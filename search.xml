<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Linux 系统 inodes 资源耗尽</title>
    <url>/2020/12/27/Linux-inodes/</url>
    <content><![CDATA[<p>inodes 资源耗尽处理</p>
<hr>
<h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>可以看下阮老师的文章理解下，参见<a href="https://www.ruanyifeng.com/blog/2011/12/inode.html" target="_blank" rel="noopener">理解inode</a></p>
<h4 id="处理"><a href="#处理" class="headerlink" title="处理"></a>处理</h4><p>马哥Linux运维有介绍相关处理方法，参见<a href="https://mp.weixin.qq.com/s/9rfSsqZdZm8QmNYn3q19gA" target="_blank" rel="noopener">Linux系统inodes资源耗尽问题</a></p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>Kafka 失效副本</title>
    <url>/2020/12/24/kafka-under-replicated/</url>
    <content><![CDATA[<p>Kafka 失效副本</p>
<hr>
<h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><p>每个分区的多个副本称之为AR（assigned replicas），包含至多一个leader副本和多个follower副本。与AR对应的另一个重要的概念就是ISR（in-sync replicas），ISR是指与leader副本保持同步状态的副本集合，当然leader副本本身也是这个集合中的一员。而ISR之外，也就是处于同步失败或失效状态的副本，副本对应的分区也就称之为同步失效分区，即under-replicated分区。</p>
<h4 id="判定"><a href="#判定" class="headerlink" title="判定"></a>判定</h4><p>怎么样判定一个分区是否有副本是处于同步失效状态的呢？从Kafka 0.9.x版本开始通过唯一的一个参数replica.lag.time.max.ms（默认大小为10,000）来控制，当ISR中的一个follower副本滞后leader副本的时间超过参数replica.lag.time.max.ms指定的值时即判定为副本失效，需要将此follower副本剔出除ISR之外。具体实现原理很简单，当follower副本将leader副本的LEO（Log End Offset，每个分区最后一条消息的位置）之前的日志全部同步时，则认为该follower副本已经追赶上leader副本，此时更新该副本的lastCaughtUpTimeMs标识。Kafka的副本管理器（ReplicaManager）启动时会启动一个副本过期检测的定时任务，而这个定时任务会定时检查当前时间与副本的lastCaughtUpTimeMs差值是否大于参数replica.lag.time.max.ms指定的值。千万不要错误的认为follower副本只要拉取leader副本的数据就会更新lastCaughtUpTimeMs，试想当leader副本的消息流入速度大于follower副本的拉取速度时，follower副本一直不断的拉取leader副本的消息也不能与leader副本同步，如果还将此follower副本置于ISR中，那么当leader副本失效，而选取此follower副本为新的leader副本，那么就会有严重的消息丢失。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.jianshu.com/p/aed1326880f1" target="_blank" rel="noopener">Kafka解析之失效副本</a></li>
</ul>
]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
  </entry>
  <entry>
    <title>Kafka __consumer_offsets 占用磁盘空间过大处理</title>
    <url>/2020/12/16/kafka-consumer-offsets/</url>
    <content><![CDATA[<p>__consumer_offsets 占用磁盘空间过大处理</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>生产环境上有一台Kafka机器磁盘告警，通过查看发现kafka的日志储存目录数据盘占用80%的存储，发现是__consumer_offsets的储存文件过大导致的</p>
<h4 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h4><p>Kafka 中用于保存消费者消费位移的主题<code>__consumer_offsets</code>与普通topic在清理策略上不同，使用的就是Log Compaction策略。</p>
<p>Log Compaction是kafka提供的一种整理offset数据的方式。Log Compaction对于有相同key的的不同value值，只保留最后一个版本。如果应用只关心key对应的最新value值，可以开启Kafka的日志清理功能，Kafka会定期将相同key的消息进行合并，只保留最新的value值。</p>
<h4 id="清理"><a href="#清理" class="headerlink" title="清理"></a>清理</h4><p>查看现有的<code>__consumer_offsets</code>清理策略</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;kafka-topics.sh --zookeeper xxx:2181 --topic __consumer_offsets --describe</span><br></pre></td></tr></table></figure>

<p>查看第一行输出可以看到 <code>cleanup.policy=compact</code></p>
<p>给<code>__consumer_offsets</code>手动添加了清理策略</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;kafka-configs.sh --zookeeper xxx:2181 --entity-type topics --entity-name __consumer_offsets --alter --add-config &#39;cleanup.policy&#x3D;delete&#39;</span><br></pre></td></tr></table></figure>

<p>添加完后，等了一会磁盘占用就会减少</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.codenong.com/cs107089637/" target="_blank" rel="noopener">KAFKA consumer_offsets 清理</a></li>
</ul>
]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
  </entry>
  <entry>
    <title>Zookeeper 事务日志和 snapshot 清理</title>
    <url>/2020/12/08/zookeeper-log-clean/</url>
    <content><![CDATA[<p>Zookeeper 运行过程会产生大量的事务日志和 snapshot 镜像文件，讨论下如何清理事务日志和snapshot</p>
<hr>
<h4 id="日志文件"><a href="#日志文件" class="headerlink" title="日志文件"></a>日志文件</h4><p>在使用ZK过程中，会有dataDir和dataLogDir两个目录，分别用于snapshot和事务日志的输出（默认情况下只有dataDir目录，snapshot和事务日志都保存在这个目录中）</p>
<p>ZK在完成若干次事务日志之后（在ZK中，凡是对数据有更新的操作，比如创建节点，删除节点或是对节点数据内容进行更新等，都会记录事务日志），ZK会触发一次快照（snapshot），将当前server上所有节点的状态以快照文件的形式dump到磁盘上去，即snapshot文件。这里的若干次事务日志是可以配置的，默认是100000，具体参看配置参数”snapCount”的介绍</p>
<h4 id="日志清理"><a href="#日志清理" class="headerlink" title="日志清理"></a>日志清理</h4><h5 id="配置自动清理"><a href="#配置自动清理" class="headerlink" title="配置自动清理"></a>配置自动清理</h5><p>ZK在3.4.0版本以后提供了自动清理snapshot和事务日志的功能通过配置 <code>autopurge.snapRetainCount</code> 和 <code>autopurge.purgeInterval</code> 这两个参数能够实现定时清理了。</p>
<p>当前ZK版本为3.4.6，因此可以使用自带的清理功能</p>
<ul>
<li>autopurge.purgeInterval: 这个参数指定了清理频率，单位是小时，需要填写一个1或更大的整数，默认是0，表示不开启自己清理功能。</li>
<li>autopurge.snapRetainCount: 这个参数和上面的参数搭配使用，这个参数指定了需要保留的文件数目。默认是保留3个。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">autopurge.snapRetainCount&#x3D;50</span><br><span class="line">autopurge.purgeInterval&#x3D;1</span><br></pre></td></tr></table></figure>

<p>每一个小时清理一次，一次保留50个文件</p>
<h5 id="自定义清理脚本"><a href="#自定义清理脚本" class="headerlink" title="自定义清理脚本"></a>自定义清理脚本</h5><p>clean_zook_log.sh 脚本内容如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">           </span><br><span class="line">#snapshot file dir</span><br><span class="line">dataDir&#x3D;&#x2F;home&#x2F;tu&#x2F;data&#x2F;zookeeper&#x2F;version-2</span><br><span class="line">#tran log dir</span><br><span class="line">dataLogDir&#x3D;&#x2F;home&#x2F;tu&#x2F;data&#x2F;zookeeper&#x2F;version-2</span><br><span class="line">#zk log dir</span><br><span class="line">logDir&#x3D;&#x2F;home&#x2F;tu&#x2F;zookeeper&#x2F;logs</span><br><span class="line">#Leave 60 files</span><br><span class="line">count&#x3D;50</span><br><span class="line">count&#x3D;$[$count+1]</span><br><span class="line">ls -t $dataLogDir&#x2F;log.* | tail -n +$count | xargs rm -f</span><br><span class="line">ls -t $dataDir&#x2F;snapshot.* | tail -n +$count | xargs rm -f</span><br><span class="line">ls -t $logDir&#x2F;zookeeper.log.* | tail -n +$count | xargs rm -f</span><br></pre></td></tr></table></figure>

<p>这个脚本保留最新的50个文件，可以将这个脚本添加到crontab中，设置为每30分钟执行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">30 * * * * &#x2F;bin&#x2F;bash &#x2F;home&#x2F;tu&#x2F;zookeeper&#x2F;bin&#x2F;clean_zook_log.sh &gt; &#x2F;dev&#x2F;null 2&gt;&amp;1</span><br></pre></td></tr></table></figure>

<h5 id="zkCleanup-sh清理"><a href="#zkCleanup-sh清理" class="headerlink" title="zkCleanup.sh清理"></a>zkCleanup.sh清理</h5><p>ZK自己有自带的清理文件 bin/zkCleanup.sh，可以直接使用这个脚本也是可以执行清理工作的，是使用的zookeeper.jar里的<code>org.apache.zookeeper.server.PurgeTxnLog</code>来做的</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.cnblogs.com/the-tops/p/5783722.html" target="_blank" rel="noopener">ZooKeepr日志清理【转】</a></li>
<li><a href="https://ningyu1.github.io/site/post/89-zookeeper-cleanlog/" target="_blank" rel="noopener">Zookeeper事务日志和snapshot清理方式</a></li>
<li><a href="https://www.cnblogs.com/linjiqin/archive/2013/03/16/2963439.html" target="_blank" rel="noopener">zookeeper配置文件详解</a></li>
<li><a href="https://mp.weixin.qq.com/s/bDwKeELWESerPznCoSdesg" target="_blank" rel="noopener">不懂 Zookeeper？没关系，看这篇就够了</a></li>
</ul>
]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
  </entry>
  <entry>
    <title>async-profile 工具</title>
    <url>/2020/05/23/async-profile/</url>
    <content><![CDATA[<p>超好用的自带火焰图的 Java 性能分析工具</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>火焰图常用来进行性能分析，async-profiler 就是一种自带火焰图的 Java 性能分析工具</p>
<blockquote>
<p>最近 Arthas 性能分析工具上线了火焰图分析功能，Arthas 使用 async-profiler 生成 CPU/内存火焰图进行性能分析，弥补了之前内存分析的不足。在 Arthas 上使用还是比较方便的，使用方式可以看官方文档。<a href="https://alibaba.github.io/arthas/profiler.html" target="_blank" rel="noopener">Arthas 火焰图官方文档</a></p>
</blockquote>
<h4 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h4><p>具体使用请跳转阅读<a href="https://juejin.im/post/5ded9ade6fb9a0164a10bcda#heading-1" target="_blank" rel="noopener">《超好用的自带火焰图的 Java 性能分析工具 Async-profiler 了解一下》</a>，该文章从安装、使用到案例都有介绍，适合入门了解。</p>
]]></content>
      <categories>
        <category>Tools</category>
      </categories>
  </entry>
  <entry>
    <title>perf 工具</title>
    <url>/2020/05/23/perf/</url>
    <content><![CDATA[<p>Linux 性能调优工具 perf 使用</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>背景知识啊，介绍啊，跳转阅读下方链接了解即可</p>
<ul>
<li><a href="https://www.ibm.com/developerworks/cn/linux/l-cn-perf1/" target="_blank" rel="noopener">Perf – Linux下的系统性能调优工具，第 1 部分</a></li>
<li><a href="https://www.ibm.com/developerworks/cn/linux/l-cn-perf2/" target="_blank" rel="noopener">Perf – Linux下的系统性能调优工具，第 2 部分</a></li>
</ul>
<h4 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h4><p>perf 是一个包含多个子工具的工具集</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ perf --help</span><br><span class="line"></span><br><span class="line"> usage: perf [--version] [--help] [OPTIONS] COMMAND [ARGS]</span><br><span class="line"></span><br><span class="line"> The most commonly used perf commands are:</span><br><span class="line">   annotate        Read perf.data (created by perf record) and display annotated code</span><br><span class="line">   archive         Create archive with object files with build-ids found in perf.data file</span><br><span class="line">   bench           General framework for benchmark suites</span><br><span class="line">   buildid-cache   Manage build-id cache.</span><br><span class="line">   buildid-list    List the buildids in a perf.data file</span><br><span class="line">   c2c             Shared Data C2C&#x2F;HITM Analyzer.</span><br><span class="line">   config          Get and set variables in a configuration file.</span><br><span class="line">   data            Data file related processing</span><br><span class="line">   diff            Read perf.data files and display the differential profile</span><br><span class="line">   evlist          List the event names in a perf.data file</span><br><span class="line">   ftrace          simple wrapper for kernel&#39;s ftrace functionality</span><br><span class="line">   inject          Filter to augment the events stream with additional information</span><br><span class="line">   kallsyms        Searches running kernel for symbols</span><br><span class="line">   kmem            Tool to trace&#x2F;measure kernel memory properties</span><br><span class="line">   kvm             Tool to trace&#x2F;measure kvm guest os</span><br><span class="line">   list            List all symbolic event types</span><br><span class="line">   lock            Analyze lock events</span><br><span class="line">   mem             Profile memory accesses</span><br><span class="line">   record          Run a command and record its profile into perf.data</span><br><span class="line">   report          Read perf.data (created by perf record) and display the profile</span><br><span class="line">   sched           Tool to trace&#x2F;measure scheduler properties (latencies)</span><br><span class="line">   script          Read perf.data (created by perf record) and display trace output</span><br><span class="line">   stat            Run a command and gather performance counter statistics</span><br><span class="line">   test            Runs sanity tests.</span><br><span class="line">   timechart       Tool to visualize total system behavior during a workload</span><br><span class="line">   top             System profiling tool.</span><br><span class="line">   version         display the version of perf binary</span><br><span class="line">   probe           Define new dynamic tracepoints</span><br><span class="line">   trace           strace inspired tool</span><br><span class="line"></span><br><span class="line"> See &#39;perf help COMMAND&#39; for more information on a specific command.</span><br></pre></td></tr></table></figure>

<p>其中最常用应该是<code>perf list</code>、<code>perf record</code>、<code>perf report</code>、<code>perf stat</code>、<code>perf top</code>这几个工具</p>
<h5 id="perf-list"><a href="#perf-list" class="headerlink" title="perf list"></a>perf list</h5><p>perf list 用来查看 perf 所支持的性能事件（即能够触发perf能够采样点的事件），其中有软件的也有硬件的。</p>
<p><img src="/images/blog/2020-05-23-2.png" alt></p>
<p>说明:</p>
<ul>
<li>Software event: 是内核软件产生的事件，比如进程切换，tick 数等，与硬件无关</li>
<li>Tracepoint event: 是内核中的静态 tracepoint 所触发的事件，这些 tracepoint 用来判断程序运行期间内核的行为细节，比如 slab 分配器的分配次数等</li>
<li>Hardware event: 是硬件产生的事件，但这里的<code>perf list</code>上没有看到对应的事件，阅读前辈们文章都有提到这个，感觉是当前版本已经去掉了</li>
</ul>
<h5 id="perf-top"><a href="#perf-top" class="headerlink" title="perf top"></a>perf top</h5><p>有些时候，只是发现系统性能无端下降，并不清楚究竟哪个进程吃资源导致的。</p>
<p>这时就需要用上<code>perf top</code>，其主要用于实时分析各个函数在某个性能事件上的热度，能够快速的定位热点函数，包括应用程序函数、</p>
<p>直接运行perf top输出示例如下:</p>
<p><img src="/images/blog/2020-05-23-3.png" alt></p>
<p>输出格式解释:</p>
<ul>
<li>符号引发的性能事件的比例，默认指占用的cpu周期比例</li>
<li>符号所在的DSO(Dynamic Shared Object)，可以是应用程序、内核、动态链接库、模块</li>
<li>DSO的类型。[.]表示此符号属于用户态的ELF文件，包括可执行文件与动态链接库)。[k]表述此符号属于内核或模块</li>
<li>符号名。有些符号不能解析为函数名，只能用地址表示</li>
</ul>
<p>使用示例:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">perf top                         # 默认配置</span><br><span class="line">perf top -G                      # 得到调用关系图</span><br><span class="line">perf top -e cycles               # 指定性能事件</span><br><span class="line">perf top -p 23015,32476          # 查看这两个进程的cpu cycles使用情况</span><br><span class="line">perf top -s comm,pid,symbol      # 显示调用symbol的进程名和进程号</span><br><span class="line">perf top --comms nginx,top       # 仅显示属于指定进程的符号</span><br><span class="line">perf top --symbols kfree         # 仅显示指定的符号</span><br></pre></td></tr></table></figure>

<h5 id="perf-stat"><a href="#perf-stat" class="headerlink" title="perf stat"></a>perf stat</h5><p>用于分析指定程序的性能概况，<code>-p</code> 指定进程号，需要按ctrl-c结束</p>
<p><img src="/images/blog/2020-05-23-4.png" alt></p>
<p>输出格式解释:</p>
<ul>
<li>task-clock：任务真正占用的处理器时间，单位为ms。CPUs utilized = task-clock / time elapsed，CPU的占用率。</li>
<li>context-switches：上下文的切换次数。</li>
<li>CPU-migrations：处理器迁移次数。Linux为了维持多个处理器的负载均衡，在特定条件下会将某个任务从一个CPU迁移到另一个CPU。</li>
<li>page-faults：缺页异常的次数。当应用程序请求的页面尚未建立、请求的页面不在内存中，或者请求的页面虽然在内存中，但物理地址和虚拟地址的映射关系尚未建立时，都会触发一次缺页异常。另外TLB不命中，页面访问权限不匹配等情况也会触发缺页异常。</li>
</ul>
<blockquote>
<p>新版本对一些好像不太支持了</p>
</blockquote>
<h5 id="perf-record"><a href="#perf-record" class="headerlink" title="perf record"></a>perf record</h5><p>收集采样信息，并将其记录在数据文件中</p>
<p>常用参数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-a：从所有CPU上收集性能数据</span><br><span class="line">-e：&lt;event&gt;：指明要分析的性能事件</span><br><span class="line">-p：&lt;pid&gt;：仅分析目标进程及其创建的线程</span><br><span class="line">-t：&lt;tid&gt; 仅分析tid线程</span><br><span class="line">-o：将采集的数据输出到指定文件</span><br><span class="line">-g：采集函数的调用关系图</span><br></pre></td></tr></table></figure>

<blockquote>
<p>默认情况下，信息会存在<code>perf.data</code>文件里，需使用<code>perf report</code>命令可以解析这个文件，如需到其他文件使用<code>-o</code>参数</p>
</blockquote>
<h5 id="perf-report"><a href="#perf-report" class="headerlink" title="perf report"></a>perf report</h5><p>读取<code>perf record</code>创建的数据文件，并给出热点分析结果</p>
<p>常用参数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-i, --input&#x3D;：输入性能数据文件</span><br><span class="line">-T, --threads：显示每一个线程的事件统计信息</span><br><span class="line">--pid&#x3D;：仅显示给的pid的进程的事件统计信息</span><br><span class="line">--tid&#x3D;：仅显示给的tid的线程的事件统计信息</span><br></pre></td></tr></table></figure>

<hr>
<p>参考链接</p>
<ul>
<li><a href="http://www.cpper.cn/2017/06/04/linux/perf/" target="_blank" rel="noopener">Linux性能调优工具perf的使用</a></li>
</ul>
]]></content>
      <categories>
        <category>Tools</category>
      </categories>
  </entry>
  <entry>
    <title>火焰图</title>
    <url>/2020/05/23/flame-graph/</url>
    <content><![CDATA[<p>火焰图（flame graph）是性能分析的利器</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>在涉猎运维知识时，多次看到大佬们使用火焰图来进行性能分析，因此跟着涨了下知识。</p>
<h4 id="含义"><a href="#含义" class="headerlink" title="含义"></a>含义</h4><p>先来体验下火焰图 – <a href="https://queue.acm.org/downloads/2016/Gregg4.svg" target="_blank" rel="noopener">SVG 图片</a>，该图来自阮一峰老师的<a href="https://www.ruanyifeng.com/blog/2017/09/flame-graph.html" target="_blank" rel="noopener">《如何读懂火焰图？》</a>，其用来展示 CPU 的调用栈。</p>
<p>y 轴表示调用栈，每一层都是一个函数。调用栈越深，火焰就越高，顶部就是正在执行的函数，下方都是它的父函数。</p>
<p>x 轴表示抽样数，如果一个函数在 x 轴占据的宽度越宽，就表示它被抽到的次数多，即执行的时间长。注意，x 轴不代表时间，而是所有的调用栈合并后，按字母顺序排列的。</p>
<p>火焰图就是看顶层的哪个函数占据的宽度最大。只要有”平顶”（plateaus），就表示该函数可能存在性能问题，也是常说的”大平顶”问题</p>
<p>颜色没有特殊含义，因为火焰图表示的是 CPU 的繁忙程度，所以一般选择暖色调。</p>
<h4 id="互动"><a href="#互动" class="headerlink" title="互动"></a>互动</h4><p>火焰图是 SVG 图片，可以与用户互动。</p>
<p><strong>鼠标悬浮</strong></p>
<p>火焰的每一层都会标注函数名，鼠标悬浮时会显示完整的函数名、抽样抽中的次数、占据总抽样次数的百分比。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysqld&#39;JOIN::exec (272,959 samples, 78.34 percent)</span><br></pre></td></tr></table></figure>

<p><strong>点击放大</strong></p>
<p>在某一层点击，火焰图会水平放大，该层会占据所有宽度，显示详细信息。左上角会同时显示”Reset Zoom”，点击该链接，图片就会恢复原样。</p>
<p><img src="/images/blog/2020-05-23-1.png" alt></p>
<p><strong>搜索</strong></p>
<p>按下 Ctrl + F 会显示一个搜索框，用户可以输入关键词或正则表达式，所有符合条件的函数名会高亮显示。</p>
<h4 id="生成工具"><a href="#生成工具" class="headerlink" title="生成工具"></a>生成工具</h4><p>在之前生成火焰图要使用 <a href="https://github.com/brendangregg/FlameGraph" target="_blank" rel="noopener">FlameGraph</a> 工具。现在已经不需要了，使用自带火焰图的 Java 性能分析工具 <a href="https://github.com/jvm-profiling-tools/async-profiler" target="_blank" rel="noopener">Async-profiler</a><br>，其已经内置了开箱即用的 SVG 文件生成功能。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.ruanyifeng.com/blog/2017/09/flame-graph.html" target="_blank" rel="noopener">如何读懂火焰图？</a></li>
<li><a href="https://juejin.im/post/5ded9ade6fb9a0164a10bcda#heading-2" target="_blank" rel="noopener">超好用的自带火焰图的 Java 性能分析工具 Async-profiler 了解一下</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/85654612" target="_blank" rel="noopener">Linux火焰图性能分析</a></li>
</ul>
]]></content>
      <categories>
        <category>Distributed</category>
      </categories>
  </entry>
  <entry>
    <title>Linux 知识练习</title>
    <url>/2020/05/12/Linux-Exercise/</url>
    <content><![CDATA[<p>Linux 运维知识练习</p>
<hr>
<h3 id="Linux-理论部分"><a href="#Linux-理论部分" class="headerlink" title="Linux 理论部分"></a>Linux 理论部分</h3><h4 id="进程管理"><a href="#进程管理" class="headerlink" title="进程管理"></a>进程管理</h4><h5 id="进程和线程有什么区别"><a href="#进程和线程有什么区别" class="headerlink" title="进程和线程有什么区别"></a>进程和线程有什么区别</h5><p>线程是CPU调度的基本单位，而进程则是资源拥有的基本单位</p>
<h5 id="进程有哪些状态"><a href="#进程有哪些状态" class="headerlink" title="进程有哪些状态"></a>进程有哪些状态</h5><p><img src="/images/blog/2020-05-12-1.png" alt></p>
<p>1.创建状态<br>进程由创建而产生。创建进程是一个非常复杂的过程，一般需要通过多个步骤才能完成：如首先由进程申请一个空白的进程控制块(PCB)，并向PCB中填写用于控制和管理进程的信息；然后为该进程分配运行时所必须的资源；最后，把该进程转入就绪状态并插入到就绪队列中。</p>
<p>2.就绪状态<br>这是指进程已经准备好运行的状态，即进程已分配到除CPU以外所有的必要资源后，只要再获得CPU，便可立即执行。如果系统中有许多处于就绪状态的进程，通常将它们按照一定的策略排成一个队列，该队列称为就绪队列。有执行资格，没有执行权的进程。</p>
<p>3.运行状态<br>这里指进程已经获取CPU，其进程处于正在执行的状态。对任何一个时刻而言，在单处理机的系统中，只有一个进程处于执行状态而在多处理机系统中，有多个进程处于执行状态。既有执行资格，又有执行权的进程。</p>
<p>4.阻塞状态<br>这里是指正在执行的进程由于发生某事件（如I/O请求、申请缓冲区失败等）暂时无法继续执行的状态，即进程执行受到阻塞。此时引起进程调度，操作系统把处理机分配给另外一个就绪的进程，而让受阻的进程处于暂停的状态，一般将这个暂停状态称为阻塞状态</p>
<p>5.终止状态<br>进程的终止也要通过两个步骤：首先，是等待操作系统进行善后处理，最后将其PCB清零，并将PCB空间返还给系统。当一个进程到达了自然结束点，或是出现了无法克服的错误，或是被操作系统所终结，或是被其他有终止权的进程所终结，它将进入终止状态。进入终止态的进程以后不能在再执行，但是操作系统中任然保留了一个记录，其中保存状态码和一些计时统计数据，供其他进程进行收集。一旦其他进程完成了对其信息的提取之后，操作系统将删除其进程，即将其PCB清零，并将该空白的PCB返回给系统。</p>
<blockquote>
<p>为什么要分开就绪和阻塞状态</p>
<p>答：因为就绪态只需要等待处理机，而阻塞态可能在等待输入输出，即使分配给处理机也是徒劳，所以两状态图不妥。对于调度进程，只需要等待就绪队列里的进程，因为阻塞状态可以转换到就绪队列里去。</p>
</blockquote>
<h5 id="什么是僵尸进程"><a href="#什么是僵尸进程" class="headerlink" title="什么是僵尸进程"></a>什么是僵尸进程</h5><p>一个进程使用fork()创建子进程，如果子进程退出，而父进程并没有调用wait()或waitpid()获取子进程的状态信息，那么子进程的某些信息如进程描述符仍然保存在系统中，这种进程称之为僵尸进程</p>
<blockquote>
<p>查看僵尸进程，利用命令ps，可以看到有标记为Z(zombie)的进程就是僵尸进程</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看进程的状态的</span><br><span class="line">$ ps -aux | grep name</span><br><span class="line">1137   pts&#x2F;0   S   0:00   -bash</span><br><span class="line">1217   pts&#x2F;0   S   0:00   .&#x2F;zombie</span><br><span class="line">1218   pts&#x2F;0   Z   0:00   [zombie]</span><br><span class="line">1578   pts&#x2F;0   R   0:00   ps   -ax</span><br></pre></td></tr></table></figure>

<h5 id="僵尸进程产生的原因"><a href="#僵尸进程产生的原因" class="headerlink" title="僵尸进程产生的原因"></a>僵尸进程产生的原因</h5><ul>
<li>子进程结束后向父进程发出SIGCHLD信号，父进程默认忽略了它</li>
<li>父进程没有调用wait()或waitpid()函数来等待子进程的结束</li>
<li>网络原因有时会引起僵尸进程</li>
</ul>
<h5 id="如果出现大量的僵尸进程会有哪些危害"><a href="#如果出现大量的僵尸进程会有哪些危害" class="headerlink" title="如果出现大量的僵尸进程会有哪些危害"></a>如果出现大量的僵尸进程会有哪些危害</h5><p>僵尸进程会在系统中保留其某些信息如进程描述符、进程id等等。以进程id为例，系统中可用的进程id是有限的，如果由于系统中大量的僵尸进程占用进程id，就会导致因为没有可用的进程id系统不能产生新的进程，这种问题可就大了，这就是僵尸进程来的危害，因此大部分情况下，我们都应当避免僵尸进程的产生。</p>
<blockquote>
<p>总而言之，僵尸进程会占用系统资源，如果很多，则会严重影响服务器的性能，最大的危害就是内存泄露</p>
</blockquote>
<h5 id="如何杀死僵尸进程"><a href="#如何杀死僵尸进程" class="headerlink" title="如何杀死僵尸进程"></a>如何杀死僵尸进程</h5><p>僵尸进程用kill命令是无法杀掉的，但是我们可以结果掉僵尸进程的父进程（如果其父进程不需要的话）；父进程挂了之后，僵尸进程就成了孤儿进程，孤儿进程不会占用系统资源，会被init程序收养，然后init程序将其回收</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看僵尸进程的父进程</span><br><span class="line">$ ps -A -o stat,ppid,pid,cmd |grep -e &quot;^[Zz]&quot;</span><br><span class="line"></span><br><span class="line"># 杀死僵尸进程对应的父进程</span><br><span class="line">$ kill -9 ppid号</span><br></pre></td></tr></table></figure>

<blockquote>
<p>一台服务器上产生了100多少僵死进程，而且每一僵死进程的父进程都不一样，如果用上面的方法，一条一条的杀会挺麻烦的。</p>
<p>一条命令直接查找僵死进程，然后将父进程杀死</p>
<p><code>ps -A -o stat,ppid,pid,cmd | grep -e &quot;^[Zz]&quot; | awk &#39;{print $2}&#39; | xargs kill -9</code></p>
</blockquote>
<h5 id="描述进程间通信有哪些⽅方法"><a href="#描述进程间通信有哪些⽅方法" class="headerlink" title="描述进程间通信有哪些⽅方法"></a>描述进程间通信有哪些⽅方法</h5><p>进程间通信（IPC，Interprocess communication）是一组编程接口，让程序员能够协调不同的进程，使之能在一个操作系统里同时运行，并相互传递、交换信息。</p>
<ul>
<li>管道: 管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有亲缘关系的进程间使用。进程的亲缘关系通常是指父子进程关系</li>
<li>有名管道: 有名管道也是半双工的通信方式，但是它允许无亲缘关系进程之间的通信</li>
<li>消息队列: 消息队列是消息的链表，存放在内核中并由消息队列表示符标示。消息队列克服了信号传递信息少，管道只能承载无格式字节流以及缓冲区大小受限制等缺点</li>
<li>共享内存: 共享内存就是映射一段能被其它进程所访问的内存，共享内存由一个进程创建，但是多个进程都可以访问。共享内存是最快的IPC，往往与其它通信机制配合使用，来实现进程间的同步和通信</li>
<li>信号量: 信号量是一个计数器，可以用来控制多个进程对共享资源的访问，它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。主要作为进程间以及同一进程内不同线程之间的同步手段</li>
<li>socket: 套接字也是进程间的通信机制，与其它通信机制不同的是，它可以用于不同机器间的进程通信</li>
<li>信号: 信号是一种比较复杂的通信方式，用于通知接受进程某个时间已经发生</li>
</ul>
<h5 id="什么是-Unix-信号"><a href="#什么是-Unix-信号" class="headerlink" title="什么是 Unix 信号"></a>什么是 Unix 信号</h5><p>信号是一种中断，是一种处理异步事件的方法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 用 kill -l 命令可以察看系统定义的信号列表</span><br><span class="line">$ kill -l</span><br><span class="line"> 1) SIGHUP	 2) SIGINT	 3) SIGQUIT	 4) SIGILL</span><br><span class="line"> 5) SIGTRAP	 6) SIGABRT	 7) SIGEMT	 8) SIGFPE</span><br><span class="line"> 9) SIGKILL	10) SIGBUS	11) SIGSEGV	12) SIGSYS</span><br><span class="line">13) SIGPIPE	14) SIGALRM	15) SIGTERM	16) SIGURG</span><br><span class="line">17) SIGSTOP	18) SIGTSTP	19) SIGCONT	20) SIGCHLD</span><br><span class="line">21) SIGTTIN	22) SIGTTOU	23) SIGIO	24) SIGXCPU</span><br><span class="line">25) SIGXFSZ	26) SIGVTALRM	27) SIGPROF	28) SIGWINCH</span><br><span class="line">29) SIGINFO	30) SIGUSR1	31) SIGUSR2</span><br></pre></td></tr></table></figure>

<p>具体含义请转自<a href="https://blog.csdn.net/u012349696/article/details/50687462" target="_blank" rel="noopener">Unix系统中常用的信号含义</a></p>
<blockquote>
<p>SIGKILL用来立即结束程序的运行，该信号不能被阻塞、捕获和忽略</p>
</blockquote>
<h5 id="当你对⼀个进程发送-一个-HUP-信号，但是没有任何作⽤，分析原因"><a href="#当你对⼀个进程发送-一个-HUP-信号，但是没有任何作⽤，分析原因" class="headerlink" title="当你对⼀个进程发送    一个 HUP 信号，但是没有任何作⽤，分析原因"></a>当你对⼀个进程发送    一个 HUP 信号，但是没有任何作⽤，分析原因</h5><p>先解释下 HUP 信号， 全名叫hangup，表示终端断线</p>
<blockquote>
<p>当用户注销（logout）或者网络断开时，终端会收到Linux HUP信号（hangup）信号从而关闭其所有子进程</p>
</blockquote>
<p>当<strong>用户退出Linux登录时，前台进程组和后台有对终端输出的进程将会收到SIGHUP信号</strong>。这个信号的默认操作为终止进程，因此前台进程组和后台有终端输出的进程就会中止，不过可以捕获这个信号，比如wget能捕获SIGHUP信号，并忽略它，这样就算退出了Linux登录，wget也能继续下载。</p>
<p>没有任何作⽤有两种可能</p>
<ul>
<li>进程忽略Linux HUP信号</li>
<li>进程运行在新的会话里从而成为不属于此终端的子进程</li>
</ul>
<p>发送 HUP 信号（即意外终止信号）到运行进程 ID 为 1001 的程序</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kill -9 1001</span><br></pre></td></tr></table></figure>

<h5 id="请简述nohup命令的原理"><a href="#请简述nohup命令的原理" class="headerlink" title="请简述nohup命令的原理"></a>请简述nohup命令的原理</h5><p>nohup 命令运行指定的命令，忽略所有挂断（SIGHUP）信号，在注销后使用 nohup 命令运行后台中的程序</p>
<p> nohup 命令会从终端解除进程的关联，进程会丢掉STDOUT，STDERR的链接。标准输出和标准错误缺省会被重定向到 nohup.out 文件中。一般我们可在结尾加上”&amp;”来将命令同时放入后台运行，也可用”&gt;filename 2&gt;&amp;1”来更改缺省的重定向文件名，”&gt;filename 2&gt;&amp;1”意思是把标准错误（2）重定向到标准输出中（1），而标准输出又导入文件filename里面，所以结果是标准错误和标准输出都导入文件filename里面了</p>
<blockquote>
<p>使用&amp;后台运行程序：</p>
<ul>
<li>结果会输出到终端</li>
<li>使用Ctrl + C发送SIGINT信号，程序免疫</li>
<li>关闭session发送SIGHUP信号，程序关闭</li>
</ul>
<p>使用nohup运行程序：</p>
<ul>
<li>结果默认会输出到nohup.out</li>
<li>使用Ctrl + C发送SIGINT信号，程序关闭</li>
<li>关闭session发送SIGHUP信号，程序免疫</li>
</ul>
<p>平日线上经常使用nohup和&amp;配合来启动程序：</p>
<ul>
<li>同时免疫SIGINT和SIGHUP信号</li>
</ul>
</blockquote>
<h5 id="如何在bash脚本中处理用户的Ctrl-C"><a href="#如何在bash脚本中处理用户的Ctrl-C" class="headerlink" title="如何在bash脚本中处理用户的Ctrl-C"></a>如何在bash脚本中处理用户的Ctrl-C</h5><p>可以使用Bash提供的<code>trap</code>命令捕获中断信号</p>
<p><code>trap</code>的用法如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">trap [-lp] [[arg] signal_spec ...]</span><br></pre></td></tr></table></figure>

<ul>
<li><code>-l</code>: 列出所有信号的序号及名称</li>
<li><code>-p</code>: 列出特定信号对应的处理指令</li>
<li><code>arg</code>: 是signal_spec指定的信号的处理指令</li>
<li><code>signal_spec</code>: 是需要捕获的信号</li>
</ul>
<p>执行下方脚本，<code>Ctrl+C</code>按键将触发<code>onCtrlC</code>函数</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">trap 'onCtrlC; exit' INT</span><br><span class="line">function onCtrlC () &#123;</span><br><span class="line">    echo 'Ctrl+C is captured'</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">while true; do</span><br><span class="line">    echo 'I am working!'</span><br><span class="line">    sleep 1</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<blockquote>
<p>如果用户中断程序的运行，这个 trap 将会被执行，可以确保onCtrlC方法被执行。<strong>onCtrlC</strong> 后面的这个 <strong>exit</strong> 命令，它的存在是必要的。如果没有它，程序会在它中断点（也就是信号接收的时刻）继续执行</p>
</blockquote>
<h4 id="bashrc和环境变量"><a href="#bashrc和环境变量" class="headerlink" title="bashrc和环境变量"></a>bashrc和环境变量</h4><h5 id="shell-按照登录类型和交互类型分类，总共有哪⼏类，在各个类别中⾄少举出⼀个例子"><a href="#shell-按照登录类型和交互类型分类，总共有哪⼏类，在各个类别中⾄少举出⼀个例子" class="headerlink" title="shell 按照登录类型和交互类型分类，总共有哪⼏类，在各个类别中⾄少举出⼀个例子"></a>shell 按照登录类型和交互类型分类，总共有哪⼏类，在各个类别中⾄少举出⼀个例子</h5><p><strong>登录shell和非登陆shell</strong></p>
<ul>
<li>登录shell: 需要用户名、密码登录后才能进入的shell（或者以–login选项启动的shell，实际上不是真的存在哪个用户来登录）</li>
<li>非登录shell: 不需要输入用户名和密码即可打开的Shell（直接bash命令就是打开一个新的非登录shell，在Gnome或KDE中打开一个“终端”（terminal）窗口程序也是一个非登录shell）</li>
</ul>
<blockquote>
<p>退出一个登录shell: exit或者logout</p>
<p>退出一个非登录shell: 只能exit</p>
</blockquote>
<blockquote>
<p>登录shell 时，其bash进程名为”-bash”</p>
<p>非登陆shell时，bash进程名为”bash”</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(base) [root@iZbp144crtihiqovt4h5m4Z ~]# su - lihm</span><br><span class="line">上一次登录：四 4月  2 16:06:04 CST 2020pts&#x2F;0 上</span><br><span class="line">[lihm@iZbp144crtihiqovt4h5m4Z ~]$ echo $0</span><br><span class="line">-bash</span><br><span class="line">[lihm@iZbp144crtihiqovt4h5m4Z ~]$ exit</span><br><span class="line">logout</span><br><span class="line">(base) [root@iZbp144crtihiqovt4h5m4Z ~]# su lihm</span><br><span class="line">[lihm@iZbp144crtihiqovt4h5m4Z root]$ echo $0</span><br><span class="line">bash</span><br><span class="line">[lihm@iZbp144crtihiqovt4h5m4Z root]$</span><br></pre></td></tr></table></figure>

<p><strong>交互式shell和非交互式shell</strong></p>
<ul>
<li>交互式模式: 在终端上执行，shell等待你的输入，并且立即执行你提交的命令</li>
<li>非交互式模式: 以shell script(非交互)方式执行。在这种模式 下，shell不与你进行交互，而是读取存放在文件中的命令,并且执行它们。当它读到文件的结尾EOF，shell也就终止</li>
</ul>
<h5 id="上述各个类别的-bash-系统配置文件和个⼈配置⽂件分别是什么，加载顺序是怎样的"><a href="#上述各个类别的-bash-系统配置文件和个⼈配置⽂件分别是什么，加载顺序是怎样的" class="headerlink" title="上述各个类别的 bash 系统配置文件和个⼈配置⽂件分别是什么，加载顺序是怎样的"></a>上述各个类别的 bash 系统配置文件和个⼈配置⽂件分别是什么，加载顺序是怎样的</h5><ul>
<li><p>交互式登录shell: <code>/etc/profile</code> -&gt; (<code>~/.bash_profile</code> | <code>~/.bash_login</code> | <code>~/.profile</code>) -&gt;（ <code>~/.bashrc</code> -&gt; <code>/etc/bashrc</code>） -&gt; <code>~/.bash_logout</code></p>
<blockquote>
<p> (~/.bash_profile | ~/.bash_login | ~/.profile) 中读取第一个存在而且可读的文件并且执行其中的命令，可以在shell启动时使用–noprofile选项来禁止这种行为</p>
<p> 是以登录shell注销的，Bash会读取并执行文件~/.bash_logout和/etc/bash.bash_logout中的命令，假如文件存在的话</p>
</blockquote>
</li>
<li><p>交互式非登陆shell: <code>~/.bashrc</code>  -&gt; <code>/etc/bashrc</code></p>
</li>
</ul>
<blockquote>
<p>一般建议将配置直接添加在 <code>~/.bashrc</code> 中，这样不管是登录式 <code>Shell</code> 还是 非登录式 <code>Shell</code> 都可以读到</p>
</blockquote>
<h5 id="PS1这个环境变量有什么⽤"><a href="#PS1这个环境变量有什么⽤" class="headerlink" title="$PS1这个环境变量有什么⽤"></a>$PS1这个环境变量有什么⽤</h5><p>是用来定义命令行的提示符，可以按照我们自己的需求来定义自己喜欢的提示符</p>
<blockquote>
<p>$： 提示符。如果是 root 用户，则会显示提示符为”#”；如果是普通用户，则会显示提示符为”$”</p>
</blockquote>
<h5 id="在什么情况下需要修改-PATH-应该如何合理地修改它"><a href="#在什么情况下需要修改-PATH-应该如何合理地修改它" class="headerlink" title="在什么情况下需要修改$PATH, 应该如何合理地修改它"></a>在什么情况下需要修改$PATH, 应该如何合理地修改它</h5><p>需要修改环境变量加载一些命令时修改$PATH</p>
<p>如果永久更改就修改配置文件，临时更改对当前会话有效则命令行修改即可</p>
<h5 id="修改过-bashrc后-如何让改变⽴即生效"><a href="#修改过-bashrc后-如何让改变⽴即生效" class="headerlink" title="修改过~/.bashrc后, 如何让改变⽴即生效"></a>修改过~/.bashrc后, 如何让改变⽴即生效</h5><p>执行<code>source profile</code>就会立即生效</p>
<h5 id="bashrc与profile有什么异同点-两者的加载顺序如何"><a href="#bashrc与profile有什么异同点-两者的加载顺序如何" class="headerlink" title="bashrc与profile有什么异同点? 两者的加载顺序如何"></a>bashrc与profile有什么异同点? 两者的加载顺序如何</h5><p>bashrc 用于交互式非登陆，profile用于交互式登陆</p>
<blockquote>
<p><code>/etc/profile</code>，<code>/etc/bashrc</code> 是系统全局环境变量设定<br><code>~/.profile</code>，<code>~/.bashrc</code>用户家目录下的私有环境变量设定</p>
</blockquote>
<p>加载顺序是 profile 再 bashrc</p>
<h4 id="FHS与proc"><a href="#FHS与proc" class="headerlink" title="FHS与proc"></a>FHS与proc</h4><h5 id="为什么系统命令会分别放到-bin-sbin-usr-bin-usr-sbin这四个⽬目录中-这些⽬录间有什么区别"><a href="#为什么系统命令会分别放到-bin-sbin-usr-bin-usr-sbin这四个⽬目录中-这些⽬录间有什么区别" class="headerlink" title="为什么系统命令会分别放到/bin, /sbin, /usr/bin, /usr/sbin这四个⽬目录中? 这些⽬录间有什么区别"></a>为什么系统命令会分别放到/bin, /sbin, /usr/bin, /usr/sbin这四个⽬目录中? 这些⽬录间有什么区别</h5><p>首先区别下/sbin和/bin:</p>
<ul>
<li><p>从命令功能来看，/sbin 下的命令属于基本的系统命令，如shutdown，reboot，用于启动系统，修复系统，/bin下存放一些普通的基本命令，如ls,chmod等，这些命令在Linux系统里的配置文件脚本里经常用到。</p>
</li>
<li><p>从用户权限的角度看，/sbin目录下的命令通常只有管理员才可以运行，/bin下的命令管理员和一般的用户都可以使用。</p>
</li>
<li><p>从可运行时间角度看，/sbin,/bin能够在挂载其他文件系统前就可以使用。</p>
<p>而/usr/bin,/usr/sbin与/sbin /bin目录的区别在于:</p>
</li>
<li><p>/bin,/sbin目录是在系统启动后挂载到根文件系统中的，所以/sbin,/bin目录必须和根文件系统在同一分区；</p>
</li>
<li><p>/usr/bin,/usr/sbin可以和根文件系统不在一个分区</p>
</li>
</ul>
<h5 id="var目录通常用来放哪些内容-var和-tmp有什么区别"><a href="#var目录通常用来放哪些内容-var和-tmp有什么区别" class="headerlink" title="/var目录通常用来放哪些内容? /var和/tmp有什么区别"></a>/var目录通常用来放哪些内容? /var和/tmp有什么区别</h5><p>/var目录主要针对常态性变动文件，包括缓存（cache）、登录文件（logfile）以及某些软件运行所产生的文件，包括程序文件（lock file，run file），或者例如MySQL数据库的文件等</p>
<ul>
<li><p>/var/cache: 应用程序本身运行过程中会产生生的一些暂存文件</p>
</li>
<li><p>/var/lib: 程序本身执行的过程中需要使用到的数据文件放置的目录。再次目录下各自的软件应该要有各自的目录。举例来说，Mysql的数据库放置到/var/lib/mysql，而rpm的数据库则放到/var/lib/rpm目录下</p>
</li>
<li><p>/var/lock: 某些设备或者是文件资源一次只能被一个应用程序所使用 ，如当系统中有一个刻录机两个人都要使用，那么需要在一个人使用的时候上锁，那么第一个人使用完毕后，第二个人才可以继续使用</p>
</li>
<li><p>/var/log: 这个是登录文件放置日志的的目录。里面比较重要的文件/var/log/messages，/var/log/harry(记录登陆者信息)等</p>
</li>
<li><p>/var/run/: 某些程序启动服务后，会将他们PID放置在这个目录下</p>
</li>
</ul>
<p>/var和/tmp区别是/var系统产生的不可自动销毁的缓存文件、日志记录，/tmp保存在使用完毕后可随时销毁的缓存文件</p>
<h5 id="boot目录里有哪些内容"><a href="#boot目录里有哪些内容" class="headerlink" title="/boot目录里有哪些内容"></a>/boot目录里有哪些内容</h5><ul>
<li>系统Kernel的配置文件</li>
<li>启动管理程序GRUB的目录，里面存放的都是GRUB在启动时所需要的画面、配置及各阶段（stage1, stage1.5, stage 2）的文件</li>
<li>Initrd文件，是系统启动时的模块供应的主要来源</li>
<li>System.map文件时系统Kernel中的变量对应表</li>
<li>vmlinuz是在启动过程中最重要的一个文件，因为这个文件就是实际系统所使用的kernel</li>
</ul>
<h5 id="usr-include和-usr-lib有什么区别"><a href="#usr-include和-usr-lib有什么区别" class="headerlink" title="/usr/include和/usr/lib有什么区别"></a>/usr/include和/usr/lib有什么区别</h5><p>/usr/include（头文件存放处），/usr/lib（库函数存放处）</p>
<h5 id="proc目录下的那些数字是什么东⻄"><a href="#proc目录下的那些数字是什么东⻄" class="headerlink" title="/proc目录下的那些数字是什么东⻄"></a>/proc目录下的那些数字是什么东⻄</h5><p>目录名即为进程的pid</p>
<h5 id="如何在proc⽂文件系统中查看CPU和内存信息"><a href="#如何在proc⽂文件系统中查看CPU和内存信息" class="headerlink" title="如何在proc⽂文件系统中查看CPU和内存信息"></a>如何在proc⽂文件系统中查看CPU和内存信息</h5><ul>
<li><p>CPU: cat /proc/cpuinfo</p>
</li>
<li><p>内存: cat /proc/meminfo</p>
</li>
</ul>
<h4 id="⽂件系统"><a href="#⽂件系统" class="headerlink" title="⽂件系统"></a>⽂件系统</h4><h5 id="什么是-inode，它包含哪些内容"><a href="#什么是-inode，它包含哪些内容" class="headerlink" title="什么是 inode，它包含哪些内容"></a>什么是 inode，它包含哪些内容</h5><p>文件数据都储存在”block”中，那应该有一个地方储存文件的元信息，比如文件的创建者、文件的创建日期、文件的大小等等。这种储存文件元信息的区域就叫做inode，中文译名为”索引节点”</p>
<p>可以用stat命令，查看某个文件的inode信息，输出中除了文件名以外的所有文件信息，都存在inode之中</p>
<h5 id="软链接和硬链接的区别是什么"><a href="#软链接和硬链接的区别是什么" class="headerlink" title="软链接和硬链接的区别是什么"></a>软链接和硬链接的区别是什么</h5><p><strong>软链接</strong></p>
<ul>
<li>软链接是存放另一个文件的路径的形式存在</li>
<li>软链接可以跨文件系统，硬链接不可以</li>
<li>软链接可以对一个不存在的文件名进行链接，硬链接必须要有源文件</li>
<li>软链接可以对目录进行链接</li>
</ul>
<p><strong>硬链接</strong></p>
<ul>
<li>硬链接，以文件副本的形式存在。但不占用实际空间</li>
<li>不允许给目录创建硬链接</li>
<li>硬链接只有在同一个文件系统中才能创建</li>
<li>删除其中一个硬链接文件并不影响其他有相同 inode 号的文件</li>
</ul>
<blockquote>
<p>不论是硬链接或软链接都不会将原本的档案复制一份，只会占用非常少量的磁碟空间</p>
</blockquote>
<h5 id="为什么不能对目录建立硬链接"><a href="#为什么不能对目录建立硬链接" class="headerlink" title="为什么不能对目录建立硬链接?"></a>为什么不能对目录建立硬链接?</h5><p>硬连接的话，相当于镜像的方式，创建一个目录的硬连接之后，操作系统需要把这个目录下所有的文件都要做一次硬连接（复制一份过去），这样操作系统在访问这个链接的时候要不断去遍历，大大增加复杂度，而且很容易进入死循环</p>
<h5 id="为什么不能跨设备建立硬链接"><a href="#为什么不能跨设备建立硬链接" class="headerlink" title="为什么不能跨设备建立硬链接"></a>为什么不能跨设备建立硬链接</h5><p>首先，不同的文件系统的文件管理方式不同，甚至有些文件系统不是索引文件系统，并不一定两个文件系统的inode有相同的含义。再者，即使有相同inode含义，硬链接的几个文件，具有相同的inode号码。不同文件系统中，也可能有使用该inode号的文件，这将产生矛盾。</p>
<h5 id="假设-B-是-A-的-软-硬-链接文件"><a href="#假设-B-是-A-的-软-硬-链接文件" class="headerlink" title="假设 B 是 A 的(软/硬)链接文件"></a>假设 B 是 A 的(软/硬)链接文件</h5><p><strong>对于硬链接和软链接, ⽐较当A或者B被删除时, 各有什什么后果？</strong></p>
<ul>
<li>硬链接<ul>
<li>A 被删除，源数据还在</li>
<li>B 被删除，源数据还在</li>
</ul>
</li>
<li>软连接<ul>
<li>A 被删除，数据丢失</li>
<li>B 被删除，源数据还在</li>
</ul>
</li>
</ul>
<p><strong>当A不存在时, 能否建⽴A的硬链接B? 能否建立A的软链接B？</strong></p>
<p>不能建立硬链接，能建立软链接</p>
<h5 id="LVM是什么，有什么作⽤"><a href="#LVM是什么，有什么作⽤" class="headerlink" title="LVM是什么，有什么作⽤"></a>LVM是什么，有什么作⽤</h5><p>可以将LVM视为”动态分区” ，这意味着你可以在Linux系统运行时从命令行创建/调整大小/删除LVM”分区” （LVM称之为”逻辑卷” ）: 不需要重新引导系统，内核就能知道新创建的或调整大小的分区</p>
<p>如果有多个硬盘，则逻辑卷可以扩展到多个磁盘: 换句话说，它们不受单个磁盘的大小限制，而不受总大小的限制</p>
<p>LV可以设置”striped” ，以便将I/O分发到并行承载LV的所有磁盘。类似于RAID-0，但是更容易设置</p>
<p>你可以创建任意LV的(只读)快照，可以在以后将原始LV恢复到快照，或者在不再需要快照时删除快照，这对于服务器备份(例如，(无法停止所有应用程序写入，创建快照并备份快照))很方便，但是也可用于在关键系统升级(克隆root分区，升级，如果出现问题，很容易恢复)之前提供”安全”</p>
<h5 id="从安全角度考虑，哪些分区应适当限制⼤小"><a href="#从安全角度考虑，哪些分区应适当限制⼤小" class="headerlink" title="从安全角度考虑，哪些分区应适当限制⼤小"></a>从安全角度考虑，哪些分区应适当限制⼤小</h5><p>？？？</p>
<h5 id="什么是-RAID，介绍至少3种-level-的-RAID"><a href="#什么是-RAID，介绍至少3种-level-的-RAID" class="headerlink" title="什么是 RAID，介绍至少3种 level 的 RAID"></a>什么是 RAID，介绍至少3种 level 的 RAID</h5><p><img src="/images/blog/2020-05-12-2.png" alt></p>
<p><strong>RAID0</strong></p>
<p>数据在从内存缓冲区写入磁盘时，根据磁盘数量将数据分成N份，这些数据同时并发写入N块磁盘，使得数据整体写入速度是一块磁盘的N倍，读取的时候也一样，因此RAID0具有极快的数据读写速度。但是RAID0不做数据备份，N块磁盘中只要有一块损坏，数据完整性就被破坏，所有磁盘的数据都会损坏</p>
<p><strong>RAID1</strong></p>
<p>数据在写入磁盘时，将一份数据同时写入两块磁盘，这样任何一块磁盘损坏都不会导致数据丢失，插入一块新磁盘就可以通过复制数据的方式自动修复，具有极高的可靠性</p>
<p><strong>RAID10</strong></p>
<p>结合RAID0和RAID1两种方案，将所有磁盘平均分成两份，数据同时在两份磁盘写入，相当于RAID1，但是在每一份磁盘里面的N/2块磁盘上，利用RAID0技术并发读写，既提高可靠性又改善性能，不过RAID10的磁盘利用率较低，有一半的磁盘用来写备份数据</p>
<h4 id="SUID机制与sudo"><a href="#SUID机制与sudo" class="headerlink" title="SUID机制与sudo"></a>SUID机制与sudo</h4><h5 id="简述SUID机制存在的意义"><a href="#简述SUID机制存在的意义" class="headerlink" title="简述SUID机制存在的意义"></a>简述SUID机制存在的意义</h5><p>SUID就是允许用户在执行程序的时候拥有这个程序属主的权限的一个机制</p>
<blockquote>
<p><a href="https://www.meiwen.com.cn/subject/qmqldqtx.html" target="_blank" rel="noopener">SUID的应用场景和简单见解</a></p>
</blockquote>
<h5 id="sudo的主配置文件路径是什么-应如何更新这个文件"><a href="#sudo的主配置文件路径是什么-应如何更新这个文件" class="headerlink" title="sudo的主配置文件路径是什么? 应如何更新这个文件?"></a>sudo的主配置文件路径是什么? 应如何更新这个文件?</h5><p><code>/etc/sudoers</code>是sudo的主配置文件</p>
<p>如果想设置sudo设置，有两种方式</p>
<ul>
<li>直接在通过<code>sudo visudo /etc/sudoers</code> 修改<code>/etc/sudoers</code>文件</li>
<li>在<code>/etc/sudoers.d/username</code>目录下添加一个文件，文件名称就用需要设定的用户名</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 建议在&#x2F;etc&#x2F;sudoers.d&#x2F;下面创建文件编辑</span><br><span class="line"># 这里给oracle2用户一些sudo权限，命令会在sudoers.d目录下创建一个文件的</span><br><span class="line">$ visudo  -f &#x2F;etc&#x2F;sudoers.d&#x2F;oracle2</span><br></pre></td></tr></table></figure>

<blockquote>
<p>常用实例讲解</p>
<p>oracle用户可以在任何地点以任何的身份执行所有命令，等同于root</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">oracle ALL&#x3D;(ALL)        ALL</span><br></pre></td></tr></table></figure>

<p>oracle2用户可以在任何地点以root的身份执行命令useradd(无需密码)和usermod（需要密码）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">oracle2 ALL&#x3D;(root) NOPASSWD:&#x2F;usr&#x2F;sbin&#x2F;useradd, PASSWD:&#x2F;usr&#x2F;sbin&#x2F;userdel</span><br></pre></td></tr></table></figure>

<p>oracle3用户只能在192.168.1.120主机远程登录并以root身份执行ifconfig eth0命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Cmnd_Alias NETCMND &#x3D; &#x2F;sbin&#x2F;ifconfig eth0</span><br><span class="line">oracle3 192.168.1.120 &#x3D; (root) NOPASSWD:NETCMND</span><br></pre></td></tr></table></figure>

<p>oracle4用户可以执行/usr/sbin下的所有命令除了/usr/sbin/userdel</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">oracle4 ALL&#x3D;(ALL) &#x2F;usr&#x2F;sbin&#x2F;,!&#x2F;usr&#x2F;sbin&#x2F;userdel</span><br></pre></td></tr></table></figure>
</blockquote>
<p>sudo提供人性化的日志功能，在<code>/var/log/secure</code>日志文件中可以查看到，用于记录所有sudo类用户的所有动作</p>
<h5 id="如果需要在shell脚本中通过sudo调用某个命令或者程序-应如何配置sudo"><a href="#如果需要在shell脚本中通过sudo调用某个命令或者程序-应如何配置sudo" class="headerlink" title="如果需要在shell脚本中通过sudo调用某个命令或者程序,  应如何配置sudo?"></a>如果需要在shell脚本中通过sudo调用某个命令或者程序,  应如何配置sudo?</h5><p>sudo 的 -S 选项允许从stdin读入密码，使用方式<code>echo [password] | sudo -S [command]</code></p>
<h5 id="怎样将某个⾼权限程序⼀部分的功能开放给sudo"><a href="#怎样将某个⾼权限程序⼀部分的功能开放给sudo" class="headerlink" title="怎样将某个⾼权限程序⼀部分的功能开放给sudo?"></a>怎样将某个⾼权限程序⼀部分的功能开放给sudo?</h5><p><strong>例如能让用户改⼦网掩码, ⽽不让他修改机器ip地址</strong></p>
<h4 id="cron"><a href="#cron" class="headerlink" title="cron"></a>cron</h4><h5 id="系统配置⽂件的路径是什么"><a href="#系统配置⽂件的路径是什么" class="headerlink" title="系统配置⽂件的路径是什么?"></a>系统配置⽂件的路径是什么?</h5><p>/etc/crontab</p>
<h5 id="cron时间描述⾥的’-‘是什么意思-‘-‘是什么意思"><a href="#cron时间描述⾥的’-‘是什么意思-‘-‘是什么意思" class="headerlink" title="cron时间描述⾥的’-‘是什么意思, ‘/‘是什么意思?"></a>cron时间描述⾥的’-‘是什么意思, ‘/‘是什么意思?</h5><p>整数间的短线（-）指定一个整数范围。如，1-4意味着整数 1、2、3、4</p>
<p>正斜线（/）可以用来指定间隔频率。在范围后加上 /<integer>意味着在范围内可以跳过 integer。如，0-59/2可以用来在分钟字段定义每两分钟。间隔频率值还可以和星号一起使用。例如，*/3的值可以用在月份字段中表示每三个月运行一次任务</integer></p>
<h5 id="reboot会在什么时候执行"><a href="#reboot会在什么时候执行" class="headerlink" title="@reboot会在什么时候执行?"></a>@reboot会在什么时候执行?</h5><p>希望在系统重启后执行某个命令</p>
<h5 id="cron的最小粒度是分钟-如何⽤cron实现每分钟跑两次-例如-分别在第0秒和第30秒-运⾏的任务"><a href="#cron的最小粒度是分钟-如何⽤cron实现每分钟跑两次-例如-分别在第0秒和第30秒-运⾏的任务" class="headerlink" title="cron的最小粒度是分钟, 如何⽤cron实现每分钟跑两次(例如, 分别在第0秒和第30秒)运⾏的任务?"></a>cron的最小粒度是分钟, 如何⽤cron实现每分钟跑两次(例如, 分别在第0秒和第30秒)运⾏的任务?</h5><p>起两个cron，其中一个cron 利用sleep休息30秒</p>
<h4 id="用户管理"><a href="#用户管理" class="headerlink" title="用户管理"></a>用户管理</h4><h5 id="新⽤户创建时-如果选择⾃动创建⽤户home⽬录-此时home⽬录中⾃动⽣成的内容是从哪⼉来的"><a href="#新⽤户创建时-如果选择⾃动创建⽤户home⽬录-此时home⽬录中⾃动⽣成的内容是从哪⼉来的" class="headerlink" title="新⽤户创建时, 如果选择⾃动创建⽤户home⽬录, 此时home⽬录中⾃动⽣成的内容是从哪⼉来的?"></a>新⽤户创建时, 如果选择⾃动创建⽤户home⽬录, 此时home⽬录中⾃动⽣成的内容是从哪⼉来的?</h5><p>把框架目录(默认为/etc/skel)下的文件复制到用户主目录下</p>
<h5 id="删除⼀个⽤户时-系统会执⾏哪些操作-改变哪些⽂件"><a href="#删除⼀个⽤户时-系统会执⾏哪些操作-改变哪些⽂件" class="headerlink" title="删除⼀个⽤户时, 系统会执⾏哪些操作, 改变哪些⽂件?"></a>删除⼀个⽤户时, 系统会执⾏哪些操作, 改变哪些⽂件?</h5><p>userdel 会查询系统账户文件，例如 /etc/password 和 /etc/group。那么它会删除所有和用户名相关的条目。在删除它之前，用户名必须存在。</p>
<p>不带选项使用 userdel，只会删除用户，-r 用于彻底删除，用户HOME目录下的信息会被移除，在其他位置上的档案也将一一找出并删除，比如用户的邮件池，在路径/var/mail/用户名下的邮件</p>
<h5 id="用户的密码存储在哪个⽂件⾥"><a href="#用户的密码存储在哪个⽂件⾥" class="headerlink" title="用户的密码存储在哪个⽂件⾥?"></a>用户的密码存储在哪个⽂件⾥?</h5><p>Linux 账号文件/etc/passwd，密码文件/etc/shadow</p>
<h5 id="禁⽌用户登录的⽅式有哪些"><a href="#禁⽌用户登录的⽅式有哪些" class="headerlink" title="禁⽌用户登录的⽅式有哪些?"></a>禁⽌用户登录的⽅式有哪些?</h5><p>命令方式</p>
<ul>
<li>已经存在的用户: usermod -s /sbin/nologin</li>
<li>新用户: useradd -s /sbin/nologin</li>
</ul>
<blockquote>
<p>/bin/false是最严格的禁止login选项，一切服务都不能用</p>
<p>而/sbin/nologin只是不允许login系统，但可以使用其他ftp等服务</p>
</blockquote>
<p>直接修改/etc/passwd</p>
<p>把需禁止用户的/bin/bash修改为/sbin/nologin</p>
<h5 id="如何踢用户下线"><a href="#如何踢用户下线" class="headerlink" title="如何踢用户下线"></a>如何踢用户下线</h5><p>Linux系统root用户可强制踢制其它登录用户</p>
<p>首先可用w命令查看登录用户信息 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ w</span><br><span class="line">14:21:31 up 1 day,  4:57,  2 users,  load average: 0.00, 0.00, 0.00</span><br><span class="line">USER     TTY      FROM              LOGIN@   IDLE   JCPU   PCPU WHAT</span><br><span class="line">root     pts&#x2F;1    192.168.1.200    14:18   21.00s  0.01s  0.01s -bash</span><br><span class="line">root     pts&#x2F;2    192.168.1.200    14:21    0.00s  0.01s  0.00s w</span><br></pre></td></tr></table></figure>

<p>强制踢人命令格式<code>pkill -kill -t tty</code>比如踢掉第一个root: <code>pkill -kill -t pts/1</code></p>
<ul>
<li>只有root用户才能踢人，但任何用户都可以踢掉自己 </li>
<li>如果同时有二个人用root用户登录，任何其中一个可以踢掉另一个 </li>
<li>pts/0就是自己开的桌面环境现的第一个终端 </li>
</ul>
<h4 id="⽂件传输-scp-vs-rsync"><a href="#⽂件传输-scp-vs-rsync" class="headerlink" title="⽂件传输: scp vs rsync"></a>⽂件传输: scp vs rsync</h4><h5 id="对比默认参数下-两种⽅式消耗的系统资源情况"><a href="#对比默认参数下-两种⽅式消耗的系统资源情况" class="headerlink" title="对比默认参数下, 两种⽅式消耗的系统资源情况"></a>对比默认参数下, 两种⽅式消耗的系统资源情况</h5><p>在都是空目录的情况下同步信息，scp和rsync的执行效率相当，在一个量级，但是当已经同步过一次之后，在后续同步内容的过程中会看到同步的效率rsync快了非常多，这是因为scp是复制，而rsync是覆盖。</p>
<p>scp消耗资源少，不会提高多少系统负荷，在这一点上，rsync就远远不及它。虽然 rsync比scp会快一点，但当小文件众多的情况，rsync会导致硬盘I/O非常高，而scp基本不影响系统使用</p>
<h5 id="在服务器端存在对应服务的条件下-哪种方式的传输是有加密的"><a href="#在服务器端存在对应服务的条件下-哪种方式的传输是有加密的" class="headerlink" title="在服务器端存在对应服务的条件下, 哪种方式的传输是有加密的?"></a>在服务器端存在对应服务的条件下, 哪种方式的传输是有加密的?</h5><p>rsync默认不是加密传输，而scp是加密传输，使用时可以按需选择</p>
<p>rsync如果有加密传输文件的需求，可以自定义加密管道管道协议，使用ssh通道或者vpn通道。使用参数：-e来指定相应的管道协议</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 使用-e参数，指定加密的协议以及协议的端口号</span><br><span class="line">$ rsync -avz &#x2F;etc&#x2F;hosts -e &#39;ssh -p 22&#39; root@172.16.1.41:&#x2F;mnt&#x2F;</span><br></pre></td></tr></table></figure>

<h5 id="请阐述在scp和rsync的具体适⽤场景"><a href="#请阐述在scp和rsync的具体适⽤场景" class="headerlink" title="请阐述在scp和rsync的具体适⽤场景"></a>请阐述在scp和rsync的具体适⽤场景</h5><p>rsync 背后算法原理可以参考<a href="https://coolshell.cn/articles/7425.html" target="_blank" rel="noopener">RSYNC 的核心算法</a></p>
<ul>
<li><p>如果是频繁更新的文件并且是小文件，则建议使用rsync</p>
</li>
<li><p>如果是很少更新的文件，建议使用scp，简单方便快捷，同时还是加密传输</p>
</li>
</ul>
<p><strong>即在什么时候应该适用scp⽽不是rsync, 在什么时候应该适用rsync⽽不是scp</strong></p>
<p>只需传输改动部分，无需重新传输整个文件时用rsync</p>
<p>点对点传输全量文件时用scp</p>
<ul>
<li>rsync只对差异文件做更新，可以做增量或全量备份；而scp只能做全量备份。简单说就是rsync只传修改了的部分，如果改动较小就不需要全部重传，所以rsync备份速度较快；默认情况下，rsync通过比较文件的最后修改时间（mtime）和文件的大小（size）来确认哪些文件需要被同步过去</li>
<li>rsync是分块校验+传输，scp是整个文件传输。rsync比scp有优势的地方在于单个大文件的一小部分存在改动时，只需传输改动部分，无需重新传输整个文件。如果传输一个新的文件，理论上rsync没有优势</li>
</ul>
<h3 id="Linux-实践部分"><a href="#Linux-实践部分" class="headerlink" title="Linux 实践部分"></a>Linux 实践部分</h3><h4 id="top"><a href="#top" class="headerlink" title="top"></a>top</h4><h5 id="如何将top的输出通过管道交给另⼀个进程"><a href="#如何将top的输出通过管道交给另⼀个进程" class="headerlink" title="如何将top的输出通过管道交给另⼀个进程?"></a>如何将top的输出通过管道交给另⼀个进程?</h5><p>top -bn 1  显示所有进程信息</p>
<ul>
<li>-b: 在批处理模式下启动top，这对于将top输出发送到其他程序或文件很有用。在此模式下，top将不接受输入，并一直运行到您使用`-n’命令行选项设置的迭代次数限制或终止为止</li>
<li>指定结束前top应生成的最大迭代次数或帧数</li>
</ul>
<p>举例，top翻页: top -bn1 | less</p>
<h5 id="如何让top显示每一个CPU的使⽤情况"><a href="#如何让top显示每一个CPU的使⽤情况" class="headerlink" title="如何让top显示每一个CPU的使⽤情况?"></a>如何让top显示每一个CPU的使⽤情况?</h5><p>如果要查看每个逻辑cpu的使用率，只需要运行top命令，<strong>按下数字键1即可</strong></p>
<h5 id="如何在top⾥杀进程"><a href="#如何在top⾥杀进程" class="headerlink" title="如何在top⾥杀进程"></a>如何在top⾥杀进程</h5><p>按下k即可，根据后面输入的PID杀死进程</p>
<h5 id="top的默认刷新时间是多少-如何修改这个默认设置"><a href="#top的默认刷新时间是多少-如何修改这个默认设置" class="headerlink" title="top的默认刷新时间是多少? 如何修改这个默认设置?"></a>top的默认刷新时间是多少? 如何修改这个默认设置?</h5><p>默认是5s，可以在启动时使用-d指定信息刷新的时间间隔，也可以在交互时键入d命令指定间隔时间</p>
<h5 id="top⾥的load-average是如何计算的"><a href="#top⾥的load-average是如何计算的" class="headerlink" title="top⾥的load average是如何计算的?"></a>top⾥的load average是如何计算的?</h5><p>指单位时间内，系统处于<strong>可运行状态</strong>和<strong>不可中断状态</strong>的平均进程数，也就是<strong>平均活跃进程数</strong></p>
<blockquote>
<p>对于单cpu和多cpu情况，系统的average load情况稍有不同。单cpu是最简单的情形，比如过去的平均一分钟里面，判断系统处于运行或者等待状态的进程数则表示系统的平均负载，但是在linux系统下稍有不同，那些处于io等待状态的进程也会被纳入去计算。这样就导致CPU利用率可能和平均负载很不同，在大部分进程都在做IO的时候，即使平均负载很大，也会没有很大的CPU利用率。另外，有些系统对于进程和线程的处理也很不一样，有的对于每一个线程都会进行计算，有的只关注进程，对于超线程技术的线程来说，可能又是别的处理方式。对于多CPU的平均负载的计算，是在单CPU的情况下再除以CPU的个数</p>
</blockquote>
<h5 id="假设top显示ffmpeg进程的CPU使⽤率为143-7-请具体解释这个数值是如何计算出来的"><a href="#假设top显示ffmpeg进程的CPU使⽤率为143-7-请具体解释这个数值是如何计算出来的" class="headerlink" title="假设top显示ffmpeg进程的CPU使⽤率为143.7%, 请具体解释这个数值是如何计算出来的"></a>假设top显示ffmpeg进程的CPU使⽤率为143.7%, 请具体解释这个数值是如何计算出来的</h5><p>ffmpeg进程在多核CPU下，每个CPU使用率的累加和</p>
<h5 id="第四列的NI是什么意思"><a href="#第四列的NI是什么意思" class="headerlink" title="第四列的NI是什么意思?"></a>第四列的NI是什么意思?</h5><p>任务nice值，代表这个进程的优先值</p>
<blockquote>
<p>在LINUX系统中，Nice值的范围从-20到+19（不同系统的值范围是不一样的），正值表示低优先级，负值表示高优先级，值为零则表示不会调整该进程的优先级。具有最高优先级的程序，其nice值最低，所以在LINUX系统中，值-20使得一项任务变得非常重要；与之相反，如果任务的nice为+19，则表示它是一个高尚的、无私的任务，允许所有其他任务比自己享有宝贵的CPU时间的更大使用份额，这也就是nice的名称的来意</p>
</blockquote>
<h4 id="ssh"><a href="#ssh" class="headerlink" title="ssh"></a>ssh</h4><h5 id="ssh的实现原理"><a href="#ssh的实现原理" class="headerlink" title="ssh的实现原理"></a>ssh的实现原理</h5><ol>
<li><p>远程主机收到用户的登录请求，把自己的公钥发给用户</p>
</li>
<li><p>用户使用这个公钥，将登录密码加密后，发送回来</p>
</li>
<li><p>远程主机用自己的私钥，解密登录密码，如果密码正确，就同意用户登录</p>
</li>
</ol>
<h5 id="ssh连接时如何指定远程端口-如何设置连接超时间"><a href="#ssh连接时如何指定远程端口-如何设置连接超时间" class="headerlink" title="ssh连接时如何指定远程端口, 如何设置连接超时间?"></a>ssh连接时如何指定远程端口, 如何设置连接超时间?</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># -p 指定端口</span><br><span class="line"># -o ConnectTimeout&#x3D;5 指定超时时间为5s，最大是2m7s左右</span><br><span class="line"># 如果要获得更大的超时时间，可以通过配置 ConnectionAttempts 来实现</span><br><span class="line">$ ssh -p 22 -o ConnectTimeout&#x3D;5 root@192.168.1.240</span><br></pre></td></tr></table></figure>

<h5 id="ssh私钥⽂件默认的权限是什么"><a href="#ssh私钥⽂件默认的权限是什么" class="headerlink" title="ssh私钥⽂件默认的权限是什么?"></a>ssh私钥⽂件默认的权限是什么?</h5><p>600，属主是当前用户</p>
<h5 id="如何利⽤ssh来进行端⼝转发"><a href="#如何利⽤ssh来进行端⼝转发" class="headerlink" title="如何利⽤ssh来进行端⼝转发?"></a>如何利⽤ssh来进行端⼝转发?</h5><p><strong>本地端口转发</strong></p>
<p>假定host1是本地主机，host2是远程主机。由于种种原因，这两台主机之间无法连通。但是，另外还有一台host3，可以同时连通前面两台主机。因此，很自然的想法就是，通过host3，将host1连上host2</p>
<p>在host1执行下面的命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ssh -L 2121:host2:21 host3</span><br></pre></td></tr></table></figure>

<p>命令中的L参数一共接受三个值，分别是”本地端口:目标主机:目标主机端口”，它们之间用冒号分隔。这条命令的意思，就是指定SSH绑定本地端口2121，然后指定host3将所有的数据，转发到目标主机host2的21端口（假定host2运行FTP，默认端口为21）</p>
<p>这样一来，只要连接host1的2121端口，就等于连上了host2的21端口</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ftp localhost:2121</span><br></pre></td></tr></table></figure>

<p>“本地端口转发”使得host1和host3之间仿佛形成一个数据传输的秘密隧道，因此又被称为”SSH隧道”</p>
<p>下面是一个比较有趣的例子</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ssh -L 5900:localhost:5900 host3</span><br></pre></td></tr></table></figure>

<p>它表示将本机的5900端口绑定host3的5900端口（这里的localhost指的是host3，因为目标主机是相对host3而言的）</p>
<p>另一个例子是通过host3的端口转发，ssh登录host2</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ssh -L 9001:host2:22 host3</span><br></pre></td></tr></table></figure>

<p>这时，只要ssh登录本机的9001端口，就相当于登录host2了，下面的-p参数表示指定登录端口</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ssh -p 9001 localhost</span><br></pre></td></tr></table></figure>

<p><strong>远程端口转发</strong></p>
<p>还是接着看上面那个例子，host1与host2之间无法连通，必须借助host3转发。但是，特殊情况出现了，host3是一台内网机器，它可以连接外网的host1，但是反过来就不行，外网的host1连不上内网的host3。这时，”本地端口转发”就不能用了，怎么办？</p>
<p>解决办法是，既然host3可以连host1，那么就从host3上建立与host1的SSH连接，然后在host1上使用这条连接就可以了</p>
<p>在host3执行下面的命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">　$ ssh -R 2121:host2:21 host1</span><br></pre></td></tr></table></figure>

<p>R参数也是接受三个值，分别是”远程主机端口:目标主机:目标主机端口”。这条命令的意思，就是让host1监听它自己的2121端口，然后将所有数据经由host3，转发到host2的21端口。由于对于host3来说，host1是远程主机，所以这种情况就被称为”远程端口绑定”</p>
<p>绑定之后，我们在host1就可以连接host2了</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ftp localhost:2121</span><br></pre></td></tr></table></figure>

<h5 id="passphrase是什么东西-有什么作用"><a href="#passphrase是什么东西-有什么作用" class="headerlink" title="passphrase是什么东西? 有什么作用?"></a>passphrase是什么东西? 有什么作用?</h5><p>passphrase是对私钥设置的口令，passphrase是用来对密钥对的私钥进行加密的，不会在网络上传播</p>
<h5 id="在⾃己的电脑上操作，通过-ssh-agent-forward-先登录-A，再从-A-登录-B"><a href="#在⾃己的电脑上操作，通过-ssh-agent-forward-先登录-A，再从-A-登录-B" class="headerlink" title="在⾃己的电脑上操作，通过 ssh agent forward 先登录 A，再从 A 登录 B"></a>在⾃己的电脑上操作，通过 ssh agent forward 先登录 A，再从 A 登录 B</h5><p><a href="https://www.jianshu.com/p/12de50582e63" target="_blank" rel="noopener">SSH &amp; SSH agent forward</a></p>
<h4 id="find"><a href="#find" class="headerlink" title="find"></a>find</h4><h5 id="find能根据哪些条件来查找⽂件"><a href="#find能根据哪些条件来查找⽂件" class="headerlink" title="find能根据哪些条件来查找⽂件"></a>find能根据哪些条件来查找⽂件</h5><ul>
<li>指搜索层级</li>
<li>根据文件名和inode查找</li>
<li>根据属主、属组查找</li>
<li>根据文件类型查找</li>
<li>空文件或目录</li>
<li>组合条件</li>
<li>根据文件大小来查找</li>
<li>根据时间戳</li>
<li>根据权限查找</li>
</ul>
<h5 id="find能否根据⽂件内容来搜索，-为什么"><a href="#find能否根据⽂件内容来搜索，-为什么" class="headerlink" title="find能否根据⽂件内容来搜索， 为什么?"></a>find能否根据⽂件内容来搜索， 为什么?</h5><p>不能，但是可以配合grep 命令来</p>
<p><code>find / | xargs grep function</code>查找系统根目录下面的所有文件的内容中包含有function字符串的文件列表</p>
<h5 id="find可以如何删除找到的⽂件-请提供三种方法"><a href="#find可以如何删除找到的⽂件-请提供三种方法" class="headerlink" title="find可以如何删除找到的⽂件? 请提供三种方法"></a>find可以如何删除找到的⽂件? 请提供三种方法</h5><p>以删除目录下所有exe文件为例</p>
<ul>
<li>find . -name -type f  “*.exe” | xargs rm -rf</li>
<li>find . -name ‘’*.exe” -type f -print -exec rm -rf {} ;</li>
<li>find  ./ -iname “*.exe” -ok rm {} ;</li>
<li>find  ./ -iname “*.exe” -delete</li>
</ul>
<h5 id="mtime-ctime-atime的区别"><a href="#mtime-ctime-atime的区别" class="headerlink" title="mtime, ctime, atime的区别"></a>mtime, ctime, atime的区别</h5><ul>
<li>atime:（access time）显示的是文件中的数据最后被访问的时间，比如系统的进程直接使用或通过一些命令和脚本间接使用。（执行一些可执行文件或脚本）</li>
<li>mtime:（modify time）显示的是文件内容被修改的最后时间，比如用vi编辑时就会被改变。（也就是Block的内容）</li>
<li>ctime:（change time）显示的是文件的权限、拥有者、所属的组、链接数发生改变时的时间。当然当内容改变时也会随之改变（即inode内容发生改变和Block内容发生改变时</li>
</ul>
<blockquote>
<p>有一个要注意的就是，在kernel 2.6.30之前，文件系统中默认会及时的更新atime，而在此之后的版本里有变化<br>Linux atime修改策略与 mount 有关，可选的值有 noatime、relatime 和 strictatime</p>
<ul>
<li>noatime: atime不会被更新，即使修改了文件内容</li>
<li>relatime:<ul>
<li>如果一个文件的 atime 比 ctime 或 mtime 更早，此时你去读取了该文件，atime 才会被更新为当前时间。</li>
<li>atime 比现在早一天，那么 atime 在文件读取时会被更新</li>
</ul>
</li>
<li>strictatime: atime 在文件每次被读取时，都能够被更新</li>
</ul>
<p>所以只有发生以下三种情况之一才会更新atime</p>
<ul>
<li>将分区 mount 的挂载的时候指定采用非 relatime 方式，使用方法就是通过<code>mount -o relatime /dir</code>来挂装目录</li>
<li>atime 小于 ctime 或者小于 mtime 的时候</li>
<li>本次的 access time 和上次的 atime 超过24个小时</li>
</ul>
</blockquote>
<h5 id="type中有哪些常⻅类型"><a href="#type中有哪些常⻅类型" class="headerlink" title="-type中有哪些常⻅类型?"></a>-type中有哪些常⻅类型?</h5><p>b      block (buffered) special</p>
<ul>
<li>c: character (unbuffered) special</li>
<li>d: directory</li>
<li>p: named pipe (FIFO)</li>
<li>f: regular file</li>
<li>l: symbolic  link;  this  is never true if the -L option or the -follow option is in effect, unless the symbolic link is broken.  If you want to search for symbolic links when -L is in effect, use -xtype.</li>
<li>s: socket</li>
<li>D: door (Solaris)</li>
</ul>
<h4 id="grep"><a href="#grep" class="headerlink" title="grep"></a>grep</h4><h5 id="了解-c-v-A-B-C-E-n-i-R参数的意义"><a href="#了解-c-v-A-B-C-E-n-i-R参数的意义" class="headerlink" title="了解-c/-v/-A/-B/-C/-E/-n/-i/-R参数的意义"></a>了解-c/-v/-A/-B/-C/-E/-n/-i/-R参数的意义</h5><ul>
<li>-c: 统计匹配到的次数</li>
<li>-v: 查找不包含指定内容的行</li>
<li>-A: 显示匹配行及前面多少行, 如: -A3, 则表示显示匹配行及前3行</li>
<li>-B: 显示匹配行及后面多少行, 如: -B3, 则表示显示匹配行及后3行</li>
<li>-C: 显示匹配行前后多少行,   如: -C3, 则表示显示批量行前后3行</li>
<li>-E: –extended-regexp的缩写，将模式解释为一个扩展的正则表达式</li>
<li>-n: 显示行号</li>
<li>-i: 不区分大小写</li>
<li>-R: 递归地读取每个目录下的所有文件。遵循所有符号链接，不像-r</li>
<li>-w: 按单词搜索</li>
<li>-e: 使用正则搜索</li>
<li>-r: 逐层遍历目录查找</li>
<li>–color: 匹配到的内容高亮显示</li>
<li>–include: 指定匹配的文件类型</li>
<li>–exclude: 过滤不需要匹配的文件类型</li>
<li>-q: 不显示任何信息</li>
<li>-o: 只显示匹配PATTERN 部分</li>
</ul>
<h5 id="对于-q-o参数-给出具体的使用场景"><a href="#对于-q-o参数-给出具体的使用场景" class="headerlink" title="对于-q/-o参数, 给出具体的使用场景"></a>对于-q/-o参数, 给出具体的使用场景</h5><p>-q 场景用于是否能够匹配，通过$?知道命令的执行结果来确认是否匹配到信息，避免泄漏信息</p>
<p>-o 场景用于知道匹配到的文件，不需要过多知道匹配到的行信息，避免泄漏 </p>
<h4 id="ls"><a href="#ls" class="headerlink" title="ls"></a>ls</h4><h5 id="对于ls-l的输出中的第⼀列-给出每个字符的含义"><a href="#对于ls-l的输出中的第⼀列-给出每个字符的含义" class="headerlink" title="对于ls -l的输出中的第⼀列, 给出每个字符的含义"></a>对于ls -l的输出中的第⼀列, 给出每个字符的含义</h5><p>第一个字符如果是d表示目录，空为文件</p>
<p>从r 读权限，w 写权限，x执行权限</p>
<h5 id="目录的⼤小是什么意思"><a href="#目录的⼤小是什么意思" class="headerlink" title="目录的⼤小是什么意思?"></a>目录的⼤小是什么意思?</h5><p>用ls命令出来的目录大小，不包括里面的文件大小</p>
<p>基本上用ls命令查看到的目录大小都是4K（假设块大小为4K）</p>
<h5 id="ls默认的排序⽅式是什么-有哪些参数能改变这⼀行为"><a href="#ls默认的排序⽅式是什么-有哪些参数能改变这⼀行为" class="headerlink" title="ls默认的排序⽅式是什么? 有哪些参数能改变这⼀行为?"></a>ls默认的排序⽅式是什么? 有哪些参数能改变这⼀行为?</h5><p>默认会以文件名排序</p>
<p>-S 基于文件大小进行排序；-t 基于文件修改时间进行排序；-r 将排序结果反向输出</p>
<p>-f 直接列出结果，而不进行排序</p>
<h5 id="对于-R-i参数-请给出具体的使⽤场景"><a href="#对于-R-i参数-请给出具体的使⽤场景" class="headerlink" title="对于-R/-i参数, 请给出具体的使⽤场景"></a>对于-R/-i参数, 请给出具体的使⽤场景</h5><p>-R 若目录下有文件，则以下之文件亦皆依序列出</p>
<p>-i 打印每个文件、目录的索引号</p>
<p>需要查看子目录下的文件时，使用-R，需要知道索引号时使用 -i</p>
<h4 id="df-du"><a href="#df-du" class="headerlink" title="df/du"></a>df/du</h4><h5 id="如何显示inode占用率"><a href="#如何显示inode占用率" class="headerlink" title="如何显示inode占用率?"></a>如何显示inode占用率?</h5><p>df -ih</p>
<h5 id="如何显示⽂件系统的类型"><a href="#如何显示⽂件系统的类型" class="headerlink" title="如何显示⽂件系统的类型?"></a>如何显示⽂件系统的类型?</h5><p>df -Th</p>
<h5 id="什么情况下⽤rm删除了一个⼤文件，df显示的空余⼤小会没有变化"><a href="#什么情况下⽤rm删除了一个⼤文件，df显示的空余⼤小会没有变化" class="headerlink" title="什么情况下⽤rm删除了一个⼤文件，df显示的空余⼤小会没有变化"></a>什么情况下⽤rm删除了一个⼤文件，df显示的空余⼤小会没有变化</h5><p>参考自<a href="https://blog.51cto.com/ixdba/1435781" target="_blank" rel="noopener">运维实战案例之文件已删除但空间不释放问题解析</a>，文中问题是删除access_log文件空间未释放</p>
<p><strong>解决思路</strong></p>
<p>一般说来不会出现删除文件后空间不释放的情况，但是也存在例外，比如文件被进程锁定，或者有进程一直在向这个文件写数据等等，要理解这个问题，就需要知道Linux下文件的存储机制和存储结构。</p>
<p>一个文件在文件系统中的存放分为两个部分：数据部分和指针部分，指针位于文件系统的meta-data中，数据被删除后，这个指针就从meta-data中清除了，而数据部分存储在磁盘中，数据对应的指针从meta-data中清除后，文件数据部分占用的空间就可以被覆盖并写入新的内容，之所以出现删除access_log文件后，空间还没释放，就是因为httpd进程还在一直向这个文件写入内容，导致虽然删除了access_log文件，但文件对应的指针部分由于进程锁定，并未从meta-data中清除，而由于指针并未被删除，那么系统内核就认为文件并未被删除，因此通过df命令查询空间并未释放也就不足为奇了</p>
<p><strong>问题排查</strong></p>
<p>既然有了解决问题的思路，那么接下来看看是否有进程一直在向acess.log文件中写数据，这里需要用到Linux下的lsof命令，通过<code>lsof | grep delete</code>命令可以获取一个已经被删除但仍然被应用程序占用的文件列表</p>
<p>从输出结果可以看到，/tmp/acess.log文件被进程httpd锁定，而httpd进程还一直向这个文件写入日志数据，从第七列可知，这个日志文件大小仅70G，而系统根分区总大小才100G，由此可知，这个文件就是导致系统根分区空间耗尽的罪魁祸首，在最后一列的“deleted”状态，说明这个日志文件已经被删除，但由于进程还在一直向此文件写入数据，空间并未释放</p>
<p><strong>解决问题</strong></p>
<p>最简单的方法是关闭或者重启httpd进程，不过这并不是最好的方法，对待这种进程不停对文件写日志的操作，要释放文件占用的磁盘空间，最好的方法是在线清空这个文件，通过以下命令完成</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo &quot; &quot; &gt;&#x2F;tmp&#x2F;acess.log</span><br></pre></td></tr></table></figure>

<p>通过这种方法，磁盘空间不但可以马上释放，也可保障进程继续向文件写入日志，这种方法经常用于在线清理Apache、Tomcat、Nginx等Web服务产生的日志文件</p>
<h5 id="如何仅显示某个⽬录下⽂件的总⼤小"><a href="#如何仅显示某个⽬录下⽂件的总⼤小" class="headerlink" title="如何仅显示某个⽬录下⽂件的总⼤小"></a>如何仅显示某个⽬录下⽂件的总⼤小</h5><p>du -sh 目录名</p>
<h5 id="请解释如何产⽣一个⽂件空洞"><a href="#请解释如何产⽣一个⽂件空洞" class="headerlink" title="请解释如何产⽣一个⽂件空洞"></a>请解释如何产⽣一个⽂件空洞</h5><p>![image-20200409135546433](/Users/tu/Library/Application Support/typora-user-images/image-20200409135546433.png)</p>
<p>在UNIX文件操作中，文件位移量可以大于文件的当前长度</p>
<blockquote>
<p>在这种情况下，对该文件的下一次写将延长该文件，并在文件中构成一个空洞。位于文件中但没有写过的字节都被设为 0</p>
</blockquote>
<p>如果 offset 比文件的当前长度更大，下一个写操作就会把文件”撑大（extend）”在文件里创造”空洞（hole）”</p>
<blockquote>
<p>没有被实际写入文件的所有字节由重复的 0 表示。空洞是否占用硬盘空间是由文件系统（file system）决定的</p>
</blockquote>
<h5 id="如果发现了-df-和-du-两个命令的输出磁盘占⽤量不一致，可能原因有哪些"><a href="#如果发现了-df-和-du-两个命令的输出磁盘占⽤量不一致，可能原因有哪些" class="headerlink" title="如果发现了 df 和 du 两个命令的输出磁盘占⽤量不一致，可能原因有哪些"></a>如果发现了 df 和 du 两个命令的输出磁盘占⽤量不一致，可能原因有哪些</h5><p>常见的df和du不一致情况就是文件删除的问题。当一个文件被删除后，在文件系统 目录中已经不可见了，所以du就不会再统计它了。然而如果此时还有运行的进程持有这个已经被删除了的文件的句柄，那么这个文件就不会真正在磁盘中被删除， 分区超级块中的信息也就不会更改。这样df仍旧会统计这个被删除了的文件</p>
<blockquote>
<p>硬盘空间消失是因为删除的文件被其他程序引用，导致空间无法回收</p>
</blockquote>
<h4 id="ps"><a href="#ps" class="headerlink" title="ps"></a>ps</h4><h5 id="ps-auxww默认是按照什么进⾏排序的"><a href="#ps-auxww默认是按照什么进⾏排序的" class="headerlink" title="ps auxww默认是按照什么进⾏排序的?"></a>ps auxww默认是按照什么进⾏排序的?</h5><p><code>auxww</code>参数解释</p>
<ul>
<li><code>a</code>选项显示出所有运行进程的内容， 而不仅仅是您的进程</li>
<li><code>u</code>选项显示出进程所归属的用户名字以及内存使用，</li>
<li><code>x</code> 选项显示出后台进程</li>
<li><code>ww</code> 选项表示把每个进程的整个命令行全部显示完， 而不是由于命令行过长就把它从屏幕上截去</li>
</ul>
<p>默认情况下，输出不排序</p>
<h5 id="如何用ps来查看进程树"><a href="#如何用ps来查看进程树" class="headerlink" title="如何用ps来查看进程树"></a>如何用ps来查看进程树</h5><p>ps -e f</p>
<p>ps -e -H</p>
<h5 id="如何⽤ps来查看单个线程的资源使⽤情况"><a href="#如何⽤ps来查看单个线程的资源使⽤情况" class="headerlink" title="如何⽤ps来查看单个线程的资源使⽤情况?"></a>如何⽤ps来查看单个线程的资源使⽤情况?</h5><p>-L用于显示线程</p>
<p>ps -eLf</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ps -eLf</span><br><span class="line">UID PID PPID LWP C NLWP STIME TTY TIME CMD</span><br><span class="line">root 1 0 1 0 1 08:31 ? 00:00:00 init [5]</span><br><span class="line">root 2 1 2 0 1 08:31 ? 00:00:00 [migration&#x2F;0]</span><br><span class="line">root 2233 2228 2233 3 8 08:35 ? 00:04:50 &#x2F;root&#x2F;firefox&#x2F;firefox-bin</span><br><span class="line">root 2233 2228 2271 0 8 08:36 ? 00:00:00 &#x2F;root&#x2F;firefox&#x2F;firefox-bin</span><br><span class="line">root 2233 2228 2272 0 8 08:36 ? 00:00:01 &#x2F;root&#x2F;firefox&#x2F;firefox-bin</span><br><span class="line">root 2233 2228 2277 0 8 08:36 ? 00:00:00 &#x2F;root&#x2F;firefox&#x2F;firefox-bin</span><br><span class="line">root 2233 2228 2278 0 8 08:36 ? 00:00:00 &#x2F;root&#x2F;firefox&#x2F;firefox-bin</span><br><span class="line">root 2233 2228 2279 0 8 08:36 ? 00:00:00 &#x2F;root&#x2F;firefox&#x2F;firefox-bin</span><br></pre></td></tr></table></figure>

<p>LWP　light weight process ID 可以称其为线程ID<br>NLWP 进程中的线程数number of lwps (threads) in the process</p>
<h4 id="cut"><a href="#cut" class="headerlink" title="cut"></a>cut</h4><h5 id="cut默认的分隔符是什么-如何设置分隔符"><a href="#cut默认的分隔符是什么-如何设置分隔符" class="headerlink" title="cut默认的分隔符是什么? 如何设置分隔符?"></a>cut默认的分隔符是什么? 如何设置分隔符?</h5><p> -d自定义分隔符，默认为制表符</p>
<h5 id="对于每⼀行-如何让cut仅显示第3到第5列"><a href="#对于每⼀行-如何让cut仅显示第3到第5列" class="headerlink" title="对于每⼀行, 如何让cut仅显示第3到第5列?"></a>对于每⼀行, 如何让cut仅显示第3到第5列?</h5><p>-f 参数指定 3-5</p>
<h5 id="对于每⼀行-如何让cut仅显示第10到第15个字符"><a href="#对于每⼀行-如何让cut仅显示第10到第15个字符" class="headerlink" title="对于每⼀行, 如何让cut仅显示第10到第15个字符?"></a>对于每⼀行, 如何让cut仅显示第10到第15个字符?</h5><p>-c 以字符为单位进行分割，-f 指定10-15</p>
<h5 id="在什么情况下-c和-b的输出有区别"><a href="#在什么情况下-c和-b的输出有区别" class="headerlink" title="在什么情况下, -c和-b的输出有区别?"></a>在什么情况下, -c和-b的输出有区别?</h5><p>-b: 以字节为单位进行分割，-c: 以字符为单位进行分割，多字节字符时就输出有区别了</p>
<h4 id="sort"><a href="#sort" class="headerlink" title="sort"></a>sort</h4><h5 id="如何对每⾏第3列进⾏排序"><a href="#如何对每⾏第3列进⾏排序" class="headerlink" title="如何对每⾏第3列进⾏排序?"></a>如何对每⾏第3列进⾏排序?</h5><p>-k3，-k指定要排序的key，key由字段组成。key格式为”POS1[,POS2]”，POS1为key起始位置，POS2为key结束位置</p>
<h5 id="请给出具体例子说明什么情况下使⽤了-n会导致sort的输出和不加-n不一致"><a href="#请给出具体例子说明什么情况下使⽤了-n会导致sort的输出和不加-n不一致" class="headerlink" title="请给出具体例子说明什么情况下使⽤了-n会导致sort的输出和不加-n不一致"></a>请给出具体例子说明什么情况下使⽤了-n会导致sort的输出和不加-n不一致</h5><p>当排序列为数值时，使用-n选项，来告诉sort要以数值来排序，不加-n则是以字典序排序</p>
<h5 id="请了解-u-r的意义"><a href="#请了解-u-r的意义" class="headerlink" title="请了解-u/-r的意义"></a>请了解-u/-r的意义</h5><p>-r默认是升序排序，使用该选项将得到降序排序的结果，-r不参与排序动作，只是操作排序完成后的结果</p>
<p>-u只输出重复行的第一行。结合”-f”使用时，重复的小写行被丢弃。 </p>
<h5 id="默认的分隔符是什么-如何指定分隔符"><a href="#默认的分隔符是什么-如何指定分隔符" class="headerlink" title="默认的分隔符是什么? 如何指定分隔符?"></a>默认的分隔符是什么? 如何指定分隔符?</h5><p>-t&lt;分隔字符&gt;: 指定排序时所用的栏位分隔字符</p>
<h5 id="请给出-T参数的使⽤场景"><a href="#请给出-T参数的使⽤场景" class="headerlink" title="请给出-T参数的使⽤场景"></a>请给出-T参数的使⽤场景</h5><p>sort命令在进行大文件排序，会自动使用外排序，此时默认会在/tmp目录下新建一个大文件，排序完成后删除，产生的临时文件是隐藏文件，解决办法是使用-T参数指定临时文件目录</p>
<h4 id="tail-head"><a href="#tail-head" class="headerlink" title="tail/head"></a>tail/head</h4><h5 id="如何取得⼀个⽂件的前几个字符"><a href="#如何取得⼀个⽂件的前几个字符" class="headerlink" title="如何取得⼀个⽂件的前几个字符?"></a>如何取得⼀个⽂件的前几个字符?</h5><p>head -c&lt;字符数&gt; finename</p>
<h5 id="tail-f是⼲什么的"><a href="#tail-f是⼲什么的" class="headerlink" title="tail -f是⼲什么的?"></a>tail -f是⼲什么的?</h5><p>tail命令用于输入文件中的尾部内容，-f 显示文件最新追加的内容</p>
<h5 id="如何用tail显示从第25⾏开始-显示⼀个40多⾏-不不知道具体数目-的⽂件的内容"><a href="#如何用tail显示从第25⾏开始-显示⼀个40多⾏-不不知道具体数目-的⽂件的内容" class="headerlink" title="如何用tail显示从第25⾏开始, 显示⼀个40多⾏(不不知道具体数目)的⽂件的内容?"></a>如何用tail显示从第25⾏开始, 显示⼀个40多⾏(不不知道具体数目)的⽂件的内容?</h5><p>tail -n +25</p>
<h5 id="请提供两种不同的办法来打印⼀个文件的第50⾏内容"><a href="#请提供两种不同的办法来打印⼀个文件的第50⾏内容" class="headerlink" title="请提供两种不同的办法来打印⼀个文件的第50⾏内容"></a>请提供两种不同的办法来打印⼀个文件的第50⾏内容</h5><p>![image-20200409162429804](/Users/tu/Library/Application Support/typora-user-images/image-20200409162429804.png)</p>
<p>head -n 50，输出最后一行就是第50⾏内容</p>
<p>tail -n +50，输出第一行就是第50⾏内容</p>
<h4 id="iostat"><a href="#iostat" class="headerlink" title="iostat"></a>iostat</h4><h5 id="直接⽆参数启动这个命令，能得到哪些数据"><a href="#直接⽆参数启动这个命令，能得到哪些数据" class="headerlink" title="直接⽆参数启动这个命令，能得到哪些数据?"></a>直接⽆参数启动这个命令，能得到哪些数据?</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ iostat</span><br><span class="line">Linux 3.10.0-514.26.2.el7.x86_64 (iZbp144crtihiqovt4h5m4Z) 	2020年04月09日 	_x86_64_	(1 CPU)</span><br><span class="line"></span><br><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">           2.31    0.00    1.23    0.01    0.00   96.44</span><br><span class="line"></span><br><span class="line">Device:            tps    kB_read&#x2F;s    kB_wrtn&#x2F;s    kB_read    kB_wrtn</span><br><span class="line">vda               0.43         1.80         3.29   38783769   70867288</span><br></pre></td></tr></table></figure>

<h5 id="如何持续监控某块硬盘的读写情况"><a href="#如何持续监控某块硬盘的读写情况" class="headerlink" title="如何持续监控某块硬盘的读写情况?"></a>如何持续监控某块硬盘的读写情况?</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ iostat -d vda -x 2</span><br><span class="line">Linux 3.10.0-514.26.2.el7.x86_64 (iZbp144crtihiqovt4h5m4Z) 	2020年04月09日 	_x86_64_	(1 CPU)</span><br><span class="line"></span><br><span class="line">Device:         rrqm&#x2F;s   wrqm&#x2F;s     r&#x2F;s     w&#x2F;s    rkB&#x2F;s    wkB&#x2F;s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">vda               0.00     0.16    0.06    0.37     1.80     3.29    23.76     0.00    6.74    6.57    6.77   0.48   0.02</span><br></pre></td></tr></table></figure>

<p>-d: 仅显示磁盘统计信息，接硬盘名可以指定某块具体硬盘，这里没有给出硬盘路径就是默认全部</p>
<p>-x: 输出扩展信息</p>
<h5 id="iostat的输出中，哪些输出对于诊断磁盘IO问题⽐较关键"><a href="#iostat的输出中，哪些输出对于诊断磁盘IO问题⽐较关键" class="headerlink" title="iostat的输出中，哪些输出对于诊断磁盘IO问题⽐较关键?"></a>iostat的输出中，哪些输出对于诊断磁盘IO问题⽐较关键?</h5><p>%iowait: 如果该值较高，表示磁盘存在I/O瓶颈</p>
<p>%util: 一秒中有百分之多少的时间用于I/O操作，即被IO消耗的CPU百分比，一般地，如果该参数是100%表示设备已经接近满负荷运行了</p>
<p>avgqu-sz: 如果avgqu-sz比较大，也表示有当量io在等待</p>
<h5 id="tps是什么东西-这个值正常情况下会在哪个范围内波动"><a href="#tps是什么东西-这个值正常情况下会在哪个范围内波动" class="headerlink" title="tps是什么东西? 这个值正常情况下会在哪个范围内波动?"></a>tps是什么东西? 这个值正常情况下会在哪个范围内波动?</h5><p>TPS: Transactions Per Second（每秒传输的事物处理个数）</p>
<p>TPS波动范围= TPS标准差/TPS平均值* 100%. 在5%内算是正常的</p>
<h4 id="ip"><a href="#ip" class="headerlink" title="ip"></a>ip</h4><h5 id="查看arp表"><a href="#查看arp表" class="headerlink" title="查看arp表"></a>查看arp表</h5><p>ARP表，它记录着主机的IP地址和MAC地址的对应关系</p>
<p>ARP协议: ARP协议是工作在网络层的协议，它负责将IP地址解析为MAC地址</p>
<p>ip neigh</p>
<h5 id="查看有哪些端⼝"><a href="#查看有哪些端⼝" class="headerlink" title="查看有哪些端⼝"></a>查看有哪些端⼝</h5><h5 id="查看ip地址"><a href="#查看ip地址" class="headerlink" title="查看ip地址"></a>查看ip地址</h5><p>Ip a</p>
<h5 id="查看路由表"><a href="#查看路由表" class="headerlink" title="查看路由表"></a>查看路由表</h5><p>ip route</p>
<h5 id="使⽤ip命令设置ip地址"><a href="#使⽤ip命令设置ip地址" class="headerlink" title="使⽤ip命令设置ip地址"></a>使⽤ip命令设置ip地址</h5><p>设置IP: ip addr add 192.168.0.123/24 dev eth0</p>
<p>删除配置的IP: ip add del 192.168.0.123/24 dev eth0</p>
<h4 id="ss-netstat"><a href="#ss-netstat" class="headerlink" title="ss/netstat"></a>ss/netstat</h4><h5 id="查看当前监听的端⼝，并显示监听端⼝的进程-PID"><a href="#查看当前监听的端⼝，并显示监听端⼝的进程-PID" class="headerlink" title="查看当前监听的端⼝，并显示监听端⼝的进程 PID"></a>查看当前监听的端⼝，并显示监听端⼝的进程 PID</h5><p>netstat -anlp | grep 80</p>
<h4 id="wget"><a href="#wget" class="headerlink" title="wget"></a>wget</h4><h5 id="如何用wget发⼀个HTTP-POST请求"><a href="#如何用wget发⼀个HTTP-POST请求" class="headerlink" title="如何用wget发⼀个HTTP POST请求"></a>如何用wget发⼀个HTTP POST请求</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget --post-data&#x3D;&quot;user&#x3D;user1&amp;pass&#x3D;pass1&amp;submit&#x3D;Login&quot;  http:&#x2F;&#x2F;domain.com&#x2F;path&#x2F;page_need_login.php</span><br></pre></td></tr></table></figure>

<h5 id="请简述wget续传-重复下载的逻辑及相关参数"><a href="#请简述wget续传-重复下载的逻辑及相关参数" class="headerlink" title="请简述wget续传/重复下载的逻辑及相关参数"></a>请简述wget续传/重复下载的逻辑及相关参数</h5><p>当文件特别大或者网络特别慢的时候，往往一个文件还没有下载完，连接就已经被切断，此时就需要断点续传。wget的断点续传是自动的，只需要使用-c参数</p>
<p>重复下载最简单的做法只需要 加上 -O 就可以</p>
<h5 id="O和-o参数有什么区别"><a href="#O和-o参数有什么区别" class="headerlink" title="-O和-o参数有什么区别?"></a>-O和-o参数有什么区别?</h5><p>使用wget -O下载并以不同的文件名保存(-O：下载文件到对应目录，并且修改文件名称)，-o将所有消息记录到日志文件</p>
<h4 id="screen"><a href="#screen" class="headerlink" title="screen"></a>screen</h4><h5 id="这个命令是⽤来做什么的"><a href="#这个命令是⽤来做什么的" class="headerlink" title="这个命令是⽤来做什么的?"></a>这个命令是⽤来做什么的?</h5><p>用户可以通过该软件同时连接多个本地或远程的命令行会话，并在其间自由切换</p>
<h5 id="如何继续上⼀次的会话"><a href="#如何继续上⼀次的会话" class="headerlink" title="如何继续上⼀次的会话?"></a>如何继续上⼀次的会话?</h5><p>查看所有会话</p>
<p>screen -ls</p>
<p>重新连接会话，12865 为会话id</p>
<p>screen -r 12865</p>
<h5 id="如何⼿工保存⼀个会话"><a href="#如何⼿工保存⼀个会话" class="headerlink" title="如何⼿工保存⼀个会话?"></a>如何⼿工保存⼀个会话?</h5><p><strong>在每个screen session 下，所有命令都以 ctrl+a(C-a) 开始</strong></p>
<p>C-a d -&gt; detach，暂时离开当前session，将目前的 screen session (可能含有多个 windows) 丢到后台执行，并会回到还没进 screen 时的状态</p>
<h4 id="touch"><a href="#touch" class="headerlink" title="touch"></a>touch</h4><h5 id="touch修改文件的atime和mtime"><a href="#touch修改文件的atime和mtime" class="headerlink" title="touch修改文件的atime和mtime"></a>touch修改文件的atime和mtime</h5><p>touch  -a filename: 更新文件的atime和ctime</p>
<p>touch -m filename: 更新文件的mtime和ctime</p>
<p>touch -r  参考文件名 目标文件名: 将目标文件的的atime和mtime更改为参考文件的时间并更新ctime</p>
<blockquote>
<p>Linux是如何更新访问时间的，这里须得明白下面一点，其更新策略为，当满足以下任意一条件时才更新访问时间</p>
<ul>
<li>访问时间早于修改时间或改变时间</li>
<li>距离上次更新时间间隔大于24h</li>
</ul>
</blockquote>
<h4 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h4><h5 id="Docker和虚拟机的区别"><a href="#Docker和虚拟机的区别" class="headerlink" title="Docker和虚拟机的区别"></a>Docker和虚拟机的区别</h5><p><strong>服务器</strong>好比运输码头: 拥有场地和各种设备（服务器硬件资源）</p>
<p><strong>服务器虚拟化</strong>好比作码头上的仓库: 拥有独立的空间堆放各种货物或集装箱(仓库之间完全独立，独立的应用系统和操作系统）</p>
<p><strong>Docker</strong>比作集装箱: 各种货物的打包</p>
<h5 id="熟悉Dockerfile编写"><a href="#熟悉Dockerfile编写" class="headerlink" title="熟悉Dockerfile编写"></a>熟悉Dockerfile编写</h5><p><a href="https://www.cnblogs.com/zpcoding/p/11450686.html" target="_blank" rel="noopener">docker—Dockerfile编写</a></p>
<p><a href="https://yeasy.gitbooks.io/docker_practice/image/build.html" target="_blank" rel="noopener">使用 Dockerfile 定制镜像</a></p>
<h5 id="熟悉docker常⽤用指令和参数-打包-查看-拉取-运行-设置环境变量-挂载⽬录"><a href="#熟悉docker常⽤用指令和参数-打包-查看-拉取-运行-设置环境变量-挂载⽬录" class="headerlink" title="熟悉docker常⽤用指令和参数: 打包/查看/拉取/运行/设置环境变量/挂载⽬录"></a>熟悉docker常⽤用指令和参数: 打包/查看/拉取/运行/设置环境变量/挂载⽬录</h5><ul>
<li><p>打包: docker image build -t koa-demo:0.0.1 .</p>
<blockquote>
<p>有Dockerfile 文件之后，可以使用<code>docker image build</code>命令创建 image 文件</p>
</blockquote>
</li>
<li><p>查看本机image文件: docker image ls</p>
</li>
<li><p>删除本机image文件: docker rmi [imageID]</p>
</li>
<li><p>拉取: docker image pull library/hello-world </p>
<blockquote>
<p><code>library/hello-world</code>是 image 文件在仓库里面的位置，其中<code>library</code>是 image 文件所在的组，<code>hello-world</code>是 image 文件的名字</p>
</blockquote>
</li>
<li><p>运行: docker container run hello-world</p>
<blockquote>
<p><code>docker container run</code>命令会从 image 文件，生成一个正在运行的容器实例</p>
<p>注意，<code>docker container run</code>命令具有自动抓取 image 文件的功能。如果发现本地没有指定的 image 文件，就会从仓库自动抓取。因此，前面的<code>docker image pull</code>命令并不是必需的步骤</p>
</blockquote>
</li>
<li><p>查看本机正在运行的容器: docker container ls</p>
</li>
<li><p>查看本机所有容器，包括终止运行的容器: docker container ls –all</p>
<blockquote>
<p><strong>image 文件生成的容器实例，本身也是一个文件，称为容器文件。</strong>也就是说，一旦容器生成，就会同时存在两个文件： image 文件和容器文件。而且关闭容器并不会删除容器文件，只是容器停止运行而已</p>
</blockquote>
</li>
<li><p>终止: docker container kill [containID]</p>
<blockquote>
<p>对于那些不会自动终止的容器，必须使用<a href="https://docs.docker.com/engine/reference/commandline/container_kill/" target="_blank" rel="noopener"><code>docker container kill</code></a> 命令手动终止</p>
</blockquote>
</li>
<li><p>删除终止运行的容器文件: docker container rm [containerID]</p>
<blockquote>
<p>终止运行的容器文件，依然会占据硬盘空间</p>
</blockquote>
</li>
<li><p>设置环境变量: docker run –env VARIABLE=VALUE image:tag</p>
<blockquote>
<p>可以使用简写 -e 替换 –env</p>
</blockquote>
</li>
<li><p>挂载目录: docker run -it -v /test:/soft centos</p>
<blockquote>
<p>启动一个centos容器，宿主机的/test目录挂载到容器的/soft目录</p>
</blockquote>
</li>
<li><p>查看镜像/容器详细信息: docker inspect NAME|ID</p>
</li>
</ul>
<h5 id="Docker⽹络类型，以及Docker对主机上的iptables的影响"><a href="#Docker⽹络类型，以及Docker对主机上的iptables的影响" class="headerlink" title="Docker⽹络类型，以及Docker对主机上的iptables的影响"></a>Docker⽹络类型，以及Docker对主机上的iptables的影响</h5><p>查看容器的详细信息（可以查看网络类型Networks）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker network ls</span><br><span class="line">NAME                DRIVER              SCOPE</span><br><span class="line">bridge              bridge              <span class="built_in">local</span></span><br><span class="line">host                host                <span class="built_in">local</span></span><br><span class="line">none                null                <span class="built_in">local</span></span><br></pre></td></tr></table></figure>

<ul>
<li><p>bridge（桥接式网络）(默认)</p>
<blockquote>
<p>启动容器时，首先会在主机上创建一个docker0的虚拟网桥，相当于交换机，同时自动分配一对网卡设备，一半在容器（eth0），一半在宿主机，并且还关联到了docker0，从而进行连接</p>
<p>Docker0网络是Docker搭建的一个虚拟桥接网络，默认网关地址是172.17.0.1。Docker默认的网络是Docker0网络，也就意味着Docker中所有没有指定网络的容器都会加入到这个桥接网络中，网络中的容器可以互相通信。</p>
</blockquote>
</li>
<li><p>Container(K8S会常用)</p>
<blockquote>
<p>与另一个运行得容器共用一个网络Network Namespace</p>
<p>使用方式: <code>--network=container:容器ID</code></p>
<p>注意共用时端口不能相同，端口谁先占用就是谁的</p>
</blockquote>
</li>
<li><p>host （主机）</p>
<blockquote>
<p>与宿主机共用一个网络</p>
<p>使用方式: <code>--network=host</code></p>
<p>使用后不需要做端口映射，性能最高，端口谁先占用就是谁的</p>
</blockquote>
</li>
<li><p>none （空）</p>
<blockquote>
<p>不为容器配置任何网络功能，不使用任何网络类型<br>使用方式: <code>--network=none</code> </p>
</blockquote>
</li>
</ul>
<p><strong>Docker对主机上的iptables的影响</strong></p>
<p>Docker引擎启动的时候会修改iptables规则</p>
<p>使用 <code>iptables-save</code> 命令查看 iptable，Docker 对 iptables 的 NAT 表和 FILTER 表都作了较大的改动</p>
<p><strong>容器对外请求数据</strong></p>
<p>如果Docker0中的容器请求外部的数据，那么他的数据包将会发送到网关172.17.0.1处。当数据包到达网关后，将会查询主机的路由表，确定数据包将从那个网卡发出。iptables负责对数据包进行snat转换，将原地址转为对应网卡的地址，因此容器对外是不可见的</p>
<p><strong>外部对容器请求数据</strong></p>
<p>外部想要访问容器内的数据，首先需要将容器的端口映射到宿主机上。这时候docker会在iptables添加转发规则，把接收到的数据转发给容器</p>
<blockquote>
<p>重启iptables会导致docker 的规则丢失，所以建议动态修改iptables</p>
</blockquote>
<h5 id="Docker的镜像是如何存储"><a href="#Docker的镜像是如何存储" class="headerlink" title="Docker的镜像是如何存储"></a>Docker的镜像是如何存储</h5><p><a href="https://www.iteblog.com/archives/9778.html" target="_blank" rel="noopener">Docker 入门教程：镜像分层</a></p>
<h3 id="后端开发"><a href="#后端开发" class="headerlink" title="后端开发"></a>后端开发</h3><h4 id="要求"><a href="#要求" class="headerlink" title="要求"></a>要求</h4><h5 id="PYTHONPATH环境变量"><a href="#PYTHONPATH环境变量" class="headerlink" title="PYTHONPATH环境变量"></a>PYTHONPATH环境变量</h5><p>PYTHONPATH是Python中一个重要的环境变量,用于在导入模块的时候搜索路径</p>
<p>Linux下设置PYTHONPATH环境变量有三种方法: 一种作用于当前终端，一种作用于当前用户，一种作用于所有用户</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 作用于当前终端，直接当前终端输入命令</span><br><span class="line"># export PYTHONPATH&#x3D;$PYTHONPATH:&lt;你的要加入的路径&gt;</span><br><span class="line"># 注1：&#39;&#x2F;home&#x2F;hadoop&#x2F;MyBI&#39;是项目MyBI的项目名</span><br><span class="line"># 注2：作用范围当前终端，一旦当前终端关闭或在另一个终端中，则无效。</span><br><span class="line"># 注3：这种方式立即生效。</span><br><span class="line">$ export PYTHONPATH&#x3D;$PYTHONPATH:&#x2F;home&#x2F;hadoop&#x2F;MyBI</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"># 作用于当前用户，修改当前用户目录下的&#39;~&#x2F;.bashrc&#39;文件</span><br><span class="line"># 加入内容：</span><br><span class="line"># export PYTHONPATH&#x3D;$PYTHONPATH:&#x2F;home&#x2F;hadoop&#x2F;MyBI</span><br><span class="line"># 也可以加入多个路径，用分号分隔</span><br><span class="line"># export PYTHONPATH&#x3D;$PYTHONPATH:&lt;你的要加入的路径1&gt;:&lt;你的要加入的路径2&gt;:等等</span><br><span class="line"># 注1：需要执行source ~&#x2F;.bashrc命令后生效（或者注销后重新登陆）</span><br><span class="line">$ vi ~&#x2F;.bashrc</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"># 作用于所有用户（需要root权限修改），修改&#39;&#x2F;etc&#x2F;profile&#39;文件</span><br><span class="line"># 加入内容：</span><br><span class="line"># export PYTHONPATH&#x3D;$PYTHONPATH:&#x2F;home&#x2F;hadoop&#x2F;MyBI</span><br><span class="line"># 注1：需要执行如下命令后生效（或者注销后重新登陆）</span><br><span class="line">$ vi &#x2F;etc&#x2F;profile</span><br><span class="line"> </span><br><span class="line"># 如果修改PATH环境变量，也是像上面的三种方式操作</span><br></pre></td></tr></table></figure>

<h5 id="查看python的包路径"><a href="#查看python的包路径" class="headerlink" title="查看python的包路径"></a>查看python的包路径</h5><p>利用pip 命令查看<code>pip show beautifulsoup4</code></p>
<p>利用包的<code>__file__</code>函数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ python</span><br><span class="line">Python 3.7.4 (default, Aug 13 2019, 15:17:50)</span><br><span class="line">[Clang 4.0.1 (tags&#x2F;RELEASE_401&#x2F;final)] :: Anaconda, Inc. on darwin</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line">&gt;&gt;&gt; import tornado</span><br><span class="line">&gt;&gt;&gt; print(tornado.__file__)</span><br><span class="line">&#x2F;Users&#x2F;tu&#x2F;.anaconda3&#x2F;lib&#x2F;python3.7&#x2F;site-packages&#x2F;tornado&#x2F;__init__.py</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Python3查看pip安装的软件包及版本</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ python3 -m pip list</span><br><span class="line">Package                            Version</span><br><span class="line">---------------------------------- ---------</span><br><span class="line">alabaster                          0.7.12</span><br><span class="line">anaconda-client                    1.7.2</span><br><span class="line">anaconda-navigator                 1.9.12</span><br><span class="line">anaconda-project                   0.8.3</span><br><span class="line">appnope                            0.1.0</span><br></pre></td></tr></table></figure>
</blockquote>
<h5 id="python的包结构"><a href="#python的包结构" class="headerlink" title="python的包结构"></a>python的包结构</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">picture&#x2F;                        Top-level package</span><br><span class="line">      __init__.py               Initialize the picture package</span><br><span class="line">      formats&#x2F;                  Subpackage for file format conversions</span><br><span class="line">              __init__.py</span><br><span class="line">              jpgread.py</span><br><span class="line">              jpgwrite.py</span><br><span class="line">              pngread.py</span><br><span class="line">              pngwrite.py</span><br><span class="line">              bmpread.py</span><br><span class="line">              bmpwrite.py</span><br><span class="line">              ...</span><br><span class="line">      filters&#x2F;                  Subpackage for filters</span><br><span class="line">              __init__.py</span><br><span class="line">              boxblur.py</span><br><span class="line">              gaussblur.py</span><br><span class="line">              sharpen.py</span><br><span class="line">              ...</span><br></pre></td></tr></table></figure>

<p>包（package）是 Python 中对模块的更高一级的抽象，Python 要求每一个「包」目录下，都必须有一个名为 <code>__init__.py</code> 的文件，一个Python 脚本就是一个 Python 模块（Module）。</p>
<h5 id="python的日志模块"><a href="#python的日志模块" class="headerlink" title="python的日志模块"></a>python的日志模块</h5><p>Python 标准库 logging 用作记录日志，默认分为六种日志级别（括号为级别对应的数值），NOTSET（0）、DEBUG（10）、INFO（20）、WARNING（30）、ERROR（40）、CRITICAL（50）</p>
<p><a href="https://juejin.im/post/5bc2bd3a5188255c94465d31" target="_blank" rel="noopener">Python日志库logging总结</a></p>
<h5 id="absolute-import这个模块的用途"><a href="#absolute-import这个模块的用途" class="headerlink" title="absolute_import这个模块的用途"></a>absolute_import这个模块的用途</h5><p>作用是绝对路径导入</p>
<p>绝对导入和相对导入之间的差异仅在从包导入模块和从包导入其他子模块时才起作用</p>
<p>例如: 关于这句<code>from __future__ import absolute_import</code>的作用<br>直观地看就是说”加入绝对引入这个新特性”。说到绝对引入，当然就会想到相对引入。那么什么是相对引入呢?比如说，你的包结构是这样的:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pkg&#x2F;</span><br><span class="line">pkg&#x2F;init.py</span><br><span class="line">pkg&#x2F;main.py</span><br><span class="line">pkg&#x2F;string.py</span><br></pre></td></tr></table></figure>

<p>如果你在main.py中写import string，那么在Python 2.4或之前，Python会先查找当前目录下有没有string.py，若找到了，则引入该模块，然后你在main.py中可以直接用string了。如果你是真的想用同目录下的string.py那就好，但是如果你是想用系统自带的标准string.py呢？那其实没有什么好的简洁的方式可以忽略掉同目录的string.py而引入系统自带的标准string.py。这时候你就需要<code>from __future__ import absolute_import</code>了。这样，你就可以用<code>import string</code>来引入系统的标准string.py，而用<code>from pkg import string</code>来引入当前目录下的string.py了</p>
<h5 id="熟悉import的⽅式-相对路径和绝对路径"><a href="#熟悉import的⽅式-相对路径和绝对路径" class="headerlink" title="熟悉import的⽅式: 相对路径和绝对路径"></a>熟悉import的⽅式: 相对路径和绝对路径</h5><p>相对路径导入方式只有from…import支持，import语句不支持，且只有使用.或..的才算是相对路径，否则就是绝对路径，就会从sys.path下搜索</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 同级目录 导入 reverse</span><br><span class="line">from . import reverse              </span><br><span class="line"></span><br><span class="line"># 上级目录 导入 frormats</span><br><span class="line">from .. import frormats            </span><br><span class="line"></span><br><span class="line"># 上级目录的filters模块下 导入 equalizer</span><br><span class="line">from ..filters import equalizer</span><br></pre></td></tr></table></figure>

<h5 id="常见HTTP的状态码"><a href="#常见HTTP的状态码" class="headerlink" title="常见HTTP的状态码"></a>常见HTTP的状态码</h5><p>1xx（临时响应）表示临时响应并需要请求者继续执行操作的状态代码。</p>
<ul>
<li>100 （继续）请求者应当继续提出请求。服务器返回此代码表示已收到请求的第一部分，正在等待其余部分</li>
<li>101 （切换协议）请求者已要求服务器切换协议，服务器已确认并准备切换。</li>
</ul>
<p>2xx （成功）表示成功处理了请求的状态代码。</p>
<ul>
<li>200 （成功）服务器已成功处理了请求。通常，这表示服务器提供了请求的网页。</li>
<li>201 （已创建）请求成功并且服务器创建了新的资源。</li>
<li>202 （已接受）服务器已接受请求，但尚未处理。</li>
<li>203 （非授权信息）服务器已成功处理了请求，但返回的信息可能来自另一来源。</li>
<li>204 （无内容）服务器成功处理了请求，但没有返回任何内容。</li>
<li>205 （重置内容）服务器成功处理了请求，但没有返回任何内容。</li>
<li>206 （部分内容）服务器成功处理了部分GET 请求。</li>
</ul>
<p>3xx （重定向）表示要完成请求，需要进一步操作。通常，这些状态代码用来重定向。</p>
<ul>
<li>300 （多种选择）针对请求，服务器可执行多种操作。服务器可根据请求者(user agent) 选择一项操作，或提供操作列表供请求者选择。</li>
<li>301 （永久移动）请求的网页已永久移动到新位置。服务器返回此响应（对GET 或HEAD请求的响应）时，会自动将请求者转到新位置。</li>
<li>302 （临时移动）服务器目前从不同位置的网页响应请求，但请求者应继续使用原有位置来进行以后的请求。</li>
<li>303 （查看其他位置）请求者应当对不同的位置使用单独的GET 请求来检索响应时，服务器返回此代码。</li>
<li>304 （未修改）自从上次请求后，请求的网页未修改过。服务器返回此响应时，不会返回网页内容，即取的缓存。</li>
<li>305 （使用代理）请求者只能使用代理访问请求的网页。如果服务器返回此响应，还表示请求者应使用代理。</li>
<li>307 （临时重定向）服务器目前从不同位置的网页响应请求，但请求者应继续使用原有位置来进行以后的请求。</li>
</ul>
<p>4xx（请求错误）这些状态代码表示请求可能出错，妨碍了服务器的处理。</p>
<ul>
<li>400 （错误请求）服务器不理解请求的语法。</li>
<li>401 （未授权）请求要求身份验证。对于需要登录的网页，服务器可能返回此响应。</li>
<li>403 （禁止）服务器拒绝请求。</li>
<li>404 （未找到）服务器找不到请求的网页。</li>
<li>405 （方法禁用）禁用请求中指定的方法。</li>
<li>406 （不接受）无法使用请求的内容特性响应请求的网页。</li>
<li>407 （需要代理授权）此状态代码与401（未授权）类似，但指定请求者应当授权使用代理。</li>
<li>408 （请求超时）服务器等候请求时发生超时。</li>
<li>409 （冲突）服务器在完成请求时发生冲突。服务器必须在响应中包含有关冲突的信息。</li>
<li>410（已删除）如果请求的资源已永久删除，服务器就会返回此响应。</li>
<li>411 （需要有效长度）服务器不接受不含有效内容长度标头字段的请求。</li>
<li>412 （未满足前提条件）服务器未满足请求者在请求中设置的其中一个前提条件。</li>
<li>413 （请求实体过大）服务器无法处理请求，因为请求实体过大，超出服务器的处理能力。</li>
<li>414 （请求的URI 过长）请求的URI（通常为网址）过长，服务器无法处理。</li>
<li>415 （不支持的媒体类型）请求的格式不受请求页面的支持。</li>
<li>416 （请求范围不符合要求）如果页面无法提供请求的范围，则服务器会返回此状态代码。</li>
<li>417 （未满足期望值）服务器未满足”期望”请求标头字段的要求。</li>
</ul>
<p>5xx（服务器错误）这些状态代码表示服务器在尝试处理请求时发生内部错误。这些错误可能是服务器本身的错误，而不是请求出错。</p>
<ul>
<li>500 （服务器内部错误）服务器遇到错误，无法完成请求。</li>
<li>501 （尚未实施）服务器不具备完成请求的功能。例如，服务器无法识别请求方法时可能会返回此代码。</li>
<li>502 （错误网关）服务器作为网关或代理，从上游服务器收到无效响应。</li>
<li>503 （服务不可用）服务器目前无法使用（由于超载或停机维护）。通常，这只是暂时状态。</li>
<li>504 （网关超时）服务器作为网关或代理，但是没有及时从上游服务器收到请求。</li>
<li>505 （HTTP 版本不受支持）服务器不支持请求中所用的HTTP 协议版本。</li>
</ul>
<h5 id="REST的设计"><a href="#REST的设计" class="headerlink" title="REST的设计"></a>REST的设计</h5><p><a href="http://www.ruanyifeng.com/blog/2014/05/restful_api.html" target="_blank" rel="noopener">RESTful API 设计指南</a></p>
<hr>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>安装 GitLab Runner</title>
    <url>/2020/05/12/gitlab-runner/</url>
    <content><![CDATA[<p>使用官方的 GitLab 存储库安装 GitLab Runner</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>需要为项目实现基于 Gitlab 的 CI/CD。gitlab-runner 是具体执行 CI/CD 的核心。因此需要搭建一个 GitLab Runner</p>
<p>文中大部分参考于官方的文档<a href="https://docs.gitlab.com/runner/install/linux-repository.html" target="_blank" rel="noopener">《Install GitLab Runner using the official GitLab repositories》</a></p>
<h4 id="环境介绍"><a href="#环境介绍" class="headerlink" title="环境介绍"></a>环境介绍</h4><ul>
<li>Linux 环境 Debian</li>
<li>Docker 作为执行器</li>
</ul>
<h4 id="安装-Docker"><a href="#安装-Docker" class="headerlink" title="安装 Docker"></a>安装 Docker</h4><p>这边 Runner 需要支持 Docker 系列的执行器，也就是构建需要在容器里执行，那么需要先安装 Docker 环境</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -sSL https:&#x2F;&#x2F;get.docker.com&#x2F; | sh</span><br><span class="line">&#x2F;etc&#x2F;init.d&#x2F;docker status</span><br><span class="line">[ ok ] Docker is running.</span><br></pre></td></tr></table></figure>

<h4 id="安装-Runner"><a href="#安装-Runner" class="headerlink" title="安装 Runner"></a>安装 Runner</h4><p>GitLab Runner 支持多平台、多方式安装，包括GNU/Linux, macOS, FreeBSD 和Windows平台的安装，以及支持基于docker的自动扩展式安装</p>
<p>下面使用官方的 GitLab 存储库在 Debian 平台安装GitLab Runner</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 添加GitLab的官方存储库</span><br><span class="line"># For Debian&#x2F;Ubuntu&#x2F;Mint</span><br><span class="line">$ curl -L https:&#x2F;&#x2F;packages.gitlab.com&#x2F;install&#x2F;repositories&#x2F;runner&#x2F;gitlab-runner&#x2F;script.deb.sh | sudo bash</span><br><span class="line"></span><br><span class="line"># 安装特定版本的 GitLab Runner</span><br><span class="line"># for DEB based systems</span><br><span class="line">$ apt-cache madison gitlab-runner</span><br><span class="line">$ apt-get install gitlab-runner&#x3D;11.11.4</span><br><span class="line"></span><br><span class="line"># 确认安装成功</span><br><span class="line">$ gitlab-runner --version</span><br><span class="line">Version:      11.11.4</span><br><span class="line">Git revision: e828d3bc</span><br><span class="line">Git branch:</span><br><span class="line">GO version:   go1.8.7</span><br><span class="line">Built:        2019-07-07T00:29:25+0000</span><br><span class="line">OS&#x2F;Arch:      linux&#x2F;amd64</span><br></pre></td></tr></table></figure>

<h4 id="注册-Runner"><a href="#注册-Runner" class="headerlink" title="注册 Runner"></a>注册 Runner</h4><p>注册 Runner 是将 Runner 与 GitLab 实例绑定的过程</p>
<p>注册 Specific Runnersr 到 GitLab 之前，需要在 Settings 中拿到 Gitlab 实例 URL 和注册 token</p>
<p>Settings-&gt;CI/CD-&gt;Runners-&gt;Specific Runners</p>
<p><img src="/images/blog/2020-05-12-4.png" alt></p>
<p>接着执行下方命令会进入一个交互式的配置流程</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 运行以下命令注册Runner</span><br><span class="line">$ gitlab-runner register</span><br><span class="line"></span><br><span class="line"># 输入GitLab实例URL</span><br><span class="line">Please enter the gitlab-ci coordinator URL (e.g. https:&#x2F;&#x2F;gitlab.com )</span><br><span class="line">https:&#x2F;&#x2F;git-sa.nie.netease.com&#x2F;</span><br><span class="line"></span><br><span class="line"># 输入token来注册</span><br><span class="line">Please enter the gitlab-ci token for this runner</span><br><span class="line">xxx</span><br><span class="line"></span><br><span class="line"># 输入Runner的描述，稍后可以在GitLab的UI中进行更改</span><br><span class="line">Please enter the gitlab-ci description for this runner</span><br><span class="line">alpha-runner</span><br><span class="line"></span><br><span class="line"># 输入与Runner关联的标签，稍后可以在GitLab的UI中进行更改（在.gitlab-ci.yml中使用tags来指定相关标签的runner运行）</span><br><span class="line">Please enter the gitlab-ci tags for this runner (comma separated):</span><br><span class="line">alpha</span><br><span class="line"></span><br><span class="line"># 输入Runner执行者（需要使用docker来部署服务，因此这里选择Docker作为执行程序）</span><br><span class="line">Please enter the executor: ssh, docker+machine, docker-ssh+machine, kubernetes, docker, parallels, virtualbox, docker-ssh, shell:</span><br><span class="line">docker</span><br><span class="line"></span><br><span class="line"># 如果选择Docker作为执行程序，则会要求定义默认映像，用于在.gitlab-ci.yml中未指定镜像使用默认镜像</span><br><span class="line">Please enter the Docker image (eg. ruby:2.6):</span><br><span class="line">dockerhub.nie.netease.com&#x2F;library&#x2F;debian_ci</span><br></pre></td></tr></table></figure>

<h4 id="配置-Docker"><a href="#配置-Docker" class="headerlink" title="配置 Docker"></a>配置 Docker</h4><p>注册一个 Docker 类型的 Runner 后，如果要在该 Runner 中使用 Docker 进行构建等任务，则需要增加以下配置，以支持 docker in docker</p>
<blockquote>
<p>安装好runner后，即会生成一份runner的配置文件，不同系统平台配置文件路径为：<br>  以root用户安装的，且为 *nix 类系统（如Debian）配置文件路径为/etc/gitlab-runner/config.toml<br>  以非root用户安装的，且为 *nix 类系统（如Debian）配置文件路径为~/.gitlab-runner/config.toml<br>  其他系统 ./config.toml</p>
</blockquote>
<p>挂载宿主docker.sock到容器内，挂载镜像registry的认证文件到容器内（如果要拉取私有镜像，则需要在容器内部署认证配置，或者在宿主配置，然后挂入容器内），修改 Docker 为 host 网络</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 修改对应runner的volumes配置</span><br><span class="line">volumes &#x3D; [&quot;&#x2F;cache&quot;, &quot;&#x2F;var&#x2F;run&#x2F;docker.sock:&#x2F;var&#x2F;run&#x2F;docker.sock&quot;, &quot;&#x2F;root&#x2F;.docker&#x2F;config.json:&#x2F;root&#x2F;.docker&#x2F;config.json&quot;]</span><br><span class="line"># 在volums下添加如下一行</span><br><span class="line">network_mode &#x3D; &quot;host&quot;</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/2020-05-12-3.png" alt></p>
]]></content>
      <categories>
        <category>Tools</category>
      </categories>
  </entry>
  <entry>
    <title>Socket 通信原理</title>
    <url>/2020/04/19/Socket/</url>
    <content><![CDATA[<p>网络套接字（Socket）</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>在同一台计算机，进程之间可以这样通信，如果是不同的计算机呢？网络上不同的计算机，也可以通信，有学传输层协议TCP、UDP，它们就是用来在不同计算机之间通行的协议，但它们毕竟只是协议，看不见摸不着，那怎们通过TCP、和UDP进行实际传输呢？</p>
<p>答案是使用网络套接字（Socket）</p>
<h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><p>Socket 中文意思为插座的意思，专业术语称之为套接字，它把 TCP/IP 封装成了调用接口供开发者调用，也就是说开发者可以通过调用Socket相关API来实现网络通讯。</p>
<p>Socket 就像一个电话插座，负责连通两端的电话，进行点对点通信，让电话可以进行通信，端口就像插座上的孔，端口不能同时被其他进程占用。而建立连接就像把插头插在这个插座上，创建一个 Socket 实例开始监听后，这个电话插座就时刻监听着消息的传入，谁拨通我这个“IP 地址和端口”，我就接通谁。</p>
<p>实际上，Socket 是在应用层和传输层之间的一个抽象层，它把 TCP/IP 层复杂的操作抽象为几个简单的接口，供应用层调用实现进程在网络中的通信。</p>
<p><img src="/images/blog/2020-04-19-1.png" alt></p>
<p>Socket 起源于 UNIX，在 UNIX 一切皆文件的思想下，进程间通信就被冠名为文件描述符（file descriptor），Socket 是一种“打开—读/写—关闭”模式的实现，服务器和客户端各自维护一个“文件”，在建立连接打开后，可以向文件写入内容供对方读取或者读取对方内容，通讯结束时关闭文件。</p>
<h4 id="Socket-接口"><a href="#Socket-接口" class="headerlink" title="Socket 接口"></a>Socket 接口</h4><p>socket(): 创建socket</p>
<p>bind(): 绑定socket到本地地址和端口，通常由服务端调用</p>
<p>listen(): TCP专用，开启监听模式</p>
<p>accept(): TCP专用，服务器等待客户端连接，一般是阻塞态</p>
<p>connect(): TCP专用，客户端主动连接服务器</p>
<p>send(): TCP专用，发送数据</p>
<p>recv(): TCP专用，接收数据</p>
<p>sendto(): UDP专用，发送数据到指定的IP地址和端口</p>
<p>recvfrom(): UDP专用，接收数据，返回数据远端的IP地址和端口</p>
<p>closesocket(): 关闭socket</p>
<p>更多详细参考<a href="https://www.cnblogs.com/yuqiao/p/5786427.html" target="_blank" rel="noopener">socket接口详解</a></p>
<h4 id="通信过程"><a href="#通信过程" class="headerlink" title="通信过程"></a>通信过程</h4><p>Socket 保证了不同计算机之间的通信，也就是网络通信。对于网站，通信模型是服务器与客户端之间的通信。两端都建立了一个 Socket 对象，然后通过 Socket 对象对数据进行传输。通常服务器处于一个无限循环，等待客户端的连接。</p>
<p>面向连接的 TCP 时序图:</p>
<p><img src="/images/blog/2020-04-19-2.png" alt></p>
<p><strong>从 TCP 连接的视角看 Socket 过程</strong></p>
<p>TCP 三次握手的 Socket 过程: </p>
<p><img src="/images/blog/2020-04-19-3.png" alt></p>
<ol>
<li>服务器调用 socket()、bind()、listen() 完成初始化后，调用 accept() 阻塞等待；</li>
<li>客户端 Socket 对象调用 connect() 向服务器发送了一个 SYN 并阻塞；</li>
<li>服务器完成了第一次握手，即发送 SYN 和 ACK 应答；</li>
<li>客户端收到服务端发送的应答之后，从 connect() 返回，再发送一个 ACK 给服务器；</li>
<li>服务器 Socket 对象接收客户端第三次握手 ACK 确认，此时服务端从 accept() 返回，建立连接。</li>
</ol>
<p>TCP 四次挥手的 Socket 过程:</p>
<p><img src="/images/blog/2020-04-19-4.png" alt></p>
<ol>
<li>某个应用进程调用 close() 主动关闭，发送一个 FIN；</li>
<li>另一端接收到 FIN 后被动执行关闭，并发送 ACK 确认；</li>
<li>之后被动执行关闭的应用进程调用 close() 关闭 Socket，并也发送一个 FIN；</li>
<li>接收到这个 FIN 的一端向另一端 ACK 确认。</li>
</ol>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><p>Java版可以看这个理解<a href="https://mp.weixin.qq.com/s/1eEBzkGp6WI5bp6697X3Gw" target="_blank" rel="noopener">搞了两周Socket通信，终于弄明白了！</a></p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.cnblogs.com/clschao/articles/9585555.html" target="_blank" rel="noopener">socket工作原理深入分析</a></li>
<li><a href="https://www.cnblogs.com/yuqiao/p/5786427.html" target="_blank" rel="noopener">socket接口详解</a></li>
<li><a href="https://mp.weixin.qq.com/s/1eEBzkGp6WI5bp6697X3Gw" target="_blank" rel="noopener">搞了两周Socket通信，终于弄明白了！</a></li>
<li><a href="https://segmentfault.com/a/1190000013712747" target="_blank" rel="noopener">Socket 通信原理</a></li>
</ul>
]]></content>
      <categories>
        <category>Network</category>
      </categories>
  </entry>
  <entry>
    <title>中断</title>
    <url>/2020/04/04/IRQ/</url>
    <content><![CDATA[<p>中断本质上是一种特殊的电信号</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>有幸阅读到李新杰的文章<a href="https://mp.weixin.qq.com/s/tETc7Ungihf8Vrv8npojnQ" target="_blank" rel="noopener">码农们都需要知道的「中断」相关知识</a>，算完整的过了一下中断的知识点</p>
<h4 id="个人理解"><a href="#个人理解" class="headerlink" title="个人理解"></a>个人理解</h4><p>CPU在同一时刻只能运行一个程序，所以底层都是通过时间片来轮换执行的。</p>
<p>计算机里都有定时机制，当一个时间片用完时，定时器会给CPU一个中断，告诉它当前正在执行的程序时间用完了，于是CPU就把它换出来，转而执行下一个程序。</p>
<p>这种时间片的轮换机制就是通过中断实现的。此外，计算机里的一些其它功能，也是通过中断实现的，如内存缺页错误。</p>
<h4 id="硬件中断和软中断的区别"><a href="#硬件中断和软中断的区别" class="headerlink" title="硬件中断和软中断的区别"></a>硬件中断和软中断的区别</h4><table>
<thead>
<tr>
<th>硬件中断</th>
<th>软中断</th>
</tr>
</thead>
<tbody><tr>
<td>由外设引发的</td>
<td>执行中断指令产生</td>
</tr>
<tr>
<td>中断号是由中断控制器提供的</td>
<td>由指令直接指出，无需使用中断控制器</td>
</tr>
<tr>
<td>可屏蔽的</td>
<td>不可屏蔽</td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>SQL 中的 NULL</title>
    <url>/2020/03/23/SQL-NULL/</url>
    <content><![CDATA[<p>NULL 就是 NULL，是「未知」  </p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>有幸阅读到 Inotime 的文章<a href="https://blog.csdn.net/lnotime/article/details/104847946" target="_blank" rel="noopener">SQL 中的 NULL 你真的懂了吗？【数据库｜SQL】</a></p>
<p><strong>NULL 就是 NULL，是「未知」</strong></p>
<p>关于 NULL 的理解比以前更清晰。其文章译自 <a href="https://mitchum.blog/null-values-in-sql-queries/" target="_blank" rel="noopener">NULL Values in SQL Queries</a></p>
]]></content>
      <categories>
        <category>SQL</category>
      </categories>
  </entry>
  <entry>
    <title>HBase 写入性能优化</title>
    <url>/2020/03/11/HBase-write-optimization/</url>
    <content><![CDATA[<p>HBase 写入性能优化</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>本文主要借阅于《HBase原理与实践》这本书，可以自行查阅 13.6 章</p>
<p>可以从HBase服务器端和业务客户端两个角度分析，确认是否还有提高的空间</p>
<h4 id="客户端优化"><a href="#客户端优化" class="headerlink" title="客户端优化"></a>客户端优化</h4><h5 id="是否可以使用-Bulkload-方案写入？"><a href="#是否可以使用-Bulkload-方案写入？" class="headerlink" title="是否可以使用 Bulkload 方案写入？"></a>是否可以使用 Bulkload 方案写入？</h5><p>Bulkload 是一个 MapReduce 程序（当然，也可以自行改成 Spark 程序）运行在Hadoop集群。程序的输入是指定数据源，输出是 HFile 文件。HFile 文件生成之后再通过 LoadIncrementalHFiles 工具将 HFile 中相关元数据加载到 HBase 中。</p>
<p>Bulkload 方案适合将已经存在于 HDFS 上的数据批量导入 HBase 集群。相比调用API的写入方案，Bulkload 方案可以更加高效、快速地导入数据，而且对 HBase 集群几乎不产生任何影响。</p>
<p>关于 Bulkload 可以阅读《HBase原理与实践》6.2 章</p>
<h5 id="是否需要写-WAL-WAL-是否需要同步写入？"><a href="#是否需要写-WAL-WAL-是否需要同步写入？" class="headerlink" title="是否需要写 WAL? WAL 是否需要同步写入？"></a>是否需要写 WAL? WAL 是否需要同步写入？</h5><p>优化原理: 数据写入流程可以理解为一次顺序写WAL+一次写缓存，通常情况下写缓存延迟很低，因此提升写性能只能从WAL入手。HBase中可以通过设置WAL的持久化等级决定是否开启WAL机制以及HLog的落盘方式。WAL的持久化分为四个等级：SKIP_WAL，ASYNC_WAL，SYNC_WAL以及FSYNC_WAL。如果用户没有指定持久化等级，HBase默认使用SYNC_WAL等级持久化数据。</p>
<p>在实际生产线环境中，部分业务可能并不特别关心异常情况下少量数据的丢失，而更关心数据写入吞吐量。比如某些推荐业务，这类业务即使丢失一部分用户行为数据可能对推荐结果也不会构成很大影响，但是对于写入吞吐量要求很高，不能造成队列阻塞。这种场景下可以考虑关闭WAL写入。退而求其次，有些业务必须写WAL，但可以接受WAL异步写入，这是可以考虑优化的，通常也会带来一定的性能提升。</p>
<p>优化推荐: 根据业务关注点在WAL机制与写入吞吐量之间做出选择，用户可以通过客户端设置WAL持久化等级。</p>
<h5 id="Put-是否可以同步批量提交？"><a href="#Put-是否可以同步批量提交？" class="headerlink" title="Put 是否可以同步批量提交？"></a>Put 是否可以同步批量提交？</h5><p>优化原理: HBase分别提供了单条put以及批量put的API接口，使用批量put接口可以减少客户端到RegionServer之间的RPC连接数，提高写入吞吐量。另外需要注意的是，批量put请求要么全部成功返回，要么抛出异常。</p>
<p>优化建议: 使用批量put写入请求。</p>
<h5 id="Put-是否可以异步批量提交？"><a href="#Put-是否可以异步批量提交？" class="headerlink" title="Put 是否可以异步批量提交？"></a>Put 是否可以异步批量提交？</h5><p>优化原理: 如果业务可以接受异常情况下少量数据丢失，可以使用异步批量提交的方式提交请求。提交分两阶段执行：用户提交写请求，数据写入客户端缓存，并返回用户写入成功；当客户端缓存达到阈值（默认2M）后批量提交给RegionServer。需要注意的是，在某些客户端异常的情况下，缓存数据有可能丢失。</p>
<p>优化建议: 在业务可以接受的情况下开启异步批量提交，用户可以设置setAutoFlush (false)</p>
<h6 id="写入-KeyValue-数据是否太大？"><a href="#写入-KeyValue-数据是否太大？" class="headerlink" title="写入 KeyValue 数据是否太大？"></a>写入 KeyValue 数据是否太大？</h6><p>KeyValue大小对写入性能的影响巨大。一旦遇到写入性能比较差的情况，需要分析写入性能下降是否因为写入KeyValue的数据太大。</p>
<p>KeyValue大小对写入性能影响曲线如下</p>
<p><img src="/images/blog/2020-03-11-1.png" alt></p>
<p>横坐标是写入的一行数据（每行数据10列）大小，左纵坐标是写入吞吐量，右纵坐标是写入平均延迟（ms）。可以看出，随着单行数据不断变大，写入吞吐量急剧下降，写入延迟在100K之后急剧增大。</p>
<h4 id="服务端优化"><a href="#服务端优化" class="headerlink" title="服务端优化"></a>服务端优化</h4><h5 id="Region-是否太少？"><a href="#Region-是否太少？" class="headerlink" title="Region 是否太少？"></a>Region 是否太少？</h5><p>优化原理: 当前集群中表的Region个数如果小于RegionServer个数，即Num (Region of Table)&lt; Num (RegionServer)，可以考虑切分Region并尽可能分布到不同的RegionServer上以提高系统请求并发度。</p>
<h5 id="写入请求是否均衡？"><a href="#写入请求是否均衡？" class="headerlink" title="写入请求是否均衡？"></a>写入请求是否均衡？</h5><p>优化原理: 写入请求如果不均衡，会导致系统并发度较低，还有可能造成部分节点负载很高，进而影响其他业务。分布式系统中特别需要注意单个节点负载很高的情况，单个节点负载很高可能会拖慢整个集群，这是因为很多业务会使用Mutli批量提交读写请求，一旦其中一部分请求落到慢节点无法得到及时响应，会导致整个批量请求超时。</p>
<p>优化建议: 检查Rowkey设计以及预分区策略，保证写入请求均衡。</p>
<h5 id="Utilize-Flash-storage-for-WAL"><a href="#Utilize-Flash-storage-for-WAL" class="headerlink" title="Utilize Flash storage for WAL"></a>Utilize Flash storage for WAL</h5><p>该特性会将WAL文件写到SSD上，对于写性能会有非常大的提升。需要注意的是，该特性建立在HDFS 2.6.0+以及HBase 1.1.0+版本基础上，以前的版本并不支持该特性。</p>
<p>使用该特性需要两个配置步骤:</p>
<p>1) 使用HDFS Archival Storage机制，在确保物理机有SSD硬盘的前提下配置HDFS的部分文件目录为SSD介质<br>2) 在hbase-site.xml中添加如下配置<br>    <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.wal.storage.policy&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;ONE_SSD&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure><br>   hbase.wal.storage.policy默认为none，用户可以指定ONE_SSD或者ALL_SSD</p>
<ul>
<li>ONE_SSD: WAL在HDFS上的一个副本文件写入SSD介质，另两个副本写入默认存储介质</li>
<li>ALL_SSD: WAL的三个副本文件全部写入SSD介质</li>
</ul>
<h4 id="写入问题"><a href="#写入问题" class="headerlink" title="写入问题"></a>写入问题</h4><ul>
<li>写阻塞<ul>
<li>MemStore 占用内存超过 RegionServer 级别高水位阈值导致阻塞 (hbase.regionserver.global.memstore.size)</li>
<li>RegionServer Active Handler 资源被耗尽  (可能跟KeyValue太大有关)</li>
<li>Store 中 HFile 文件数量达到阈值就会阻塞写入 (hbase.hstore.blockingStoreFiles)</li>
</ul>
</li>
<li>写延迟<ul>
<li>WAL 写入延迟，IO 资源是否争抢 (Utilize Flash storage for WAL)</li>
<li>JVM young gc (CCSMap)</li>
</ul>
</li>
</ul>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/RBXctAm9YGMPCQHR1NN8TQ" target="_blank" rel="noopener">Apache HBase 问题排查思路</a></li>
<li><a href="https://mp.weixin.qq.com/s/ippW4kRsdbO45JqQPfbMOw" target="_blank" rel="noopener">从原理到参数解析，HBase 刷写与合并机制介绍</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>ZooKeeper 监控指标</title>
    <url>/2020/03/07/Zookeeper-Monitor/</url>
    <content><![CDATA[<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>ZooKeeper 是一个开放源码的分布式应用程序协调服务，很多分布式应用程序可以基于它实现同步服务，如 HDFS、HBase、Kafka</p>
<h4 id="状态"><a href="#状态" class="headerlink" title="状态"></a>状态</h4><p>ZooKeeper 提供了四字命令(The Four Letter Words)，用来获取 ZooKeeper 服务的当前状态及相关信息</p>
<p><strong>有哪些命令可以使用？</strong></p>
<table>
<thead>
<tr>
<th>四字命令</th>
<th>功能描述</th>
</tr>
</thead>
<tbody><tr>
<td>conf</td>
<td>打印配置</td>
</tr>
<tr>
<td>cons</td>
<td>列出所有连接到这台服务器的客户端全部连接/会话详细信息。包括”接受/发送”的包数量、会话id、操作延迟、最后的操作执行等等信息</td>
</tr>
<tr>
<td>crst</td>
<td>重置所有连接的连接和会话统计信息</td>
</tr>
<tr>
<td>dump</td>
<td>列出那些比较重要的会话和临时节点。这个命令只能在leader节点上有用</td>
</tr>
<tr>
<td>envi</td>
<td>打印出服务环境的详细信息</td>
</tr>
<tr>
<td>reqs</td>
<td>列出未经处理的请求</td>
</tr>
<tr>
<td>ruok</td>
<td>即”Are you ok”，测试服务是否处于正确状态。如果确实如此，那么服务返回”imok”，否则不做任何相应</td>
</tr>
<tr>
<td>stat</td>
<td>输出关于性能和连接的客户端的列表</td>
</tr>
<tr>
<td>srst</td>
<td>重置服务器的统计</td>
</tr>
<tr>
<td>srvr</td>
<td>列出连接服务器的详细信息</td>
</tr>
<tr>
<td>wchs</td>
<td>列出服务器watch的详细信息</td>
</tr>
<tr>
<td>wchc</td>
<td>通过session列出服务器watch的详细信息，它的输出是一个与watch相关的会话的列表</td>
</tr>
<tr>
<td>wchp</td>
<td>通过路径列出服务器watch的详细信息。它输出一个与session相关的路径</td>
</tr>
<tr>
<td>mntr</td>
<td>输出可用于检测集群健康状态的变量列表</td>
</tr>
</tbody></table>
<p>通过下列命令来获取这些监控信息 <code>echo commands  |  nc ip port</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ echo mntr | nc 192.168.1.229 2181</span><br><span class="line">zk_version      3.4.5-cdh6.3.1--1, built on 09&#x2F;26&#x2F;2019 09:28 GMT    # 版本</span><br><span class="line">zk_avg_latency  0                                                   # 平均延时</span><br><span class="line">zk_max_latency  51                                                  # 最大延时</span><br><span class="line">zk_min_latency  0                                                   # 最小延时</span><br><span class="line">zk_packets_received     825166                                      # 收包数</span><br><span class="line">zk_packets_sent 844514                                              # 发包数</span><br><span class="line">zk_num_alive_connections        10                                  # 连接数</span><br><span class="line">zk_outstanding_requests 0                                           # 堆积请求数</span><br><span class="line">zk_server_state leader                                              # 状态</span><br><span class="line">zk_znode_count  399                                                 # znode 数量</span><br><span class="line">zk_watch_count  74                                                  # watch 数量</span><br><span class="line">zk_ephemerals_count     9                                           # 临时节点(znode)</span><br><span class="line">zk_approximate_data_size        28043                               # 数据大小</span><br><span class="line">zk_open_file_descriptor_count   59                                  # 打开的文件描述符数量</span><br><span class="line">zk_max_file_descriptor_count    32768                               # 最大文件描述符数量</span><br><span class="line">zk_fsync_threshold_exceed_count 0</span><br><span class="line">zk_followers    2                                                   # follower 数量，leader角色才会有这个输出</span><br><span class="line">zk_synced_followers     2                                           # 同步的 follower 数量</span><br><span class="line">zk_pending_syncs        0                                           # 准备同步数，leader角色才会有这个输出</span><br><span class="line">zk_last_proposal_size   32                                          # 最近一次 Proposal 消息大小</span><br><span class="line">zk_max_proposal_size    1337                                        # 最大 Proposal 消息大小</span><br><span class="line">zk_min_proposal_size    32                                          # 最小 Proposal 消息大小</span><br></pre></td></tr></table></figure>

<p>还有很多信息可以获取，更多可以自行搜索。</p>
<h4 id="监控"><a href="#监控" class="headerlink" title="监控"></a>监控</h4><p>监控平台的话目前业界已经比较成熟的阿里开源 ZooKeeper 监控框架 <a href="https://github.com/alibaba/taokeeper" target="_blank" rel="noopener">TaoKeeper</a></p>
<p>TaoKeeper 同作原理通过SSH连接到ZooKeeper部署的机器上，再在上面执行 ZooKeeper 的四字命令来得到统计信息，再分析保存到 MySQL 数据库中。</p>
<p>当然也可以使用四字命令+nc写脚本去做告警也行，针对以下几个维度去</p>
<p>open_connections  单机打开连接连接数，建议超过平常的两倍就报警，这样就能知道比平常连接数激增</p>
<p>latency  响应一个客户端请求的时间，建议这个时间大于10个TickTime就报警</p>
<blockquote>
<p>tickTime 这个时间是作为 Zookeeper 服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个 tickTime 时间就会发送一个心跳。默认是 2000 毫秒</p>
</blockquote>
<p>outstanding_requests   排队请求的数量，当ZooKeeper超过了它的处理能力时，这个值会增大，建议设置报警阀值为10</p>
<blockquote>
<p>zookeeper globalOutstandingLimit 配置指定了等待处理的最大请求数量的限制</p>
</blockquote>
<p>open_file_descriptor_count   打开文件数量，当这个值大于允许值”zk_max_file_descriptor_count” 85% 时报警</p>
<p>Java Heap Size  JVM 堆栈内存使用情况大于分配的最大内存 80% 时报警</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.cnblogs.com/xinfang520/p/10721770.html" target="_blank" rel="noopener">Hadoop记录- zookeeper 监控指标</a></li>
<li><a href="https://www.cnblogs.com/allenhaozi/p/11416817.html" target="_blank" rel="noopener">zookeeper globalOutstandingLimit</a></li>
<li><a href="https://www.cnblogs.com/linuxbug/p/4840506.html" target="_blank" rel="noopener">zookeeper监控告警</a></li>
<li><a href="https://blog.csdn.net/hackerwin7/article/details/43985049" target="_blank" rel="noopener">Zookeeper 监控原型开发</a></li>
<li><a href="https://www.jianshu.com/p/4f11d7bfc9ce" target="_blank" rel="noopener">ZooKeeper监控工具(六)</a></li>
<li><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-zookeeper/index.html" target="_blank" rel="noopener">分布式服务框架 Zookeeper — 管理分布式环境中的数据</a></li>
</ul>
]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
  </entry>
  <entry>
    <title>NameNode 在高可用中防止脑裂</title>
    <url>/2020/03/05/NameNode-HA-fencing/</url>
    <content><![CDATA[<p>CDH 从图表生成器添加图表</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>在分布式系统中脑裂又称为双主现象，由于网络问题可能会导致出现两个 NameNode 同时为 Active 状态，此时两个 NameNode 都可以对外提供服务，无法保证数据一致性。ActiveStandbyElector 通过 Fencing 机制防止脑裂现象。</p>
<h4 id="机制"><a href="#机制" class="headerlink" title="机制"></a>机制</h4><p>当某个 NameNode 竞选成功，成功创建 ActiveStandbyElectorLock 临时节点后会创建另一个名为 ActiveBreadCrumb 的持久节点，该节点保存了 NameNode 的地址信息，正常情况下删除 ActiveStandbyElectorLock 节点时会主动删除 ActiveBreadCrumb，但如果由于异常情况导致 Zookeeper Session关闭，此时临时节点 ActiveStandbyElectorLock 会被删除，但持久节点 ActiveBreadCrumb 并不会删除，当有新的 NameNode 竞选成功后它会发现已经存在一个旧的 NameNode 遗留下来的 ActiveBreadCrumb 节点，此时会通知 ZKFC 对旧的 ANN 进行 fencing，</p>
<p>在进行 fencing 的时候，会执行以下的操作:</p>
<ul>
<li>首先尝试调用这个旧 Active NameNode 的 HAServiceProtocol RPC 接口的 transitionToStandby 方法，看能不能把它转换为 Standby 状态。</li>
<li>如果 transitionToStandby 方法调用失败，那么就执行 Hadoop 配置文件之中预定义的隔离措施，Hadoop 目前主要提供两种隔离措施，通常会选择 sshfence：<ul>
<li>sshfence: 通过 SSH 登录到目标机器上，执行命令 fuser 将对应的进程杀死；</li>
<li>shellfence: 执行一个用户自定义的 shell 脚本来将对应的进程隔离；</li>
</ul>
</li>
</ul>
<p>具体如何使用可以参阅<a href="https://blog.csdn.net/tom_fans/article/details/89677681" target="_blank" rel="noopener">Hadoop NameNode HA fencing</a></p>
<p>只有在成功地执行完成 fencing 之后，选主成功的 ActiveStandbyElector 才会回调 ZKFailoverController 的 becomeActive 方法将对应的 NameNode 转换为 Active 状态，开始对外提供服务</p>
<p><img src="/images/blog/2020-03-05-1.png" alt></p>
<p><img src="/images/blog/2020-03-05-2.png" alt></p>
<h4 id="JournalNode-方面"><a href="#JournalNode-方面" class="headerlink" title="JournalNode 方面"></a>JournalNode 方面</h4><p>同一时刻只有一个 NameNode 可以写入 QJM, 这样就不担心脑裂问题。</p>
<p>Epoch 是一个单调递增的整数，用来标识每一次 Active NameNode 的生命周期，每发生一次 NameNode 的主备切换，Epoch 就会加 1。这实际上是一种 fencing 机制。</p>
<p>Epoch 的过程完全类似:</p>
<ol>
<li>Active NameNode 首先向 JournalNode 集群发送 getJournalState RPC 请求，每个 JournalNode 会返回自己保存的最近的那个 Epoch(代码中叫 lastPromisedEpoch)。</li>
<li>NameNode 收到大多数的 JournalNode 返回的 Epoch 之后，在其中选择最大的一个加 1 作为当前的新 Epoch，然后向各个 JournalNode 发送 newEpoch RPC 请求，把这个新的 Epoch 发给各个 JournalNode。</li>
<li>每一个 JournalNode 在收到新的 Epoch 之后，首先检查这个新的 Epoch 是否比它本地保存的 lastPromisedEpoch 大，如果大的话就把 lastPromisedEpoch 更新为这个新的 Epoch，并且向 NameNode 返回它自己的本地磁盘上最新的一个 EditLogSegment 的起始事务 id，为后面的数据恢复过程做好准备。如果小于或等于的话就向 NameNode 返回错误。</li>
<li>NameNode 收到大多数 JournalNode 对 newEpoch 的成功响应之后，就会认为生成新的 Epoch 成功</li>
</ol>
<p>在生成新的 Epoch 之后，每次 NameNode 在向 JournalNode 集群提交 EditLog 的时候，都会把这个 Epoch 作为参数传递过去。每个 JournalNode 会比较传过来的 Epoch 和它自己保存的 lastPromisedEpoch 的大小，如果传过来的 epoch 的值比它自己保存的 lastPromisedEpoch 小的话，那么这次写相关操作会被拒绝。一旦大多数 JournalNode 都拒绝了这次写操作，那么这次写操作就失败了。如果原来的 Active NameNode 恢复正常之后再向 JournalNode 写 EditLog，那么因为它的 Epoch 肯定比新生成的 Epoch 小，并且大多数的 JournalNode 都接受了这个新生成的 Epoch，所以拒绝写入的 JournalNode 数目至少是大多数，这样原来的 Active NameNode 写 EditLog 就肯定会失败，失败之后这个 NameNode 进程会直接退出，这样就实现了对原来的 Active NameNode 的隔离了。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.jianshu.com/p/139fb06bea48" target="_blank" rel="noopener">NameNode在高可用中的防止脑裂</a></li>
<li><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-name-node/index.html" target="_blank" rel="noopener">Hadoop NameNode 高可用 (High Availability) 实现解析</a></li>
<li><a href="https://blog.csdn.net/tom_fans/article/details/89677681" target="_blank" rel="noopener">Hadoop NameNode HA fencing</a></li>
</ul>
]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title>ZooKeeper 客户端连接数</title>
    <url>/2020/03/05/Zookeeper-maxClientCnxns/</url>
    <content><![CDATA[<p>配置参数将限制连接到 ZooKeeper 的客户端的数量，限制并发连接的数量</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>Zookeeper 容易出现连接数暴增事件，因此可以通过相应的配置来限制某 IP 的连接数</p>
<h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><p><strong>maxClientCnxns</strong></p>
<p>Limits the number of concurrent connections (at the socket level) that a single client, identified by IP address, may make to a single member of the ZooKeeper ensemble. This is used to prevent certain classes of DoS attacks, including file descriptor exhaustion. The default is 60. Setting this to 0 entirely removes the limit on concurrent connections.</p>
<blockquote>
<p>翻译过来就是将单个客户端（通过IP地址标识）可以与ZooKeeper集成中的单个成员建立的并发连接数（在套接字级别）受到限制。 这用于防止某些类的DoS攻击，包括文件描述符耗尽。 默认值为60。将其设置为0将完全消除并发连接的限制。</p>
</blockquote>
<p>需要明确地是这个限制的使用范围，仅仅是单台客户端机器与单台ZK服务器之间的连接数限制，不是针对指定客户端ID，也不是ZK集群的连接数限制，也不是单台ZK对所有客户端的连接数限制。</p>
<p>如果某 IP 连接数过多会报如下错误</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2011-10-28 09:39:44,856 – WARN  [NIOServerCxn.Factory:0.0.0.0&#x2F;0.0.0.0:5858:NIOServerCnxn$Factory@253] – Too many connections from &#x2F;172.*.*.* – max is 60</span><br></pre></td></tr></table></figure>

<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.51cto.com/zlfwmm/1712403" target="_blank" rel="noopener">ZooKeeper客户端连接数过多</a></li>
<li><a href="https://blog.csdn.net/weixin_34238633/article/details/92595022" target="_blank" rel="noopener">记一次zookeeper连接数暴增事件</a></li>
<li><a href="https://www.jianshu.com/p/63f147de795b" target="_blank" rel="noopener">集群 zk 连接数过多问题排查</a></li>
</ul>
]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
  </entry>
  <entry>
    <title>HBase 读取性能优化</title>
    <url>/2020/03/05/HBase-read-optimization/</url>
    <content><![CDATA[<p>HBase 读取性能优化</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>本文主要借阅于《HBase原理与实践》这本书，可以自行查阅 13.4 和 13.5 章</p>
<p>读请求延迟较大通常存在三种场景，分别为：</p>
<ol>
<li><p>仅有某业务延迟较大，集群其他业务都正常</p>
</li>
<li><p>整个集群所有业务都反映延迟较大</p>
</li>
<li><p>某个业务起来之后集群其他部分业务延迟较大</p>
</li>
</ol>
<p>这三种场景是表象，通常某业务反应延迟异常，首先需要明确具体是哪种场景，然后针对性解决问题。</p>
<p>主要分为四个方面: 客户端优化、服务器端优化、列族设计优化以及 HDFS 相关优化</p>
<h4 id="客户端优化"><a href="#客户端优化" class="headerlink" title="客户端优化"></a>客户端优化</h4><p>客户端作为业务读写的入口，姿势使用不正确通常会导致本业务读延迟较高实际上存在一些使用姿势的推荐用法</p>
<h5 id="scan-缓存是否设置合理？"><a href="#scan-缓存是否设置合理？" class="headerlink" title="scan 缓存是否设置合理？"></a>scan 缓存是否设置合理？</h5><p>优化原理: HBase业务通常一次 scan 就会返回大量数据，因此客户端发起一次 scan 请求，实际并不会一次就将所有数据加载到本地，而是分成多次 RPC 请求进行加载，这样设计一方面因为大量数据请求可能会导致网络带宽严重消耗进而影响其他业务，另一方面因为数据量太大可能导致本地客户端发生OOM。在这样的设计体系下，用户会首先加载一部分数据到本地，然后遍历处理，再加载下一部分数据到本地处理，如此往复，直至所有数据都加载完成。数据加载到本地就存放在scan缓存中，默认为100条数据。</p>
<p>通常情况下，默认的scan缓存设置是可以正常工作的。但是对于一些大scan（一次scan可能需要查询几万甚至几十万行数据），每次请求100条数据意味着一次scan需要几百甚至几千次RPC请求，这种交互的代价无疑是很大的。因此可以考虑将scan缓存设置增大，比如设为500或者1000条可能更加合适。《HBase原理与实践》作者之前做过一次试验，在一次scan 10w+条数据量的条件下，将scan缓存从100增加到1000条，可以有效降低scan请求的总体延迟，延迟降低了25%左右。</p>
<p>优化建议: 大scan场景下将scan缓存从100增大到500或者1000，用以减少RPC次数。</p>
<h5 id="get-是否使用批量请求？"><a href="#get-是否使用批量请求？" class="headerlink" title="get 是否使用批量请求？"></a>get 是否使用批量请求？</h5><p>优化原理: HBase分别提供了单条get以及批量get的API接口，使用批量get接口可以减少客户端到RegionServer之间的RPC连接数，提高读取吞吐量。另外需要注意的是，批量get请求要么成功返回所有请求数据，要么抛出异常。</p>
<p>优化建议: 使用批量get进行读取请求。需要注意的是，对读取延迟非常敏感的业务，批量请求时每次批量数不能太大，最好进行测试。</p>
<h5 id="请求是否可以显式指定列簇或者列？"><a href="#请求是否可以显式指定列簇或者列？" class="headerlink" title="请求是否可以显式指定列簇或者列？"></a>请求是否可以显式指定列簇或者列？</h5><p>优化原理: HBase是典型的列簇数据库，意味着同一列簇的数据存储在一起，不同列簇的数据分开存储在不同的目录下。一个表有多个列簇，如果只是根据rowkey而不指定列簇进行检索，不同列簇的数据需要独立进行检索，性能必然会比指定列簇的查询差很多，很多情况下甚至会有2～3倍的性能损失。</p>
<p>优化建议：尽量指定列簇或者列进行精确查找。</p>
<h5 id="离线批量读取请求是否设置禁止缓存？"><a href="#离线批量读取请求是否设置禁止缓存？" class="headerlink" title="离线批量读取请求是否设置禁止缓存？"></a>离线批量读取请求是否设置禁止缓存？</h5><p>优化原理: 通常在离线批量读取数据时会进行一次性全表扫描，一方面数据量很大，另一方面请求只会执行一次。这种场景下如果使用scan默认设置，就会将数据从HDFS加载出来放到缓存。可想而知，大量数据进入缓存必将其他实时业务热点数据挤出，其他业务不得不从HDFS加载，进而造成明显的读延迟毛刺。</p>
<p>优化建议: 离线批量读取请求设置禁用缓存，scan.setCacheBlocks (false)。</p>
<h4 id="服务端优化"><a href="#服务端优化" class="headerlink" title="服务端优化"></a>服务端优化</h4><p>一般服务端端问题一旦导致业务读请求延迟较大的话，通常是集群级别的，即整个集群的业务都会反映读延迟较大。</p>
<h5 id="读请求是否均衡？"><a href="#读请求是否均衡？" class="headerlink" title="读请求是否均衡？"></a>读请求是否均衡？</h5><p>优化原理: 假如业务所有读请求都落在集群某一台RegionServer上的某几个Region上，很显然，这一方面不能发挥整个集群的并发处理能力，另一方面势必造成此台 RegionServer 资源严重消耗（比如IO耗尽、handler耗尽等），导致落在该台 RegionServer 上的其他业务受到波及。也就是说读请求不均衡不仅会造成本身业务性能很差，还会严重影响其他业务。</p>
<p>观察确认: 观察所有RegionServer的读请求QPS曲线，确认是否存在读请求不均衡现象。</p>
<p>优化建议: Rowkey必须进行散列化处理（比如MD5散列），同时建表必须进行预分区处理。</p>
<h5 id="BlockCache设置是否合理？"><a href="#BlockCache设置是否合理？" class="headerlink" title="BlockCache设置是否合理？"></a>BlockCache设置是否合理？</h5><p>优化原理: BlockCache作为读缓存，对于读性能至关重要。默认情况下BlockCache和MemStore的配置相对比较均衡（各占40%），可以根据集群业务进行修正，比如读多写少业务可以将BlockCache占比调大。另一方面，BlockCache的策略选择也很重要，不同策略对读性能来说影响并不是很大，但是对GC的影响却相当显著，尤其在BucketCache的offheap模式下GC表现非常优秀。</p>
<p>观察确认: 观察所有 RegionServer 的缓存未命中率、配置文件相关配置项以及GC日志，确认 BlockCache 是否可以优化。</p>
<p>优化建议: 如果JVM内存配置量小于20G，BlockCache策略选择LRUBlockCache；否则选择BucketCache策略的 offheap 模式。</p>
<h5 id="HFile文件是否太多？"><a href="#HFile文件是否太多？" class="headerlink" title="HFile文件是否太多？"></a>HFile文件是否太多？</h5><p>优化原理: HBase在读取数据时通常先到MemStore和BlockCache中检索（读取最近写入数据和热点数据），如果查找不到则到文件中检索。HBase的类LSM树结构导致每个store包含多个HFile文件，文件越多，检索所需的IO次数越多，读取延迟也就越高。文件数量通常取决于Compaction的执行策略，一般和两个配置参数有关：hbase.hstore. compactionThreshold和hbase.hstore.compaction.max.size，前者表示一个store中的文件数超过阈值就应该进行合并，后者表示参与合并的文件大小最大是多少，超过此大小的文件不能参与合并。这两个参数需要谨慎设置，如果前者设置太大，后者设置太小，就会导致Compaction合并文件的实际效果不明显，很多文件得不到合并，进而导致HFile文件数变多。</p>
<p>观察确认: 观察RegionServer级别以及Region级别的HFile数，确认HFile文件是否过多。</p>
<p>优化建议: hbase.hstore.compactionThreshold设置不能太大，默认为3个。</p>
<h5 id="Compaction是否消耗系统资源过多？"><a href="#Compaction是否消耗系统资源过多？" class="headerlink" title="Compaction是否消耗系统资源过多？"></a>Compaction是否消耗系统资源过多？</h5><p>优化原理: Compaction是将小文件合并为大文件，提高后续业务随机读性能，但是也会带来IO放大以及带宽消耗问题（数据远程读取以及三副本写入都会消耗系统带宽）。正常配置情况下，Minor Compaction并不会带来很大的系统资源消耗，除非因为配置不合理导致MinorCompaction太过频繁，或者Region设置太大发生Major Compaction。</p>
<p>观察确认: 观察系统IO资源以及带宽资源使用情况，再观察Compaction队列长度，确认是否由于Compaction导致系统资源消耗过多。</p>
<p>优化建议: 对于大Region读延迟敏感的业务（100G以上）通常不建议开启自动MajorCompaction，手动低峰期触发。小Region或者延迟不敏感的业务可以开启MajorCompaction，但建议限制流量。</p>
<h4 id="列簇设计优化"><a href="#列簇设计优化" class="headerlink" title="列簇设计优化"></a>列簇设计优化</h4><h5 id="布隆过滤器是否设置？"><a href="#布隆过滤器是否设置？" class="headerlink" title="布隆过滤器是否设置？"></a>布隆过滤器是否设置？</h5><p>优化原理: 布隆过滤器主要用来过滤不存在待检索rowkey的HFile文件，避免无用的IO操作。</p>
<p>布隆过滤器取值有两个——row以及rowcol，需要根据业务来确定具体使用哪种。如果业务中大多数随机查询仅仅使用row作为查询条件，布隆过滤器一定要设置为row；如果大多数随机查询使用row+column作为查询条件，布隆过滤器需要设置为rowcol。如果不确定业务查询类型，则设置为row。</p>
<p>优化建议: 任何业务都应该设置布隆过滤器，通常设置为row，除非确认业务随机查询类型为row+column，则设置为rowcol。<strong>默认为 row</strong></p>
<h5 id="TTL-是否设置合理？"><a href="#TTL-是否设置合理？" class="headerlink" title="TTL 是否设置合理？"></a>TTL 是否设置合理？</h5><p>优化原理: TTL(Time to Live) 用于限定数据的超时时间，HBase cell 超过时间后会被自动删除，对某些数据不是永久保存，并大量写入的场景下非常适用，减少数据规模</p>
<p>优化建议: CF 默认的 TTL 值是 FOREVER，也就是永不过期，可以根据具体的业务场景设置超时时间</p>
<h4 id="HDFS-相关优化"><a href="#HDFS-相关优化" class="headerlink" title="HDFS 相关优化"></a>HDFS 相关优化</h4><h5 id="数据本地率是不是很低？"><a href="#数据本地率是不是很低？" class="headerlink" title="数据本地率是不是很低？"></a>数据本地率是不是很低？</h5><p>优化原理: 如果数据本地率很低，数据读取时会产生大量网络IO请求，导致读延迟较高。</p>
<p>观察确认: 观察所有RegionServer的数据本地率（见jmx中指标PercentFileLocal，在TableWeb UI可以看到各个Region的Locality）。</p>
<p>优化建议: 尽量避免Region无故迁移。对于本地率较低的节点，可以在业务低峰期执行major_compact。</p>
<blockquote>
<p>执行major_compact提升数据本地率的理论依据是，major_compact本质上是将Region中的所有文件读取出来然后写到一个大文件，写大文件必然会在本地DataNode生成一个副本，这样Region的数据本地率就会提升到100%。</p>
</blockquote>
<h5 id="Short-Circuit-Local-Read功能是否开启？"><a href="#Short-Circuit-Local-Read功能是否开启？" class="headerlink" title="Short-Circuit Local Read功能是否开启？"></a>Short-Circuit Local Read功能是否开启？</h5><p>优化原理: 当前HDFS读取数据都需要经过 DataNode，客户端会向DataNode发送读取数据的请求，DataNode接受到请求之后从硬盘中将文件读出来，再通过TCP发送给客户端。Short Circuit策略允许客户端绕过DataNode直接读取本地数据。</p>
<p>优化建议: 开启Short Circuit Local Read 功能，需要在<code>hbase-site.xml</code>或者<code>hdfs-site.xml</code>配置文件中增加如下配置项</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.client.read.shortcircuit&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.domain.socket.path&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;&#x2F;var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;dn_socket&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.client.read.shortcircuit.buffer.size&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;131072&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>需要注意的是，dfs.client.read.shortcircuit.buffer.size参数默认是1M，对于HBase系统来说有可能会造成OOM，详见HBASE-8143 HBase on Hadoop 2 with local short circuit reads(ssr) causes OOM</p>
</blockquote>
<h5 id="Hedged-Read功能是否开启？"><a href="#Hedged-Read功能是否开启？" class="headerlink" title="Hedged Read功能是否开启？"></a>Hedged Read功能是否开启？</h5><p>优化原理: HBase数据在HDFS中默认存储三个副本，通常情况下HBase会根据一定算法优先选择一个DataNode进行数据读取。然而在某些情况下，有可能因为磁盘问题或者网络问题等引起读取超时，根据Hedged Read策略，如果在指定时间内读取请求没有返回，HDFS客户端将会向第二个副本发送第二次数据请求，并且谁先返回就使用谁，之后返回的将会被丢弃。</p>
<p>优化建议: 开启Hedged Read功能，需要在<code>hbase-site.xml</code>配置文件中增加如下配置项</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.client.hedged.read.threadpool.size&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;20&lt;&#x2F;value&gt;&lt;!-- 20 threads --&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.client.hedged.read.threshold.millis&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;10&lt;&#x2F;value&gt;&lt;!-- 10 milliseconds --&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>参数dfs.client.hedged.read.threadpool.size表示用于hedged read的线程池线程数量，默认为0，表示关闭hedged read功能；参数dfs.client.hedged.read.threshold.millis表示HDFS数据读取超时时间，超过这个阈值，HDFS客户端将会再发起一次读取请求。</p>
</blockquote>
<h4 id="读性能优化归纳"><a href="#读性能优化归纳" class="headerlink" title="读性能优化归纳"></a>读性能优化归纳</h4><p>提到读延迟较大无非三种常见的表象，单个业务慢、集群随机读慢以及某个业务随机读之后其他业务受到影响导致随机读延迟很大。</p>
<p>了解完常见的可能导致读延迟较大的一些问题之后，我们将这些问题进行如下归类，读者可以在看到现象之后在对应的问题列表中进行具体定位</p>
<p><img src="/images/blog/2020-03-05-3.png" alt></p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.cnblogs.com/yingjie2222/p/6084255.html" target="_blank" rel="noopener">HBase读延迟的12种优化套路</a></li>
<li><a href="https://www.cnblogs.com/yanzibuaa/p/7568528.html" target="_blank" rel="noopener">HBase的TTL介绍</a></li>
<li><a href="https://blog.csdn.net/jewes/article/details/40189263" target="_blank" rel="noopener">详解HDFS Short Circuit Local Reads</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>CDH 从图表生成器添加图表</title>
    <url>/2020/03/03/CDH-chart/</url>
    <content><![CDATA[<p>CDH 从图表生成器添加图表</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>CDH 是一款开源的集部署、监控、操作等于一体的 Hadoop 生态组件管理工具，也提供收费版（比免费版多提供数据备份恢复、故障定位等特性）。</p>
<p>CDH 提供的监控界面在体验上是非常优秀的，但是有时候有些监控项需要点到具体的组件实例了查看，挺麻烦的，因此我想直接加在组件面板里</p>
<h4 id="图表"><a href="#图表" class="headerlink" title="图表"></a>图表</h4><h5 id="HBase-各个RS读写请求"><a href="#HBase-各个RS读写请求" class="headerlink" title="HBase 各个RS读写请求"></a>HBase 各个RS读写请求</h5><p>HBase 各个RS读写请求</p>
<p><img src="/images/blog/2020-03-03-1.png" alt></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select read_requests_rate, write_requests_rate</span><br></pre></td></tr></table></figure>


<h5 id="Zookeeper-打开的连接数"><a href="#Zookeeper-打开的连接数" class="headerlink" title="Zookeeper 打开的连接数"></a>Zookeeper 打开的连接数</h5><p>Zookeeper 打开的连接数</p>
<p><img src="/images/blog/2020-03-03-4.png" alt></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT open_connections</span><br></pre></td></tr></table></figure>







]]></content>
      <categories>
        <category>CDH</category>
      </categories>
  </entry>
  <entry>
    <title>Unix 中的 I/O 模型</title>
    <url>/2020/02/29/Unix-IO/</url>
    <content><![CDATA[<p>叙述一下 Unix 下为解决不同 I/O 问题所设计的 I/O 模型</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>说到IO模型，都会牵扯到同步、异步、阻塞、非阻塞这几个词。从词的表面上看，很多人都觉得很容易理解。但是细细一想，却总会发现有点摸不着头脑。因此想梳理一下IO模型，来加深对这些知识的理解</p>
<p>需要说明以下几点: </p>
<ul>
<li>IO有内存IO、网络IO和磁盘IO三种，通常我们说的IO指的是后两者</li>
<li>阻塞和非阻塞，是函数/方法的实现方式，即在数据就绪之前是立刻返回还是等待，即发起IO请求是否会被阻塞</li>
<li>以文件IO为例,一个IO读过程是文件数据从磁盘→内核缓冲区→用户内存的过程</li>
<li>同步与异步的区别主要在于数据从内核缓冲区→用户内存这个过程需不需要用户进程等待，即实际的IO读写是否阻塞请求进程</li>
<li>网络IO把磁盘换做网卡即可</li>
</ul>
<h4 id="开篇"><a href="#开篇" class="headerlink" title="开篇"></a>开篇</h4><p>在 Unix 系统下，不论是标准输入还是借助套接字接受网络输入，其实都会有两个步骤，很多文章都提到</p>
<ol>
<li>等待数据准备好（Waiting for the data to be ready）</li>
<li>从内核向进程复制数据（Copying the data from the kernel to the process）</li>
</ol>
<p><img src="/images/blog/2020-04-05-1.png" alt></p>
<h4 id="用户空间和内核空间"><a href="#用户空间和内核空间" class="headerlink" title="用户空间和内核空间"></a>用户空间和内核空间</h4><p>OS 的核心是内核，可以访问底层硬件设备，为了保证用户进程不能直接操作内核从而保证内核的安全，OS 将虚拟空间划分为两部分，一部分为内核空间，一部分为用户空间。</p>
<p>内核空间中存放的是内核代码和数据，例如 Linux 的 OS 和驱动便运行在内核空间，可以操作底层硬件，如果从磁盘读取数据，那么数据会被先载入内核空间的缓冲区中；而进程的用户空间中存放的是用户程序的代码和数据，通常来讲就是应用程序常驻的区域。</p>
<p>因此整个 Linux 内部结构可以分为三部分，从最底层到最上层依次是：硬件、内核空间、用户空间。如下图: </p>
<p><img src="/images/blog/2020-04-05-2.png" alt></p>
<p>二者间无法直接通信，必须通过系统调用，一般来说系统调用的成本很高，涉及CPU上下文切换的成本</p>
<h4 id="内核态和用户态"><a href="#内核态和用户态" class="headerlink" title="内核态和用户态"></a>内核态和用户态</h4><ul>
<li>当一个进程经过系统调用而陷入内核代码中执行时，称进程处于内核运行态，简称内核态</li>
<li>当进程在执行用户自己的代码时，则称其处于用户运行态，简称用户态</li>
</ul>
<h4 id="高性能的Server有什么特点"><a href="#高性能的Server有什么特点" class="headerlink" title="高性能的Server有什么特点"></a>高性能的Server有什么特点</h4><p>说完上面的之后，可能疑惑这和 RPC 的通信设计有什么关系呢？其实正是由于这种内存空间的划分，所以 I/O 一般会在两个地方阻塞，一个是等待数据报到达时，一个是从内核空间拷贝到用户空间时，而阻塞多数情况下我们是无法接受的，因为其损耗性能。</p>
<p>而<strong>高性能的 server 到底在关注什么？</strong>一句话总结: <strong>用尽可能少的系统开销处理尽可能多的连接请求</strong>。因此诞生了不同的 I/O 模型，它们的不同点总结起来就是<strong>对这两个阻塞阶段的处理方式不同</strong></p>
<h4 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h4><p>Unix 下存在五种 I/O 模型:      </p>
<ul>
<li>阻塞 I/O</li>
<li>非阻塞 I/O</li>
<li>I/O 复用（select和poll）</li>
<li>信号驱动 I/O（SIGIO）</li>
<li>异步 I/O</li>
</ul>
<h5 id="阻塞-I-O"><a href="#阻塞-I-O" class="headerlink" title="阻塞 I/O"></a>阻塞 I/O</h5><p>水平有限，可以阅读下方链接即可</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/45745980" target="_blank" rel="noopener">UNIX 中的 I/O 模型</a></li>
<li><a href="https://mp.weixin.qq.com/s/dIDESfH5CBO07fODIYCxHg" target="_blank" rel="noopener">IO模型</a></li>
<li><a href="https://blog.csdn.net/lmy86263/article/details/55681371" target="_blank" rel="noopener">Unix中的IO模型：帮你弄清阻塞VS非阻塞、同步VS异步</a></li>
<li><a href="https://www.cnblogs.com/51try-again/p/11078674.html" target="_blank" rel="noopener">Linux IO模式及 select、poll、epoll详解</a></li>
<li><a href="https://www.ibm.com/developerworks/cn/linux/l-async/index.html" target="_blank" rel="noopener">使用异步 I/O 大大提高应用程序的性能</a></li>
</ul>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>NameNode RPC 延迟</title>
    <url>/2020/02/27/NameNode-RPC-Latency/</url>
    <content><![CDATA[<p>NameNode RPC 延迟分析</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>CDH 有个 NameNode RPC 延迟测试，用于检查NameNode响应请求所花费的平均时间的移动平均值不超过某个值。 </p>
<p>此运行状况测试失败，可能表明 NameNode 配置错误，NameNode 写入其数据目录之一时遇到问题，或者可能表明容量规划问题。</p>
<h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><p>检查 NameNode <code>RpcQueueTime_avg_time</code> 是否异常，如果耗费时间较长，这表明大部分RPC延迟都花在了请求排队上，请尝试增加 NameNode NameNode Handler Count，即<code>dfs.namenode.handler.count</code></p>
<p>如果 NameNode <code>RpcProcessingTime_avg_time</code> 指示大部分 RPC 延迟是由于请求处理引起的，请检查以确保存储 HDFS 元数据的每个目录运行正常，比如权限异常、磁盘问题</p>
<p>在 CDH NameNode 图表面板上可以看到这两个监控项曲线</p>
<p><img src="/images/blog/2020-02-27-1.png" alt></p>
<p>当然更多时候是人员不规范使用 HDFS 造成的，这种更多需要借助 hdfs-audit 分析 rpc 画像，找到幕后真凶，请参考之前写的<a href="https://lihuimintu.github.io/2019/12/25/elk-hdfs/" target="_blank" rel="noopener">采集 hdfs-audit 分析 rpc 画像</a></p>
<h4 id="监控设置"><a href="#监控设置" class="headerlink" title="监控设置"></a>监控设置</h4><p>可以使用 NameNode RPC 延迟阈值 和 NameNode RPC 延迟监视窗口 来配置此监控</p>
<p><img src="/images/blog/2020-02-27-2.png" alt></p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://docs.cloudera.com/documentation/enterprise/latest/topics/cm_ht_namenode.html#concept_kx8_nzn_yk" target="_blank" rel="noopener">NameNode RPC Latency</a></li>
</ul>
]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title>IDEA 远程调试</title>
    <url>/2020/02/25/IDEA-Remote/</url>
    <content><![CDATA[<p>使用IDEA进行远程调试</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>想远程调试下测试环境的代码，因此记录下使用过程</p>
<h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><p><strong>Edit Configurations</strong></p>
<p><img src="/images/blog/2020-02-25-1.png" alt></p>
<p><strong>Remote</strong></p>
<p><img src="/images/blog/2020-02-25-2.png" alt></p>
<p><strong>Config</strong></p>
<p><img src="/images/blog/2020-02-25-3.png" alt></p>
<h4 id="启动远程项目"><a href="#启动远程项目" class="headerlink" title="启动远程项目"></a>启动远程项目</h4><p>正常启动命令如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">java -jar ***.jar</span><br></pre></td></tr></table></figure>

<p>开启远程调试</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">java -jar -agentlib:jdwp&#x3D;transport&#x3D;dt_socket,server&#x3D;y,suspend&#x3D;n,address&#x3D;5005 **.jar</span><br></pre></td></tr></table></figure>

<blockquote>
<p>其中 <code>-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005</code> 是从上一步方框里拷贝出来的</p>
</blockquote>
<h4 id="设置断点，开始调试"><a href="#设置断点，开始调试" class="headerlink" title="设置断点，开始调试"></a>设置断点，开始调试</h4><p>远程 debug 模式已经开启，现在可以在需要调试的代码中打断点了</p>
<p><img src="/images/blog/2020-02-25-4.png" alt></p>
<p>如图中所示，如果断点内有√，则表示选取的断点正确</p>
<h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><p>要保证远程调试监听的端口没有被占用，其次防火墙对端口放行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">netstat -anlp | grep 5005</span><br></pre></td></tr></table></figure>

<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.cnblogs.com/victorbu/p/10904732.html" target="_blank" rel="noopener">IDEA 远程调试</a></li>
<li><a href="https://www.jianshu.com/p/302dc10217c0" target="_blank" rel="noopener">使用IDEA进行远程调试</a></li>
</ul>
]]></content>
      <categories>
        <category>Tools</category>
      </categories>
  </entry>
  <entry>
    <title>TCP 可信传输协议</title>
    <url>/2020/02/16/TCP/</url>
    <content><![CDATA[<p>TCP 是基于全双工的可信传输协议</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><h4 id="TCP-状态"><a href="#TCP-状态" class="headerlink" title="TCP 状态"></a>TCP 状态</h4><p>了解 TCP 各个状态，对理解知识和定位问题有很大帮助，具体参看<a href="https://mp.weixin.qq.com/s/5XSM-yLwLc-HyoDdCCNyCA" target="_blank" rel="noopener">TCP连接的状态详解以及故障排查（上）</a>，下方列出会用到的状态</p>
<p>LISTENING: 表示侦听来自远方的TCP端口的连接请求</p>
<p>SYN-SENT: 客户端 SYN_SENT 状态</p>
<p>SYN-RECEIVED: 服务器端状态 SYN_RCVD</p>
<p>ESTABLISHED: 代表一个打开的连接</p>
<p>FIN-WAIT-1: 等待远程TCP连接中断请求，或先前的连接中断请求的确认</p>
<p>FIN-WAIT-2: 从远程TCP等待连接中断请求</p>
<p>CLOSE-WAIT: 等待从本地用户发来的连接中断请求</p>
<p>LAST-ACK: 等待原来的发向远程TCP的连接中断请求的确认</p>
<p>TIME-WAIT: 等待足够的时间以确保远程TCP接收到连接中断请求的确认</p>
<p>CLOSED: 没有任何连接状态</p>
<h4 id="三次握手、四次挥手"><a href="#三次握手、四次挥手" class="headerlink" title="三次握手、四次挥手"></a>三次握手、四次挥手</h4><p>具体可以参看<a href="https://mp.weixin.qq.com/s/NL7Jzh0lYoA395yzaGxBHw" target="_blank" rel="noopener">跟着动画学习TCP三次握手和四次挥手</a>，动画做的挺好的，也讲的挺通俗易懂的。</p>
<p>可以使用<code>tcpdump</code>命令来抓包</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ tcpdump -iany -nn tcp port 80</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</span><br><span class="line">listening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes</span><br><span class="line">19:18:41.179701 IP 127.0.0.1.42524 &gt; 127.0.0.1.80: Flags [S], seq 712584015, win 43690, options [mss 65495,sackOK,TS val 4236991780 ecr 0,nop,wscale 7], length 0</span><br><span class="line">14:59:13.050531 IP 127.0.0.1.80 &gt; 127.0.0.1.42524: Flags [S.], seq 2917476214, ack 712584016, win 43690, options [mss 65495,sackOK,TS val 4236991780 ecr 4236991780,nop,wscale 7], length 0</span><br><span class="line">19:18:41.179727 IP 127.0.0.1.42524 &gt; 127.0.0.1.80: Flags [.], ack 1, win 342, options [nop,nop,TS val 4236991780 ecr 4236991780], length 0</span><br></pre></td></tr></table></figure>

<blockquote>
<p>-nn: 表示不解析域名，直接显示IP，在netstat命令中，也有这个选项<br>-i: 指定监听的网卡，如果为 -iany 则表示监听所有的网卡</p>
</blockquote>
<h4 id="TCP-特点"><a href="#TCP-特点" class="headerlink" title="TCP 特点"></a>TCP 特点</h4><p>首先，TCP 是<strong>基于连接的</strong>，也就是在进行数据传输之前，客户端与服务端(或者说是通信的双方)需要先建立一个可信的连接。在数据传输结束后，再通过一种协定的方式断开连接，由通信的双方释放资源。这里涉及到的，就是常说的”三次握手”、”四次挥手”</p>
<p>其次，TCP 是可靠的，它定义了一种数据包的”超时重传机制”，简单说，就是每一个数据包在发送出去后的都会等待一个响应。如果指定时间内没有收到响应，由发送方进行一定次数的重传来保证数据的可靠传输。</p>
<p>最后，TCP 是基于流的，这是指在传输数据时应用层不需要关注数据包的边界，TCP在数据传输时会自动根据网络环境将数据进行<font color="#FF0000">缓冲、分组、合并</font>。这点跟基于报文的协议(UDP)是截然不同的。当然，基于流的传输也保证了数据收发的有序性，因此每个数据包都附带上一个属于当前连接的序列号。</p>
<h4 id="TCP-与-UDP-区别"><a href="#TCP-与-UDP-区别" class="headerlink" title="TCP 与 UDP 区别"></a>TCP 与 UDP 区别</h4><table>
<thead>
<tr>
<th>TCP</th>
<th>UDP</th>
</tr>
</thead>
<tbody><tr>
<td>面向连接</td>
<td>无连接</td>
</tr>
<tr>
<td>提供可靠服务</td>
<td>不保证可靠</td>
</tr>
<tr>
<td>点到点</td>
<td>一对一，一对多，多对一和多对多</td>
</tr>
<tr>
<td>对系统资源要求较多</td>
<td>对系统资源要求较少</td>
</tr>
<tr>
<td>实时性低</td>
<td>实时性高</td>
</tr>
<tr>
<td>流模式</td>
<td>数据报模式，基于报文的</td>
</tr>
</tbody></table>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/NL7Jzh0lYoA395yzaGxBHw" target="_blank" rel="noopener">跟着动画学习TCP三次握手和四次挥手</a></li>
<li><a href="https://mp.weixin.qq.com/s/5XSM-yLwLc-HyoDdCCNyCA" target="_blank" rel="noopener">TCP连接的状态详解以及故障排查（上）</a></li>
<li><a href="https://mp.weixin.qq.com/s/Y4EELMzinGunW7MEAPjPgA" target="_blank" rel="noopener">TCP连接的状态详解以及故障排查（下）</a></li>
<li><a href="https://mp.weixin.qq.com/s/_UmaGl8yYLtajSgqMPjNYw" target="_blank" rel="noopener">校招面试必考的TCP</a></li>
<li><a href="https://www.jellythink.com/archives/478" target="_blank" rel="noopener">Linux tcpdump命令详解</a></li>
<li><a href="https://www.cnblogs.com/jason2013/articles/4346639.html" target="_blank" rel="noopener">TCP流模式和UDP数据报模式的区别</a></li>
<li><a href="https://mp.weixin.qq.com/s/Je5ZHyGPQe5p0ANV5B-gOQ" target="_blank" rel="noopener">TCP三次握手和四次挥手详解</a></li>
<li><a href="https://mp.weixin.qq.com/s/1eEBzkGp6WI5bp6697X3Gw" target="_blank" rel="noopener">搞了两周Socket通信，终于弄明白了！</a></li>
</ul>
]]></content>
      <categories>
        <category>Network</category>
      </categories>
  </entry>
  <entry>
    <title>Hive 查询表行数</title>
    <url>/2020/02/13/Hive-RowNums/</url>
    <content><![CDATA[<p>在 Hive 中比较快速地查到表的行数信息</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>有时在大数据处理时需要查询 Hive 一些数据表的行数，使用关系型数据库久了会使用<code>select count(*) from &lt;table&gt;;</code>方式进行查询，但是这种方式执行时间比较长，Hive 本身就有记录数据行数，所以可以直接拿到数据行数</p>
<h4 id="方式"><a href="#方式" class="headerlink" title="方式"></a>方式</h4><h5 id="元数据表"><a href="#元数据表" class="headerlink" title="元数据表"></a>元数据表</h5><p>从元数据表中查询元数据</p>
<p>查询表的总条数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select a.TBL_ID, a.TBL_NAME, b.PARAM_KEY, b.PARAM_VALUE </span><br><span class="line">  from TBLS as a join TABLE_PARAMS as b </span><br><span class="line">  where a.TBL_ID &#x3D; b.TBL_ID and TBL_NAME&#x3D;&quot;call_center&quot; and PARAM_KEY&#x3D;&quot;numRows&quot;;</span><br><span class="line">+--------+-------------+-----------+-------------+</span><br><span class="line">| TBL_ID | TBL_NAME    | PARAM_KEY | PARAM_VALUE |</span><br><span class="line">+--------+-------------+-----------+-------------+</span><br><span class="line">|    134 | call_center | numRows   | 60          |</span><br><span class="line">+--------+-------------+-----------+-------------+</span><br></pre></td></tr></table></figure>

<h5 id="explain"><a href="#explain" class="headerlink" title="explain"></a>explain</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">explain select * from xxx limit 2;</span><br></pre></td></tr></table></figure>

<h5 id="HUE"><a href="#HUE" class="headerlink" title="HUE"></a>HUE</h5><p>从HUE上直接查看</p>
<p><img src="/images/blog/2020-02-13-1.png" alt></p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.jianshu.com/p/b46c8d1f13f1" target="_blank" rel="noopener">Hive查询表行数</a></li>
<li><a href="https://blog.csdn.net/name_szd/article/details/80618515" target="_blank" rel="noopener">hive查询数据库总条数</a></li>
<li><a href="https://www.cnblogs.com/qingyunzong/p/8710356.html#_label2_0" target="_blank" rel="noopener">Hive学习之路 （三）Hive元数据信息对应MySQL数据库表</a></li>
</ul>
]]></content>
      <categories>
        <category>Hive</category>
      </categories>
  </entry>
  <entry>
    <title>Linux CPU、Memory、IO、Network 分析</title>
    <url>/2020/02/07/CPU-Memory-IO-Network/</url>
    <content><![CDATA[<p>Linux性能监控 - CPU、Memory、IO、Network</p>
<hr>
<h4 id="CPU"><a href="#CPU" class="headerlink" title="CPU"></a>CPU</h4><p>CPU 查看这些重要参数: 中断、上下文切换、可运行队列、CPU 利用率来监测性能。</p>
<p>每个参数的健康区间:</p>
<ul>
<li>CPU利用率: User Time &lt;= 70%，System Time &lt;= 35%，User Time + System Time &lt;= 70%。</li>
<li>上下文切换: 与CPU利用率相关联，如果CPU利用率状态良好，大量的上下文切换也是可以接受的。</li>
<li>可运行队列: 每个处理器的可运行队列 &lt;=3 个线程。如双处理器系统的可运行队列里不应该超过6个线程。</li>
</ul>
<p>运用到的工具有<a href="https://www.jellythink.com/archives/419" target="_blank" rel="noopener">Linux vmstat命令详解</a>、<a href="https://www.jellythink.com/archives/421" target="_blank" rel="noopener">Linux top命令详解</a>、<a href="https://www.cnblogs.com/mayou18/p/9546431.html" target="_blank" rel="noopener">Linux mpstat-显示各个可用CPU的状态</a></p>
<p>top 查看 CPU 利用率，如果 User Time 比较高，使用 <a href="https://github.com/oldratlee/useful-scripts/blob/dev-2.x/docs/java.md#-show-busy-java-threads" target="_blank" rel="noopener">show-busy-java-threads</a> 工具排查，其会从所有运行的Java进程中找出最消耗CPU的线程（缺省5个），打印出其线程栈。<a href="https://lihuimintu.github.io/2019/06/03/Occupying-CPU-high/" target="_blank" rel="noopener">Linux 系统 CPU 过高异常排查</a></p>
<p>如果 System Time 比 User Time 高，以及高频度的上下文切换（cs），说明应用程序进行了大量的系统调用。</p>
<p>vmstat 输出例子</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----</span><br><span class="line"> r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st</span><br><span class="line"> 3  0 361396 196772  55820 359372    0    0    13    21    1    1  2  0 98  0  0</span><br><span class="line"> 1  0 361392 196524  55820 359616    8    0   236     0  411  527  1  0 90  9  0</span><br><span class="line"> 2  1 361392 196524  55828 359608    0    0     0    48  370  503  1  1 98  0  0</span><br><span class="line"> 4  0 361392 196524  55828 359616    0    0     0     0  442  559  1  0 99  0  0</span><br></pre></td></tr></table></figure>

<p>需要关注的参数</p>
<ul>
<li>r: 当前运行队列中线程的数目，代表线程处于可运行状态，但CPU还未能执行</li>
<li>b: 等待IO的进程数量；<strong>如果该值一直都很大，说明IO比较繁忙，处理较慢</strong></li>
<li>in: 每秒中断数</li>
<li>cs: 每秒上下文切换数</li>
<li>us: 用户占用CPU的百分比</li>
<li>sys: 内核占用CPU的百分比</li>
<li>id: CPU 完全空闲的百分比</li>
</ul>
<h4 id="Memory"><a href="#Memory" class="headerlink" title="Memory"></a>Memory</h4><p>分析 vmstat 输出</p>
<ul>
<li>si: 每秒从交换区写到内存的大小</li>
<li>so: 每秒写入交换区的内存大小</li>
</ul>
<p>内存够用的时候，这2个值都是0，如果这2个值长期大于0时，系统性能会受到影响，磁盘IO和CPU资源都会被消耗。有时我们看到空闲内存（free）很少的或接近于0时，就认为内存不够用了，不能光看这一点，还要结合si和so，如果free很少，但是si和so也很少（大多时候是0），那么不用担心，系统性能这时不会受到影响的</p>
<p><a href="https://mp.weixin.qq.com/s/XAhASXPkIELTvwwY8QPXng" target="_blank" rel="noopener">查询进程占用内存情况方法</a></p>
<h4 id="IO"><a href="#IO" class="headerlink" title="IO"></a>IO</h4><p>通过 <code>vmstat</code> 的输出，重点关注<code>b</code>、<code>bi</code>、<code>bo</code>和<code>wa</code>字段。这几个值变大，都意味着IO的消耗增加。</p>
<p>对于读请求大的服务器，一般<code>b</code>、<code>bi</code>、<code>wa</code>都会比较大，而对于写入量大的服务器，一般<code>b</code>、<code>bo</code>、<code>wa</code>都会比较大。</p>
<p>借助<a href="https://www.jellythink.com/archives/438" target="_blank" rel="noopener">Linux iostat命令详解</a>可以查看相关参数</p>
<ul>
<li>%iowait: 如果该值较高，表示磁盘存在I/O瓶颈</li>
<li>await: 平均每次设备I/O操作的等待时间 (毫秒)，一般地，系统I/O响应时间应该低于5ms，如果大于 10ms就比较大了</li>
<li>%util：一秒中有百分之多少的时间用于I/O操作，即被IO消耗的CPU百分比，一般地，如果该参数是100%表示设备已经接近满负荷运行了</li>
</ul>
<p>借助<code>pidstat -d 1</code>定位出导致瓶颈的进程，参阅<a href="https://www.jellythink.com/archives/444" target="_blank" rel="noopener">Linux pidstat命令详解</a></p>
<p>现在定位到进程级别了，可能需要知道这个进程到底打开了哪些文件，借助<code>lsof -p 20711</code>命令列出指定20711进程打开的文件列表，参阅<a href="https://www.jellythink.com/archives/449" target="_blank" rel="noopener">Linux lsof命令详解</a></p>
<h4 id="Network"><a href="#Network" class="headerlink" title="Network"></a>Network</h4><p>网络的监测是所有 Linux 子系统里面最复杂的，这里贴了解到的运维命令</p>
<p>iftop 命令查看端到端流量</p>
<p>NetHogs实时监控进程的网络带宽占用情况，参阅<a href="https://lihuimintu.github.io/2019/12/06/Linux-nethogs/" target="_blank" rel="noopener">Linux NetHogs 监控工具</a></p>
<p>还有些<code>ip</code>，<code>netstat</code>，<code>tcpdump</code>，<code>sar</code> 来分析网络性能问题，参阅<a href="https://www.jellythink.com/archives/486" target="_blank" rel="noopener">Linux性能监测：网络篇</a></p>
<p><strong>UDP监控</strong></p>
<p>对于UDP服务，查看所有监听的UDP端口的网络情况</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$  watch netstat -unlp</span><br><span class="line">Every 2.0s: netstat -unlp                                                                                                                      Fri Feb  7 21:36:55 2020</span><br><span class="line"></span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State	PID&#x2F;Program name</span><br><span class="line">udp        0	  0 0.0.0.0:627             0.0.0.0:*                           6813&#x2F;rpcbind</span><br><span class="line">udp        0	  0 0.0.0.0:7191            0.0.0.0:*                           8137&#x2F;python2</span><br><span class="line">udp        0	  0 0.0.0.0:68              0.0.0.0:*                           780&#x2F;dhclient</span><br><span class="line">udp        0	  0 0.0.0.0:111             0.0.0.0:*                           1&#x2F;systemd</span><br><span class="line">udp        0	  0 10.0.0.122:123          0.0.0.0:*                           3924&#x2F;ntpd</span><br><span class="line">udp        0	  0 127.0.0.1:123           0.0.0.0:*                           3924&#x2F;ntpd</span><br><span class="line">udp        0	  0 0.0.0.0:123             0.0.0.0:*                           3924&#x2F;ntpd</span><br><span class="line">udp6	   0	  0 :::627                  :::*                                6813&#x2F;rpcbind</span><br><span class="line">udp6	   0	  0 :::7191                 :::*                                8137&#x2F;python2</span><br><span class="line">udp6	   0	  0 :::111                  :::*                                6813&#x2F;rpcbind</span><br><span class="line">udp6	   0	  0 :::123                  :::*                                3924&#x2F;ntpd</span><br></pre></td></tr></table></figure>

<p>对于<code>Recv-Q</code>和<code>Send-Q</code>两个指标值为0，或者没有长时间大于0的数值是比较正常的。</p>
<p>对于UDP服务，查看丢包情况（网卡收到了，但是应用层没有处理过来造成的丢包）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ netstat -su</span><br><span class="line">...</span><br><span class="line">Udp:</span><br><span class="line">    14706185 packets received</span><br><span class="line">    272545 packets to unknown port received.</span><br><span class="line">    11026 packet receive errors</span><br><span class="line">    46474392 packets sent</span><br><span class="line">    10964 receive buffer errors</span><br><span class="line">    0 send buffer errors</span><br><span class="line">    InCsumErrors: 62</span><br><span class="line">....</span><br></pre></td></tr></table></figure>

<p><code>packet receive errors</code> 这一项数值增长了，则表明在丢包。</p>
<p><strong>TCP监控</strong></p>
<p>对于TCP服务而言，这个就比较复杂；因为TCP涉及到重传，所以我们就需要重点关注这个重传率。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ netstat -st | grep segments</span><br><span class="line">    689039681 segments received</span><br><span class="line">    864949096 segments send out</span><br><span class="line">    2233493 segments retransmited</span><br><span class="line">    5186 bad segments received.</span><br></pre></td></tr></table></figure>

<p>查看<code>segments send out</code>和<code>segments retransmited</code>指标，对比一段时间内，这两个指标的增长率就是对应的重传率(<a href="https://www.jellythink.com/archives/486" target="_blank" rel="noopener">Linux性能监测：网络篇</a>文末尾有计算重传率的Shell脚本)，发生重传说明网络传输有丢包，基本上从3个点去定位：客户端网络情况、服务端网络情况、中间链路网络情况。</p>
<p><strong>网卡吞吐率</strong></p>
<p>可以通过<code>sar -n DEV 2 3</code>命令来查看</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sar -n DEV 2 3</span><br><span class="line">Linux 3.10.0-862.14.4.el7.x86_64 (region-0-0-122) 	2020年02月07日 	_x86_64_	(4 CPU)</span><br><span class="line"></span><br><span class="line">21时43分32秒     IFACE   rxpck&#x2F;s   txpck&#x2F;s    rxkB&#x2F;s    txkB&#x2F;s   rxcmp&#x2F;s   txcmp&#x2F;s  rxmcst&#x2F;s</span><br><span class="line">21时43分34秒      eth0    116.50    119.50     11.77     19.72      0.00      0.00      0.00</span><br><span class="line">21时43分34秒        lo     66.00     66.00    144.26    144.26      0.00      0.00      0.00</span><br><span class="line"></span><br><span class="line">21时43分34秒     IFACE   rxpck&#x2F;s   txpck&#x2F;s    rxkB&#x2F;s    txkB&#x2F;s   rxcmp&#x2F;s   txcmp&#x2F;s  rxmcst&#x2F;s</span><br><span class="line">21时43分36秒      eth0    155.00    152.00     19.60     44.66      0.00      0.00      0.00</span><br><span class="line">21时43分36秒        lo     29.00     29.00     94.53     94.53      0.00      0.00      0.00</span><br><span class="line"></span><br><span class="line">21时43分36秒     IFACE   rxpck&#x2F;s   txpck&#x2F;s    rxkB&#x2F;s    txkB&#x2F;s   rxcmp&#x2F;s   txcmp&#x2F;s  rxmcst&#x2F;s</span><br><span class="line">21时43分38秒      eth0    116.00    118.00     11.86     19.80      0.00      0.00      0.00</span><br><span class="line">21时43分38秒        lo     33.00     33.00     68.67     68.67      0.00      0.00      0.00</span><br><span class="line"></span><br><span class="line">平均时间:     IFACE   rxpck&#x2F;s   txpck&#x2F;s    rxkB&#x2F;s    txkB&#x2F;s   rxcmp&#x2F;s   txcmp&#x2F;s  rxmcst&#x2F;s</span><br><span class="line">平均时间:      eth0    129.17    129.83     14.41     28.06      0.00      0.00      0.00</span><br><span class="line">平均时间:        lo     42.67     42.67    102.49    102.49      0.00      0.00      0.00</span><br></pre></td></tr></table></figure>


<p>将<code>rxkB/s</code>和<code>txkB/s</code>进行相加，得到网卡设备的实际吞吐率，然后再和网卡的硬件指标进行比对即可。</p>
<p>比如一个网卡的<code>rxkB/s</code>指标为21999.10，<code>txkB/s</code>指标为482.56，那这个网卡的吞吐率大概在<code>22Mbytes/s</code>，即<code>176 Mbits/sec</code>，没有达到<code>1Gbit/sec</code>的硬件上限。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.cnblogs.com/linuxbug/p/4909980.html" target="_blank" rel="noopener">Linux性能监控 - CPU、Memory、IO、Network</a></li>
</ul>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>缺页中断算法 - LRU</title>
    <url>/2020/02/05/LRU/</url>
    <content><![CDATA[<p>最近最久未使用置换算法</p>
<hr>
<h4 id="缺页中断"><a href="#缺页中断" class="headerlink" title="缺页中断"></a>缺页中断</h4><p>缺页中断就是 CPU 要访问的页不在主存，需要操作系统将其调入主存后再进行访问</p>
<p>访问的页面不在内存时，会产生一次缺页中断，缺页中断是由于所要访问的页面不存在主内存时触发，属于由硬件所产生的一种特殊的中断，也称之为硬中断。</p>
<p>缺页本身是一种中断，与软中断一样，需要经过4个处理步骤</p>
<ol>
<li>保护CPU现场 </li>
<li>分析中断原因 </li>
<li>转入缺页中断处理程序进行处理 </li>
<li>恢复CPU现场，继续执行</li>
</ol>
<p>缺页中断更多可以阅读<a href="https://liam.page/2017/09/01/page-fault/" target="_blank" rel="noopener">程序员的自我修养（七）：内存缺页错误</a></p>
<h4 id="页面置换算法"><a href="#页面置换算法" class="headerlink" title="页面置换算法"></a>页面置换算法</h4><p>进程运行过程中，如果发生缺页中断，而此时内存中有没有空闲的物理块时，<br>为了能够把所缺的页面装入内存，系统必须从内存中选择一页调出到磁盘的对换区。<br>但此时应该把那个页面换出，则需要根据一定的页面置换算法（Page Replacement Algorithm)来确定。</p>
<p>页面置换算法有 OPT、FIFO、LRU 三种算法。OPT、FIFO 大家自行阅读<a href="https://blog.csdn.net/Youth_Mr6/article/details/82767332" target="_blank" rel="noopener">缺页中断算法(FIFO,LRU)</a></p>
<h4 id="LRU"><a href="#LRU" class="headerlink" title="LRU"></a>LRU</h4><p>最近最久未使用置换算法（Least Recently Used）</p>
<p>置换最近一段时间以来最长时间未访问过的页面。根据程序局部性原理，刚被访问的页面，可能马上又要被访问；而较长时间内没有被访问的页面，可能最近不会被访问。 </p>
<p>采用固定分配局部置换的策略，假定系统为某进程在内存中分配了3个物理页，页面访问顺序为2、3、2、1、5、2、4、5、3、2、5、2。假定系统未采用预调页策略，即未事先调入任何页面。</p>
<p><img src="/images/blog/2020-02-05-1.png" alt></p>
<p>中断次数为7，缺页中断率为7/12*100% = 58.3%</p>
<h4 id="数据结构实现实现-LRU"><a href="#数据结构实现实现-LRU" class="headerlink" title="数据结构实现实现 LRU"></a>数据结构实现实现 LRU</h4><p>哈希表 + 双向链表</p>
<p>用哈希表，辅以双向链表记录键值对的信息。所以可以在 <code>O(1)</code> 时间内完成 put 和 get 操作，同时也支持 <code>O(1)</code> 删除第一个添加的节点。</p>
<p><img src="/images/blog/2020-02-05-2.png" alt></p>
<p>使用双向链表的一个好处是不需要额外信息删除一个节点，同时可以在常数时间内从头部或尾部插入删除节点。</p>
<p>一个需要注意的是，在双向链表实现中，这里使用一个伪头部和伪尾部标记界限，这样在更新的时候就不需要检查是否是 null 节点。</p>
<p><img src="/images/blog/2020-02-05-3.png" alt></p>
<p>感兴趣可以到 LeetCode 做这道题——<a href="https://leetcode-cn.com/problems/lru-cache/solution/lru-huan-cun-ji-zhi-by-leetcode/" target="_blank" rel="noopener">146. LRU缓存机制</a></p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/Youth_Mr6/article/details/82767332" target="_blank" rel="noopener">缺页中断算法(FIFO,LRU)</a></li>
<li><a href="https://leetcode-cn.com/problems/lru-cache/solution/lru-huan-cun-ji-zhi-by-leetcode/" target="_blank" rel="noopener">146. LRU缓存机制</a></li>
</ul>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>Zookeeper 解决脑裂原理</title>
    <url>/2020/02/04/ZK-Split-Brain/</url>
    <content><![CDATA[<p>这是分布式系统中一个很实际的问题</p>
<hr>
<h4 id="转载来源"><a href="#转载来源" class="headerlink" title="转载来源"></a>转载来源</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">作者：1点25</span><br><span class="line">链接：https:&#x2F;&#x2F;juejin.im&#x2F;post&#x2F;5d36c2f25188257f6a209d37</span><br><span class="line">来源: 掘金</span><br></pre></td></tr></table></figure>

<h4 id="脑裂"><a href="#脑裂" class="headerlink" title="脑裂"></a>脑裂</h4><p>脑裂(split-brain)就是“大脑分裂”，也就是本来一个“大脑”被拆分了两个或多个“大脑”，我们都知道，如果一个人有多个大脑，并且相互独立的话，那么会导致人体“手舞足蹈”，“不听使唤”。<br>脑裂通常会出现在集群环境中，比如ElasticSearch、Zookeeper集群，而这些集群环境有一个统一的特点，就是它们有一个大脑，比如ElasticSearch集群中有Master节点，Zookeeper集群中有Leader节点。</p>
<h4 id="Zookeeper-集群中的脑裂场景"><a href="#Zookeeper-集群中的脑裂场景" class="headerlink" title="Zookeeper 集群中的脑裂场景"></a>Zookeeper 集群中的脑裂场景</h4><p>对于一个集群，想要提高这个集群的可用性，通常会采用多机房部署，比如现在有一个由6台zkServer所组成的一个集群，部署在了两个机房</p>
<p><img src="/images/blog/2020-02-04-1.png" alt></p>
<p>正常情况下，此集群只会有一个Leader，那么如果机房之间的网络断了之后，两个机房内的zkServer还是可以相互通信的，如果<strong>不考虑过半机制</strong>，那么就会出现每个机房内部都将选出一个Leader。</p>
<p><img src="/images/blog/2020-02-04-2.png" alt></p>
<p>这就相当于原本一个集群，被分成了两个集群，出现了两个“大脑”，这就是脑裂。<br>对于这种情况，我们也可以看出来，原本应该是统一的一个集群对外提供服务的，现在变成了两个集群同时对外提供服务，如果过了一会，断了的网络突然联通了，那么此时就会出现问题了，两个集群刚刚都对外提供服务了，数据该怎么合并，数据冲突怎么解决等等问题。<br>刚刚在说明脑裂场景时，有一个前提条件就是没有考虑过半机制，所以实际上Zookeeper集群中是不会出现脑裂问题的，而不会出现的原因就跟过半机制有关。</p>
<h4 id="过半机制"><a href="#过半机制" class="headerlink" title="过半机制"></a>过半机制</h4><p>在领导者选举的过程中，如果某台zkServer获得了超过半数的选票，则此zkServer就可以成为Leader了。</p>
<p>过半机制的源码实现其实非常简单：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class QuorumMaj implements QuorumVerifier &#123;</span><br><span class="line">    private static final Logger LOG &#x3D; LoggerFactory.getLogger(QuorumMaj.class);</span><br><span class="line">    </span><br><span class="line">    int half;</span><br><span class="line">    </span><br><span class="line">    &#x2F;&#x2F; n表示集群中zkServer的个数（准确的说是参与者的个数，参与者不包括观察者节点）</span><br><span class="line">    public QuorumMaj(int n)&#123;</span><br><span class="line">        this.half &#x3D; n&#x2F;2;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; 验证是否符合过半机制</span><br><span class="line">    public boolean containsQuorum(Set&lt;Long&gt; set)&#123;</span><br><span class="line">        &#x2F;&#x2F; half是在构造方法里赋值的</span><br><span class="line">        &#x2F;&#x2F; set.size()表示某台zkServer获得的票数</span><br><span class="line">        return (set.size() &gt; half);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>大家仔细看一下上面方法中的注释，核心代码就是下面两行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">this.half &#x3D; n&#x2F;2;</span><br><span class="line">return (set.size() &gt; half);</span><br></pre></td></tr></table></figure>

<p>举个简单的例子： 如果现在集群中有5台zkServer，那么half=5/2=2，那么也就是说，领导者选举的过程中至少要有三台zkServer投了同一个zkServer，才会符合过半机制，才能选出来一个Leader。</p>
<p>那么有一个问题我们想一下，<strong>选举的过程中为什么一定要有一个过半机制验证？</strong>因为这样不需要等待所有zkServer都投了同一个zkServer就可以选举出来一个Leader了，这样比较快，所以叫快速领导者选举算法呗。</p>
<p>那么再来想一个问题，<strong>过半机制中为什么是大于，而不是大于等于呢？</strong></p>
<p>这就是跟脑裂问题有关系了，比如回到上文出现脑裂问题的场景：</p>
<p><img src="/images/blog/2020-02-04-3.png" alt></p>
<p>当机房中间的网络断掉之后，机房1内的三台服务器会进行领导者选举，但是此时过半机制的条件是set.size() &gt; 3，也就是说至少要4台zkServer才能选出来一个Leader，所以对于机房1来说它不能选出一个Leader，同样机房2也不能选出一个Leader，这种情况下整个集群当机房间的网络断掉后，整个集群将没有Leader。</p>
<p>而如果过半机制的条件是set.size() &gt;= 3，那么机房1和机房2都会选出一个Leader，这样就出现了脑裂。所以我们就知道了，为什么过半机制中是大于，而不是大于等于。就是为了防止脑裂。</p>
<p>如果假设我们现在只有5台机器，也部署在两个机房</p>
<p><img src="/images/blog/2020-02-04-4.png" alt></p>
<p>此时过半机制的条件是set.size() &gt; 2，也就是至少要3台服务器才能选出一个Leader，此时机房件的网络断开了，对于机房1来说是没有影响的，Leader依然还是Leader，对于机房2来说是选不出来Leader的，此时整个集群中只有一个Leader。</p>
<p>所以，我们可以总结得出，有了过半机制，对于一个Zookeeper集群，要么没有Leader，要没只有1个Leader，这样就避免了脑裂问题。</p>
<blockquote>
<p>Q: 在阅读时我就想如果机房1有1个leader和1个follower,机房2有3个follower, 这种情况呢？<br>A: 每当新leader产生时，会生成一个epoch，这个epoch是递增的，followers如果确认了新的leader存在，知道其epoch，就会拒绝epoch小于现任leader epoch的所有请求。所以旧leader无法被follower认可。</p>
<p>假设某个leader假死，其余的followers选举出了一个新的leader。这时，旧的leader复活并且仍然认为自己是leader，这个时候它向其他followers发出写请求也是会被拒绝的。因为每当新leader产生时，会生成一个epoch，这个epoch是递增的，followers如果确认了新的leader存在，知道其epoch，就会拒绝epoch小于现任leader epoch的所有请求。那有没有follower不知道新的leader存在呢，有可能，但肯定不是大多数，否则新leader无法产生。Zookeeper的写也遵循quorum机制，因此，得不到大多数支持的写是无效的，旧leader即使各种认为自己是leader，依然没有什么作用。</p>
</blockquote>
<h4 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h4><p>解决 Split-Brain 的问题，一般有3种方式</p>
<ul>
<li>Quorums（法定人数）: 比如3个节点的集群，Quorums = 2, 也就是说集群可以容忍1个节点失效，这时候还能选举出1个lead，集群还可用。比如4个节点的集群，它的Quorums = 3，Quorums要超过3，相当于集群的容忍度还是1，如果2个节点失效，那么整个集群还是无效的</li>
<li>Redundant communications: 冗余通信的方式，集群中采用多种通信方式，防止一种通信方式失效导致集群中的节点无法通信。</li>
<li>Fencing: 共享资源的方式，比如能看到共享资源就表示在集群中，能够获得共享资源的锁的就是Leader，看不到共享资源的，就不在集群中。HDFS NameNode 使用的就是这种方式</li>
</ul>
<p>ZooKeeper默认采用了Quorums这种方式，即只有集群中超过半数节点投票才能选举出Leader。这样的方式可以确保leader的唯一性,要么选出唯一的一个leader，要么选举失败。</p>
<p>在ZooKeeper中Quorums有2个作用:</p>
<ul>
<li>集群中最少的节点数用来选举Leader保证集群可用</li>
<li>通知客户端数据已经安全保存，前集群中最少数量的节点数已经保存了该数据。一旦这些节点保存了该数据，客户端将被通知已经安全保存了，可以继续其他任务。而集群中剩余的节点将会最终也保存了该数据</li>
</ul>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/u013374645/article/details/93140148" target="_blank" rel="noopener">面试题-Zookeeper是如何解决脑裂问题</a></li>
<li><a href="https://blog.csdn.net/bigtree_3721/article/details/78389492" target="_blank" rel="noopener">ZooKeeper的Quorums机制</a></li>
</ul>
]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
  </entry>
  <entry>
    <title>CDH 集群中 DN 热换盘处理</title>
    <url>/2020/02/03/DN-Fault-Raid/</url>
    <content><![CDATA[<p>在集群使用的过程中会遇到数据节点的磁盘故障，在不停数据节点的情况下，如何为数据节点进行热插拔换盘操作</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>HDFS 随着使用时间会出现数据借点磁盘故障现象，网上查阅到的大部分都是停止数据节点，换盘后重启。我司处理的方法是热插拔处理。</p>
<h4 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h4><p>具体参阅 Fayson 的<a href="https://cloud.tencent.com/developer/article/1158329" target="_blank" rel="noopener">如何在CDH集群中为数据节点热插拔硬盘</a></p>
<p>Fayson 自己也说操作步骤更类似于加盘操作，磁盘坏掉如果磁盘的盘符未变更则只需要将磁盘格式化挂载在原来的目录下。</p>
<p>变通方式是先将其取消坏盘挂载，更换新盘后格式化再挂载回来，DN 执行刷新数据目录操作即可。</p>
<p>这样会出现节点内磁盘不均衡的现象，需要借助节点内平衡 DiskBalancer，更多参考 <a href="https://issues.apache.org/jira/browse/HDFS-1312" target="_blank" rel="noopener">HDFS-1312</a></p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://cloud.tencent.com/developer/article/1158329" target="_blank" rel="noopener">如何在CDH集群中为数据节点热插拔硬盘</a></li>
<li><a href="https://ilinuxkernel.com/?p=958" target="_blank" rel="noopener">Linux硬盘盘符分配</a></li>
</ul>
]]></content>
      <categories>
        <category>CDH</category>
      </categories>
  </entry>
  <entry>
    <title>内存缺页错误</title>
    <url>/2020/02/01/Page-Fault/</url>
    <content><![CDATA[<p>缺页中断就是要访问的页不在主存，需要操作系统将其调入主存后再进行访问。</p>
<hr>
<h4 id="转载来源"><a href="#转载来源" class="headerlink" title="转载来源"></a>转载来源</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">作者：Liam Huang</span><br><span class="line">链接：https:&#x2F;&#x2F;liam.page&#x2F;2017&#x2F;09&#x2F;01&#x2F;page-fault&#x2F;</span><br></pre></td></tr></table></figure>

<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>众所周知，CPU 不能直接和硬盘进行交互。CPU 所作的一切运算，都是通过 CPU 缓存间接与内存进行操作的。若是 CPU 请求的内存数据在物理内存中不存在，那么 CPU 就会报告「缺页错误（Page Fault）」，提示内核产生中断，将所缺的页面装入内存。</p>
<p>在内核处理缺页错误时，就有可能进行磁盘的读写操作。这样的操作，相对 CPU 的处理是非常缓慢的。因此，发生大量的缺页错误，势必会对程序的性能造成很大影响。因此，在对性能要求很高的环境下，应当尽可能避免这种情况。</p>
<p>此篇介绍缺页错误本身，并结合一个实际示例作出一些实践分析。这里主要在 Linux 的场景下做讨论；其他现代操作系统，基本也是类似的。</p>
<h4 id="内存页和缺页错误"><a href="#内存页和缺页错误" class="headerlink" title="内存页和缺页错误"></a>内存页和缺页错误</h4><p>现代 CPU 都支持分段和分页的内存寻址模式。在 Linux 当中，实际起作用的只有分页模式。</p>
<p>具体来说，分页模式在逻辑上将虚拟内存和物理内存同时等分成固定大小的块。这些块在虚拟内存上称之为「页」，而在物理内存上称之为「页帧」，并交由 CPU 中的 MMU 模块(内存管理单元)来负责页帧和页之间的映射管理。</p>
<p>引入分页模式的好处，可以大致概括为两个方面: </p>
<ul>
<li>允许虚存空间远大于实际物理内存大小的情况。这是因为，分页之后，操作系统读入磁盘的文件时，无需以文件为单位全部读入，而可以以内存页为单位，分片读入。同时，考虑到 CPU 不可能一次性需要使用整个内存中的数据，因此可以交由特定的算法，进行内存调度: 将长时间不用的页帧内的数据暂存到磁盘上。</li>
<li>减少了内存碎片的产生。这是因为，引入分页之后，内存的分配管理都是以页大小（通常是 4KiB，扩展分页模式下是 4MiB）为单位的；虚拟内存中的页总是对应物理内存中实际的页帧。这样一来，在虚拟内存空间中，页内连续的内存在物理内存上也一定是连续的，不会产生碎片。</li>
</ul>
<h4 id="缺页错误"><a href="#缺页错误" class="headerlink" title="缺页错误"></a>缺页错误</h4><p>当进程在进行一些计算时，CPU 会请求内存中存储的数据。在这个请求过程中，CPU 发出的地址是逻辑地址（虚拟地址），然后交由 CPU 当中的 MMU 单元进行内存寻址，找到实际物理内存上的内容。若是目标虚存空间中的内存页（因为某种原因），在物理内存中没有对应的页帧，那么 CPU 就无法获取数据。这种情况下，CPU 是无法进行计算的，于是它就会报告一个缺页错误（Page Fault）。</p>
<p>因为 CPU 无法继续进行进程请求的计算，并报告了缺页错误，用户进程必然就中断了。这样的中断称之为缺页中断，因为由 CPU 以外的硬件产生的中断，也称之为硬中断。在报告 Page Fault 之后，进程会从用户态切换到系统态，交由操作系统内核的 Page Fault Handler 处理缺页错误。</p>
<h4 id="分类和处理"><a href="#分类和处理" class="headerlink" title="分类和处理"></a>分类和处理</h4><p>基本来说，缺页错误可以分为两类: 硬缺页错误（Hard Page Fault）和软缺页错误（Soft Page Fault）。这里，前者又称为主要缺页错误（Major Page Fault）；后者又称为次要缺页错误（Minor Page Fault）。当缺页中断发生后，Page Fault Handler 会判断缺页的类型，进而处理缺页错误，最终将控制权交给用户态代码。</p>
<p>若是此时物理内存里，已经有一个页帧正是此时 CPU 请求的内存页，那么这是一个软缺页错误；于是，Page Fault Hander 会指示 MMU 建立相应的页帧到页的映射关系。这一操作的实质是进程间共享内存——比如动态库（共享对象），比如 mmap 的文件。</p>
<p>若是此时物理内存中，没有相应的页帧，那么这就是一个硬缺页错误；于是 Page Fault Hander 会指示 CPU，从已经打开的磁盘文件中读取相应的内容到物理内存，而后交由 MMU 建立这份页帧到页的映射关系。</p>
<p>不难发现，软缺页错误只是在内核态里轻轻地走了一遭，而硬缺页错误则涉及到磁盘 I/O。因此，处理起来，硬缺页错误要比软缺页错误耗时长得多。这就是为什么我们要求高性能程序必须在对外提供服务时，尽可能少地发生硬缺页错误。</p>
<blockquote>
<p>除了硬缺页错误和软缺页错误之外，还有一类缺页错误是因为访问非法内存引起的。前两类缺页错误中，进程尝试访问的虚存地址尚为合法有效的地址，只是对应的物理内存页帧没有在物理内存当中。后者则不然，进程尝试访问的虚存地址是非法无效的地址。比如尝试对 nullptr 解引用，就会访问地址为 0x0 的虚存地址，这是非法地址。此时 CPU 报出无效缺页错误（Invalid Page Fault）。操作系统对无效缺页错误的处理各不相同：Windows 会使用异常机制向进程报告；*nix 则会通过向进程发送 SIGSEGV 信号（11），引发<a href="https://liam.page/2017/05/27/tutorial-to-GDB-taking-ncurses-as-an-example/" target="_blank" rel="noopener">内存转储</a>。</p>
</blockquote>
<h4 id="缺页错误的原因"><a href="#缺页错误的原因" class="headerlink" title="缺页错误的原因"></a>缺页错误的原因</h4><p>之前提到，物理内存中没有 CPU 所需的页帧，就会引发缺页错误。这一现象背后的原因可能有很多。</p>
<p>例如说，进程通过 mmap 系统调用，直接建立了磁盘文件和虚拟内存的映射关系。然而，在 mmap 调用之后，并不会立即从磁盘上读取这一文件。而是在实际需要文件内容时，通过 CPU 触发缺页错误，要求 Page Fault Handler 去将文件内容读入内存。</p>
<p>又例如说，一个进程启动了很久，但是长时间没有活动。若是计算机处在很高的内存压力下，则操作系统会将这一进程长期未使用的页帧内容，从物理内存转储到磁盘上。这个过程称为换出（swap out）。在 *nix 系统下，用于转储这部分内存内容的磁盘空间，称为交换空间；在 Windows 上，这部分磁盘空间，则被称为虚拟内存，对应磁盘上的文件则称为页面文件。在这个过程中，进程在内存中保存的任意内容，都可能被换出到交换空间：可以是数据内容，也可以是进程的代码段内容。</p>
<p>Windows 用户看到这里，应该能明白这部分空间为什么叫做「虚拟内存」——因为它于真实的内存条相对，是在硬盘上虚拟出来的一份内存。通过这样的方式，「好像」将内存的容量扩大了。同样，为什么叫「页面文件」也一目了然。因为事实上，文件内保存的就是一个个内存页帧。在 Windows 上经常能观察到「假死」的现象，就和缺页错误有关。这种现象，实际就是长期不运行某个程序，导致程序对应的内存被换出到磁盘；在需要响应时，由于需要从磁盘上读取大量内容，导致响应很慢，产生假死现象。这种现象发生时，若是监控系统硬错误数量，就会发现在短时间内，目标进程产生了大量的硬错误。</p>
<p>在 Windows XP 流行的年代，有很多来路不明的「系统优化建议」。其中一条就是「扩大页面文件的大小，有助于加快系统速度」。事实上，这种方式只能加大内存「看起来」的容量，却给内存整体（将物理内存和磁盘页面文件看做一个整体）的响应速度带来了巨大的负面影响。因为，尽管容量增大了，但是访问这部分增大的容量时，进程实际上需要先陷入内核态，从磁盘上读取内容做好映射，再继续执行。更有甚者，这些建议会要求「将页面文件分散在多个不同磁盘分区」，并美其名曰「分散压力」。事实上，从页面文件中读取内存页帧本就已经很慢；若是还要求磁盘不断在不同分区上寻址，那就更慢了。可见谣言害死人。</p>
<h4 id="观察缺页错误"><a href="#观察缺页错误" class="headerlink" title="观察缺页错误"></a>观察缺页错误</h4><p>通过<code>top</code> 命令查看观察软中断和硬中断的 CPU 使用率，hi(hard interrupt) 表示硬中断、si(soft interrupt) 表示软中断</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">%Cpu(s): 53.3 us,  1.2 sy,  0.0 ni, 44.6 id,  0.9 wa,  0.0 hi,  0.1 si,  0.0 st</span><br></pre></td></tr></table></figure>

<p>通过<code>mpstat -P ALL 2</code> 每隔两秒查看下所有核状态信息，其中<code>%irq</code>为硬中断，<code>%soft</code>为软中断，如果硬中断比较高可以看看是不是大量读取磁盘引起的(<code>iostat -d 1</code>每隔1s 查看磁盘读写速度)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ mpstat -P ALL 2</span><br><span class="line">Linux 3.10.0-862.14.4.el7.x86_64 (region-0-0-122) 	2020年02月07日 	_x86_64_	(4 CPU)</span><br><span class="line"></span><br><span class="line">16时23分31秒  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle</span><br><span class="line">16时23分33秒  all   52.64    0.00    0.88    0.38    0.00    0.00    0.00    0.00    0.00   46.10</span><br><span class="line">16时23分33秒    0  100.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00</span><br><span class="line">16时23分33秒    1    4.06    0.00    1.52    1.52    0.00    0.00    0.00    0.00    0.00   92.89</span><br><span class="line">16时23分33秒    2   99.50    0.00    0.50    0.00    0.00    0.00    0.00    0.00    0.00    0.00</span><br><span class="line">16时23分33秒    3    5.08    0.00    1.52    0.00    0.00    0.00    0.00    0.00    0.00   93.40</span><br><span class="line"></span><br><span class="line">16时23分33秒  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle</span><br><span class="line">16时23分35秒  all   52.41    0.00    1.01    0.13    0.00    0.13    0.00    0.00    0.00   46.33</span><br><span class="line">16时23分35秒    0  100.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00</span><br><span class="line">16时23分35秒    1    4.57    0.00    2.54    0.51    0.00    0.00    0.00    0.00    0.00   92.39</span><br><span class="line">16时23分35秒    2  100.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00</span><br><span class="line">16时23分35秒    3    3.55    0.00    2.03    0.51    0.00    0.51    0.00    0.00    0.00   93.40</span><br><span class="line"></span><br><span class="line">16时23分35秒  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle</span><br><span class="line">16时23分37秒  all   54.74    0.00    1.52    0.13    0.00    0.25    0.00    0.00    0.00   43.36</span><br><span class="line">16时23分37秒    0   99.50    0.00    0.50    0.00    0.00    0.00    0.00    0.00    0.00    0.00</span><br><span class="line">16时23分37秒    1    9.28    0.00    2.06    0.00    0.00    0.52    0.00    0.00    0.00   88.14</span><br><span class="line">16时23分37秒    2  100.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00</span><br><span class="line">16时23分37秒    3    8.16    0.00    3.57    0.00    0.00    0.51    0.00    0.00    0.00   87.76</span><br></pre></td></tr></table></figure>

<p><code>dstat</code>可以查看 CPU 软硬中断次数，hiq、siq分别为硬中断和软中断次数。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ dstat</span><br><span class="line">You did not select any stats, using -cdngy by default.</span><br><span class="line">----total-cpu-usage---- -dsk&#x2F;total- -net&#x2F;total- ---paging-- ---system--</span><br><span class="line">usr sys idl wai hiq siq| read  writ| recv  send|  in   out | int   csw</span><br><span class="line"> 62   1  36   0   0   0|  84k  355k|   0     0 |   0     0 |5766  4315</span><br><span class="line"> 52   1  47   0   0   0|   0   232k|  21k   54k|   0     0 |5279  4004</span><br><span class="line"> 52   1  47   0   0   0|   0    56k|  15k   22k|   0     0 |5327  4045</span><br></pre></td></tr></table></figure>


<p>ps 是一个强大的命令，我们可以用 -o 选项指定希望关注的项目。比如</p>
<ul>
<li>min_flt: 进程启动至今软缺页中断数量</li>
<li>maj_flt: 进程启动至今硬缺页中断数量</li>
<li>cmd: 执行的命令</li>
<li>args: 执行的命令的参数（从 $0$ 开始）</li>
<li>uid: 执行命令的用户的 ID</li>
<li>gid: 执行命令的用户所在组的 ID</li>
</ul>
<p>因此，我们可以用 <code>ps -o min_flt,maj_flt,cmd,args,uid,gid 14434</code> 来观察进程号为 14434 的进程的缺页错误</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ps -o min_flt,maj_flt,cmd,args,uid,gid 14434</span><br><span class="line">   MINFL  MAJFL CMD                         COMMAND                       UID   GID</span><br><span class="line">  2413550   355 &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_181-clou &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_181-clou   981   978</span><br></pre></td></tr></table></figure>

<p>结合 <code>watch</code> 命令，则可关注进程当前出发缺页中断的状态</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">watch -n 1 --difference &quot;ps -o min_flt,maj_flt,cmd,args,uid,gid 14434&quot;</span><br></pre></td></tr></table></figure>

<p>你还可以结合 <code>sort</code> 命令，动态观察产生缺页错误最多的几个进程。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ watch -n 1 &quot;ps -eo min_flt,maj_flt,cmd,args,uid,gid | sort -nrk1 | head -n 8&quot;</span><br><span class="line">Every 1.0s: ps -eo min_flt,maj_flt,cmd,args,uid,gid | sort -nrk1 | head -n 8                                                                   Fri Feb  7 17:18:43 2020</span><br><span class="line"></span><br><span class="line">437395316 262 &#x2F;usr&#x2F;bin&#x2F;python2 &#x2F;opt&#x2F;cloud &#x2F;usr&#x2F;bin&#x2F;python2 &#x2F;opt&#x2F;cloud     0     0</span><br><span class="line">69510242   75 &#x2F;bin&#x2F;bash &#x2F;opt&#x2F;cloudera&#x2F;cm- &#x2F;bin&#x2F;bash &#x2F;opt&#x2F;cloudera&#x2F;cm-   981   978</span><br><span class="line">23151349 1164 &#x2F;usr&#x2F;local&#x2F;aegis&#x2F;aegis_clie &#x2F;usr&#x2F;local&#x2F;aegis&#x2F;aegis_clie     0     0</span><br><span class="line">5772594  1197 &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;systemd-jo &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;systemd-jo     0     0</span><br><span class="line">2902146    46 &#x2F;usr&#x2F;sbin&#x2F;irqbalance --fore &#x2F;usr&#x2F;sbin&#x2F;irqbalance --fore     0     0</span><br><span class="line">2413598   355 &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_181-clou &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_181-clou   981   978</span><br><span class="line">1591327   619 &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_181-clou &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_181-clou   997   995</span><br><span class="line">1588418   197 &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;systemd -- &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;systemd --     0     0</span><br></pre></td></tr></table></figure>

<h4 id="一个硬缺页错误导致的问题"><a href="#一个硬缺页错误导致的问题" class="headerlink" title="一个硬缺页错误导致的问题"></a>一个硬缺页错误导致的问题</h4><p>我司的某一高性能服务采取了 mmap 的方式，从磁盘加载大量数据。由于调研测试需要，多名组内成员共享一台调研机器。现在的问题是，当共享的人数较多时，新启动的服务进程会在启动时耗费大量时间——以几十分钟计。那么，这是为什么呢？</p>
<blockquote>
<p>因为涉及到公司机密，这里不方便给截图。留待以后，做模拟实验后给出。</p>
</blockquote>
<p>以 top 命令观察，机器卡顿时，CPU 负载并不高：32 核只有 1.3 左右的 1min 平均负载。但是，iostat 观察到，磁盘正在以 10MiB/s 级别的速度，不断进行读取。由此判断，这种情况下，目标进程一定有大量的 Page Fault 产生。使用上述 <code>watch -n 1 --difference &quot;ps -o min_flt,maj_flt,cmd,args,uid,gid &lt;pid&gt;&quot;</code> 观察，发现目标进程确实有大量硬缺页错误产生，肯定了这一推断。</p>
<p>然而，诚然进程需要载入大量数据，但是以 mmap 的方式映射，为何会已有大量同类服务存在的情况下，大量读取硬盘呢？这就需要更加深入的分析了。</p>
<p>事实上，这里隐含了一个非常细小的矛盾。一方面，该服务需要从磁盘加载大量数据；另一方面，该服务对性能要求非常高。我们知道，mmap 只是对文件做了映射，不会在调用 mmap 时立即将文件内容加载进内存。这就导致了一个问题：当服务启动对外提供服务时，可能还有数据未能加载进内存；而这种加载是非常慢的，严重影响服务性能。因此，可以推断，为了解决这个问题，程序必然在 mmap 之后，尝试将所有数据加载进物理内存。</p>
<p>这样一来，先前遇到的现象就很容易解释了。</p>
<ul>
<li>一方面，因为公用机器的人很多，必然造成内存压力大，从而存在大量换出的内存；</li>
<li>另一方面，新启动的进程，会逐帧地扫描文件；</li>
<li>这样一来，新启动的进程，就必须在极大的内存压力下，不断逼迫系统将其它进程的内存换出，而后换入自己需要的内存，不断进行磁盘 I/O；</li>
<li>故此，新启动的进程会耗费大量时间进行不必要的磁盘 I/O。</li>
</ul>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>HDFS 处理小文件方法</title>
    <url>/2020/01/28/HDFS-small-file/</url>
    <content><![CDATA[<p>Hadoop 中处理小文件</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>HDFS 随着使用时间会存在小文件现象，这里将网上调研的处理方法收集一下</p>
<p>解决此问题的方法主要为两个方面</p>
<ol>
<li><p>从源头解决小文件问题，此方法需要业务方配合解决</p>
</li>
<li><p>合并平台上已有的小文件。因此更多工作是围绕第二种个方法来做的，有多种方法，比如HAR 归档啊，利用Hive针对parquet 文件格式合并、Ozone 小文件利器。</p>
</li>
</ol>
<p>HAR 把历史数据进行归档，读取它还多了一层索引查找，比较鸡肋，不怎么好用。Ozone 目前技术较新，对Hadoop 有版本要求，需要Hadoop 3.x 版本。因此平常更多是利用 Hive 来针对 parquet 文件格式合并</p>
<h4 id="处理方法"><a href="#处理方法" class="headerlink" title="处理方法"></a>处理方法</h4><p>什么是小文件？小文件是怎么来的？可以参阅<a href="https://mp.weixin.qq.com/s/QdaNFfq37bBlAQtNDGLmVw" target="_blank" rel="noopener">如何在Hadoop中处理小文件</a></p>
<p>利用 HAR 方法处理小文件可以参阅<a href="https://mp.weixin.qq.com/s/rlM3ZPmWz-ZT72w59lvrRw" target="_blank" rel="noopener">0508-如何使用Hadoop的Archive处理小文件</a></p>
<p>利用 Hive 方法处理小文件可以参阅<a href="https://mp.weixin.qq.com/s/xw6MjNvpI97m0aygyW1HaA" target="_blank" rel="noopener">如何在Hadoop中处理小文件-续</a>、<a href="https://mp.weixin.qq.com/s/ki2k9fBPco6PwMZEzff60A" target="_blank" rel="noopener">0704-5.16.2-如何使用Hive合并小文件</a></p>
<p>利用 Impala 方法处理小文件可以参阅<a href="https://mp.weixin.qq.com/s/BTBqUqfbczq08LKRCuC_Ig" target="_blank" rel="noopener">如何使用Impala合并小文件</a></p>
<p>利用 Ozone 来解决<a href="https://mp.weixin.qq.com/s/x_xCjSs5Aed8z6moxp_uYQ" target="_blank" rel="noopener">Hadoop小文件利器Ozone</a></p>
<p>在有赞使用 SparkSQL 中提到存在小文件现象，采用社区 <a href="https://issues.apache.org/jira/browse/SPARK-24940" target="_blank" rel="noopener">SPARK-24940</a>方式处理，借助 SQL hint 的方式合并小文件。参阅<a href="https://www.iteblog.com/archives/2501.html" target="_blank" rel="noopener">Spark SQL 查询中 Coalesce 和 Repartition 暗示（Hint）</a></p>
<p>利用 Spark 处理小文件跟 Spark SQL 也是差不多的，通过 Spark 的 coalesce() 方法和 repartition() 方法，设置并行个数可以通过输入文件的总大小和期望输出文件的大小进行预计算而得。</p>
<h4 id="小文件寻找"><a href="#小文件寻找" class="headerlink" title="小文件寻找"></a>小文件寻找</h4><p>知道处理方法后，问题就是如何寻找小文件，最先之前是利用<code>haddop fs -count &lt;path&gt;</code>人工肉眼寻找</p>
<p>高级操作还是得借用 HDFS 的元数据 FsImage 来分析，感兴趣可以自行参阅下方博客</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/TWGROH7TRgXRwl3TBlGLhw" target="_blank" rel="noopener">0464-如何离线分析HDFS的FsImage查找集群小文件</a></li>
<li><a href="https://mp.weixin.qq.com/s/RT8tCF_ZEvMYewjGgABSYQ" target="_blank" rel="noopener">hdfs如何通过解析fsimage来监控目录</a></li>
</ul>
<hr>
<p>参考链接</p>
<ul>
<li><a href="http://bigdatadecode.club/HDFS-little-file-action.html" target="_blank" rel="noopener">HDFS小文件合并实战</a></li>
<li><a href="https://juejin.im/post/5c3f2713f265da61285a5b75#heading-8" target="_blank" rel="noopener">SparkSQL 在有赞的实践</a></li>
<li><a href="https://blog.csdn.net/weixin_37944880/article/details/86694829" target="_blank" rel="noopener">Spark小文件合并</a></li>
</ul>
]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title>Hive set 命令使用</title>
    <url>/2020/01/27/Hive-set/</url>
    <content><![CDATA[<p>Hive set 命令使用</p>
<hr>
<h4 id="转载来源"><a href="#转载来源" class="headerlink" title="转载来源"></a>转载来源</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">作者：lwf006164</span><br><span class="line">链接：https:&#x2F;&#x2F;blog.csdn.net&#x2F;lwf006164&#x2F;article&#x2F;details&#x2F;96754526</span><br><span class="line">来源：Hive篇.set命令使用</span><br></pre></td></tr></table></figure>

<h4 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h4><p>Hive 命令行下执行 set 命令，仅当前会话有效</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 设置显示当前使用的数据库</span><br><span class="line">set hive.cli.print.current.db&#x3D;true;</span><br><span class="line"></span><br><span class="line">#  设置显示表名</span><br><span class="line">set hive.cli.print.header&#x3D;true;</span><br></pre></td></tr></table></figure>

<p>Hive 脚本中配置 set 命令，当前机器用户有效   </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 进入编辑模式，在执行 hive 命令进入 shell 命令行的时候默认会加载这个脚本</span><br><span class="line">vi ~&#x2F;.hiverc</span><br><span class="line"></span><br><span class="line"># 将下面的两行命令添加到该文件中</span><br><span class="line"># 设置显示当前使用的数据库</span><br><span class="line">set hive.cli.print.current.db&#x3D;true;</span><br><span class="line"></span><br><span class="line"># 设置显示表名</span><br><span class="line">set hive.cli.print.header&#x3D;true;</span><br></pre></td></tr></table></figure>

<p>查看 Hive 历史操作命令集可以浏览 <code>~/.hivehistory</code></p>
]]></content>
      <categories>
        <category>Hive</category>
      </categories>
  </entry>
  <entry>
    <title>HDFS 中 atime 与 mtime 解析</title>
    <url>/2020/01/27/HDFS-atime-ctime/</url>
    <content><![CDATA[<p>HDFS 中 atime 和 mtime</p>
<hr>
<h4 id="转载来源"><a href="#转载来源" class="headerlink" title="转载来源"></a>转载来源</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">作者：混绅士</span><br><span class="line">链接：http:&#x2F;&#x2F;bigdatadecode.club&#x2F;HDFS%E4%B8%ADatime%E5%92%8Cmtime.html</span><br><span class="line">来源：HDFS中atime与mtime解析</span><br></pre></td></tr></table></figure>

<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>如果不知道 atime、mtime 的建议先了解下 Linux 中 atime 和 mtime，这样有助于学习 HDFS 中 atime 与 mtime</p>
<h4 id="查看"><a href="#查看" class="headerlink" title="查看"></a>查看</h4><p>转载文章中未介绍如何查看 HDFS atime、mtime，因此说明下。</p>
<p>跟 Linux 一样，也是使用 stat 命令来查看</p>
<p>用法: <code>hadoop fs -stat [format] &lt;path&gt; ...</code></p>
<blockquote>
<p>以指定的格式打印有关 <path></path> 处的文件/目录的统计信息。格式接受八进制（％a）和符号（％A），字节（％b）文件大小，类型（％F），所有者组名称（％g），名称（％n），块大小（％o），复制（％r），<br>所有者用户名（％u），访问日期（％x，％X）和修改日期（％y，％Y）。％x和％y将UTC日期显示为“yyyy-MM-dd HH:mm:ss”，并且％X和％Y显示自1970年1月1日UTC以来的毫秒数。如果未指定格式，则默认使用％y。 —— <a href="https://www.jianshu.com/p/49d89c197310" target="_blank" rel="noopener">https://www.jianshu.com/p/49d89c197310</a></p>
</blockquote>
<p>如查看 HBase 根目录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hadoop fs -stat &quot;type:%F perm:%a %u:%g size:%b mtime:%y atime:%x name:%n&quot; &#x2F;hbase</span><br><span class="line">type:directory perm:755 hbase:hbase size:0 mtime:2020-01-14 12:54:42 atime:1970-01-01 00:00:00 name:hbase</span><br></pre></td></tr></table></figure>

<h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><p>先来了解下 HDFS 中的这 atime 与 mtime 的变化规则</p>
<p>在看代码之前，先想下 atime 和 mtime 有可能在哪些地方会修改</p>
<p>hdfs底层api对文件都有哪些操作？</p>
<p>无非就是读写两种操作，读肯定修改的是atime，写修改的是mtime，是否修改atime还得确认。<br>这里我们还漏掉一种操作，那就是mv。</p>
<h5 id="atime"><a href="#atime" class="headerlink" title="atime"></a>atime</h5><p>去年写过一篇文章 <a href="http://bigdatadecode.club/HDFS%20read%E8%A7%A3%E6%9E%90.html" target="_blank" rel="noopener">HDFS read解析(一)之Open文件流</a>介绍HDFS读操作流程，这里就不再累赘了，直接贴出关键代码。(有兴趣的同学可以自行查看)</p>
<blockquote>
<p>这里还是简单说下读的流程：客户端向NN发起一个读请求，NN将相关的block信息返回给客户端，客户端再与对应的DN建立连接读取信息。<br>  在这个过程中，先与NN交互然后再与DN交互，那么每个文件的atime相关元数据信息都存在NN中，那么atime相关的修改也肯定发生在与NN交互的这个过程中。</p>
</blockquote>
<p>从之前的文章中可知入口函数是 <code>FSNamesystem.getBlockLocations</code>，关键代码如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">LocatedBlocks getBlockLocations(String clientMachine, String srcArg,</span><br><span class="line">    long offset, long length) throws IOException &#123;</span><br><span class="line">...</span><br><span class="line">  if (res.updateAccessTime()) &#123;</span><br><span class="line">    String src &#x3D; srcArg;</span><br><span class="line">    writeLock();</span><br><span class="line">    final long now &#x3D; now();</span><br><span class="line">    try &#123;</span><br><span class="line">      final INodesInPath iip &#x3D; dir.resolvePath(pc, src);</span><br><span class="line">      src &#x3D; iip.getPath();</span><br><span class="line">      INode inode &#x3D; iip.getLastINode();</span><br><span class="line">      &#x2F;&#x2F; 再次判断是否可以更新atime</span><br><span class="line">      boolean updateAccessTime &#x3D; inode !&#x3D; null &amp;&amp;</span><br><span class="line">          now &gt; inode.getAccessTime() + getAccessTimePrecision();</span><br><span class="line">      if (!isInSafeMode() &amp;&amp; updateAccessTime) &#123;</span><br><span class="line">        &#x2F;&#x2F; 设置atime</span><br><span class="line">        boolean changed &#x3D; FSDirAttrOp.setTimes(dir,</span><br><span class="line">            inode, -1, now, false, iip.getLatestSnapshotId());</span><br><span class="line">        if (changed) &#123;</span><br><span class="line">          getEditLog().logTimes(src, -1, now);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; catch (Throwable e) &#123;</span><br><span class="line">      LOG.warn(&quot;Failed to update the access time of &quot; + src, e);</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">      writeUnlock(operationName);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>res.updateAccessTime()</code> 决定了是否更新atime，其值是在getBlockLocationsInt中赋值的，代码如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">boolean updateAccessTime &#x3D; isAccessTimeSupported() &amp;&amp; !isInSafeMode()</span><br><span class="line">        &amp;&amp; !iip.isSnapshot()</span><br><span class="line">        &amp;&amp; now &gt; inode.getAccessTime() + getAccessTimePrecision();</span><br></pre></td></tr></table></figure>

<p>其中关键的因素是<code>isAccessTimeSupported()</code>和<code>getAccessTimePrecision()</code>，这个两个方法都与<code>accessTimePrecision</code>有关，<br>此值是由<code>dfs.namenode.accesstime.precision</code>设置的，默认是3600000。</p>
<p>当此值大于0，<code>isAccessTimeSupported()</code>返回true，<code>getAccessTimePrecision()</code>得到的值是<code>dfs.namenode.accesstime.precision</code>的值。</p>
<p>从上述代码中可以看出更新atime的一个条件是<strong>两次读取间隔相隔<code>dfs.namenode.accesstime.precision</code>秒，默认是1小时。</strong></p>
<p>这里遗留两个问题:</p>
<ol>
<li>新建文件时atime如何赋值</li>
<li>修改文件内容时atime如何赋值</li>
</ol>
<p>关于这个两个问题我在下一节在写流程中解答。请继续向下看</p>
<h5 id="mtime"><a href="#mtime" class="headerlink" title="mtime"></a>mtime</h5><p>同样去年也写过一篇关于写的文章<a href="http://bigdatadecode.club/HDFS%20write%E8%A7%A3%E6%9E%90.html" target="_blank" rel="noopener">HDFS write解析</a>介绍HDFS读操作流程，这里就不再累赘了，直接贴出关键代码。(有兴趣的同学可以自行查看)</p>
<p>写相关的操作包括create、close和append<br>写文件有两种方式，一种是调用<code>create(Path)</code>方法，另一种是调用<code>append(Path)</code>方法</p>
<p><strong>create</strong></p>
<p>通过调用<code>create(Path)</code>，最终会调用<code>FSDirectory.addFile</code>方法，<br>在此方法中会new一个<code>INodeFile</code>，此时会设置 mtime 和 atime 为同一个值，代码如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INodesInPath addFile(INodesInPath existing, String localName, PermissionStatus</span><br><span class="line">    permissions, short replication, long preferredBlockSize,</span><br><span class="line">    String clientName, String clientMachine)</span><br><span class="line">  throws FileAlreadyExistsException, QuotaExceededException,</span><br><span class="line">    UnresolvedLinkException, SnapshotAccessControlException, AclException &#123;</span><br><span class="line">  long modTime &#x3D; now();</span><br><span class="line">  INodeFile newNode &#x3D; newINodeFile(allocateNewInodeId(), permissions, modTime,</span><br><span class="line">      modTime, replication, preferredBlockSize);</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>有打开一个文件就有关闭一个文件，接下来看下关闭文件时 atime 和 mtime 会有什么变化。</p>
<p><strong>close</strong></p>
<p><code>closeFile()</code>在<code>finalizeINodeFileUnderConstruction</code>中调用，在此方法中会设置mtime，看下代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private void finalizeINodeFileUnderConstruction(String src,</span><br><span class="line">    INodeFile pendingFile, int latestSnapshot) throws IOException &#123;</span><br><span class="line">...</span><br><span class="line">  pendingFile.toCompleteFile(now());</span><br><span class="line">...</span><br><span class="line">  closeFile(src, pendingFile);</span><br><span class="line">...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; INodeFile.java</span><br><span class="line">public INodeFile toCompleteFile(long mtime) &#123;</span><br><span class="line">  Preconditions.checkState(isUnderConstruction(),</span><br><span class="line">      &quot;file is no longer under construction&quot;);</span><br><span class="line">  FileUnderConstructionFeature uc &#x3D; getFileUnderConstructionFeature();</span><br><span class="line">  if (uc !&#x3D; null) &#123;</span><br><span class="line">    assertAllBlocksComplete();</span><br><span class="line">    removeFeature(uc);</span><br><span class="line">    this.setModificationTime(mtime);</span><br><span class="line">  &#125;</span><br><span class="line">  return this;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从代码中可以看出，close文件时只对mtime进行了修改。</p>
<p><strong>append</strong></p>
<p>append只是打开一个文件流，并不会修改mtime或者atime，只是在close的时候修改mtime</p>
<h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p><strong>atime</strong></p>
<ol>
<li>两次读间隔大于默认的1小时时，更新atime。默认间隔通过<code>dfs.namenode.accesstime.precision</code>控制。</li>
<li>新建一个文件时atime赋值为当前时间(注意，当关闭一个文件时atime不会修改)</li>
</ol>
<p><strong>mtime</strong></p>
<ol>
<li>新建一个文件时mtime赋值为当前时间(同时会修改atime)</li>
<li>关闭一个文件时mtime赋值为当前时间(此时并不会修改atime)</li>
</ol>
]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title>Kafka 零拷贝原理</title>
    <url>/2020/01/24/Kafka-Zero-Copy/</url>
    <content><![CDATA[<p>从字面意思理解就是数据不需要来回的拷贝，大大提升了系统的性能</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>“零拷贝”是指计算机操作的过程中，CPU 不需要为数据在内存之间的拷贝消耗资源。而它通常是指计算机在网络上发送文件时，不需要将文件内容拷贝到用户空间（User Space）而直接在内核空间（Kernel Space）中传输到网络的方式。</p>
<p>零拷贝并不是不需要拷贝，而是减少不必要的拷贝次数。通常是说在 IO 读写过程中</p>
<h4 id="缓冲区"><a href="#缓冲区" class="headerlink" title="缓冲区"></a>缓冲区</h4><p>缓冲区是所有 I/O 的基础，I/O 讲的无非就是把数据移进或移出缓冲区</p>
<p>进程执行I/O操作，就是向操作系统发出请求，让它要么把缓冲区的数据排干(写)，要么填充缓冲区(读)</p>
<p>下面看一个 Java 进程发起读请求加载数据大致的流程图</p>
<p><img src="/images/blog/2020-01-24-1.png" alt></p>
<ol>
<li><p>进程发起读请求之后，内核接收到读请求之后，会先检查内核空间中是否已经存在进程所需要的数据，<br>如果已经存在，则直接把数据拷贝给进程的缓冲区</p>
</li>
<li><p>如果内核缓冲区没有命中随即向磁盘控制器发出命令，要求从磁盘读取数据，磁盘控制器把数据直接写入内核read缓冲区，这一步通过DMA完成</p>
</li>
<li><p>接下来内核将数据拷贝到进程的缓冲区</p>
</li>
</ol>
<p>DMA 是一种硬件和软件之间的数据传输的技术，且 DMA 进行数据传输的过程中几乎不需要 CPU 参与</p>
<h4 id="虚拟内存"><a href="#虚拟内存" class="headerlink" title="虚拟内存"></a>虚拟内存</h4><p>所有现代操作系统都使用虚拟内存，使用虚拟的地址取代物理地址，这样有两个好处:</p>
<ol>
<li>一个以上的虚拟地址可以指向同一个物理内存地址</li>
<li>虚拟内存空间可大于实际可用的物理地址</li>
</ol>
<p>利用第一条特性可以把内核空间地址和用户空间的虚拟地址映射到同一个物理地址，<br>这样DMA就可以填充对内核和用户空间进程同时可见的缓冲区了，大致如下图所示:</p>
<p><img src="/images/blog/2020-01-24-3.png" alt></p>
<h4 id="零拷贝技术"><a href="#零拷贝技术" class="headerlink" title="零拷贝技术"></a>零拷贝技术</h4><p>本人水平有限，零拷贝技术可以阅读下方链接</p>
<ul>
<li><a href="https://www.jianshu.com/p/497e7640b57c" target="_blank" rel="noopener">零拷贝的原理及Java实现</a></li>
<li><a href="https://blog.csdn.net/lzb348110175/article/details/100853071" target="_blank" rel="noopener">Kafka 的零拷贝技术</a></li>
<li><a href="https://mp.weixin.qq.com/s/otuUvACiVDafGgDl6xNd2A" target="_blank" rel="noopener">面试被问到“零拷贝”！你真的理解吗？</a></li>
<li><a href="https://mp.weixin.qq.com/s/vaKRVvfUnFjhHzfkwZDbKQ" target="_blank" rel="noopener">图解Kafka的零拷贝技术到底有多牛？</a></li>
<li><a href="https://blog.csdn.net/ljheee/article/details/99652448" target="_blank" rel="noopener">Kafka零拷贝</a></li>
</ul>
<p>Kafka 通过 sendfile 实现的零拷贝 I/O，理想状态下的零拷贝I/O需要接触 DMA 完成。</p>
]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
  </entry>
  <entry>
    <title>HDFS Federation 发展历程</title>
    <url>/2020/01/22/HDFS-Federation/</url>
    <content><![CDATA[<p>HDFS Federation（联邦）相关知识，如 HDFS Router-based Federation</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>在阅读一些 Hadoop3 新特性的文章中，我看到有提”多 NameNode”，然后简单某度搜索到的链接都是提 HDFS Federation（联邦）</p>
<p><img src="/images/blog/2020-01-22-2.png" alt></p>
<p>下意识以为”多 NameNode”指的是 Federation 正式在 Hadoop3 上实现可用，在一次面试过程中跟面试官扯 Hadoop3 Federation 新特性，人家反问一句这不是 Hadoop2 就有了吗。。。我瞬间懵了。</p>
<p>事后回顾到才知道 HDFS RBF 才是 Hadoop3 Router-based Federation 新特性。”多 NameNode” 是指 HDFS HA 支持多 Standby 节点机制 <a href="https://issues.apache.org/jira/browse/HDFS-6440" target="_blank" rel="noopener">HDFS-6440</a></p>
<p>在配置平衡策略时看到有个”BlockPool”，想到这是跟 Federation 有关，因此想整理一下对这方面的知识程度</p>
<p><img src="/images/blog/2020-01-22-1.png" alt></p>
<h4 id="HDFS-Federation"><a href="#HDFS-Federation" class="headerlink" title="HDFS Federation"></a>HDFS Federation</h4><p>对 Federation 学习个人大部分是通过以下链接学习了解，大家可以按链接顺序自行学习。</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/8VrQFIuQUVEa20JfG2EZkQ" target="_blank" rel="noopener">HDFS Federation（联邦）简介</a></li>
<li><a href="https://mp.weixin.qq.com/s/YCzhYCv-fcbOHfM9wvMchg" target="_blank" rel="noopener">如何通过CM为HDFS启用Federation</a></li>
<li><a href="https://mp.weixin.qq.com/s/X-8DmvgAsLsFEZADYGklBA" target="_blank" rel="noopener">如何通过CM禁用Federation</a></li>
<li><a href="https://mp.weixin.qq.com/s/2QDnm54_ObpXMyD952GovQ" target="_blank" rel="noopener">HDFS Router-based Federation</a></li>
<li><a href="https://mp.weixin.qq.com/s/BPfhDxiSaUkoOSyrMuAHIw" target="_blank" rel="noopener">Router-Based HDFS Federation 在滴滴大数据的应用</a></li>
<li><a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs-rbf/HDFSRouterFederation.html" target="_blank" rel="noopener">HDFS Router-based Federation 官方文档</a></li>
<li><a href="https://mp.weixin.qq.com/s/0qY-LMHQT-yGJPh-ybbZUg" target="_blank" rel="noopener">Apache Hadoop 的 HDFS federation 前世今生</a></li>
</ul>
<p>由于 Router-based HDFS federation 还算比较新的特性，所以社区分了几个阶段修复或添加了一些新的功能，<br>社区在 2.9 和 3.0 版本中发布 HDFS RBF 方案解决统一命名空间问题，参见 <a href="https://issues.apache.org/jira/browse/HDFS-10467" target="_blank" rel="noopener">HDFS-10467</a>，<br>在 Apache Hadoop 3.2.0 版本修复或添加了一些功能，参见 <a href="https://issues.apache.org/jira/browse/HDFS-12615" target="_blank" rel="noopener">HDFS-12615</a>，<br>以及 Router-based HDFS federation 稳定性相关的 ISSUE <a href="https://issues.apache.org/jira/browse/HDFS-13891" target="_blank" rel="noopener">HDFS-13891</a>，<br>这个 ISSUE 可能会在 Apache Hadoop 3.3.0 版本发布(过往记忆大数据的<a href="https://mp.weixin.qq.com/s/0qY-LMHQT-yGJPh-ybbZUg" target="_blank" rel="noopener">Apache Hadoop 的 HDFS federation 前世今生</a>文末提的。。。不知道会不会发布)。</p>
<p>小米在<a href="https://mp.weixin.qq.com/s/2QDnm54_ObpXMyD952GovQ" target="_blank" rel="noopener">HDFS Router-based Federation</a>文末提了 Rebalance 社区没有实现跨子集群的迁移。</p>
<p>翻看官方文档的确有提不支持跨子集群的迁移:</p>
<blockquote>
<p>Some operations are not available in Router-based federation. The Router throws exceptions for those. Examples users may encounter include the following.</p>
<ul>
<li>Rename file/folder in two different nameservices.</li>
<li>Copy file/folder in two different nameservices.</li>
<li>Write into a file/folder being rebalanced.</li>
</ul>
</blockquote>
<p>但是跨集群的数据平衡已经在做了，可以参见 <a href="https://issues.apache.org/jira/browse/HDFS-13123" target="_blank" rel="noopener">HDFS-13123</a>。<br>小米自己也说内部实现了跨子集群的 Federation Rename，可以在未来将这一特性合并到社区的 Rebalance 方案中，期待ing。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/CEIKrDZrR6EZIKJ9x2Se0Q" target="_blank" rel="noopener">HDFS3.2升级在滴滴的实践</a></li>
<li><a href="http://www.voidcn.com/article/p-umrtxeag-pk.html" target="_blank" rel="noopener">HDFS HA支持多Standby节点机制</a></li>
</ul>
]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title>HBase 表存储层次</title>
    <url>/2020/01/21/HBase-Table/</url>
    <content><![CDATA[<p>对 HBase 表存储层次梳理</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>HBase 表是由一个或者多个 region 组成，一个 region 由一个或者多个列蔟组成，一个列蔟由一个 store 组成，一个 store 由唯一 memstore 加上一个或者多个 HFile 组成，HFile 又是由 block 组成，而 block 是由多个 cell 组成。</p>
<h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><h5 id="Region"><a href="#Region" class="headerlink" title="Region"></a>Region</h5><p>Region 类似于数据库的分片和分区的概念，每个 Region 负责一小部分 Rowkey 范围的数据的读写和维护，这样即使是一张巨大的表，由于被切割到不同的 Region，访问起来的时延也很低。<br>Region 包含了对应的起始行到结束行的所有信息。</p>
<p><img src="/images/blog/2020-01-21-1.png" alt></p>
<h5 id="Store"><a href="#Store" class="headerlink" title="Store"></a>Store</h5><p>Store 是存储所有写入表中信息的实体，并且当数据需要从表中读取时也将被用到。A Store is the same thing as a ColumnFamily. 一个 Store 等价一个列簇 </p>
<h5 id="Memstore"><a href="#Memstore" class="headerlink" title="Memstore"></a>Memstore</h5><p>HBase 是基于 LSM-Tree 模型的，所有的数据更新插入操作都首先写入 Memstore 中（同时会顺序写到日志HLog中），达到指定大小之后再将这些修改操作批量写入磁盘，生成一个新的 HFile 文件，这种设计可以极大地提升 HBase 的写入性能。每一个 Store 里面都有一个 Memstore, MemStore 保存对 Store 的内存修改。修改是 Cells / KeyValues。</p>
<h5 id="StoreFile"><a href="#StoreFile" class="headerlink" title="StoreFile"></a>StoreFile</h5><p>StoreFile 为 HFile 上的一层类封装。A StoreFile is a facade of HFile.</p>
<h5 id="HFile"><a href="#HFile" class="headerlink" title="HFile"></a>HFile</h5><p>HFile 对应的列蔟。当内存写满必须要刷新到磁盘时，HFile 就会被创建，所以 HFile 存放的是某个时刻的  MemStore 刷写时的快照，一个完整的行的数据可能存放在多个 HFile 里。 随着时间推移，HFile 最终会被压缩（即合并）成大文件。HFile 是 HBase 用来存储数据的文件格式。</p>
<p>HFile 由不同种类的 block 块组成（索引块和数据块）。HFile 存储在 HDFS 上，因此获得 HDFS 的持久化和多副本的优势。</p>
<h5 id="Block"><a href="#Block" class="headerlink" title="Block"></a>Block</h5><p>HFile 由 block 组成，不要与 HDFS 的 block 混淆了。一个 HDFS 数据块可以包含很多 HFile block。HFile block 通常在 8kb 和 1MB 之间，默认大小是 64kb(在建表语句中可以通过参数 BlockSize 指定)。如果一个表配置了压缩选项，HBase 仍会产生了 64kb 大小 block，然后压缩 block。根据数据的大小以及压缩格式，压缩后的 block 存储在磁盘的大小也不一样。</p>
<p>每个 block 的大小可以在创建表列簇的时候通过参数 <code>blocksize ＝&gt; &#39;65535&#39;</code> 进行指定，默认为 64k，大号的 Block 有利于顺序 Scan，小号 Block 利于随机查询，因而需要权衡。</p>
<p>如果将 block size 配置得很小，将会产生过多的 HFile block 索引，这将会给内存造成压力 （《HBase 实战》P27 《HBase应用架构》P19）</p>
<h5 id="KeyValue"><a href="#KeyValue" class="headerlink" title="KeyValue"></a>KeyValue</h5><p><img src="/images/blog/2020-01-21-2.png" alt></p>
<p>KeyValue 是 HBase 数据存储的核心，KeyValue 并不是简单的 KV 数据对，是一个具有复杂元素的结构体</p>
<p>由 keylength、valuelength、key、value 四个部分组成，其中 Key 又由 Row Length、Row、Column Family Length、Column Family、Column Qualifier、TimeStamp、Key Type 七部分组成。 </p>
<table>
<thead>
<tr>
<th>字段</th>
<th>说明</th>
<th>长度</th>
</tr>
</thead>
<tbody><tr>
<td>Key Length</td>
<td>存储Key的长度</td>
<td>4个字节</td>
</tr>
</tbody></table>
<p>Value Length | 存储Value的长度 | 4个字节</p>
<p>Row Length | 存储 Row 的长度，即 Rowkey 的长度 |  2个字节</p>
<p>Row | 存储 Rowkey | Row Length</p>
<p>Column Family Length | 存储列簇 Column Family 的长度 | 1个字节</p>
<p>Column Family | 存储 Column Family 实际内容 | Column Family Length</p>
<p>Column Qualifier | 存储 Column Qualifier 对应的数据 |  -          </p>
<p>Time Stamp | 存储时间戳 TimeStamp | 8个字节</p>
<p>Key Type | 存储 Key 类型 KeyType | 1个字节</p>
<p>Value | 存储单元格 Cell 对应的实际的值Value | Value Length</p>
<p>“key type” 字段代表不同可能的HBase操作</p>
<ul>
<li>Put</li>
<li>Delete</li>
<li>DeleteColumn</li>
<li>DeleteFamily</li>
</ul>
<p>KeyValue 保存了这个 value 的所有信息，这也是为什么面向列的原因</p>
<blockquote>
<p>Q: Row，Column Family 都有一个 length 标志，为什么 Column Qualifier 没有呢？<br>A: 由于 Key 中其它的字段占用大小已经知道，并且知道整个 Key 的大小，因此没有存储 Column Qualifier 的大小。 Column Qualifier 的 Length 可以由 key length 减去其他 length 得到，这样可以减少一定的存储量。</p>
</blockquote>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/bitcarmanlee/article/details/78979836" target="_blank" rel="noopener">Hbase KeyValue结构详解</a></li>
<li><a href="https://blog.csdn.net/ping_hu/article/details/77115998" target="_blank" rel="noopener">HBase的KeyValue分析</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>Spark-Submit 参数说明</title>
    <url>/2020/01/20/Spark-submit/</url>
    <content><![CDATA[<p>spark-submit 详细参数说明</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>部分初级开发者需要使用 Spark-submit 提交 spark 作业到 yarn 上，经常问些参数设置的问题，<br>基于此需求梳理下</p>
<h4 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h4><p>在命令行输入 <code>spark-submit -h</code>，可以看到 spark-submit 的所用参数如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ bin&#x2F;spark-submit -h</span><br><span class="line">Usage: spark-submit [options] &lt;app jar | python file | R file&gt; [app arguments]</span><br><span class="line">Usage: spark-submit --kill [submission ID] --master [spark:&#x2F;&#x2F;...]</span><br><span class="line">Usage: spark-submit --status [submission ID] --master [spark:&#x2F;&#x2F;...]</span><br><span class="line">Usage: spark-submit run-example [options] example-class [example args]</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">  --master MASTER_URL         spark:&#x2F;&#x2F;host:port, mesos:&#x2F;&#x2F;host:port, yarn,</span><br><span class="line">                              k8s:&#x2F;&#x2F;https:&#x2F;&#x2F;host:port, or local (Default: local[*]).</span><br><span class="line">  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (&quot;client&quot;) or</span><br><span class="line">                              on one of the worker machines inside the cluster (&quot;cluster&quot;)</span><br><span class="line">                              (Default: client).</span><br><span class="line">  --class CLASS_NAME          Your application&#39;s main class (for Java &#x2F; Scala apps).</span><br><span class="line">  --name NAME                 A name of your application.</span><br><span class="line">  --jars JARS                 Comma-separated list of jars to include on the driver</span><br><span class="line">                              and executor classpaths.</span><br><span class="line">  --packages                  Comma-separated list of maven coordinates of jars to include</span><br><span class="line">                              on the driver and executor classpaths. Will search the local</span><br><span class="line">                              maven repo, then maven central and any additional remote</span><br><span class="line">                              repositories given by --repositories. The format for the</span><br><span class="line">                              coordinates should be groupId:artifactId:version.</span><br><span class="line">  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while</span><br><span class="line">                              resolving the dependencies provided in --packages to avoid</span><br><span class="line">                              dependency conflicts.</span><br><span class="line">  --repositories              Comma-separated list of additional remote repositories to</span><br><span class="line">                              search for the maven coordinates given with --packages.</span><br><span class="line">  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place</span><br><span class="line">                              on the PYTHONPATH for Python apps.</span><br><span class="line">  --files FILES               Comma-separated list of files to be placed in the working</span><br><span class="line">                              directory of each executor. File paths of these files</span><br><span class="line">                              in executors can be accessed via SparkFiles.get(fileName).</span><br><span class="line"></span><br><span class="line">  --conf PROP&#x3D;VALUE           Arbitrary Spark configuration property.</span><br><span class="line">  --properties-file FILE      Path to a file from which to load extra properties. If not</span><br><span class="line">                              specified, this will look for conf&#x2F;spark-defaults.conf.</span><br><span class="line"></span><br><span class="line">  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).</span><br><span class="line">  --driver-java-options       Extra Java options to pass to the driver.</span><br><span class="line">  --driver-library-path       Extra library path entries to pass to the driver.</span><br><span class="line">  --driver-class-path         Extra class path entries to pass to the driver. Note that</span><br><span class="line">                              jars added with --jars are automatically included in the</span><br><span class="line">                              classpath.</span><br><span class="line"></span><br><span class="line">  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).</span><br><span class="line"></span><br><span class="line">  --proxy-user NAME           User to impersonate when submitting the application.</span><br><span class="line">                              This argument does not work with --principal &#x2F; --keytab.</span><br><span class="line"></span><br><span class="line">  --help, -h                  Show this help message and exit.</span><br><span class="line">  --verbose, -v               Print additional debug output.</span><br><span class="line">  --version,                  Print the version of current Spark.</span><br><span class="line"></span><br><span class="line"> Cluster deploy mode only:</span><br><span class="line">  --driver-cores NUM          Number of cores used by the driver, only in cluster mode</span><br><span class="line">                              (Default: 1).</span><br><span class="line"></span><br><span class="line"> Spark standalone or Mesos with cluster deploy mode only:</span><br><span class="line">  --supervise                 If given, restarts the driver on failure.</span><br><span class="line">  --kill SUBMISSION_ID        If given, kills the driver specified.</span><br><span class="line">  --status SUBMISSION_ID      If given, requests the status of the driver specified.</span><br><span class="line"></span><br><span class="line"> Spark standalone and Mesos only:</span><br><span class="line">  --total-executor-cores NUM  Total cores for all executors.</span><br><span class="line"></span><br><span class="line"> Spark standalone and YARN only:</span><br><span class="line">  --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode,</span><br><span class="line">                              or all available cores on the worker in standalone mode)</span><br><span class="line"></span><br><span class="line"> YARN-only:</span><br><span class="line">  --queue QUEUE_NAME          The YARN queue to submit to (Default: &quot;default&quot;).</span><br><span class="line">  --num-executors NUM         Number of executors to launch (Default: 2).</span><br><span class="line">                              If dynamic allocation is enabled, the initial number of</span><br><span class="line">                              executors will be at least NUM.</span><br><span class="line">  --archives ARCHIVES         Comma separated list of archives to be extracted into the</span><br><span class="line">                              working directory of each executor.</span><br><span class="line">  --principal PRINCIPAL       Principal to be used to login to KDC, while running on</span><br><span class="line">                              secure HDFS.</span><br><span class="line">  --keytab KEYTAB             The full path to the file that contains the keytab for the</span><br><span class="line">                              principal specified above. This keytab will be copied to</span><br><span class="line">                              the node running the Application Master via the Secure</span><br><span class="line">                              Distributed Cache, for renewing the login tickets and the</span><br><span class="line">                              delegation tokens periodically.</span><br></pre></td></tr></table></figure>

<p>相关意义如下:</p>
<table>
<thead>
<tr>
<th>参数名</th>
<th>默认值</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td>–master</td>
<td>local[*]</td>
<td>master 的地址，提交任务到哪里执行</td>
</tr>
<tr>
<td>–deploy-mode</td>
<td>client</td>
<td>在本地 (client) 启动 driver 或在 cluster 上启动，默认是 client</td>
</tr>
<tr>
<td>–class</td>
<td>-</td>
<td>应用程序的主类，仅针对 java 或 scala 应用</td>
</tr>
<tr>
<td>–name</td>
<td>-</td>
<td>应用程序的名称</td>
</tr>
<tr>
<td>–jars</td>
<td>-</td>
<td>用逗号分隔的本地 jar 包，设置后，这些 jar 将包含在 driver 和 executor 的 classpath 下</td>
</tr>
<tr>
<td>–packages</td>
<td>-</td>
<td>包含在 driver 和 executor 的 classpath 中的 jar 的 maven 坐标</td>
</tr>
<tr>
<td>–exclude-packages</td>
<td>-</td>
<td>为了避免冲突 而指定不包含的 package</td>
</tr>
<tr>
<td>–repositories</td>
<td>-</td>
<td>逗号分隔的其他远程存储库列表，用于搜索用–packages给定的maven坐标</td>
</tr>
<tr>
<td>–conf PROP=VALUE</td>
<td>-</td>
<td>指定 spark 配置属性的值，例如 -conf spark.executor.extraJavaOptions=”-XX:MaxPermSize=256m”</td>
</tr>
<tr>
<td>–properties-file</td>
<td>-</td>
<td>加载的配置文件，默认为 conf/spark-defaults.conf</td>
</tr>
<tr>
<td>–driver-memory</td>
<td>1024M</td>
<td>Driver内存</td>
</tr>
<tr>
<td>–driver-cores</td>
<td>1</td>
<td>Driver 的核数。只能在 client 模式下使用</td>
</tr>
<tr>
<td>–driver-java-options</td>
<td>-</td>
<td>传给 driver 的额外的 Java 选项</td>
</tr>
<tr>
<td>–driver-library-path</td>
<td>-</td>
<td>传给 driver 的额外的库路径</td>
</tr>
<tr>
<td>–driver-class-path</td>
<td>-</td>
<td>传给 driver 的额外的类路径</td>
</tr>
<tr>
<td>–total-executor-cores</td>
<td>-</td>
<td>所有 executor 总共的核数。仅仅在 mesos 或者 standalone 下使用</td>
</tr>
<tr>
<td>–num-executors</td>
<td>2</td>
<td>启动的 executor 数量。在 yarn 下使用</td>
</tr>
<tr>
<td>–executor-core</td>
<td>1</td>
<td>每个 executor 的核数。在 yarn 或者 standalone下使用</td>
</tr>
<tr>
<td>–executor-memory</td>
<td>1G</td>
<td>每个 executor 的内存</td>
</tr>
<tr>
<td>–queue</td>
<td>“default”</td>
<td>提交到 Yarn 上队列</td>
</tr>
</tbody></table>
<h4 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h4><p>将官方的 example 包使用 client 模式提交到 Yarn 上</p>
<p><code>spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode client --driver-memory 4g --num-executors 2 --executor-memory 2g --executor-cores 2 spark-examples*.jar 10</code></p>
<p><code>client</code> 模式输出结果，会在控制台展示</p>
<p>使用 cluster 模式只需将 <code>--deploy-mode</code> 参数换成 <code>cluster</code>，使用 <code>cluster</code> 模式，控制台上面是没有输出结果的</p>
<p>需要利用 Yarn 来查看  </p>
<ul>
<li><code>yarn logs -applicationId {applicationId} &gt;&gt; yarnApp.log</code> </li>
<li>Yarn Web UI</li>
</ul>
<h4 id="遇到问题"><a href="#遇到问题" class="headerlink" title="遇到问题"></a>遇到问题</h4><p><strong><a href="https://www.cnblogs.com/itboys/p/10579352.html" target="_blank" rel="noopener">Spark代码中设置appName在client模式和cluster模式中不一样问题</a></strong></p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/qq_29303759/article/details/82659185" target="_blank" rel="noopener">spark-submit 详细参数说明</a></li>
<li><a href="https://blog.csdn.net/MASILEJFOAISEGJIAE/article/details/89317964" target="_blank" rel="noopener">Spark使用示例：分别使用client模式和cluster运行SparkPi程序</a></li>
</ul>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
  </entry>
  <entry>
    <title>YARN Memory CPU 配置</title>
    <url>/2020/01/19/Yarn-Container/</url>
    <content><![CDATA[<p>介绍如何配置 YARN 对内存和CPU的使用</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>以下内容是我学习的理解，主要还是转载武基鹏的<a href="http://blog.itpub.net/30089851/viewspace-2127851/" target="_blank" rel="noopener">YARN的Memory和CPU调优配置详解</a>。</p>
<p>Hadoop YARN 同时支持内存和 CPU 两种资源的调度</p>
<p>YARN 作为一个资源调度器，应该考虑到集群里面每一台机子的计算资源，然后根据 application 申请的资源进行分配 Container</p>
<p>Container 是 YARN 里面资源分配的基本单位，一个 Container 就是一组分配的系统资源，现阶段只包含内存、CPU 系统资源 </p>
<p>Container 和集群节点的关系是一个节点会运行多个 Container，但一个 Container 不会跨节点</p>
<p>在 YARN 集群中，平衡内存、CPU、磁盘的资源的很重要的，作为一般建议，每个磁盘和每个核心允许两个容器可以为群集利用率提供最佳平衡。</p>
<p>确定群集节点的适当 YARN 内存配置时，从可用的硬件资源开始分析  </p>
<ul>
<li>RAM（内存量）</li>
<li>CORES（CPU内核数）</li>
<li>DISKS（磁盘数）</li>
</ul>
<p>关键参数默认值如下表:  </p>
<table>
<thead>
<tr>
<th>参数</th>
<th>默认值</th>
</tr>
</thead>
<tbody><tr>
<td>yarn.nodemanager.resource.memory-mb</td>
<td>-1 (代表NodeManager占机器总内存80%)</td>
</tr>
<tr>
<td>yarn.scheduler.minimum-allocation-mb</td>
<td>1024MB</td>
</tr>
<tr>
<td>yarn.scheduler.maximum-allocation-mb</td>
<td>8192MB</td>
</tr>
<tr>
<td>yarn.nodemanager.resource.cpu-vcores</td>
<td>-1 (代表值为8个虚拟CPU)</td>
</tr>
<tr>
<td>yarn.scheduler.minimum-allocation-vcores</td>
<td>1</td>
</tr>
<tr>
<td>yarn.scheduler.maximum-allocation-vcores</td>
<td>4</td>
</tr>
</tbody></table>
<h4 id="内存配置"><a href="#内存配置" class="headerlink" title="内存配置"></a>内存配置</h4><p>关于内存相关的配置可以参考 hortonwork 公司的文档<a href="https://docs.cloudera.com/HDPDocuments/HDP2/HDP-2.1.1/bk_installing_manually_book/content/rpm-chap1-11.html" target="_blank" rel="noopener">Determine HDP Memory Configuration Settings</a>来配置你的集群</p>
<p>YARN 可用的内存资源应该要考虑预留内存，预留内存是系统进程和其他Hadoop进程（例如HBase）所需的内存。</p>
<p>预留内存 = 操作系统内存预留 + HBase 内存预留（如果HBase在同一节点上）</p>
<h5 id="保留内存建议"><a href="#保留内存建议" class="headerlink" title="保留内存建议"></a>保留内存建议</h5><table>
<thead>
<tr>
<th>每个节点的总内存</th>
<th>建议的预留系统内存</th>
<th>建议的保留 HBase 内存</th>
</tr>
</thead>
<tbody><tr>
<td>4GB</td>
<td>1GB</td>
<td>1GB</td>
</tr>
<tr>
<td>8GB</td>
<td>2GB</td>
<td>1GB</td>
</tr>
<tr>
<td>16GB</td>
<td>2GB</td>
<td>2GB</td>
</tr>
<tr>
<td>24GB</td>
<td>4GB</td>
<td>4GB</td>
</tr>
<tr>
<td>48GB</td>
<td>6GB</td>
<td>8GB</td>
</tr>
<tr>
<td>64GB</td>
<td>8GB</td>
<td>8GB</td>
</tr>
<tr>
<td>72GB</td>
<td>8GB</td>
<td>8GB</td>
</tr>
<tr>
<td>96GB</td>
<td>12GB</td>
<td>16GB</td>
</tr>
<tr>
<td>128GB</td>
<td>24GB</td>
<td>24GB</td>
</tr>
<tr>
<td>256GB</td>
<td>32GB</td>
<td>32GB</td>
</tr>
<tr>
<td>512GB</td>
<td>64GB</td>
<td>64GB</td>
</tr>
</tbody></table>
<p>下一个计算是确定每个节点允许的最大容器数。可以使用以下公式:</p>
<blockquote>
<p>containers = min (2 * CORES, 1.8 * DISKS, (Total available RAM) / MIN_CONTAINER_SIZE)</p>
<p>说明:<br>CORES 为机器CPU核数<br>DISKS 为机器上挂载的磁盘个数<br>Total available RAM 为机器总内存<br>MIN_CONTAINER_SIZE 是指 container 最小的容量大小</p>
</blockquote>
<p><code>MIN_CONTAINER_SIZE</code> 这需要根据具体情况去设置，在较小的内存节点中，<br>最小容器大小也应该较小，下表概述了推荐值: </p>
<table>
<thead>
<tr>
<th>每台机子可用内存</th>
<th>container 最小值</th>
</tr>
</thead>
<tbody><tr>
<td>小于4GB</td>
<td>256MB</td>
</tr>
<tr>
<td>4GB到8GB之间</td>
<td>512MB</td>
</tr>
<tr>
<td>8GB到24GB之间</td>
<td>1024MB</td>
</tr>
<tr>
<td>大于24GB</td>
<td>2048MB</td>
</tr>
</tbody></table>
<p>每个 container 的平均使用内存大小计算方式为</p>
<blockquote>
<p>RAM-per-container = max(MIN_CONTAINER_SIZE, (Total Available RAM) / containers))</p>
</blockquote>
<table>
<thead>
<tr>
<th>配置文件</th>
<th>配置设置</th>
<th>默认值</th>
<th>计算值</th>
</tr>
</thead>
<tbody><tr>
<td>yarn-site.xml</td>
<td>yarn.nodemanager.resource.memory-mb</td>
<td>8192MB</td>
<td>= containers * RAM-per-container</td>
</tr>
<tr>
<td>yarn-site.xml</td>
<td>yarn.scheduler.minimum-allocation-mb</td>
<td>1024MB</td>
<td>= RAM-per-container</td>
</tr>
<tr>
<td>yarn-site.xml</td>
<td>yarn.scheduler.maximum-allocation-mb</td>
<td>8192MB</td>
<td>= containers * RAM-per-container</td>
</tr>
</tbody></table>
<p>举个例子: 对于 128G 内存、32核CPU 的机器，挂载了 7 个磁盘，根据上面的说明，系统保留内存为 24G，<br>不适应 HBase 情况下，系统剩余可用内存为 104G，计算 containers 值如下</p>
<blockquote>
<p>containers = min (2 * 32, 1.8 * 7 , (128-24)/2) = min (64, 12.6 , 52) = 13</p>
</blockquote>
<p>计算 RAM-per-container 值如下</p>
<blockquote>
<p>RAM-per-container = max (2, (124-24)/13) = max (2, 8) = 8</p>
</blockquote>
<p>可以使用脚本 <a href="../codeRep/yarn-utils.py">yarn-utils.py</a> 来计算上面的值</p>
<p>执行下面命令，python 版本为 python2</p>
<blockquote>
<p>python2 yarn-utils.py -c 32 -m 128 -d 7 -k False </p>
</blockquote>
<p>这样的话，每个container内存为8G，似乎有点多，还是更愿意根据集群使用情况任务将其调整为2G内存，则集群中下面的参数配置值如下</p>
<table>
<thead>
<tr>
<th>配置文件</th>
<th>配置设置</th>
<th>计算值</th>
</tr>
</thead>
<tbody><tr>
<td>yarn-site.xml</td>
<td>yarn.nodemanager.resource.memory-mb</td>
<td>= 52 * 2 =104 G</td>
</tr>
<tr>
<td>yarn-site.xml</td>
<td>yarn.scheduler.minimum-allocation-mb</td>
<td>= 2G</td>
</tr>
<tr>
<td>yarn-site.xml</td>
<td>yarn.scheduler.maximum-allocation-mb</td>
<td>= 52 * 2 = 104G</td>
</tr>
</tbody></table>
<h4 id="CPU-配置"><a href="#CPU-配置" class="headerlink" title="CPU 配置"></a>CPU 配置</h4><p>YARN中目前的CPU被划分成虚拟CPU（CPU virtual Core），这里的虚拟CPU是YARN自己引入的概念，<br>初衷是，考虑到不同节点的CPU性能可能不同，每个CPU具有的计算能力也是不一样的，<br>比如某个物理CPU的计算能力可能是另外一个物理CPU的2倍，这时候，<br>你可以通过为第一个物理CPU多配置几个虚拟CPU弥补这种差异。<br>用户提交作业时，可以指定每个任务需要的虚拟CPU个数。</p>
<p>在YARN中，CPU相关配置参数如下</p>
<ul>
<li><code>yarn.nodemanager.resource.cpu-vcores</code> 表示能够分配给Container的CPU核数，默认配置为-1，代表值为8个虚拟CPU，推荐该值的设置和物理CPU的核数数量相同，若不够，则需要调小该值。</li>
<li><code>yarn.scheduler.minimum-allocation-vcores</code> 的默认值为1，表示每个Container容器在处理任务的时候可申请的最少CPU个数为1个。</li>
<li><code>yarn.scheduler.maximum-allocation-vcores</code> 的默认值为4，表示每个Container容器在处理任务的时候可申请的最大CPU个数为4个。</li>
</ul>
<p>在环境上建议留一个给操作系统</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/qq_25302531/article/details/80623791" target="_blank" rel="noopener">浅谈YARN中Container容器（内存、CPU分配）</a></li>
<li><a href="http://blog.itpub.net/30089851/viewspace-2127851/" target="_blank" rel="noopener">YARN的Memory和CPU调优配置详解</a></li>
<li><a href="https://docs.cloudera.com/HDPDocuments/HDP2/HDP-2.1.1/bk_installing_manually_book/content/rpm-chap1-11.html" target="_blank" rel="noopener">9.确定HDP内存配置设置</a></li>
<li><a href="https://blog.csdn.net/suifeng3051/article/details/48135521" target="_blank" rel="noopener">Yarn 内存分配管理机制及相关参数配置</a></li>
</ul>
]]></content>
      <categories>
        <category>Yarn</category>
      </categories>
  </entry>
  <entry>
    <title>HBase CMS GC 配置参考</title>
    <url>/2020/01/18/HBase-CMS/</url>
    <content><![CDATA[<p>HBase发展到当下，对其进行的各种优化从未停止，而 GC 优化更是其中的重中之重</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>HBase发展到当下，对其进行的各种优化从未停止，而 GC 优化更是其中的重中之重</p>
<p>参考自一下文章:</p>
<ul>
<li><a href="http://hbasefly.com/2016/05/21/hbase-gc-1/" target="_blank" rel="noopener">HBase GC的前生今世 – 身世篇</a></li>
<li><a href="http://hbasefly.com/2016/05/29/hbase-gc-2/" target="_blank" rel="noopener">HBase GC的前生今世 – 演进篇</a></li>
<li><a href="http://hbasefly.com/2016/08/09/hbase-cms-gc/" target="_blank" rel="noopener">HBase最佳实践－CMS GC调优</a></li>
<li><a href="https://mp.weixin.qq.com/s/GEwD1B-XqFIudWP_EbGgdQ" target="_blank" rel="noopener">HBase实战：记一次Safepoint导致长时间STW的踩坑之旅</a></li>
</ul>
<h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-Xms32g -Xmx32g -Xmn512m </span><br><span class="line">-XX:+UseParNewGC </span><br><span class="line">-XX:ParallelGCThreads&#x3D;8 </span><br><span class="line">-XX:+UseConcMarkSweepGC</span><br><span class="line"> -XX:+UseCMSCompactAtFullCollection </span><br><span class="line">-XX:CMSFullGCsBeforeCompaction&#x3D;1 </span><br><span class="line">-XX:CMSInitiatingOccupancyFraction&#x3D;65 </span><br><span class="line">-XX:+UseCMSInitiatingOccupancyOnly </span><br><span class="line">-XX:+PrintGC </span><br><span class="line">-XX:+PrintGCDetails </span><br><span class="line">-XX:+PrintGCDateStamps </span><br><span class="line">-XX:+PrintGCApplicationStoppedTime </span><br><span class="line">-XX:+PrintHeapAtGC  </span><br><span class="line">-XX:+PrintTenuringDistribution </span><br><span class="line">-Xloggc:&#x2F;var&#x2F;log&#x2F;hbase&#x2F;gc-regionserver-hbase.log </span><br><span class="line">-XX:+UseGCLogFileRotation  </span><br><span class="line">-XX:NumberOfGCLogFiles&#x3D;10  </span><br><span class="line">-XX:GCLogFileSize&#x3D;100M</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>记一次 HBase Master is initializing 问题处理</title>
    <url>/2020/01/18/HBase-Master-is-initializing/</url>
    <content><![CDATA[<p>HBase org.apache.hadoop.hbase.PleaseHoldException: Master is initializing 错误处理</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>测试集群上做了jar包替换验证测试，测试完之后还原jar，重启出现进入 hbase shell 中执行命令遇到问题。</p>
<p><img src="/images/blog/2020-01-18-1.png" alt></p>
<p><code>org.apache.hadoop.hbase.PleaseHoldException: Master is initializing</code> 代表Master正在初始化中，出现这种错误的原因有很多</p>
<p>关键还是得看 Master 日志，盲目搜索效率不高</p>
<p>一开始只是搜索 ERROR 日志，并没有看到很关键的信息，后面看其他博客贴的日志片段是 WARN，进而<br>转换思路看 WARN，一看这个是 meta 表未分配到 RS 上啊，之前处理过，用 hbck2 解决即可</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">WARN org.apache.hadoop.hbase.master.HMaster: hbase:meta,,1.1588230740 is NOT online; state&#x3D;&#123;1588230740 state&#x3D;OPENING, </span><br><span class="line">ts&#x3D;1579334053311, server&#x3D;region-168-1-240,16020,1579334037426&#125;; ServerCrashProcedures&#x3D;true. Master startup cannot </span><br><span class="line">progress, in holding-pattern until region onlined.</span><br></pre></td></tr></table></figure>

<p>hbck2 配置参考<a href="https://lihuimintu.github.io/2019/12/16/hbase-hbck2/" target="_blank" rel="noopener">CDH-HBase 使用 HBCK2 运维</a>即可</p>
<p>手动去 assign 一下 meta 表即可，hbase:meta 表的 encoded name 是一个时间戳，比如上面日志的 encoded name 就是 <code>1588230740</code> ，<br>具体命令参考下方</p>
<p><code>hbase org.apache.hbase.HBCK2 assigns -o 1588230740</code></p>
<p>查看 Master 日志，可以看到成功分配</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INFO org.apache.hadoop.hbase.master.procedure.MasterProcedureScheduler: Took xlock for pid&#x3D;7, state&#x3D;RUNNABLE:REGION_TRANSITION_QUEUE; AssignProcedure table&#x3D;hbase:meta, region&#x3D;1588230740, override&#x3D;true</span><br><span class="line">INFO org.apache.hadoop.hbase.master.assignment.AssignProcedure: Assigned, not reassigning rit&#x3D;OPEN, location&#x3D;region-168-1-240,16020,1579334037426</span><br><span class="line">INFO org.apache.hadoop.hbase.procedure2.ProcedureExecutor: Finished pid&#x3D;6, state&#x3D;SUCCESS; AssignProcedure table&#x3D;hbase:meta, region&#x3D;1588230740, override&#x3D;true in 160msec</span><br></pre></td></tr></table></figure>

<p>执行命令验证成功</p>
<p><img src="/images/blog/2020-01-18-2.png" alt></p>
<h4 id="吐槽"><a href="#吐槽" class="headerlink" title="吐槽"></a>吐槽</h4><p>之前听范老师说过: 运维三大法办- 监控、日志、源码。还是要细心</p>
<p>hbck2 的使用可以多看看官方文档说明，学习如何使用</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/zzf1510711060/article/details/80919660" target="_blank" rel="noopener">HBase org.apache.hadoop.hbase.PleaseHoldException: Master is initializing 错误解决方法</a></li>
<li><a href="https://lihuimintu.github.io/2019/12/16/hbase-hbck2/" target="_blank" rel="noopener">CDH-HBase 使用 HBCK2 运维</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>记一次 Arthas 实操</title>
    <url>/2020/01/17/Arthas/</url>
    <content><![CDATA[<p>Arthas 是 Alibaba 开源的Java诊断工具，深受开发者喜爱</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>Arthas 是 Alibaba 开源的Java诊断工具，深受开发者喜爱。</p>
<p>Arthas 官方文档十分详细，本文也参考了官方文档内容，同时在开源在的 Github 的项目里的 Issues 里不仅有问题反馈，更有大量的使用案例，也可以进行学习参考。</p>
<p>开源地址: <a href="https://github.com/alibaba/arthas" target="_blank" rel="noopener">https://github.com/alibaba/arthas</a></p>
<p>官方文档: <a href="https://alibaba.github.io/arthas" target="_blank" rel="noopener">https://alibaba.github.io/arthas</a></p>
<h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p><img src="/images/blog/2020-01-17-1.png" alt></p>
<p>在使用 Yarn 跑测试任务时，发现中文输出乱码。</p>
<p>网上搜索到<a href="https://blog.csdn.net/wankunde/article/details/84658467" target="_blank" rel="noopener">hadoop web中查看文件内容乱码解决</a>，<br>感觉可以试试能不能解决问题，但是又不想把 hadoop-common 项目下载下来完整编译打包上传。</p>
<p>这时就想到 Arthas 好想可以解决这个问题，以前特地过了下基础教程学习使用了一下。但是一直未实战，感觉这次可以试试</p>
<h4 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h4><p>准备工作，下载相关文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 服务端下载这个类的 CDH6.3.1 的源码</span><br><span class="line">wget https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;cloudera&#x2F;hadoop-common&#x2F;cdh6.3.1&#x2F;hadoop-common-project&#x2F;hadoop-common&#x2F;src&#x2F;main&#x2F;java&#x2F;org&#x2F;apache&#x2F;hadoop&#x2F;http&#x2F;HtmlQuoting.java</span><br><span class="line"></span><br><span class="line"># 下载 Arthas</span><br><span class="line">wget https:&#x2F;&#x2F;alibaba.github.io&#x2F;arthas&#x2F;arthas-boot.jar</span><br></pre></td></tr></table></figure>

<p>修改 org.apache.hadoop.http.HtmlQuoting </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 导入该类</span><br><span class="line">import java.nio.charset.Charset;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; quoteHtmlChars 方法</span><br><span class="line">public static String quoteHtmlChars(String item) &#123;</span><br><span class="line">    if (item &#x3D;&#x3D; null) &#123;</span><br><span class="line">      return null;</span><br><span class="line">    &#125;</span><br><span class="line">    try &#123;</span><br><span class="line">      byte[] bytes &#x3D; item.getBytes(Charset.defaultCharset().name());</span><br><span class="line">      if (needsQuoting(bytes, 0, bytes.length)) &#123;</span><br><span class="line">        ByteArrayOutputStream buffer &#x3D; new ByteArrayOutputStream();</span><br><span class="line">        try &#123;</span><br><span class="line">          quoteHtmlChars(buffer, bytes, 0, bytes.length);</span><br><span class="line">          return buffer.toString(&quot;UTF-8&quot;);</span><br><span class="line">        &#125; catch (IOException ioe) &#123;</span><br><span class="line">          &#x2F;&#x2F; Won&#39;t happen, since it is a bytearrayoutputstream</span><br><span class="line">          return null;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        return item;</span><br><span class="line">     &#125;</span><br><span class="line">    &#125; catch (IOException zz) &#123;</span><br><span class="line">       return null;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>启动 Arthas</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo -u yarn java -jar &#x2F;opt&#x2F;arthas-boot.jar</span><br><span class="line"></span><br><span class="line"># 选择 RM、NM 进程，然后将其相关替换</span><br><span class="line"></span><br><span class="line"># 三板斧</span><br><span class="line">sc -d *HtmlQuoting # 获取classLoaderHash</span><br><span class="line">mc -c 1963006a &#x2F;opt&#x2F;HbaseSessions.java -d &#x2F;opt</span><br><span class="line">redefine &#x2F;opt&#x2F;org&#x2F;apache&#x2F;hadoop&#x2F;http&#x2F;HtmlQuoting.class </span><br><span class="line"></span><br><span class="line"># 查看是否替换成功</span><br><span class="line">jad org.apache.hadoop.http.HtmlQuoting -c 1963006a</span><br></pre></td></tr></table></figure>

<p>启动 Spark 任务发现还是中文乱码，并没有解决问题，但是这次实操 Arthas 经历挺好的，熟悉下使用</p>
<h4 id="遇到问题"><a href="#遇到问题" class="headerlink" title="遇到问题"></a>遇到问题</h4><p><strong>AttachNotSupportedException</strong> <a href="https://github.com/alibaba/arthas/issues/440" target="_blank" rel="noopener">issue-440</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@region-168-1-228 ~]# java -jar arthas-boot.jar</span><br><span class="line">[INFO] arthas-boot version: 3.1.7</span><br><span class="line">[INFO] Found existing java process, please choose one and hit RETURN.</span><br><span class="line">* [1]: 13120 org.apache.hadoop.hdfs.server.datanode.DataNode</span><br><span class="line">  [2]: 13617 org.apache.hadoop.yarn.server.nodemanager.NodeManager</span><br><span class="line">  [3]: 13619 org.apache.hadoop.hbase.regionserver.HRegionServer</span><br><span class="line">  [4]: 13620 org.apache.hadoop.yarn.server.resourcemanager.ResourceManager</span><br><span class="line">  [5]: 1124 org.apache.zookeeper.server.quorum.QuorumPeerMain</span><br><span class="line">  [6]: 13118 org.apache.hadoop.hdfs.qjournal.server.JournalNode</span><br><span class="line">  [7]: 44015 com.cloudera.kafka.wrap.Kafka</span><br><span class="line">4</span><br><span class="line">[INFO] Start download arthas from remote server: https:&#x2F;&#x2F;maven.aliyun.com&#x2F;repository&#x2F;public&#x2F;com&#x2F;taobao&#x2F;arthas&#x2F;arthas-packaging&#x2F;3.1.7&#x2F;arthas-packaging-3.1.7-bin.zip</span><br><span class="line">[INFO] Download arthas success.</span><br><span class="line">[INFO] arthas home: &#x2F;root&#x2F;.arthas&#x2F;lib&#x2F;3.1.7&#x2F;arthas</span><br><span class="line">[INFO] Try to attach process 13620</span><br><span class="line">[ERROR] Start arthas failed, exception stack trace: </span><br><span class="line">com.sun.tools.attach.AttachNotSupportedException: Unable to open socket file: target process not responding or HotSpot VM not loaded</span><br><span class="line">        at sun.tools.attach.LinuxVirtualMachine.&lt;init&gt;(LinuxVirtualMachine.java:106)</span><br><span class="line">        at sun.tools.attach.LinuxAttachProvider.attachVirtualMachine(LinuxAttachProvider.java:78)</span><br><span class="line">        at com.sun.tools.attach.VirtualMachine.attach(VirtualMachine.java:250)</span><br><span class="line">        at com.taobao.arthas.core.Arthas.attachAgent(Arthas.java:85)</span><br><span class="line">        at com.taobao.arthas.core.Arthas.&lt;init&gt;(Arthas.java:28)</span><br><span class="line">        at com.taobao.arthas.core.Arthas.main(Arthas.java:123)</span><br><span class="line">[ERROR] attach fail, targetPid: 13620</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/2020-01-17-2.png" alt></p>
<p>根据意见参考，执行 arthas 使用 yarn 用户执行的解决</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/wankunde/article/details/84658467" target="_blank" rel="noopener">hadoop web中查看文件内容乱码解决</a></li>
<li><a href="https://stackoverflow.com/questions/11277494/why-java-nio-charset-charsets-compile-error" target="_blank" rel="noopener">why java.nio.charset.Charsets compile error? </a></li>
</ul>
]]></content>
      <categories>
        <category>Tools</category>
      </categories>
  </entry>
  <entry>
    <title>CDH 中使用 Hive on Spark</title>
    <url>/2020/01/15/Hive-On-Spark/</url>
    <content><![CDATA[<p>Content here</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>使用 Hive insert SQL 后查看 Yarn 发现其跑的是 MR 方式</p>
<p><img src="/images/blog/2020-01-15-1.png" alt></p>
<p>这里想改用 Spark 引起来缩短 HiveQL 的响应时间</p>
<p>有两种方式</p>
<ul>
<li>SparkSQL</li>
<li>Hive on Spark</li>
</ul>
<p>两种方式都可以，看个人习惯</p>
<p>Hive on Spark 大体与 SparkSQL 结构类似，只是 SQL 引擎不同，但是计算引擎都是 Spark</p>
<p>本文主要介绍 Hive on Spark</p>
<h4 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h4><p>CDH Hive 配置中可以看到有官方的提示配置文档</p>
<p><img src="/images/blog/2020-01-15-2.png" alt></p>
<p>要将 Hive 配置为在 Spark 上运行，请执行以下两个步骤</p>
<ol>
<li>配置 Hive 依赖项为 Spark 服务</li>
<li>配置 Hive 客户端以使用 Spark 执行引擎</li>
</ol>
<p><strong>配置 Hive 依赖项为 Spark 服务</strong></p>
<p>按照官方文档操作即可</p>
<ol>
<li>在 Cloudera Manager 管理控制台中，转到 Hive 服务</li>
<li>单击配置选项卡</li>
<li>搜索 Spark On YARN 服务。要配置 Spark 服务，请选择 Spark 服务名称。要删除依赖项，请选择 none</li>
<li>点击保存更改。</li>
<li>进入Spark服务。</li>
<li>在 HiveServer2 所在的主机上添加 Spark 的 gateway 角色(即客户端)</li>
<li>重启 Hive、Spark 服务</li>
</ol>
<p><strong>配置 Hive 客户端以使用 Spark 执行引擎</strong></p>
<p>CDH 中的 Hive 支持两个执行引擎: MapReduce 和 Spark</p>
<p>要配置执行引擎，请执行以下步骤之一</p>
<p>beeline/hive: 运行 <code>set hive.execution.engine=engine</code> 命令，engine 选项要么wei <code>mr</code> 要么为 <code>spark</code>，<br>默认为 <code>mr</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set hive.execution.engine&#x3D;spark;</span><br><span class="line"></span><br><span class="line"># 查看当前的设置执行引擎</span><br><span class="line">set hive.execution.engine;</span><br></pre></td></tr></table></figure>

<p>Cloudera Manager（影响所有查询，不推荐）:</p>
<ol>
<li>转到 Hive 服务</li>
<li>单击配置选项卡</li>
<li>搜索 “execution”</li>
<li>将”Default Execution Engine”属性设置为 MapReduce 或 Spark。默认值为 MapReduce</li>
<li>重启 Hive 服务</li>
</ol>
<h4 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h4><p>官方文档中提到性能</p>
<p><img src="/images/blog/2020-01-15-3.png" alt></p>
<p>暂未研究，有兴趣的可以自行看看</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://docs.cloudera.com/documentation/enterprise/latest/topics/admin_hos_oview.html" target="_blank" rel="noopener">在CDH中的Spark上运行Apache Hive</a></li>
<li><a href="https://docs.cloudera.com/documentation/enterprise/latest/topics/admin_hive_configure.html#concept_i3p_2lv_cv" target="_blank" rel="noopener">Hive执行引擎</a></li>
</ul>
]]></content>
      <categories>
        <category>Hive</category>
      </categories>
  </entry>
  <entry>
    <title>Linux block 与 inode</title>
    <url>/2020/01/10/Linux-block-inode/</url>
    <content><![CDATA[<p>Linux 磁盘管理 block 与 inode</p>
<hr>
<h4 id="详解"><a href="#详解" class="headerlink" title="详解"></a>详解</h4><p><strong>Sector（扇区）与 Block（块）</strong></p>
<p>1) 硬盘的最小存储单位: sector（扇区），每个扇区储存512字节；操作系统会一次性连续读取多个扇区，即一次性读取多个扇区称为一个block（块）</p>
<p>2) 文件存取的最小单位: block（块），由多个扇区组成；block的大小常见的有1KB、2KB、4KB，在linux中常见设置为4KB，即连续8个扇区组成一个block</p>
<p>每个block只能存放一个文件，如果文件的大小比block大，会申请更多的block；如果文件的大小比block小，仍会占用一个block，剩余的空间会浪费</p>
<p>例：有1万个文件，大小为10B，block为4KB</p>
<p>理论上占用空间大小：10000 * 10B = 97.656MB</p>
<p>实际上占用空间大小：10000 * 4KB = 40GB</p>
<p><strong>superblock、inode 与 block</strong></p>
<p>操作系统对于文件数据的存放包括两个部分：1文件内容、2权限及文件属性</p>
<p>在硬盘分区中，还有一个超级区块（superblock）</p>
<p>1）superblock: 记录文件系统的整体信息，包括inode与block的总量、使用大小、剩余大小以及文件系统的格式与相关信息等</p>
<p>2）inode: 记录文件的属性、权限，同时会记录该文件的数据所在的block编号</p>
<p>3）block: 存储文件的内容</p>
<p><strong>inode 与 block</strong></p>
<p>每个inode与block都有编号，而每个文件都会占用一个inode，inode内则有文件数据放置的block号码；能够找到文件的inode就可以找到该文件所放置数据的block号码，从而读取文件内容</p>
<p>在格式化时可以指定默认的inode与block的大小；-b指定默认block值，-I指定默认inode值，例：mkfs.ext4 –b 4096 –I 256 /dev/sdb</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="http://www.mamicode.com/info-detail-2435756.html" target="_blank" rel="noopener">Linux磁盘管理（block与inode）</a></li>
</ul>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>监控 Yarn 上作业状态</title>
    <url>/2020/01/07/Yarn-App-Monitor/</url>
    <content><![CDATA[<p>监控 yarn 上 spark 或者 mr 应用的存活状态</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>有开发人员有疑惑想监控 yarn 上 spark 或者 mr 应用的存活状态。</p>
<p>实际通过 Yarn 提供的 API 即可做到</p>
<h4 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h4><p><code>pom.xml</code> 文件，添加相关的配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;repositories&gt;</span><br><span class="line">    &lt;repository&gt;</span><br><span class="line">        &lt;id&gt;cloudera&lt;&#x2F;id&gt;</span><br><span class="line">        &lt;url&gt;https:&#x2F;&#x2F;repository.cloudera.com&#x2F;artifactory&#x2F;cloudera-repos&#x2F;&lt;&#x2F;url&gt;</span><br><span class="line">    &lt;&#x2F;repository&gt;</span><br><span class="line">&lt;&#x2F;repositories&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;hadoop-common&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;version&gt;3.0.0-cdh6.3.1&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;hadoop-client&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;version&gt;3.0.0-cdh6.3.1&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;hadoop-yarn-api&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;version&gt;3.0.0-cdh6.3.1&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;hadoop-yarn-client&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;version&gt;3.0.0-cdh6.3.1&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;&#x2F;dependencies&gt;</span><br></pre></td></tr></table></figure>

<p>具体实现代码其实很简单就是，通过 yarnclient 获取 resourcemanager 上 Application 状态</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.yarn.api.records.ApplicationReport;</span><br><span class="line">import org.apache.hadoop.yarn.api.records.YarnApplicationState;</span><br><span class="line">import org.apache.hadoop.yarn.client.api.YarnClient;</span><br><span class="line">import org.apache.hadoop.yarn.conf.YarnConfiguration;</span><br><span class="line">import org.apache.hadoop.yarn.exceptions.YarnException;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line">import java.util.Collections;</span><br><span class="line">import java.util.Comparator;</span><br><span class="line">import java.util.EnumSet;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">public class client &#123;</span><br><span class="line"></span><br><span class="line">    public static ApplicationReport getAppReport(String applictionName) &#123;</span><br><span class="line">        Configuration conf &#x3D; new YarnConfiguration();</span><br><span class="line">        &#x2F;&#x2F; 修改为 RM 的地址</span><br><span class="line">        conf.set(&quot;yarn.resourcemanager.address&quot;, &quot;master-240:8032&quot;);</span><br><span class="line">        conf.set(&quot;yarn.resourcemanager.admin.address&quot;, &quot;master-240:8033&quot;);</span><br><span class="line">        YarnClient yarnClient &#x3D; YarnClient.createYarnClient();</span><br><span class="line">        yarnClient.init(conf);</span><br><span class="line">        yarnClient.start();</span><br><span class="line">        EnumSet&lt;YarnApplicationState&gt; appStates &#x3D; EnumSet.noneOf(YarnApplicationState.class);</span><br><span class="line">        if (appStates.isEmpty()) &#123;</span><br><span class="line">            appStates.add(YarnApplicationState.RUNNING);</span><br><span class="line">            appStates.add(YarnApplicationState.ACCEPTED);</span><br><span class="line">            appStates.add(YarnApplicationState.SUBMITTED);</span><br><span class="line">            appStates.add(YarnApplicationState.FINISHED);</span><br><span class="line">            appStates.add(YarnApplicationState.FAILED);</span><br><span class="line">            appStates.add(YarnApplicationState.KILLED);</span><br><span class="line">        &#125;</span><br><span class="line">        List&lt;ApplicationReport&gt; appsReports &#x3D; null;</span><br><span class="line">        try &#123;</span><br><span class="line">            &#x2F;&#x2F; 如果不指定YarnApplicationState的话就搜索所有状态 Application</span><br><span class="line">            &#x2F;&#x2F;appsReports &#x3D; yarnClient.getApplications(appStates);</span><br><span class="line">            appsReports &#x3D; yarnClient.getApplications();</span><br><span class="line">&#x2F;&#x2F;            appsReports.sort(new Comparator&lt;ApplicationReport&gt;() &#123;</span><br><span class="line">&#x2F;&#x2F;                @Override</span><br><span class="line">&#x2F;&#x2F;                public int compare(ApplicationReport o1, ApplicationReport o2) &#123;</span><br><span class="line">&#x2F;&#x2F;                    return -o1.getApplicationId().compareTo(o2.getApplicationId());</span><br><span class="line">&#x2F;&#x2F;                &#125;</span><br><span class="line">&#x2F;&#x2F;            &#125;);</span><br><span class="line">&#x2F;&#x2F;            appsReports.sort((ApplicationReport o1, ApplicationReport o2) -&gt; -(o1.getApplicationId().compareTo(o2.getApplicationId())));</span><br><span class="line">&#x2F;&#x2F;            Collections.sort(appsReports, Comparator.comparing(ApplicationReport::getApplicationId).reversed());</span><br><span class="line">            &#x2F;&#x2F; 反转 applicationId</span><br><span class="line">            Comparator&lt;ApplicationReport&gt; comp &#x3D; (ApplicationReport o1, ApplicationReport o2) -&gt; o1.getApplicationId().compareTo(o2.getApplicationId());</span><br><span class="line">            appsReports.sort(comp.reversed());</span><br><span class="line">        &#125; catch (YarnException | IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        assert appsReports !&#x3D; null;</span><br><span class="line">        ApplicationReport aimAppReport &#x3D; null;</span><br><span class="line">        for (ApplicationReport appReport : appsReports) &#123;</span><br><span class="line">            &#x2F;&#x2F; 获取任务</span><br><span class="line">            String appName &#x3D; appReport.getName();</span><br><span class="line">            if (appName.equals(applictionName)) &#123;</span><br><span class="line">                aimAppReport &#x3D; appReport;</span><br><span class="line">                break;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        yarnClient.stop();</span><br><span class="line">        return aimAppReport;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args)&#123;</span><br><span class="line">        ApplicationReport applicationReport &#x3D; getAppReport(&quot;Spark shell&quot;);</span><br><span class="line">        System.out.println(&quot;ApplicationId &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt; &quot;+ applicationReport.getApplicationId());</span><br><span class="line">        System.out.println(&quot;YarnApplicationState &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt; &quot;+ applicationReport.getYarnApplicationState());</span><br><span class="line">        System.out.println(&quot;name &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt; &quot;+ applicationReport.getName());</span><br><span class="line">        System.out.println(&quot;queue &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt; &quot;+ applicationReport.getQueue());</span><br><span class="line">        System.out.println(&quot;queue &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt; &quot;+ applicationReport.getUser());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这样，我们通过app name字段可以获取到存活的 spark 等任务</p>
<p><img src="/images/blog/2020-01-07-1.png" alt></p>
<p>客户端连接还可以将 yarn-site.xml 放到 resources 目录下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Configuration conf &#x3D; new YarnConfiguration();</span><br><span class="line">&#x2F;&#x2F; 这里将会加载 yarn 配置，配置地址如果是 hostname 的话，要在 client 机器上 hosts 文件配置好 host ip 映射</span><br><span class="line">conf.addResource(new Path(&quot;src&#x2F;main&#x2F;resources&#x2F;yarn-site.xml&quot;));</span><br></pre></td></tr></table></figure>

<p>如果是高可用配置的话还可以配置参数的形式访问</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Configuration conf &#x3D; new YarnConfiguration();</span><br><span class="line">conf.set(&quot;yarn.resourcemanager.ha.enabled&quot;, &quot;true&quot;);</span><br><span class="line">&#x2F;&#x2F; 下方三个参数每个 yarn 集群不一样，可以从 yarn-site.xml 中寻找</span><br><span class="line">conf.set(&quot;yarn.resourcemanager.ha.rm-ids&quot;, &quot;rm61,rm54&quot;);</span><br><span class="line">conf.set(&quot;yarn.resourcemanager.address.rm54&quot;, &quot;192.168.1.229:8032&quot;);</span><br><span class="line">conf.set(&quot;yarn.resourcemanager.address.rm61&quot;, &quot;192.168.1.228:8032&quot;);</span><br></pre></td></tr></table></figure>

<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/rlnLo2pNEfx9c/article/details/83353496" target="_blank" rel="noopener">写个yarn的监控</a></li>
<li><a href="https://blog.csdn.net/qq_29493353/article/details/85788171" target="_blank" rel="noopener">yarn通过客户端提交application</a></li>
<li><a href="https://www.jianshu.com/p/c1fb266e4809" target="_blank" rel="noopener">YARN HA</a></li>
</ul>
]]></content>
      <categories>
        <category>Yarn</category>
      </categories>
  </entry>
  <entry>
    <title>Spark spark.yarn.jars 使用说明</title>
    <url>/2020/01/06/Spark-yarn-jar/</url>
    <content><![CDATA[<p>spark 优化将依赖包传入HDFS 使用 spark.yarn.jar </p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>问题描述请转移 <a href="https://blog.csdn.net/weizhonggui/article/details/85240804" target="_blank" rel="noopener">十：WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set,解决案例</a></p>
<p>启动 Spark 任务时，在没有配置 spark.yarn.archive 或者 spark.yarn.jars 时， 会看到不停地上传jar，非常耗时</p>
<h4 id="处理"><a href="#处理" class="headerlink" title="处理"></a>处理</h4><p>如果使用了 spark.yarn.archive 配置将会替换 spark.yarn.jars 的配置，所以这里使用<br>spark.yarn.jars 可以大大地减少任务的启动时间，整个处理过程如下。</p>
<p>上传依赖jar包, <code>/user/spark/jars</code> 为 hfds 上的目录，如果没有自行新建；<br>spark/jars/* 为 spark 服务自带的 jar 包</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop fs -put spark&#x2F;jars&#x2F;* &#x2F;user&#x2F;spark&#x2F;jars</span><br></pre></td></tr></table></figure>

<p>配置spark-defaut.conf，下方 <code>hdfs://reh</code> 为我这 HDFS nameserver 名字，你自行改成你自己的</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark.yarn.jars&#x3D;local:&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.3.1-1.cdh6.3.1.p0.1470567&#x2F;lib&#x2F;spark&#x2F;jars&#x2F;*,local:&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.3.1-1.cdh6.3.1.p0.1470567&#x2F;lib&#x2F;spark&#x2F;hive&#x2F;*,hdfs:&#x2F;&#x2F;reh&#x2F;user&#x2F;spark&#x2F;jars&#x2F;*.jar</span><br></pre></td></tr></table></figure>

<p>注：本地配置local,hdfs标记为hdfs目录即可</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/weizhonggui/article/details/85240804" target="_blank" rel="noopener">十：WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set,解决案例</a></li>
<li><a href="https://www.cnblogs.com/yyy-blog/p/11110388.html" target="_blank" rel="noopener">spark优化——依赖包传入HDFS_spark.yarn.jar和spark.yarn.archive的使用</a></li>
</ul>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
  </entry>
  <entry>
    <title>Linux shell 命令行参数</title>
    <url>/2020/01/04/Linux-getopts-getopt/</url>
    <content><![CDATA[<p>Linux shell 命令行参数解析处理</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>在bash中，可以用以下三种方式来处理命令行参数</p>
<ul>
<li>直接处理：使用$1,$2,…,$n进行解析</li>
<li>getopts：单个字符选项的情况（如：-n 10 -f file.txt等选项）</li>
<li>getopt：可以处理单个字符选项，也可以处理长选项long-option（如：–prefix=/home等）</li>
</ul>
<p>直接处理的方式网上很多，也很简单，直接跳过</p>
<h4 id="getopts"><a href="#getopts" class="headerlink" title="getopts"></a>getopts</h4><p>getopts 是 bash 的内部命令</p>
<p>getopts 的使用形式：getopts OPTION_STRING VAR;</p>
<p>OPTION_STRING 是类似 -u,-p 这种单字符选项</p>
<p>测试代码:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">while getopts &quot;a:b:c:&quot; opt; do</span><br><span class="line">  case $opt in</span><br><span class="line">    a)</span><br><span class="line">        use&#x3D;$OPTARG</span><br><span class="line">        echo &quot;user is $use&quot; ;;</span><br><span class="line">    b)</span><br><span class="line">        passwd&#x3D;$OPTARG</span><br><span class="line">        echo &quot;passwd is $passwd&quot; ;;</span><br><span class="line">    c)</span><br><span class="line">        #cccc&#x3D;$OPTARG</span><br><span class="line">	    echo &quot;this is -c option. OPTARG&#x3D;[$OPTARG] OPTIND&#x3D;[$OPTIND]&quot;</span><br><span class="line">	;;</span><br><span class="line">        #echo &quot;cccc is $cccc&quot; ;;</span><br><span class="line">    \?)</span><br><span class="line">        echo &quot;invalid arg&quot; ;;</span><br><span class="line">  esac</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<p>运行结果:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># sh test.sh -u tom -p 123456</span><br><span class="line">user is tom</span><br><span class="line">passwd is 123456</span><br></pre></td></tr></table></figure>

<h4 id="getopt"><a href="#getopt" class="headerlink" title="getopt"></a>getopt</h4><p>getopt 是一个外部命令，不是 bash 内置命令，Linux 发行版通常会自带</p>
<p>老版本的getopt问题较多，增强版getopt比较好用，执行命令getopt -T; echo $?，如果输出4，则代表是增强版的</p>
<p>测试代码:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">filepath&#x3D;$(cd &#96;dirname $0&#96;; pwd)</span><br><span class="line"></span><br><span class="line">show_usage&#x3D;&quot;args: [-s , -e , -n , -c]\</span><br><span class="line">                                  [--sdate&#x3D;, --edate&#x3D;, --numprocs&#x3D;, --cfile&#x3D;]&quot;</span><br><span class="line"></span><br><span class="line">if [[ -z $@ ]];then</span><br><span class="line">echo $show_usage</span><br><span class="line">exit 0</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">GETOPT_ARGS&#x3D;&#96;getopt -o s:e:n:c: -al sdate:,edate:,numprocs:,cfile: -- &quot;$@&quot;&#96;</span><br><span class="line"></span><br><span class="line">#echo &quot;$GETOPT_ARGS&quot;</span><br><span class="line">eval set -- &quot;$GETOPT_ARGS&quot;</span><br><span class="line"></span><br><span class="line">while [ -n &quot;$1&quot; ]</span><br><span class="line">do</span><br><span class="line">        case &quot;$1&quot; in</span><br><span class="line">                -s|--sdate) sdate&#x3D;$2; echo $sdate; shift 2;;</span><br><span class="line">                -e|--edate) edate&#x3D;$2; echo $edate; shift 2;;</span><br><span class="line">                -n|--numprocs) numprocs&#x3D;$2; echo $numprocs; shift 2;;</span><br><span class="line">                -c|--cfile) cfile&#x3D;$2; echo $cfile; shift 2;;</span><br><span class="line">                --) break ;;</span><br><span class="line">                *) echo $1,$2,$show_usage; break ;;</span><br><span class="line">        esac</span><br><span class="line">done</span><br></pre></td></tr></table></figure>


<p>运行结果:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># bash test2.sh --edate 11.11 -numprocs 123456</span><br><span class="line">11.11</span><br><span class="line">123456</span><br></pre></td></tr></table></figure>


<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.jianshu.com/p/6393259f0a13" target="_blank" rel="noopener">linux shell命令行选项与参数用法详解–getopts、getopt</a></li>
<li><a href="https://www.cnblogs.com/houyongchong/p/8482652.html" target="_blank" rel="noopener">shell getopts用法</a></li>
<li><a href="https://www.cnblogs.com/tommyjiang/p/10629848.html" target="_blank" rel="noopener">shell之getopt的用法</a></li>
</ul>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>YARN Fair scheduler</title>
    <url>/2020/01/02/Yarn-Fair-Scheduler/</url>
    <content><![CDATA[<p>YARN 默认调度 Fair scheduler</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>工作中这个问题碰到好几次了。调度规则(scheduling rule)，啥是调度策略(scheduling policy)，还有 CDH 本身有个动态资源池的概念，<br>让我学了又忘，忘了又学，这次通过文档方式来给自己一个交代。</p>
<h4 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h4><p><strong>什么是资源？</strong></p>
<p>对于一个资源管理系统，首先要确定什么是资源，然后将每种资源量化，最后对量化的资源进行管理。</p>
<p>YARN对资源的抽象很简单，只有内存和vcore，这两种资源。每个NodeManager节点贡献一定的内存和vcore，由ResourceManager统一管理。</p>
<h4 id="调度规则"><a href="#调度规则" class="headerlink" title="调度规则"></a>调度规则</h4><p>YARN的三种调度规则(scheduling rule)</p>
<ul>
<li>FIFO Scheduler</li>
<li>Capacity Scheduler</li>
<li>Fair Scheduler</li>
</ul>
<p>调度规则具体可以看<a href="https://www.cnblogs.com/sodawoods-blogs/p/8877197.html" target="_blank" rel="noopener">YARN中FIFO、Capacity以及Fari调度器的详细介绍</a><br>，记得之前有次看到说随着发展，Capacity 与 Fari 区别越来越小了</p>
<h4 id="调度策略"><a href="#调度策略" class="headerlink" title="调度策略"></a>调度策略</h4><p><a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/FairScheduler.html" target="_blank" rel="noopener">Fair Scheduler支持的调度策略</a></p>
<blockquote>
<p>Additionally, the fair scheduler allows setting a different custom policy for each queue to allow sharing the queue’s resources in any which way the user wants. A custom policy can be built by extending org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.SchedulingPolicy. FifoPolicy, FairSharePolicy (default), and DominantResourceFairnessPolicy are built-in and can be readily used.</p>
</blockquote>
<p>队列内部的默认调度策略是基于内存的共享策略，也可以配置成 FIFO 和 multi-resource with Dominant Resource Fairness (即DRF)</p>
<p>对于CDH版本来说有些不同，<a href="https://docs.cloudera.com/documentation/enterprise/latest/topics/cm_mc_resource_pools.html#concept_xkk_l1d_wr__section_utc_gbl_vl" target="_blank" rel="noopener">CDH版本的 Fair Scheduler</a> 默认是采用DRF 策略</p>
<blockquote>
<p>Dominant Resource Fairness (DRF) (default) - An extension of fair scheduling for more than one resource. DRF determines CPU and memory resource shares based on the availability of those resources and the job requirements.</p>
</blockquote>
<p><img src="/images/blog/2020-01-02-3.png" alt></p>
<p>也就是说 CDH 版本的 YARN 默认采用的调度策略是 Fair Scheduler 的 DRF 策略，即基于 vcore 和内存的策略，而不是只基于内存的调度策略。</p>
<h4 id="Fair-Scheduler"><a href="#Fair-Scheduler" class="headerlink" title="Fair Scheduler"></a>Fair Scheduler</h4><p>FairScheduler 默认让所有的 apps 都运行，但是也可以通过配置文件限制每个用户以及每个queue运行的apps的数量。<br>这是针对一个用户一次提交hundreds of apps产生大量中间结果数据或者大量的context-switching的情况。</p>
<p>大的任务job1提交并执行，占用了集群的全部资源，开始执行。之后小的job2执行时，获得系统一半的资源，开始执行。因此每个job可以公平地使用系统的资源。当job2执行完毕，并且集群中没有其他的job加入时，job1又可以获得全部的资源继续执行。</p>
<p><img src="/images/blog/2020-02-09-1.png" alt></p>
<p>两个用户A和B。A提交job1时集群内没有正在运行的app，因此job1独占集群中的资源。用户B的job2提交时，job2在job1释放一半的containers之后，开始执行。job2还没执行完的时候，用户B提交了job3，job2释放它占用的一半containers之后，job3获得资源开始执行。</p>
<p><img src="/images/blog/2020-02-09-2.png" alt></p>
<h4 id="抢占"><a href="#抢占" class="headerlink" title="抢占"></a>抢占</h4><p>Capacity Scheduler 与 Fair Scheduler 都可以将集群分为队列且允许抢占机制</p>
<p>当一个job提交到一个繁忙集群中的空队列时，job并不会马上执行，而是阻塞直到正在运行的job释放系统资源。为了使提交job的执行时间更具预测性（可以设置等待的超时时间），调度器支持抢占。</p>
<p>抢占就是允许调度器杀掉占用超过其应占份额资源队列的containers，这些containers资源便可被分配到应该享有这些份额资源的队列中。需要注意抢占会降低集群的执行效率，因为被终止的containers需要被重新执行。</p>
<p>可以阅读下方三个参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/suifeng3051/article/details/49508261" target="_blank" rel="noopener">Yarn 调度器Scheduler详解</a></li>
<li><a href="http://bigdatadecode.club/YARN%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BFair%20Scheduler%20part1.html" target="_blank" rel="noopener">YARN源码分析之Fair Scheduler part1</a></li>
<li><a href="http://bigdatadecode.club/YARN%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BFair%20Scheduler%20part2.html" target="_blank" rel="noopener">YARN源码分析之Fair Scheduler part2</a></li>
</ul>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.jianshu.com/p/ada7f7b58298" target="_blank" rel="noopener">CDH版本的YARN默认调度Fair scheduler 和DRF简述</a></li>
<li><a href="https://www.cnblogs.com/sodawoods-blogs/p/8877197.html" target="_blank" rel="noopener">YARN中FIFO、Capacity以及Fari调度器的详细介绍</a></li>
<li><a href="https://www.jianshu.com/p/990094565a9d" target="_blank" rel="noopener">Yarn 队列调度策略</a></li>
</ul>
]]></content>
      <categories>
        <category>Yarn</category>
      </categories>
  </entry>
  <entry>
    <title>Maven 项目打成 jar 包运行</title>
    <url>/2020/01/02/mavan-jar/</url>
    <content><![CDATA[<p>普通 Java Maven 项目打成 jar 包方式放服务器上运行</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>动手写了个普通 Java 的 Maven 项目，将其打包放到服务器上运行</p>
<h4 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h4><p>在 <code>pom.xml</code> 中添加如下配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;build&gt;</span><br><span class="line">    &lt;pluginManagement&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-assembly-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.5.5&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;archive&gt;</span><br><span class="line">                        &lt;manifest&gt;</span><br><span class="line">                            &lt;!-- 这里是启动类全路径名 --&gt;</span><br><span class="line">                            &lt;mainClass&gt;com.xxx.XXXX&lt;&#x2F;mainClass&gt;</span><br><span class="line">                        &lt;&#x2F;manifest&gt;</span><br><span class="line">                    &lt;&#x2F;archive&gt;</span><br><span class="line">                    &lt;descriptorRefs&gt;</span><br><span class="line">                        &lt;descriptorRef&gt;jar-with-dependencies&lt;&#x2F;descriptorRef&gt;</span><br><span class="line">                    &lt;&#x2F;descriptorRefs&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.1&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;!-- jdk 版本 --&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                    &lt;encoding&gt;utf-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;&#x2F;plugins&gt;</span><br><span class="line">    &lt;&#x2F;pluginManagement&gt;</span><br><span class="line">&lt;&#x2F;build&gt;</span><br></pre></td></tr></table></figure>

<p>请将 com.xxx.XXXX 改成你项目的主类</p>
<p>终端下执行<code>mvn clean package assembly:single</code></p>
<p><img src="/images/blog/2020-01-02-1.png" alt></p>
<p>出现如图标志说明打包成功</p>
<p>可以在target 目录下看到两个 jar 包，带 jar-with-dependencies 后缀的是带依赖的jar包</p>
<p><img src="/images/blog/2020-01-02-2.png" alt></p>
<p>这里选择将其上传到服务器上运行 <code>java -jar cdhAgent-1.0-SNAPSHOT-jar-with-dependencies.jar</code></p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/qq_27252133/article/details/82750599" target="_blank" rel="noopener">使用maven-assembly-plugin插件打包</a></li>
<li><a href="https://blog.csdn.net/qq_16038125/article/details/60168389" target="_blank" rel="noopener">构建工具-Maven-使用maven-assembly-plugin将依赖打包进jar并指定JDK版本</a></li>
<li><a href="https://www.cnblogs.com/binhua-liu/p/5604841.html" target="_blank" rel="noopener">如何配置pom.xml用maven打包java工程</a></li>
</ul>
]]></content>
      <categories>
        <category>Tools</category>
      </categories>
  </entry>
  <entry>
    <title>二阶段提交（2PC）</title>
    <url>/2020/01/01/2PC/</url>
    <content><![CDATA[<p>在分布式环境下，所有节点进行事务提交，保持一致性的算法</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>今天是2020年的元旦，在这先祝自己开心快乐</p>
<p>2PC 在之前学习 HBase Snapshot 时了解过，这次通过文档的记录的方式再过一遍</p>
<p>本人主要是阅读下方学习了解的</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/kXtliHCqtrTjlWFm1aHtVQ" target="_blank" rel="noopener">分布式基础，啥是两阶段提交？</a></li>
<li><a href="https://mp.weixin.qq.com/s/QS01KCQ4u2DhNHww_A2Xtw" target="_blank" rel="noopener">漫话：如何给女朋友解释什么是2PC（二阶段提交）？</a></li>
</ul>
<h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><p>2PC 是解决分布式系统中数据一致性问题，即分布式事务</p>
<p><strong>为什么分布式事务为什么难？</strong></p>
<p>在分布式环境下，每个节点都可以知晓自己操作的成功或者失败，却无法知道其他节点操作的成功或失败。当一个分布式事务跨多个节点时，保持事务的原子性与一致性，是非常困难的。</p>
<p>举个例子: 在电商网站下单的时候，需要有多个分布式服务同时服务，如支付系统进行支付、红包系统进行红包扣减、库存系统扣减库存、物流系统更新物流信息等。</p>
<p>但是，如果其中某一个系统在执行过程中失败了，或者由于网络原因没有收到请求，那么，整个系统可能就有不一致的现象了，<br>即：付了钱，扣了红包，但是库存没有扣减</p>
<h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><p>通过引入一个协调者（Coordinator）来统一掌控所有参与者（Participant）的操作结果，<br>并指示它们是否要把操作结果进行真正的提交（commit）或者回滚（rollback）</p>
<p>2PC分为两个阶段:</p>
<ul>
<li>投票阶段（voting phase）参与者通知协调者，协调者反馈结果</li>
<li>提交阶段（commit phase）收到参与者的反馈后，协调者再向参与者发出通知，根据反馈情况决定各参与者是否要提交还是回滚</li>
</ul>
<h5 id="准备阶段"><a href="#准备阶段" class="headerlink" title="准备阶段"></a>准备阶段</h5><p>准备阶段分为以下三个步骤</p>
<ol>
<li>协调者节点向所有参与者节点询问是否可以执行提交操作，并开始等待各参与者节点的响应</li>
<li>参与者节点执行询问发起为止的所有事务操作，并将Undo信息和Redo信息写入日志 (HBase SnapShot 是将执行操作写入到临时文件，回滚就直接删除，执行就移动到目标目录)</li>
<li>各参与者节点响应协调者节点发起的询问。如果参与者节点的事务操作实际执行成功，则它返回一个”OK”消息, 如果参与者节点的事务操作实际执行失败，则它返回一个”NO”消息</li>
</ol>
<p><img src="/images/blog/2020-01-01-1.png" alt></p>
<h5 id="提交阶段"><a href="#提交阶段" class="headerlink" title="提交阶段"></a>提交阶段</h5><p>如果协调者收到了参与者的失败消息或者超时，直接给每个参与者发送回滚消息；否则，发送提交消息；参与者根据协调者的指令执行提交或者回滚操作</p>
<p>当协调者节点从所有参与者节点获得的相应消息都为”OK”时</p>
<ol>
<li>协调者节点向所有参与者节点发出”正式提交”的请求</li>
<li>参与者节点正式完成操作，并释放在整个事务期间内占用的资源</li>
<li>参与者节点向协调者节点发送”完成”消息</li>
<li>协调者节点受到所有参与者节点反馈的”完成”消息后，完成事务</li>
</ol>
<p><img src="/images/blog/2020-01-01-2.png" alt></p>
<p>如果任一参与者节点在第一阶段返回的响应消息为”NO”时，或者 协调者节点在第一阶段的询问超时之前无法获取所有参与者节点的响应消息时</p>
<ol>
<li>协调者节点向所有参与者节点发出”回滚操作”的请求</li>
<li>参与者节点利用之前写入的Undo信息执行回滚，并释放在整个事务期间内占用的资源</li>
<li>参与者节点向协调者节点发送”回滚完成”消息</li>
<li>协调者节点受到所有参与者节点反馈的”回滚完成”消息后，取消事务</li>
</ol>
<p><img src="/images/blog/2020-01-01-3.png" alt></p>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><p>1、同步阻塞问题。执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。</p>
<p>2、单点故障。由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。（如果是协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题）</p>
<p>3、数据不一致。在二阶段提交的阶段二中，当协调者向参与者发送commit请求之后，发生了局部网络异常或者在发送commit请求过程中协调者发生了故障，这回导致只有一部分参与者接受到了commit请求。而在这部分参与者接到commit请求之后就会执行commit操作。但是其他部分未接到commit请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据部一致性的现象。</p>
<p>4、二阶段无法解决的问题：协调者再发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。</p>
<h4 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h4><ul>
<li><a href="https://mp.weixin.qq.com/s/lxc-JvDRkoR9tXLpsTqGeA" target="_blank" rel="noopener">漫话：如何给女朋友解释什么是3PC？</a></li>
</ul>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/kXtliHCqtrTjlWFm1aHtVQ" target="_blank" rel="noopener">分布式基础，啥是两阶段提交？</a></li>
<li><a href="https://mp.weixin.qq.com/s/QS01KCQ4u2DhNHww_A2Xtw" target="_blank" rel="noopener">漫话：如何给女朋友解释什么是2PC（二阶段提交）？</a></li>
</ul>
]]></content>
      <categories>
        <category>Distributed</category>
      </categories>
  </entry>
  <entry>
    <title>Cloudera Manager API 使用</title>
    <url>/2019/12/27/CM-API/</url>
    <content><![CDATA[<p>Cloudera Manager API 简单使用</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>Hadoop 是一个开源项目，所以很多公司在这个基础进行商业化，Cloudera 对 Hadoop 做了相应的改变，<br>Cloudera 公司的发行版，我们将该版本称为 CDH(Cloudera Distribution Hadoop)。</p>
<p>我司使用的就是 CDH 版本，方便快捷部署，同时指标可视化，健康检测也做的挺好的。</p>
<p>Clouder Manager 是管理 CDH 的管理平台，通过接入 Cloudera Manager 获取到一些信息，<br>然后基于信息做些监控报警和宕机重启操作</p>
<p>因为之前使用的是 CDH 5.7.1 版本的，API 直接可用，但是现在新集群使用的是 CDH 6.3.1 ，API 改版使用 Swagger 方式来<br>操作了，按以前的方法各种踩坑。</p>
<h4 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h4><p><a href="http://cloudera.github.io/cm_api/" target="_blank" rel="noopener">Cloudera Manager API 官网地址</a></p>
<p>参考官网实例基本够用，同时注意选择 <a href="http://cloudera.github.io/cm_api/docs/releases/" target="_blank" rel="noopener">API 版本</a></p>
<p><img src="/images/blog/2019-12-27-1.png" alt></p>
<p>访问的cm地址可以直接查看 api 版本 <code>http://host:port/api/version</code>，参考自第一个链接</p>
<p>swagger 的方式与之前真不大相同，这里把写的 Python demo 保存下，方便以后查阅，注意注册一个可读账号来访问</p>
<p>python 2.7.9</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F;python</span><br><span class="line"># -*- coding: UTF-* -*-</span><br><span class="line"></span><br><span class="line">import cm_client</span><br><span class="line">from cm_client.rest import ApiException</span><br><span class="line">from pprint import pprint</span><br><span class="line"></span><br><span class="line"># Configure HTTP basic authorization: basic</span><br><span class="line">cm_client.configuration.username &#x3D; &#39;hadoop&#39;</span><br><span class="line">cm_client.configuration.password &#x3D; &#39;xxxx&#39;</span><br><span class="line"></span><br><span class="line"># Create an instance of the API class</span><br><span class="line">api_host &#x3D; &#39;http:&#x2F;&#x2F;192.168.1.240&#39;</span><br><span class="line">port &#x3D; &#39;7180&#39;</span><br><span class="line">api_version &#x3D; &#39;v30&#39;</span><br><span class="line"># Construct base URL for API</span><br><span class="line"># http:&#x2F;&#x2F;cmhost:7180&#x2F;api&#x2F;v30</span><br><span class="line">api_url &#x3D; api_host + &#39;:&#39; + port + &#39;&#x2F;api&#x2F;&#39; + api_version</span><br><span class="line">api_client &#x3D; cm_client.ApiClient(api_url)</span><br><span class="line">cluster_api_instance &#x3D; cm_client.ClustersResourceApi(api_client)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Lists all known clusters.</span><br><span class="line">api_response &#x3D; cluster_api_instance.read_clusters(view&#x3D;&#39;SUMMARY&#39;)</span><br><span class="line">for cluster in api_response.items:</span><br><span class="line">    print (cluster.name , &quot;-&quot;, cluster.full_version)</span><br><span class="line">    if cluster.full_version.startswith(&quot;6.&quot;):</span><br><span class="line">        services_api_instance &#x3D; cm_client.ServicesResourceApi(api_client)</span><br><span class="line">        services &#x3D; services_api_instance.read_services(cluster.name, view&#x3D;&#39;FULL&#39;)</span><br><span class="line">        for service in services.items:</span><br><span class="line">            print (service.name, &quot;-&quot;, service.type)</span><br><span class="line">            if service.type &#x3D;&#x3D; &#39;HDFS&#39;:</span><br><span class="line">                hive &#x3D; service</span><br><span class="line">                print (&quot;lihm&quot;)</span><br><span class="line">                print (hive.name, hive.service_state, hive.health_summary)</span><br><span class="line">                for health_check in hive.health_checks:</span><br><span class="line">                    print (health_check.name, &quot;---&quot;, health_check.summary)</span><br><span class="line">                role_api_instance &#x3D; cm_client.RolesResourceApi(api_client)</span><br><span class="line">                roles &#x3D; role_api_instance.read_roles(cluster.name, hive.name)</span><br><span class="line">                for role in roles.items:</span><br><span class="line">                    if role.type &#x3D;&#x3D; &#39;NAMENODE&#39;:</span><br><span class="line">                        nn &#x3D; role</span><br><span class="line">                        print (nn.name, nn.role_state, nn.health_summary, nn.host_ref.host_id)</span><br><span class="line">                        for hc in nn.health_checks:</span><br><span class="line">                            print (hc.name, &quot;---&quot;, hc.summary)</span><br><span class="line">                # cmd &#x3D; services_api_instance.restart_command(cluster.name, hive.name)</span><br><span class="line">                # print (&quot;cmd.active&quot;)</span><br></pre></td></tr></table></figure>

<h4 id="场景"><a href="#场景" class="headerlink" title="场景"></a>场景</h4><p>从运维 CDH 集群以来，使用的场景无非就是监控、告警</p>
<ul>
<li>使用 Python 方式来做脚本获取信息，并接入第三方告警系统中，如 open-falcon</li>
<li>使用 Java 方式获取指标信息，接入自己开发的管理平台，可视化展示一些集群指标信息</li>
<li>使用 Java 方式获取角色异常退出信息并操作重启退出角色</li>
</ul>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/Leon_liuqinburen/article/details/82145355" target="_blank" rel="noopener">cm_api.api_client.ApiException: (error 404)</a></li>
</ul>
]]></content>
      <categories>
        <category>CDH</category>
      </categories>
  </entry>
  <entry>
    <title>采集 hdfs-audit 分析 rpc 画像</title>
    <url>/2019/12/25/elk-hdfs/</url>
    <content><![CDATA[<p>快速采集 hdfs-audit 分析并进行展示监控</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>本文主要是基于小火牛的<a href="https://mp.weixin.qq.com/s/CNYaZSRELuzzLIY_xYo_YQ" target="_blank" rel="noopener">《如何快速采集分析平台日志，并进行展示监控？》</a></p>
<p>HDFS 存在某业务导致NameNode RPC通信频繁，希望通过 ELK 快速分析 NameNode RPC 操作并对接 Grafana 展示</p>
<h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>ELK 这里使用是 7.5 版本，Kafka 使用是 CDH6.3 自带的，grafana 使用是 6.5</p>
<p>FileBeat、Kafka 搭建过程网上也很多，可以参考下我之前写的</p>
<ul>
<li><a href="https://lihuimintu.github.io/2019/08/05/Filebeat-Kafka/" target="_blank" rel="noopener">Filebeat 采集日志到 Kafka 配置及使用</a></li>
</ul>
<p>贴一下 <code>filebeat.yml</code> 的配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">filebeat.inputs:</span><br><span class="line">- type: log</span><br><span class="line">  enabled: true</span><br><span class="line">  paths:</span><br><span class="line">    - &#x2F;var&#x2F;log&#x2F;hadoop-hdfs&#x2F;hdfs-audit.log</span><br><span class="line">  harvester_buffer_size: 32768</span><br><span class="line">  scan_frequency: 1s</span><br><span class="line">  backoff: 10ms</span><br><span class="line">  # backoff &lt;&#x3D; max_backoff &lt;&#x3D; scan_frequency</span><br><span class="line"></span><br><span class="line">processors:</span><br><span class="line">  - drop_fields:    # 删除字段，不再kibana里面展示，默认情况kibana里面会自动展示这些beat字段, 前缀带@符号的好像删不掉</span><br><span class="line">      fields: [&quot;input&quot;, &quot;ecs&quot;, &quot;agent&quot;, &quot;host&quot;, &quot;log&quot;]</span><br><span class="line"></span><br><span class="line">output.kafka:</span><br><span class="line">  enabled: true</span><br><span class="line">  hosts: [&quot;xxxx:9092&quot;]</span><br><span class="line">  topic: hdfsAudit</span><br><span class="line">  required_acks: 1 # ACK的可靠等级.0&#x3D;无响应,1&#x3D;等待本地消息,-1&#x3D;等待所有副本提交.默认1</span><br><span class="line">  keep_alive: 10s # 连接的存活时间.如果为0,表示短连,发送完就关闭.默认为0秒</span><br><span class="line"></span><br><span class="line">logging.level: error # 最低日志级别，建议在开发时期开启日志并把日志调整为debug或者info级别，在生产环境下调整为error级别</span><br><span class="line">name: xxxx</span><br></pre></td></tr></table></figure>

<p>Elasticsearch、Kibana 搭建过程网上很多，本人参考自下方链接</p>
<ul>
<li><a href="https://blog.csdn.net/llwy1428/article/details/89714709" target="_blank" rel="noopener">记一次 Centos7.4 手动搭建 Elasticsearch 7.4.0 集群</a></li>
<li><a href="https://blog.csdn.net/llwy1428/article/details/89326461" target="_blank" rel="noopener">记一次 Centos7.4 Elasticsearch 集群集成 Kibana</a></li>
</ul>
<p>遇到的坑</p>
<ol>
<li>需要普通用户启动，解压后目录最好放在普通用户家目录下，别放在 root 家目录</li>
<li><code>elasticsearch.yml</code> 中 cluster.initial_master_nodes 参数填的内容是 node.name 参数的值</li>
</ol>
<p>logstash 搭建过程网上很多，这里是直接解压到 root 家目录下<br>在 config 目录下新增一个 kafka.conf</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">input &#123;</span><br><span class="line">    kafka &#123;</span><br><span class="line">        bootstrap_servers &#x3D;&gt; &quot;xxxx:9092&quot;</span><br><span class="line">        topics &#x3D;&gt; [&quot;hdfsAudit&quot;]</span><br><span class="line">        group_id &#x3D;&gt; &quot;logstash&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">filter &#123;</span><br><span class="line">    mutate &#123;</span><br><span class="line">        rename &#x3D;&gt; [&quot;message&quot;, &quot;fileMessage&quot;]</span><br><span class="line">    &#125;</span><br><span class="line">    json &#123;</span><br><span class="line">        source &#x3D;&gt; &quot;fileMessage&quot;</span><br><span class="line">    &#125;</span><br><span class="line">    grok &#123;</span><br><span class="line">        match &#x3D;&gt; &#123;</span><br><span class="line">            &quot;message&quot; &#x3D;&gt; &quot;(?&lt;logd&gt;(%&#123;YEAR&#125;[.&#x2F;-]%&#123;MONTHNUM&#125;[.&#x2F;-]%&#123;MONTHDAY&#125;)) %&#123;TIME:logt&#125; %&#123;LOGLEVEL:level&#125;%&#123;GREEDYDATA&#125;ugi&#x3D;(?&lt;ugi&gt;([\w]*)&#x2F;?)%&#123;GREEDYDATA&#125;ip&#x3D;&#x2F;%&#123;IPV4:ip&#125;%&#123;GREEDYDATA&#125;cmd&#x3D;(?&lt;cmd&gt;([\w]*)&#x2F;?)%&#123;GREEDYDATA&#125;src&#x3D;(?&lt;src&gt;([\w&#x2F;._-]*)&#x2F;?)%&#123;GREEDYDATA&#125;dst&#x3D;%&#123;GREEDYDATA&#125;&quot;</span><br><span class="line">        &#125;</span><br><span class="line">        add_field &#x3D;&gt; &#123;</span><br><span class="line">            &quot;logdate&quot; &#x3D;&gt; &quot;%&#123;logd&#125; %&#123;logt&#125;&quot;</span><br><span class="line">        &#125;</span><br><span class="line">        remove_field &#x3D;&gt; [&quot;fileMessage&quot;, &quot;logt&quot;, &quot;logd&quot;, &quot;message&quot;]</span><br><span class="line">    &#125;</span><br><span class="line">    date &#123; </span><br><span class="line">        match &#x3D;&gt; [ &quot;logdate&quot;,&quot;ISO8601&quot; ]</span><br><span class="line">        target &#x3D;&gt; &quot;@times&quot;</span><br><span class="line">        remove_field &#x3D;&gt; [&#39;logdate&#39;]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">    stdout &#123;</span><br><span class="line">        codec &#x3D;&gt; rubydebug</span><br><span class="line">    &#125;</span><br><span class="line">    elasticsearch &#123;</span><br><span class="line">        index &#x3D;&gt; &quot;logstash-hdfs-auit-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">        hosts &#x3D;&gt; [&quot;xxxx:9200&quot;]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>这里我一开始使用小火牛推荐的dissect来解析，遇到各种问题，实在能力不够解决不了，就用回grok了</p>
</blockquote>
<p>使用 <code>bin/logstash -f config/kafka.conf</code> 启动后观察是否有报错，或者解析不成功的。可行后就使用 nohup 后台启动</p>
<p>Grafana 安装网上很多，官网也有说明</p>
<ul>
<li><a href="https://grafana.com/grafana/download" target="_blank" rel="noopener">Download Grafana</a></li>
</ul>
<p>Logstash、Elasticsearch、Kibana 最好找一台机器部署，因为很吃资源，尽量别影响到集群</p>
<h4 id="展示"><a href="#展示" class="headerlink" title="展示"></a>展示</h4><p>Grafana 配置 NameNode RPC 操作</p>
<p><img src="/images/blog/2019-12-31-1.png" alt></p>
<p>然后创建Dashboard依次配置以下几种查询展示</p>
<p><strong>集群整体RPC每分钟连接次数</strong></p>
<p><img src="/images/blog/2019-12-31-2.png" alt></p>
<p><strong>操作类型每分钟计数</strong></p>
<p><img src="/images/blog/2019-12-31-3.png" alt></p>
<p><strong>每分钟操作类型排行前五</strong></p>
<p><img src="/images/blog/2019-12-31-4.png" alt></p>
<blockquote>
<p>饼图配置可是折腾了我一会，原来还需要安装插件，语法还不会写，orz</p>
</blockquote>
<p>Grafana 默认版本中没有饼图插件，需要自行安装，安装使用参考下方</p>
<ul>
<li><a href="https://blog.csdn.net/liangcha007/article/details/88547126" target="_blank" rel="noopener">Grafana安装饼图插件</a></li>
<li><a href="http://www.yoonper.com/post.php?id=99" target="_blank" rel="noopener">Grafana入门 - 创建饼图面板</a></li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>这一次是采坑花了时间精力去折腾的，整出来后还是有些成就感的</p>
<p>RPC的监控只是监控大数据平台的一个指标，路还很长，再接再厉</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/CNYaZSRELuzzLIY_xYo_YQ" target="_blank" rel="noopener">如何快速采集分析平台日志，并进行展示监控？</a></li>
<li><a href="https://blog.csdn.net/llwy1428/article/details/89714709" target="_blank" rel="noopener">记一次 Centos7.4 手动搭建 Elasticsearch 7.4.0 集群</a></li>
<li><a href="https://blog.csdn.net/llwy1428/article/details/89326461" target="_blank" rel="noopener">记一次 Centos7.4 Elasticsearch 集群集成 Kibana</a></li>
<li><a href="https://blog.csdn.net/holdlg/article/details/89378722" target="_blank" rel="noopener">elasticsearch 7 单机配置</a></li>
<li><a href="https://www.jianshu.com/p/f47bf814e032" target="_blank" rel="noopener">Logstash Filter（四）Mutate</a></li>
<li><a href="https://www.cnblogs.com/duanxuan/p/6517462.html" target="_blank" rel="noopener">Logstash处理json格式日志文件的三种方法</a></li>
<li><a href="http://www.yoonper.com/post.php?id=99" target="_blank" rel="noopener">Grafana入门 - 创建饼图面板</a></li>
<li><a href="https://www.zhukun.net/archives/8300" target="_blank" rel="noopener">ElasticSearch对接Grafana展示Nginx日志数据</a></li>
<li><a href="https://blog.csdn.net/liangcha007/article/details/88547126" target="_blank" rel="noopener">Grafana安装饼图插件</a></li>
</ul>
]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title>记一次 HDFS Corrupt block 事件</title>
    <url>/2019/12/23/hdfs-corrupt/</url>
    <content><![CDATA[<p>记一次断电造成的 HDFS Corrupt block 影响到 HBase 的事件</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>测试环境断电，同事把 HBase 启动起来之后遇到 RS 不可用，反馈后让我去排查下原因</p>
<h4 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h4><p>先把停止的 RS 启动，CM未报明显错误，但是观察发现 Region 数为空</p>
<p><img src="/images/blog/2019-12-23-1.png" alt></p>
<p>既然这样就看日志定位下，先看 RS 日志未有收获，那再看下 HMaster 日志</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2019-12-23 10:00:25,981 INFO org.apache.hadoop.hbase.master.SplitLogManager: total&#x3D;1, unassigned&#x3D;0, tasks&#x3D;&#123;&#x2F;hbase&#x2F;splitWAL&#x2F;WALs%2Fregion-227%2C16020%2C1576486413270-splitting%2Fregion-227%252C16020%252C1576486413270.region-227%252C16020%252C1576486413270.regiongroup-0.1</span><br><span class="line">576833983591&#x3D;last_update &#x3D; 1577066410474 last_version &#x3D; 2 cur_worker_name &#x3D; region-228,16020,1576840559418 status &#x3D; in_progress incarnation &#x3D; 0 resubmits &#x3D; 0 batch &#x3D; installed &#x3D; 1 done &#x3D; 0 error &#x3D; 0&#125;</span><br><span class="line">2019-12-23 10:00:26,759 INFO org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination: Task &#x2F;hbase&#x2F;splitWAL&#x2F;WALs%2Fregion-227%2C16020%2C1576486413270-splitting%2Fregion-227%252C16020%252C1576486413270.region-227%252C16020%252C1576486413270.regiongroup-0.15768339</span><br><span class="line">83591 entered state&#x3D;ERR region-228,16020,1576840559418</span><br><span class="line">2019-12-23 10:00:26,759 WARN org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination: Error splitting &#x2F;hbase&#x2F;splitWAL&#x2F;WALs%2Fregion-227%2C16020%2C1576486413270-splitting%2Fregion-227%252C16020%252C1576486413270.region-227%252C16020%252C1576486413270.regiongroup</span><br><span class="line">-0.1576833983591</span><br><span class="line">2019-12-23 10:00:26,759 WARN org.apache.hadoop.hbase.master.SplitLogManager: error while splitting logs in [hdfs:&#x2F;&#x2F;reh&#x2F;hbase&#x2F;WALs&#x2F;region-227,16020,1576486413270-splitting] installed &#x3D; 1 but only 0 done</span><br><span class="line">2019-12-23 10:00:26,759 WARN org.apache.hadoop.hbase.master.procedure.ServerCrashProcedure: Failed state&#x3D;SERVER_CRASH_SPLIT_LOGS, retry pid&#x3D;3, state&#x3D;RUNNABLE:SERVER_CRASH_SPLIT_LOGS, locked&#x3D;true; ServerCrashProcedure server&#x3D;region-227,16020,1576486413270, splitWal&#x3D;true, meta&#x3D;false; cycles&#x3D;11833</span><br><span class="line">java.io.IOException: error or interrupted while splitting logs in [hdfs:&#x2F;&#x2F;reh&#x2F;hbase&#x2F;WALs&#x2F;region-227,16020,1576486413270-splitting] Task &#x3D; installed &#x3D; 1 done &#x3D; 0 error &#x3D; 1</span><br><span class="line">        at org.apache.hadoop.hbase.master.SplitLogManager.splitLogDistributed(SplitLogManager.java:271)</span><br><span class="line">        at org.apache.hadoop.hbase.master.MasterWalManager.splitLog(MasterWalManager.java:402)</span><br><span class="line">        at org.apache.hadoop.hbase.master.MasterWalManager.splitLog(MasterWalManager.java:387)</span><br><span class="line">        at org.apache.hadoop.hbase.master.MasterWalManager.splitLog(MasterWalManager.java:284)</span><br><span class="line">        at org.apache.hadoop.hbase.master.procedure.ServerCrashProcedure.splitLogs(ServerCrashProcedure.java:253)</span><br><span class="line">        at org.apache.hadoop.hbase.master.procedure.ServerCrashProcedure.executeFromState(ServerCrashProcedure.java:159)</span><br><span class="line">        at org.apache.hadoop.hbase.master.procedure.ServerCrashProcedure.executeFromState(ServerCrashProcedure.java:59)</span><br><span class="line">        at org.apache.hadoop.hbase.procedure2.StateMachineProcedure.execute(StateMachineProcedure.java:189)</span><br><span class="line">        at org.apache.hadoop.hbase.procedure2.Procedure.doExecute(Procedure.java:965)</span><br><span class="line">        at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.execProcedure(ProcedureExecutor.java:1742)</span><br><span class="line">        at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.executeProcedure(ProcedureExecutor.java:1481)</span><br><span class="line">        at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.access$1200(ProcedureExecutor.java:78)</span><br><span class="line">        at org.apache.hadoop.hbase.procedure2.ProcedureExecutor$WorkerThread.run(ProcedureExecutor.java:2058)</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/2019-12-23-2.png" alt></p>
<p>可以推断是拆分日志时出错或中断导致的问题，该问题影响到了 RS 的 region 恢复</p>
<p>既然这个 log 文件有问题，优先恢复服务，将其 move 到临时文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo -u hdfs hadoop fs -mv hdfs:&#x2F;&#x2F;reh&#x2F;hbase&#x2F;WALs&#x2F;region-227,16020,1576486413270-splitting&#x2F;region-227%2C16020%2C1576486413270.region-227%2C16020%2C1576486413270.regiongroup-0.1576833983591 &#x2F;user&#x2F;root</span><br></pre></td></tr></table></figure>

<p>可以看到 Region 有所恢复</p>
<p><img src="/images/blog/2019-12-23-3.png" alt></p>
<p>RS 分管的 Region 也恢复在线</p>
<p><img src="/images/blog/2019-12-23-4.png" alt></p>
<p>接着来看看为啥 block 会出问题</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@region-227 ~]# sudo -u hdfs hdfs fsck &#x2F;</span><br><span class="line">Connecting to namenode via http:&#x2F;&#x2F;region-227:9870&#x2F;fsck?ugi&#x3D;hdfs&amp;path&#x3D;%2F</span><br><span class="line">FSCK started by hdfs (auth:SIMPLE) from &#x2F;192.168.1.227 for path &#x2F; at Mon Dec 23 10:58:33 CST 2019</span><br><span class="line"></span><br><span class="line">&#x2F;user&#x2F;root&#x2F;region-227%2C16020%2C1576486413270.region-227%2C16020%2C1576486413270.regiongroup-0.1576833983591: CORRUPT blockpool BP-911051597-192.168.1.240-1572922263924 block blk_1073813352</span><br><span class="line"></span><br><span class="line">&#x2F;user&#x2F;root&#x2F;region-227%2C16020%2C1576486413270.region-227%2C16020%2C1576486413270.regiongroup-0.1576833983591: CORRUPT 1 blocks of total size 57732 B.</span><br><span class="line">Status: CORRUPT</span><br><span class="line"> Number of data-nodes:	3</span><br><span class="line"> Number of racks:		1</span><br><span class="line"> Total dirs:			1851</span><br><span class="line"> Total symlinks:		0</span><br><span class="line"></span><br><span class="line">Replicated Blocks:</span><br><span class="line"> Total size:	1299310671 B</span><br><span class="line"> Total files:	1428 (Files currently being written: 6)</span><br><span class="line"> Total blocks (validated):	1074 (avg. block size 1209786 B) (Total open file blocks (not validated): 4)</span><br><span class="line">  ********************************</span><br><span class="line">  UNDER MIN REPL&#39;D BLOCKS:	1 (0.09310987 %)</span><br><span class="line">  dfs.namenode.replication.min:	1</span><br><span class="line">  CORRUPT FILES:	1</span><br><span class="line">  CORRUPT BLOCKS: 	1</span><br><span class="line">  CORRUPT SIZE:		57732 B</span><br><span class="line">  ********************************</span><br><span class="line"> Minimally replicated blocks:	1073 (99.90689 %)</span><br><span class="line"> Over-replicated blocks:	0 (0.0 %)</span><br><span class="line"> Under-replicated blocks:	0 (0.0 %)</span><br><span class="line"> Mis-replicated blocks:		0 (0.0 %)</span><br><span class="line"> Default replication factor:	3</span><br><span class="line"> Average block replication:	2.9972067</span><br><span class="line"> Missing blocks:		0</span><br><span class="line"> Corrupt blocks:		1</span><br><span class="line"> Missing replicas:		0 (0.0 %)</span><br><span class="line"> Blocks queued for replication:	0</span><br><span class="line"></span><br><span class="line">Erasure Coded Block Groups:</span><br><span class="line"> Total size:	0 B</span><br><span class="line"> Total files:	0</span><br><span class="line"> Total block groups (validated):	0</span><br><span class="line"> Minimally erasure-coded block groups:	0</span><br><span class="line"> Over-erasure-coded block groups:	0</span><br><span class="line"> Under-erasure-coded block groups:	0</span><br><span class="line"> Unsatisfactory placement block groups:	0</span><br><span class="line"> Average block group size:	0.0</span><br><span class="line"> Missing block groups:		0</span><br><span class="line"> Corrupt block groups:		0</span><br><span class="line"> Missing internal blocks:	0</span><br><span class="line"> Blocks queued for replication:	0</span><br><span class="line">FSCK ended at Mon Dec 23 10:58:33 CST 2019 in 37 milliseconds</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The filesystem under path &#39;&#x2F;&#39; is CORRUPT</span><br></pre></td></tr></table></figure>

<p>HDFS 有三副本策略，3份全丢状态就为 corrupt，看来该 block blk_1073813352 无法恢复了</p>
<p>到 DN 磁盘目录下搜索看是否块文件缺失</p>
<p><img src="/images/blog/2019-12-23-5.png" alt></p>
<p>看来块没有缺失，但是什么问题造成无法恢复呢？</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2019-12-23 10:00:01,739 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: region-227:9866:DataXceiver error processing READ_BLOCK operation  src: &#x2F;192.168.1.228:59434 dst: &#x2F;192.168.1.227:9866</span><br><span class="line">java.io.IOException: Replica is not readable, block&#x3D;BP-911051597-192.168.1.240-1572922263924:blk_1073813352_72571, replica&#x3D;ReplicaWaitingToBeRecovered, blk_1073813352_72571, RWR</span><br><span class="line">  getNumBytes()     &#x3D; 57724</span><br><span class="line">  getBytesOnDisk()  &#x3D; 57724</span><br><span class="line">  getVisibleLength()&#x3D; -1</span><br><span class="line">  getVolume()       &#x3D; &#x2F;dfs&#x2F;dn</span><br><span class="line">  getBlockURI()     &#x3D; file:&#x2F;dfs&#x2F;dn&#x2F;current&#x2F;BP-911051597-192.168.1.240-1572922263924&#x2F;current&#x2F;rbw&#x2F;blk_1073813352</span><br><span class="line">        at org.apache.hadoop.hdfs.server.datanode.BlockSender.&lt;init&gt;(BlockSender.java:283)</span><br><span class="line">        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:598)</span><br><span class="line">        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)</span><br><span class="line">        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)</span><br><span class="line">        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:291)</span><br><span class="line">        at java.lang.Thread.run(Thread.java:748)</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/2019-12-23-6.png" alt></p>
<p><code>DataXceiver error processing READ_BLOCK operation</code> 说明读出现了问题</p>
<p>根据<a href="https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java#L283" target="_blank" rel="noopener">BlockSender.java:283</a>源码判断是跟 <code>replicaVisibleLength</code> 有关</p>
<p><img src="/images/blog/2019-12-23-7.png" alt></p>
<p><code>replicaVisibleLength</code> 又由 <code>getReplica(block, datanode)</code> 得到的 <code>replica</code> 实例调用<code>getVisibleLength()</code>得到</p>
<p><img src="/images/blog/2019-12-23-8.png" alt></p>
<p>查阅<a href="https://www.maiyewang.com/2018/11/26/%E3%80%8Ahadoop-2-x-hdfs%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90%E3%80%8B2-hdfs-%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE/" target="_blank" rel="noopener">资料</a>指导 <code>visible length</code> 表示从数据节点获取数据块副本真实的数据长度</p>
<p><img src="/images/blog/2019-12-23-9.png" alt></p>
<p>再对比之前报错信息的 <code>getVisibleLength()= -1</code> 可知块数据长度变为-1导致无法读取</p>
<p>接着看 -1 是怎么导致的，到这里就有点全靠猜测了</p>
<p>分析源码找到 <a href="https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.java#L99" target="_blank" rel="noopener">FinalizedReplica 99行</a>，猜测其跟 NumBytes 有关，但是根据报错信息上显示<br><code>getNumBytes() = 57724</code>，就有点猜不透了</p>
<p><img src="/images/blog/2019-12-23-10.png" alt></p>
<p>在项目中搜索 -1 看到有个方法可能是关键， <a href="https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java#L835" target="_blank" rel="noopener">DFSStripedOutputStream 835行</a></p>
<p><img src="/images/blog/2019-12-23-11.png" alt></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Return the length of each block in the block group. Unhealthy blocks have a length of -1.</span><br></pre></td></tr></table></figure>

<p>该方法注释说返回 block 的长度，但是对于不健康的块返回 -1</p>
<p>看看判断 block 健康的方法，跳到了<a href="https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java#L1940" target="_blank" rel="noopener">DataStreamer 1940行</a></p>
<p><img src="/images/blog/2019-12-23-12.png" alt></p>
<p><img src="/images/blog/2019-12-23-13.png" alt></p>
<p>DataStreamer 是 HDFS 写流程中将 packet 数据包流式传输到管线的类</p>
<p>到了这里我猜估计就是写过程遇到些故障(断电)，块损坏，然后 DataStreamer 将其置换为故障，然后 DFSStripedOutputStream 就将其长度变为 -1</p>
<p>这就是我猜想的大概，水平有限，谅解谅解</p>
<p>不往下看了，剩下找不到突破口了</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>既然块 3 副本都损坏了，那只能删除这个文件了，这个文件是 HBase 未落盘的 Hlog 切分，无法恢复说明就存在<br>数据丢失的情况了。断电造成的影响是极度可怕的，所以生产环境要考虑完善。</p>
<p>这里还有个问题我未解决，就是为什么切分要恢复的日志未恢复会阻塞 RS 的 Region 上线，带着思考题去研究下这方面的知识。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.51cto.com/xiaolanlan/2071642" target="_blank" rel="noopener">记一次HDFS的block corrupt事件</a></li>
</ul>
]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title>HDFS HA 节点中都处于 standby 状态</title>
    <url>/2019/12/19/HDFS-HA-state-standby/</url>
    <content><![CDATA[<p>Content here</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>在执行 hbase 命令时，报错信息如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.RuntimeException: </span><br><span class="line">org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby</span><br></pre></td></tr></table></figure>

<p>初步判断是 HDFS 的 HA 节点异常</p>
<p>搜索引擎找到相应的错误<a href="https://www.cnblogs.com/stone-learning/p/9249611.html" target="_blank" rel="noopener">Operation category READ is not supported in state standby 故障解决</a></p>
<p>手动执行命令失效<code>Illegal argument: Unable to determine service address for namenode &#39;namenode1&#39;</code></p>
<p>因为我的NameNode不叫<code>namenode1</code>, <code>namenode2</code></p>
<p>在<a href="https://blog.csdn.net/zhangshenghang/article/details/82840602" target="_blank" rel="noopener">HDFS查看异常：Operation category READ is not supported in state standby. Visit</a>中了解到跟下方配置有关，<br>手动搜索<code>hdfs-site.xml</code>，注意有可能你的不叫<code>.nameservice1</code>，这个是自己设置的，你只需要搜索其前缀<code>dfs.ha.namenodes.</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.ha.namenodes.nameservice1&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;namenode223,namenode47&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>

<p>结果我在<code>hdfs-site.xml</code>中未找到， 在<a href="https://blog.csdn.net/weizhonggui/article/details/85068960" target="_blank" rel="noopener">Operation category READ/WRITE is not supported in state standby解决案例</a><br>看到其使用 nn1, nn2 ，随手试试发现可行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hdfs haadmin -getServiceState nn1                                                                                                                                                                </span><br><span class="line">standby                                                               </span><br><span class="line">$ hdfs haadmin -getServiceState nn2</span><br><span class="line">standby</span><br></pre></td></tr></table></figure>

<p>两个 namenode 节点均处于 standby 状态，随后执行</p>
<p>强行手工将 namenode1 状态转换为 active</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hdfs haadmin -transitionToActive --forcemanual nn1</span><br></pre></td></tr></table></figure>

<p>手动执行hbase命令，工作正常，故障解决</p>
]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title>HDFS NameNode HA 实现综述</title>
    <url>/2019/12/16/hdfs-namenode-ha/</url>
    <content><![CDATA[<p>通过 JN 集群共享 NN 节点状态，通过 ZKFC 选举active NN，自动监控 NN 节点状态，自动切换</p>
<hr>
<h4 id="转载来源"><a href="#转载来源" class="headerlink" title="转载来源"></a>转载来源</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">作者：lipeng_bigdata</span><br><span class="line">链接：https:&#x2F;&#x2F;blog.csdn.net&#x2F;lipeng_bigdata&#x2F;article&#x2F;details&#x2F;53637562</span><br><span class="line">来源：CSDN</span><br></pre></td></tr></table></figure>

<h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><p>HDFS 中 NameNode 等的 HA 是基于 ZooKeeper 实现的</p>
<p>它应用了ZooKeeper集群的如下功能或特性</p>
<ol>
<li>只要半数以上节点还存活，就继续能对外提供服务</li>
<li>ZooKeeper通过Paxos算法提供了leader选举功能，其它follower learn leader</li>
<li>ZooKeeper提供了watcher机制，只要ZooKeeper上znode增减，或内容发生变化，或其子znode有增减，客户端都可以通过注册的watcher获得通知</li>
<li>ZooKeeper提供了持久化节点和临时节点，尤其是临时节点EPHEMERAL，其在ZooKeeper客户端连接断掉后，会自动删除</li>
</ol>
<p>正是基于ZooKeeper的上述特性，HDFS的NameNode实现了HA，<br>NameNode的状态大致可分为Active和Standby两种，<br>NameNode竞争在ZooKeeper指定路径上注册临时节点，<br>将自己的host、port、nameserviceId、namenodeId等数据写入节点，<br>哪个NameNode争先写入成功，哪个就成为Active NameNode。<br>然后，会有后台工作线程周期性检查NameNode状态，<br>并在一定条件下（比如Active NameNode节点发生故障等）发生竞选，<br>由NameNode再去竞争，实现故障转移和状态切换。</p>
<p>上述就是HDFS NameNode HA实现的大体思路。</p>
<p>而HDFS NameNode HA的架构和实现细节是怎么样的？下面我来介绍下。</p>
<h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><p>首先上一张Hadoop HDFS NameNode HA的设计图，如下: </p>
<p><img src="/images/blog/2019-12-16-1.png" alt></p>
<p>简单描述下这张图</p>
<p>首先，NameNode（NN）、DataNode（DN）均是HDFS中的组件或节点，NN有两个，分为Active和Standby状态，DN有多个，DN周期性向Active和Standby做数据块汇报</p>
<p>其次，处于中间位置的，实现HDFS NameNode HA最关键的组件是FailoverController，它是运行在NameNode节点上的守护进程，将Active NN节点的信息通过竞争的方式注册到ZooKeeper上，并实现以下三个基本功能</p>
<p>1) 有一个后台线程，周期性的检查NameNode的健康状况，即粗的红色拐弯箭头的Monitor Health，触发状态选举等  </p>
<p>2) 定期保持与ZooKeeper的心跳，完成Leader选举（即Active节点选举）和检查Active NN ZNode节点状态等工作，即最上面与ZooKeeper交互的蓝色箭头</p>
<p>3) 向NameNode发送一些命令，完成状态切换</p>
<p>所以，NN HA 的主体思路就是</p>
<p>1、NN竞争在ZooKeeper上注册，即create一个临时节点，写入NN的host、port、nameserviceId、namenodeI等信息，那个写入成功，那个就是Active状态；</p>
<p>2、注册成功后，通过create后的watcher机制，FailoverController会发送命令给各个NN，让其确定各自状态和职责；</p>
<p>3、Monitor Health会周期性连接NN，检查NN状态，并由可能触发重新选举，即重复1-2；</p>
<p>4、FailoverController会与ZooKeeper保持心跳，注册的临时节点消失后，也会触发重新选举。</p>
<p>那么，HDFS 是如何实现 NameNode HA 的呢？对应上图，Hadoop 中实现了以下模型</p>
<p><img src="/images/blog/2019-12-16-2.png" alt></p>
<p>上图的三个关键组件即是：</p>
<p>1、DFSZKFailoverController（继承自抽象类ZKFailoverController）：ZKFC，故障转移控制器，守护进程，负责总体故障转移和命令发布；</p>
<p>2、ActiveStandbyElector：Active、Standby选举器，实现选举功能；</p>
<p>3、HealthMonitor：健康监视器，实现NN状态的定时监控。</p>
<p>上述组件各自是如何实现的，可以参阅下方链接</p>
<ul>
<li><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-name-node/index.html" target="_blank" rel="noopener">Hadoop NameNode 高可用 (High Availability) 实现解析</a></li>
<li><a href="https://blog.csdn.net/likaiasddsa/article/details/89678455" target="_blank" rel="noopener">HDFS NameNode HA架构分析</a></li>
</ul>
]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title>CDH-HBase 使用 HBCK2 运维</title>
    <url>/2019/12/16/hbase-hbck2/</url>
    <content><![CDATA[<p>CDH-HBase 使用 HBCK2 运维</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>周末 CDH6.3 的集群断电，导致 HBase 出现 RIT 状态。</p>
<p>赶紧把之前学的 hbck2 的知识实践顺便回顾下</p>
<h4 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h4><p>将项目拉取到本地 <code>git clone https://github.com/apache/hbase-operator-tools.git --depth 1</code></p>
<p>编译出jar包上传到集群上 <code>mvn clean package -Dmaven.skip.test=true</code></p>
<p>CDH 集群的话将其上传至<code>/opt/cloudera/parcels/CDH/lib/hbase/lib</code>路径下</p>
<h4 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase org.apache.hbase.HBCK2 &lt;命令&gt;</span><br><span class="line"></span><br><span class="line"># 验证是否可以使用</span><br><span class="line">hbase org.apache.hbase.HBCK2 -v</span><br></pre></td></tr></table></figure>

<p>结果当头一棒，不支持 2.1.0-cdh6.3.1</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">12:48:02.084 [main] INFO  org.apache.hadoop.hbase.client.ConnectionImplementation - Closing master protocol: MasterService</span><br><span class="line">Exception in thread &quot;main&quot; java.lang.UnsupportedOperationException: bypass not supported on server version&#x3D;2.1.0-cdh6.3.1; needs at least a server that matches or exceeds [2.0.3, 2.1.1, 2.2.0, 3.0.0]</span><br><span class="line">        at org.apache.hbase.HBCK2.checkHBCKSupport(HBCK2.java:134)</span><br><span class="line">        at org.apache.hbase.HBCK2.bypass(HBCK2.java:335)</span><br><span class="line">        at org.apache.hbase.HBCK2.doCommandLine(HBCK2.java:686)</span><br><span class="line">        at org.apache.hbase.HBCK2.run(HBCK2.java:631)</span><br><span class="line">        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)</span><br><span class="line">        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)</span><br><span class="line">        at org.apache.hbase.HBCK2.main(HBCK2.java:865)</span><br></pre></td></tr></table></figure>

<p>版本不支持，翻阅文档使用<code>-s</code>跳过版本检查，即 <code>hbase org.apache.hbase.HBCK2 -s</code></p>
<p>更多的命令参考可以参阅下方链接</p>
<ul>
<li><a href="https://github.com/apache/hbase-operator-tools/tree/master/hbase-hbck2" target="_blank" rel="noopener">Apache HBase HBCK2 Tool</a></li>
<li><a href="https://mp.weixin.qq.com/s/GVMWwB1WsKcdvZGfvX1lcA" target="_blank" rel="noopener">HBase 2.0之修复工具HBCK2运维指南</a></li>
</ul>
<h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><p>Region in transition 的信息是在 active hmater WEB UI 页面上查看，<br>如果没有RIT状态的 region，其不会显示，所以正常 HBase 集群是看不到 Region in transition 的内容</p>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>Maven 两种方式跳过编译 test</title>
    <url>/2019/12/14/Maven-SkipTest/</url>
    <content><![CDATA[<p>Maven 跳过单元测试 -maven.test.skip 和 skipTests 的区别</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>使用 IDEA 自带的 Maven 插件编译打包遇到没有跳过testCompile的情况，</p>
<p><img src="/images/blog/2019-12-14-1.png" alt></p>
<p>可我明明设置了跳过test</p>
<p><img src="/images/blog/2019-12-14-2.png" alt></p>
<p>不了解的可以参见<a href="https://my.oschina.net/u/3866531/blog/2250997" target="_blank" rel="noopener">IDEA中Maven打包时如何跳过测试</a></p>
<p>这很不科学，肯定有种猫腻，想起《误杀》中的台词”当你看过一千部电影，这世上就没什么离奇事”</p>
<p>优先解决问题，既然 IDEA 不行就换命令试试，结果通过命令知道了大概了</p>
<h4 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h4><p>两种方式跳过编译 test</p>
<ul>
<li>mvn clean install -DskipTests</li>
<li>mvn clean install -Dmaven.test.skip=true</li>
</ul>
<p>-DskipTests，不执行测试用例，但编译测试用例类生成相应的class文件至target/test-classes下</p>
<p>-Dmaven.test.skip=true，不执行测试用例，也不编译测试用例类</p>
<p>这回知道了。使用maven.test.skip，不但跳过单元测试的运行，也跳过测试代码的编译。使用 mvn package -DskipTests 跳过单元测试，但是会继续编译。</p>
<p>如果没时间修改单元测试的bug，或者单元测试编译错误，使用第二个，不要用第一个</p>
<p>使用 -Dmaven.test.skip=true 美滋滋解决问题了</p>
<h4 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h4><p>了解背景后猜测 IDEA 的跳过test 其实是使用 skipTests 方式，根据图二中的一个红框的tip可以知道的确是使用的skipTest</p>
<p>完结，撒花</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/yizhizouxiaqu/article/details/7449556" target="_blank" rel="noopener">maven两种方式跳过编译test</a></li>
<li><a href="https://blog.csdn.net/rungong123/article/details/88415601" target="_blank" rel="noopener">maven跳过单元测试-maven.test.skip和skipTests的区别</a></li>
<li><a href="https://my.oschina.net/u/3866531/blog/2250997" target="_blank" rel="noopener">IDEA中Maven打包时如何跳过测试 </a></li>
</ul>
]]></content>
      <categories>
        <category>Tools</category>
      </categories>
  </entry>
  <entry>
    <title>HDFS 异构存储</title>
    <url>/2019/12/12/HDFS-Archival-Storage/</url>
    <content><![CDATA[<p>HDFS Archival Storage 是 Hadoop-2.6.0 新增的一个特性，是Hadoop异构存储中的一部分</p>
<hr>
<h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>在大多数Hadoop集群中，随着越来越多的数据被存储更长的时间，对存储的需求已经超过了计算。Hadoop需要一种经济高效且易于管理的解决方案来满足这种存储需求。</p>
<p>Archival Storage 之前的解决方案</p>
<ul>
<li>删除旧的未使用数据，这是以人工识别筛选不必要数据并手动删除它们的操作成本为代价</li>
<li>将更多节点添加到集群，这是单纯解决存储容量而为群集增加了不必要的计算容量</li>
</ul>
<p>Hadoop 需要一种解决方案，以使不断增长的存储容量与计算容量脱钩。具有更高密度和价格便宜，计算能力低的存储节点正变得可用，并且可以用作集群中的冷存储。</p>
<p>根据策略，可以将热存储中的数据移至冷存储中。将更多节点添加到冷存储可以独立于群集中的计算容量来扩展存储。</p>
<p><a href="http://hadoop.apache.org/docs/r2.7.5/hadoop-project-dist/hadoop-hdfs/ArchivalStorage.html" target="_blank" rel="noopener">Archival Storage, SSD &amp; Memory</a>    </p>
<p>HDFS Archival Storage 是 Hadoop-2.6.0 新增的一个特性，详情请参考<a href="https://issues.apache.org/jira/browse/HDFS-6584" target="_blank" rel="noopener">HDFS-6584</a></p>
<h4 id="存储类型"><a href="#存储类型" class="headerlink" title="存储类型"></a>存储类型</h4><p>ARCHIVE，DISK，SSD 和 RAM_DISK</p>
<p>异构存储<a href="https://issues.apache.org/jira/browse/HDFS-2832" target="_blank" rel="noopener">HDFS-2832</a>将数据节点存储模型从单个存储（可能对应于多个物理存储介质）更改为存储的集合，每个存储都对应于一个物理存储介质。还添加了存储类型DISK和SSD的概念，其中DISK是默认存储类型，不填默认是为disk。</p>
<p>添加了一种新的存储类型ARCHIVE，它具有很高的存储密度（PB级存储），但计算能力却很小，可用于支持归档存储。</p>
<p>添加了另一个新的存储类型RAM_DISK，以支持在内存中写入单个副本文件</p>
<h4 id="存储策略"><a href="#存储策略" class="headerlink" title="存储策略"></a>存储策略</h4><p>引入了存储策略的新概念，以允许根据存储策略将文件存储在不同的存储类型中。</p>
<blockquote>
<ul>
<li>Hot - 用于存储和计算。流行且仍用于处理的数据将保留在此策略中。块热时，所有副本都存储在DISK中</li>
<li>Cold - 仅适用于计算量有限的存储。不再使用的数据或需要归档的数据从热存储移动到冷存储。当块处于冷状态时，所有副本都存储在ARCHIVE中</li>
<li>Warm - 部分热和部分冷。当一个块变热时，其某些副本存储在DISK中，其余副本存储在ARCHIVE中。</li>
<li>All_SSD - 用于将所有副本存储在SSD中</li>
<li>One_SSD - 用于将副本之一存储在SSD中。其余副本存储在DISK中</li>
<li>Lazy_Persist - 用于在内存中写入具有单个副本的块。首先将副本写入RAM_DISK，然后将其延迟保存在DISK中</li>
</ul>
</blockquote>
<p><img src="/images/blog/2019-12-12-1.png" alt></p>
<p>更正式地说，存储策略由以下字段组成</p>
<ol>
<li>策略编号</li>
<li>策略名称</li>
<li>块放置的存储类型列表</li>
<li>用于文件创建的后备存储类型列表</li>
<li>用于复制的后备存储类型列表</li>
</ol>
<p>当有足够的空间时，将根据#3中指定的存储类型列表存储块副本。当列表#3中的某些存储类型空间不足时，在#4和#5中指定的后备存储类型列表将分别用于替换空间不足的存储类型，以用于文件创建和复制</p>
<p>以下是典型的存储策略表</p>
<p><img src="/images/blog/2019-12-12-2.png" alt></p>
<p>请注意，Lazy_Persist 策略仅对单个副本块更有效果。对于具有多个副本的块，所有副本将被写入DISK，因为仅将一个副本写入RAM_DISK不会提高整体性能。  </p>
<p>Lazy_Persist 是从 Apache Hadoop 2.6.0 开始支持，详情参考 <a href="https://issues.apache.org/jira/browse/HDFS-6581" target="_blank" rel="noopener">HDFS-6581</a></p>
<h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><ul>
<li><p>dfs.storage.policy.enabled 用于启用/禁用存储策略功能。默认值为true</p>
</li>
<li><p>dfs.datanode.data.dir-在每个数据节点上，以逗号分隔的存储位置应使用其存储类型进行标记。这允许存储策略根据策略将块放置在不同的存储类型上</p>
</li>
</ul>
<blockquote>
<p>A datanode storage location /grid/dn/disk0 on DISK should be configured with [DISK]file:///grid/dn/disk0    </p>
<p>A datanode storage location /grid/dn/ssd0 on SSD can should configured with [SSD]file:///grid/dn/ssd0     </p>
<p>A datanode storage location /grid/dn/archive0 on ARCHIVE should be configured with [ARCHIVE]file:///grid/dn/archive0   </p>
<p>A datanode storage location /grid/dn/ram0 on RAM_DISK should be configured with [RAM_DISK]file:///grid/dn/ram0  </p>
</blockquote>
<p>修改<code>hdfs-site.xml</code>配置文件，在原路径前增加存储属性，如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.datanode.data.dir&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;[ARCHIVE]file:&#x2F;&#x2F;&#x2F;opt&#x2F;hadoop&#x2F;dfs.data&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>

<p>如果数据节点存储位置的默认存储类型没有显式标记的存储类型，则它将为DISK</p>
<p>目前Hadoop-2.6.0可以配置的存储属性包括[DISK]、[ARCHIVE]，Hadoop-2.7.0开始支持[SSD]和[RAM_DISK]</p>
<p>修改完之后重启DataNode，最好两台两台的滚动重启</p>
<h4 id="Mover"><a href="#Mover" class="headerlink" title="Mover"></a>Mover</h4><p>一种新的数据迁移工具</p>
<p>添加了新的数据迁移工具，用于归档数据。该工具类似于Balancer。它会定期扫描HDFS中的文件，以检查块放置是否满足存储策略。对于违反存储策略的块，它将副本移动到其他存储类型，以满足存储策略要求。请注意，只要有可能，它总是尝试在同一节点内移动块副本。如果这不可能（例如，当一个节点没有目标存储类型时），它将通过网络将块副本复制到另一个节点。</p>
<p>Command 如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hdfs mover [-p &lt;files&#x2F;dirs&gt; | -f &lt;local file name&gt;]</span><br></pre></td></tr></table></figure>

<p>-p &lt;文件/目录&gt;    指定以空格分隔的要迁移的HDFS文件/目录列表<br>-f &lt;本地文件&gt;    指定一个本地文件，其中包含要迁移的HDFS文件/目录的列表  </p>
<p>请注意，当同时省略-p和-f选项时，默认路径为根目录。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 列出存储策略</span><br><span class="line"># 列出所有存储策略</span><br><span class="line">hdfs storagepolicies -listPolicies</span><br><span class="line"></span><br><span class="line"># 设置存储策略</span><br><span class="line"># 将存储策略设置为文件或目录</span><br><span class="line">hdfs storagepolicies -setStoragePolicy -path &lt;path&gt; -policy &lt;policy&gt;</span><br><span class="line"></span><br><span class="line"># 获取存储策略</span><br><span class="line"># 获取文件或目录的存储策略</span><br><span class="line">hdfs storagepolicies -getStoragePolicy -path &lt;path&gt;</span><br></pre></td></tr></table></figure>

<p><strong>应用程序的使用</strong></p>
<p>从Apache Hadoop 2.8.0 ，应用程序可以在程序中使用 FileSystem.setStoragePolicy 方法设置存储策略</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">fs.setStoragePolicy(path, &quot;LAZY_PERSIST&quot;);</span><br></pre></td></tr></table></figure>

<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/lipeng_bigdata/article/details/53374391" target="_blank" rel="noopener">HDFS Archival Storage</a></li>
<li><a href="https://www.jianshu.com/p/4ac9129f9a50" target="_blank" rel="noopener">HDFS内存存储原理（Lazy Persist）</a></li>
<li><a href="https://blog.csdn.net/CPP_MAYIBO/article/details/97621141" target="_blank" rel="noopener">HDFS 内存存储支持——LAZY_PERSIST存储策略</a></li>
</ul>
]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title>jmxtrans + influxdb + granafa 监控套件</title>
    <url>/2019/12/10/jmxtrans-influxdb-grafana/</url>
    <content><![CDATA[<p>大数据常用的监控组件的组合</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>jmxtrans+influxdb+Grafana。jmxtrans用于收集服务的jmx信息（通过配置json文件），然后入库到influxDB。通过配置Grafana，把influxDB里面的数据通过web展示出来。</p>
<h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><h5 id="influxDB"><a href="#influxDB" class="headerlink" title="influxDB"></a>influxDB</h5><p>InfluxDb原先有web界面，但是这个web管理界面在1.1以后的版本中被删除。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 下载地址 https:&#x2F;&#x2F;portal.influxdata.com&#x2F;downloads&#x2F;#influxdb</span><br><span class="line">wget https:&#x2F;&#x2F;dl.influxdata.com&#x2F;influxdb&#x2F;releases&#x2F;influxdb-1.7.9.x86_64.rpm</span><br><span class="line"></span><br><span class="line"># 安装</span><br><span class="line">rpm -ivh influxdb-1.7.9.x86_64.rpm</span><br><span class="line"></span><br><span class="line"># 启动</span><br><span class="line">systemctl start influxdb</span><br></pre></td></tr></table></figure>

<p><strong>配置文件</strong></p>
<ul>
<li>默认配置文件：/etc/influxdb/influxdb.conf  — 配置文件详解可以参考<a href="https://www.cnblogs.com/guyeshanrenshiwoshifu/p/9188368.html" target="_blank" rel="noopener">Influxdb配置文件详解—influxdb.conf</a></li>
<li>默认日志文件：/var/log/influxdb/influxd.log</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 启动influxDb shell</span><br><span class="line">influx</span><br><span class="line"></span><br><span class="line"># 查询数据库</span><br><span class="line">show databases;</span><br><span class="line"></span><br><span class="line"># 跳转数据库</span><br><span class="line">use hbaseJmx;</span><br><span class="line"></span><br><span class="line"># 创建数据库</span><br><span class="line">create database hbaseJmx;</span><br><span class="line"></span><br><span class="line"># 查看表</span><br><span class="line">show measurements;</span><br><span class="line"></span><br><span class="line"># 查询表</span><br><span class="line">select * from test limit 1;</span><br></pre></td></tr></table></figure>

<h5 id="jmxtrans"><a href="#jmxtrans" class="headerlink" title="jmxtrans"></a>jmxtrans</h5><p>官网：<a href="http://www.jmxtrans.org/" target="_blank" rel="noopener">http://www.jmxtrans.org/</a><br>下载地址：<a href="http://central.maven.org/maven2/org/jmxtrans/jmxtrans/" target="_blank" rel="noopener">http://central.maven.org/maven2/org/jmxtrans/jmxtrans/</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 下载 270 版本</span><br><span class="line">wget http:&#x2F;&#x2F;central.maven.org&#x2F;maven2&#x2F;org&#x2F;jmxtrans&#x2F;jmxtrans&#x2F;270&#x2F;jmxtrans-270.rpm</span><br><span class="line"></span><br><span class="line"># 安装</span><br><span class="line">rpm -ivh jmxtrans-270.rpm</span><br><span class="line"></span><br><span class="line"># 启动</span><br><span class="line">systemctl start jmxtrans</span><br></pre></td></tr></table></figure>

<p>配置文件: /etc/jmxtrans/wrapper.conf</p>
<h5 id="grafana"><a href="#grafana" class="headerlink" title="grafana"></a>grafana</h5><p>下载文档: <a href="https://grafana.com/grafana/download?platform=linux" target="_blank" rel="noopener">https://grafana.com/grafana/download?platform=linux</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 下载</span><br><span class="line">wget https:&#x2F;&#x2F;dl.grafana.com&#x2F;oss&#x2F;release&#x2F;grafana-6.5.1-1.x86_64.rpm</span><br><span class="line"></span><br><span class="line"># 安装</span><br><span class="line">rpm -ivh grafana-6.5.1-1.x86_64.rpm</span><br><span class="line"></span><br><span class="line"># 启动</span><br><span class="line">systemctl start grafana-server</span><br></pre></td></tr></table></figure>

<p><strong>配置文件</strong></p>
<ul>
<li>配置文件：/etc/sysconfig/grafana-server  </li>
<li>环境变量文件：/etc/grafana/grafana.ini  </li>
<li>日志文件：/var/log/grafana/grafana.log   </li>
</ul>
<p>网页访问，端口号3000，账号密码都是admin</p>
<h4 id="收集-Jmx"><a href="#收集-Jmx" class="headerlink" title="收集 Jmx"></a>收集 Jmx</h4><p>这里以 HBase 为例，收集HBase里面的监控项，通过 Jmxtrans 写入到 influxDB ,然后通过 grafana 展示</p>
<p>配置hbase，将hbase-env.sh中以下配置项的注释打开，打开后为</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export HBASE_JMX_BASE&#x3D;&quot;-Dcom.sun.management.jmxremote.ssl&#x3D;false -Dcom.sun.management.jmxremote.authenticate&#x3D;false&quot;</span><br><span class="line">export HBASE_MASTER_OPTS&#x3D;&quot;$HBASE_MASTER_OPTS $HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port&#x3D;10101&quot;</span><br><span class="line">export HBASE_REGIONSERVER_OPTS&#x3D;&quot;$HBASE_REGIONSERVER_OPTS $HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port&#x3D;10102&quot;</span><br><span class="line">export HBASE_THRIFT_OPTS&#x3D;&quot;$HBASE_THRIFT_OPTS $HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port&#x3D;10103&quot;</span><br><span class="line">export HBASE_ZOOKEEPER_OPTS&#x3D;&quot;$HBASE_ZOOKEEPER_OPTS $HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port&#x3D;10104&quot;</span><br><span class="line">export HBASE_REST_OPTS&#x3D;&quot;$HBASE_REST_OPTS $HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port&#x3D;10105&quot;</span><br></pre></td></tr></table></figure>

<p>我这使用的是CDH的HBase，因此一开始在<code>hbase-env.sh 的 HBase 客户端环境高级配置代码段（安全阀）</code>添加了以上配置内容（去掉export， 否则会报错）</p>
<p><img src="/images/blog/2019-12-10-1.png" alt></p>
<p>重启后发现未生效，服务端口并未启动，但是查看 <code>/etc/hbase/conf/hbase-env.sh</code> 有对应的配置。</p>
<p><img src="/images/blog/2019-12-10-2.png" alt></p>
<p>最后通过全网搜索在<a href="https://blog.xiaoxiaomo.com/2019/04/01/%E7%9B%91%E6%8E%A7-jmxtrans+influxdb+Grafana/" target="_blank" rel="noopener">小小默的博客</a>中找到了原因，该配置无法识别$变量</p>
<p>后面我选择在<code>HBase RegionServer 的 Java 配置选项</code>的最后加了该配置，相应的是Master 的配置就在<code>HBase Master 的 Java 配置选项</code>添加</p>
<p><img src="/images/blog/2019-12-10-3.png" alt></p>
<p>重启 HBase 集群之后可以在服务器查看对应端口是否启动成功</p>
<p><img src="/images/blog/2019-12-10-4.png" alt></p>
<p>Jmxtrans 组件会读取 /var/lib/jmxtrans 目录下所有数据源配置文件(json格式文件)，实时从数据源中获取数据，解析数据后存储到目标源中</p>
<p>在/var/lib/jmxtrans/目录下创建 hbaseJmx.json 配置文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  &quot;servers&quot;:[&#123;</span><br><span class="line">    &quot;port&quot;:&quot;10102&quot;,</span><br><span class="line">    &quot;host&quot;:&quot;RS地址&quot;,</span><br><span class="line">    &quot;queries&quot;:[&#123;</span><br><span class="line">    &quot;obj&quot;:&quot;Hadoop:service&#x3D;HBase,name&#x3D;JvmMetrics&quot;,</span><br><span class="line">    &quot;attr&quot;:[&quot;GcCount&quot;],</span><br><span class="line">    &quot;resultAlias&quot;:&quot;GcCount&quot;,</span><br><span class="line">    &quot;outputWriters&quot;:[&#123;</span><br><span class="line">        &quot;@class&quot;:&quot;com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory&quot;,</span><br><span class="line">        &quot;url&quot;:&quot;http:&#x2F;&#x2F;influxDB地址:8086&#x2F;&quot;,</span><br><span class="line">        &quot;username&quot;:&quot;admin&quot;,</span><br><span class="line">        &quot;password&quot;:&quot;admin&quot;,</span><br><span class="line">        &quot;database&quot;:&quot;hbaseJmx&quot;</span><br><span class="line">     &#125;]</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">    &quot;obj&quot;:&quot;Hadoop:service&#x3D;HBase,name&#x3D;RegionServer,sub&#x3D;Regions&quot;,</span><br><span class="line">    &quot;attr&quot;:[&quot;Namespace_cnet2_table_graph_v_region_723302e21faa1d7335ceda36b96bba02_metric_storeFileSize&quot;],</span><br><span class="line">    &quot;resultAlias&quot;:&quot;Regions&quot;,</span><br><span class="line">    &quot;outputWriters&quot;:[&#123;</span><br><span class="line">        &quot;@class&quot;:&quot;com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory&quot;,</span><br><span class="line">        &quot;url&quot;:&quot;http:&#x2F;&#x2F;influxDB地址:8086&#x2F;&quot;,</span><br><span class="line">        &quot;username&quot;:&quot;admin&quot;,</span><br><span class="line">        &quot;password&quot;:&quot;admin&quot;,</span><br><span class="line">        &quot;database&quot;:&quot;hbaseJmx&quot;</span><br><span class="line">     &#125;]</span><br><span class="line">    &#125;]</span><br><span class="line">  &#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上面的配置中第二部分的 <code>Hadoop:service=HBase,name=RegionServer,sub=Regions</code> 我监听了一个Region的storeFileSize，这是我的Region，不适用你的HBase，所以去掉即可，我这里只是展示多指标的配置样式</p>
<p>数据项说明</p>
<p><img src="/images/blog/2019-12-10-6.png" alt></p>
<p>如果查询所有的属性，则在query子元素中不定义attr属性，influxdb 会将所有的attr属性作为表字段存储</p>
<p>关于HBase监控指标项可以参考这<a href="https://www.cnblogs.com/fengzzi/p/10033728.html" target="_blank" rel="noopener">Hbase监控指标项</a></p>
<p>查看influxDB中的数据，记得要提前创建好 hbaseJmx 库</p>
<p><img src="/images/blog/2019-12-10-5.png" alt></p>
<p>配置grafana这里跳过了，大家自行阅读其他博客，截图太累了。。。。大家可以参阅下方两个博客了解下  </p>
<ul>
<li><a href="https://www.cnblogs.com/fengzzi/p/10033751.html" target="_blank" rel="noopener">jmxtrans + influxdb + granafa 监控套件使用手册</a></li>
</ul>
<h4 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h4><p>jmxtrans + influxdb + granafa 的应用挺广的，比如Zk，kafka、cannel，这里我贴几个我搜索到的资料</p>
<ul>
<li><a href="https://blog.csdn.net/weixin_34125592/article/details/88727897" target="_blank" rel="noopener">jmxtrans+influxdb+grafana监控zookeeper实战</a></li>
<li><a href="https://www.cnblogs.com/wangzhisdu/p/8005962.html" target="_blank" rel="noopener">基于jmxtrans+influxdb+grafana实现对canal监控</a></li>
<li><a href="https://blog.csdn.net/qq_27078095/article/details/63291402" target="_blank" rel="noopener">jmxtrans+influxdb+granafa监控hbase</a></li>
<li><a href="https://www.cnblogs.com/ymsk/p/10685015.html" target="_blank" rel="noopener">Kafka使用jmxtrans+influxdb+grafana监控JMX指标</a></li>
</ul>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.cnblogs.com/wzbk/p/10569683.html" target="_blank" rel="noopener">InfluxDB概念和基本操作</a></li>
<li><a href="https://blog.csdn.net/qq_36421826/article/details/81483575" target="_blank" rel="noopener">Hbase监控搭建</a></li>
<li><a href="https://blog.xiaoxiaomo.com/2019/04/01/%E7%9B%91%E6%8E%A7-jmxtrans+influxdb+Grafana/" target="_blank" rel="noopener">监控–jmxtrans+influxdb+Grafana</a></li>
<li><a href="http://www.yanglajiao.com/article/u012333307/42276557" target="_blank" rel="noopener">jmxtrans梳理</a></li>
</ul>
]]></content>
      <categories>
        <category>Tools</category>
      </categories>
  </entry>
  <entry>
    <title>牛逼的 Bitmap 算法</title>
    <url>/2019/12/09/bitmap/</url>
    <content><![CDATA[<p>牛逼的 Bitmap 算法</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>之前就知道位图很牛逼，感兴趣的可以看玻璃猫的文章<a href="https://mp.weixin.qq.com/s/XqRYoSmRFguVgdGx9WvPDQ" target="_blank" rel="noopener">【漫画】牛逼的Bitmap算法</a>，我只是个技术搬运工</p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title>Cloudera Manager 中 Event Server 报 No such file or directory</title>
    <url>/2019/12/09/CM-Event-Server-NoSuchFileOrDirectory/</url>
    <content><![CDATA[<p>Cloudera Manager 中 Event Server 报 No such file or directory</p>
<hr>
<h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p>如题，错误日志如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2019-12-09 15:50:11,520 ERROR com.cloudera.cmf.eventcatcher.server.EventCatcherService: Error starting EventServer</span><br><span class="line">java.io.FileNotFoundException: &#x2F;var&#x2F;lib&#x2F;cloudera-scm-eventserver&#x2F;v3&#x2F;_qdi.cfs (没有那个文件或目录)</span><br><span class="line">        at java.io.RandomAccessFile.open0(Native Method)</span><br><span class="line">        at java.io.RandomAccessFile.open(RandomAccessFile.java:316)</span><br><span class="line">        at java.io.RandomAccessFile.&lt;init&gt;(RandomAccessFile.java:243)</span><br><span class="line">        at org.apache.lucene.store.MMapDirectory.openInput(MMapDirectory.java:214)</span><br><span class="line">        at org.apache.lucene.index.CompoundFileReader.&lt;init&gt;(CompoundFileReader.java:64)</span><br><span class="line">        at org.apache.lucene.index.CompoundFileReader.&lt;init&gt;(CompoundFileReader.java:52)</span><br><span class="line">        at org.apache.lucene.index.IndexWriter.getFieldInfos(IndexWriter.java:1222)</span><br><span class="line">        at org.apache.lucene.index.IndexWriter.getCurrentFieldInfos(IndexWriter.java:1242)</span><br><span class="line">        at org.apache.lucene.index.IndexWriter.&lt;init&gt;(IndexWriter.java:1175)</span><br><span class="line">        at com.cloudera.cmf.eventcatcher.server.SingleIndexManager.makeIndexWriter(SingleIndexManager.java:139)</span><br><span class="line">        at com.cloudera.cmf.eventcatcher.server.SingleIndexManager.&lt;init&gt;(SingleIndexManager.java:112)</span><br><span class="line">        at com.cloudera.cmf.eventcatcher.server.EventCatcherService.&lt;init&gt;(EventCatcherService.java:286)</span><br><span class="line">        at com.cloudera.cmf.eventcatcher.server.EventCatcherService.main(EventCatcherService.java:169)</span><br></pre></td></tr></table></figure>

<p>即执行 mv /var/lib/cloudera-scm-eventserver /var/lib/cloudera-scm-eventserver-old</p>
<p>然后再启动服务即可。</p>
<p>ClouderaManager的其他服务有问题时，也可同样如何处理</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.cnblogs.com/hark0623/p/4596709.html" target="_blank" rel="noopener">ClouderaManager中Event Server报No such file or directory</a></li>
</ul>
]]></content>
      <categories>
        <category>CDH</category>
      </categories>
  </entry>
  <entry>
    <title>10 道 BAT 大厂海量数据面试题</title>
    <url>/2019/12/07/Mass-data-interview-questions/</url>
    <content><![CDATA[<p>大数据经典海量数据处理面试题</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>先来看一下都有哪些题目</p>
<ul>
<li>如何从大量的 URL 中找出相同的 URL？（百度）</li>
<li>如何从大量数据中找出高频词？（百度）</li>
<li>如何找出某一天访问百度网站最多的 IP？（百度）</li>
<li>如何在大量的数据中找出不重复的整数？（百度）</li>
<li>如何在大量的数据中判断一个数是否存在？（腾讯）</li>
<li>如何查询最热门的查询串？（腾讯）</li>
<li>如何统计不同电话号码的个数？（百度）</li>
<li>如何从 5 亿个数中找出中位数？（百度）</li>
<li>如何按照 query 的频度排序？（百度）</li>
<li>如何找出排名前 500 的数？（腾讯）</li>
</ul>
<p>答案呢？</p>
<p>跳转阅读杨立滨收集的答案<a href="https://mp.weixin.qq.com/s/rjGqxUvrEqJNlo09GrT1Dw" target="_blank" rel="noopener">10 道 BAT 大厂海量数据面试题（附题解+方法总结）</a></p>
<h4 id="处理方法"><a href="#处理方法" class="headerlink" title="处理方法"></a>处理方法</h4><p>对海量数据的处理方法一般都是有以下步骤</p>
<ul>
<li>布隆过滤器</li>
<li>哈希</li>
<li>位图</li>
<li>堆</li>
<li>双层桶划分—-其实本质上就是【分而治之】的思想，重在分的技巧上！</li>
<li>B树</li>
<li>倒排索引</li>
<li>外排序</li>
<li>trie树</li>
<li>分布式处理 mapreduce</li>
</ul>
<p>具体的可以跳转阅读<a href="https://blog.csdn.net/wypersist/article/details/80114709" target="_blank" rel="noopener">大数据十道经典海量数据处理面试题与十个方法大总结</a>第2部分</p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title>HBase 2.0 新特性</title>
    <url>/2019/12/06/HBase-Two-New-features/</url>
    <content><![CDATA[<p>HBase 2.0 新特性</p>
<hr>
<h4 id="转载来源"><a href="#转载来源" class="headerlink" title="转载来源"></a>转载来源</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">作者：GOD_WAR</span><br><span class="line">链接：https:&#x2F;&#x2F;blog.csdn.net&#x2F;young_0609&#x2F;article&#x2F;details&#x2F;94213280</span><br><span class="line">来源：CSDN</span><br></pre></td></tr></table></figure>

<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>HBase 2.0 做了那么多的改动，总有一些新特性值得我们去关注。这里做一个简单的介绍</p>
<p>所有 HBase 2.0 相关的 <a href="https://s.apache.org/hbase-2.0.0-JIRA-changes" target="_blank" rel="noopener">jira，subtask</a></p>
<blockquote>
<p>HBase2.0.0版本自2018年4月30日正式发布起， 到现在已经过了接近15个月(2019年07月)。现在的状态是HBase2.0.x已经EOL了，后面不会再发新的Release版本了，HBase2.1已经发布到HBase2.1.6了，个人预计将来也不会维护太长的时间。今后的HBase2.x的稳定版本将会是HBase2.2.x和HBase2.3.x，尤其是HBase2.2.x，可能成为未来真正意义上经过大厂线上严苛考验的版本。<br>– 转自OpenInx <a href="http://openinx.github.io/2019/07/15/future-work-for-HBase2.x/" target="_blank" rel="noopener">《社区HBase未来值得做的一些工作》</a></p>
</blockquote>
<h4 id="新特性"><a href="#新特性" class="headerlink" title="新特性"></a>新特性</h4><p><strong>A new Region assignment manager</strong><br><a href="https://issues.apache.org/jira/browse/HBASE-14350" target="_blank" rel="noopener">HBASE-14350</a>、<a href="https://issues.apache.org/jira/browse/HBASE-14614" target="_blank" rel="noopener">HBASE-14614</a><br>AssignmentManager V2(“AMv2”)基于Procedure V2实现，能够更快速的分配Region，维护的region状态机存储不再依赖于ZooKeeper，能够更好的应对Region长时间RIT问题。</p>
<p><strong>Offheaping of Read/Write</strong><br><a href="https://issues.apache.org/jira/browse/HBASE-11425" target="_blank" rel="noopener">HBASE-11425</a>、<a href="https://issues.apache.org/jira/browse/HBASE-15179" target="_blank" rel="noopener">HBASE-15179</a><br>减少对Heap内存的使用，改用Offheap区的内存，有效减少GC压力</p>
<p><strong>In-Memory Compaction</strong><br><a href="https://issues.apache.org/jira/browse/HBASE-17343" target="_blank" rel="noopener">HBASE-17343</a><br>重新设计了CompactingMemStore 替代 DefaultMemStore，CompactingMemStore中的数据达到一定大小以后，不是直接Flush成HDFS中的HFile文件，而是先Flush到内存中的一个不可改写的Segment，这样，内存中的多个Segments可以预先合并，当达到一定的大小以后，才Flush成HDFS中的HFile文件，这样做的好处是希望有效降低Compaction所带来的写IO放大问题。</p>
<p><strong>NettyRpcServer</strong><br><a href="https://issues.apache.org/jira/browse/HBASE-17263" target="_blank" rel="noopener">HBASE-17263</a><br>其实并不新鲜，早在1.x 淘宝就有使用，现在 HBase2.0 开始默认使用NettyRpcServer<br>使用Netty替代HBase原生的RPC server，大大提升了HBase RPC的吞吐能力，降低了延迟</p>
<p><strong>Async RPC Client</strong><br><a href="https://issues.apache.org/jira/browse/HBASE-16833" target="_blank" rel="noopener">HBASE-16833</a>、<a href="(https://issues.apache.org/jira/browse/HBASE-15921)">HBASE-15921</a><br>HBase2.0 Client不在是原来同步等待，而是利用异步RPC机制，大大提高Client端请求并发度，有效提高资源利用率，扩大吞吐。</p>
<p><strong>RegionServer Group</strong><br><a href="https://issues.apache.org/jira/browse/HBASE-6721" target="_blank" rel="noopener">HBASE-6721</a><br>在HBase 2.0中我们可以将RegionServer划分到多个逻辑Group中，这样可以提供多租户的能力。</p>
<p><strong>Support for MOB</strong><br><a href="https://issues.apache.org/jira/browse/HBASE-11339" target="_blank" rel="noopener">HBASE-11339</a><br>MOB特性使得HBase支持存储小于10MB 的中等媒体对象数据，这些小对象文件采用独立的HFile文件进行存储，相比原来直接存储大对象插入hbase，其读写效率更高；Mob数据存储还是以hfile格式存储，兼容HBase现有特性，如snapshot、bulkload、replication等。MOB数据文件有独立的compaction和expire clean机制，稳定性更可控。</p>
<p><strong>Support CF-level Storage Policy</strong><br><a href="https://issues.apache.org/jira/browse/HBASE-14061" target="_blank" rel="noopener">HBASE-14061</a><br>支持通过”hbase.hstore.block.storage.policy”配置来设置HFile的存储策略，并且还支持CF级设置以覆盖配置文件中的设置。目前支持的存储策略包括ALL_SSD/ONE_SSD/HOT/WARM/COLD   </p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.cppentry.com/bencandy.php?fid=118&id=201370" target="_blank" rel="noopener">Apache HBase2.0正式发布——内附一手导读材料</a></li>
<li><a href="https://mp.weixin.qq.com/s/kRw2Ed0BG7Zo-3czw2EToA" target="_blank" rel="noopener">HBase实践 说好不哭，但HBase2.0真的好用到哭</a></li>
<li><a href="http://openinx.github.io/2019/07/15/future-work-for-HBase2.x/" target="_blank" rel="noopener">社区HBase未来值得做的一些工作</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>Linux NetHogs 监控工具</title>
    <url>/2019/12/06/Linux-nethogs/</url>
    <content><![CDATA[<p>监控Linux的进程或应用程序的网络流量工具</p>
<hr>
<h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>NetHogs是一款开源、免费的，终端下的网络流量监控工具，它可监控Linux的进程或应用程序的网络流量。NetHogs只能实时监控进程的网络带宽占用情况。NetHogs支持IPv4和IPv6协议，支持本地网卡以及PPP链接。</p>
<h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum -y install nethogs</span><br></pre></td></tr></table></figure>

<h4 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ nethogs</span><br><span class="line">NetHogs version 0.8.5</span><br><span class="line"></span><br><span class="line">    PID USER     PROGRAM                                                                                                           DEV        SENT      RECEIVED</span><br><span class="line">  14434 hbase    &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_181-cloudera&#x2F;bin&#x2F;java                                                                          eth0       69.064	  31.629 KB&#x2F;sec</span><br><span class="line">  10471 kafka    &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_181-cloudera&#x2F;bin&#x2F;java                                                                          eth0        6.201	   6.095 KB&#x2F;sec</span><br><span class="line">   5581 mysql    &#x2F;usr&#x2F;sbin&#x2F;mysqld                                                                                                  eth0       14.833	   4.132 KB&#x2F;sec</span><br><span class="line">   6361 cloude.. &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_181-cloudera&#x2F;bin&#x2F;java                                                                          eth0        1.396	   1.181 KB&#x2F;sec</span><br><span class="line">  32041 yarn     wget                                                                                                              eth0        0.054	   0.604 KB&#x2F;sec</span><br><span class="line">  32119 yarn     curl                                                                                                              eth0        0.113	   0.504 KB&#x2F;sec</span><br><span class="line">  32066 yarn     curl                                                                                                              eth0        0.101	   0.485 KB&#x2F;sec</span><br><span class="line">   7236 root     &#x2F;usr&#x2F;local&#x2F;aegis&#x2F;aegis_client&#x2F;aegis_10_75&#x2F;AliYunDun                                                               eth0       20.661	   0.401 KB&#x2F;sec</span><br><span class="line">  24534 yarn     &#x2F;var&#x2F;tmp&#x2F;kinsing                                                                                                  eth0        0.202	   0.087 KB&#x2F;sec</span><br><span class="line">   8821 yarn     &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_181-cloudera&#x2F;bin&#x2F;java                                                                          eth0        0.279	   0.085 KB&#x2F;sec</span><br><span class="line">  14023 hdfs     &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_181-cloudera&#x2F;bin&#x2F;java                                                                          eth0        0.288	   0.062 KB&#x2F;sec</span><br><span class="line">  20867 root     sshd: root@pts&#x2F;0                                                                                                  eth0        0.453	   0.052 KB&#x2F;sec</span><br><span class="line">  23170 yarn     &#x2F;var&#x2F;tmp&#x2F;kinsing                                                                                                  eth0        0.189	   0.040 KB&#x2F;sec</span><br><span class="line">      ? root     unknown TCP                                                                                                                   0.000	   0.000 KB&#x2F;sec</span><br><span class="line"></span><br><span class="line">  TOTAL                                                                                                                                      113.833      45.359 KB&#x2F;sec</span><br></pre></td></tr></table></figure>

<p>交互式控制指令</p>
<ul>
<li>m: Cycle between display modes (kb/s, kb, b, mb) 切换网速显示单位</li>
<li>r: Sort by received. 按接收流量排序</li>
<li>s : Sort by sent. 按发送流量排序</li>
<li>q : Quit and return to the shell prompt. 退出NetHogs命令工具</li>
</ul>
<p>命令行参数</p>
<ul>
<li>-d delay for refresh rate. 数据刷新时间 如nethogs -d 1 就是每秒刷新一次</li>
<li>-h display available commands usage. 显示命名帮助、使用信息</li>
<li>-p sniff in promiscious mode (not recommended).</li>
<li>-t tracemode.</li>
<li>-V prints Version info.</li>
</ul>
<p>监视特定网卡<code>nethogs eth0</code></p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.cnblogs.com/kerrycode/p/4748970.html" target="_blank" rel="noopener">Linux NetHogs监控工具介绍</a></li>
<li><a href="https://mp.weixin.qq.com/s/JFQZLPmOmW3xDx7yapYRzQ" target="_blank" rel="noopener">每个系统管理员都要知道的 30 个 Linux 系统监控工具</a></li>
</ul>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>HBase 读写 Java API</title>
    <url>/2019/12/06/HBase-Java-Client/</url>
    <content><![CDATA[<p>Content here</p>
<hr>
<h4 id="转载来源"><a href="#转载来源" class="headerlink" title="转载来源"></a>转载来源</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">作者：牧梦者</span><br><span class="line">链接：https:&#x2F;&#x2F;www.cnblogs.com&#x2F;swordfall&#x2F;p&#x2F;10301707.html</span><br><span class="line">来源：博客园</span><br></pre></td></tr></table></figure>

<h4 id="HBase-读写的方式概况"><a href="#HBase-读写的方式概况" class="headerlink" title="HBase 读写的方式概况"></a>HBase 读写的方式概况</h4><p>主要分为：</p>
<ul>
<li>纯 Java API 读写 HBase 的方式</li>
<li>Spark 读写 HBase 的方式</li>
<li>Flink 读写 HBase 的方式</li>
<li>HBase 通过 Phoenix 读写的方式</li>
</ul>
<p>第一种方式是 HBase 自身提供的比较原始的高效操作方式，而第二、第三则分别是 Spark、Flink 集成 HBase 的方式，最后一种是第三方插件 Phoenix 集成的 JDBC 方式，Phoenix 集成的 JDBC 操作方式也能在 Spark、Flink 中调用</p>
<p>注意：使用HBase2.1.2版本，以下代码都是基于该版本开发的。</p>
<h4 id="连接-HBase"><a href="#连接-HBase" class="headerlink" title="连接 HBase"></a>连接 HBase</h4><p>这里我们采用静态方式连接HBase，不同于2.1.2之前的版本，无需创建HBase线程池，HBase2.1.2提供的代码已经封装好，只需创建调用即可</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line">  * 声明静态配置</span><br><span class="line">  *&#x2F;</span><br><span class="line">static Configuration conf &#x3D; null;</span><br><span class="line">static Connection conn &#x3D; null;</span><br><span class="line">static &#123;</span><br><span class="line">       conf &#x3D; HBaseConfiguration.create();</span><br><span class="line">       conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;hadoop01,hadoop02,hadoop03&quot;);</span><br><span class="line">       conf.set(&quot;hbase.zookeeper.property.client&quot;, &quot;2181&quot;);</span><br><span class="line">       try&#123;</span><br><span class="line">           conn &#x3D; ConnectionFactory.createConnection(conf);</span><br><span class="line">       &#125;catch (Exception e)&#123;</span><br><span class="line">           e.printStackTrace();</span><br><span class="line">       &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="创建-HBase-的表"><a href="#创建-HBase-的表" class="headerlink" title="创建 HBase 的表"></a>创建 HBase 的表</h4><p>创建HBase表，是通过Admin来执行的，表和列簇则是分别通过TableDescriptorBuilder和ColumnFamilyDescriptorBuilder来构建</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line"> * 创建只有一个列簇的表</span><br><span class="line"> * @throws Exception</span><br><span class="line"> *&#x2F;</span><br><span class="line">public static void createTable() throws Exception&#123;</span><br><span class="line">    Admin admin &#x3D; conn.getAdmin();</span><br><span class="line">    if (!admin.tableExists(TableName.valueOf(&quot;test&quot;)))&#123;</span><br><span class="line">        TableName tableName &#x3D; TableName.valueOf(&quot;test&quot;);</span><br><span class="line">        &#x2F;&#x2F;表描述器构造器</span><br><span class="line">        TableDescriptorBuilder tdb &#x3D; TableDescriptorBuilder.newBuilder(tableName);</span><br><span class="line">        &#x2F;&#x2F;列族描述器构造器</span><br><span class="line">        ColumnFamilyDescriptorBuilder cdb &#x3D; ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(&quot;user&quot;));</span><br><span class="line">        &#x2F;&#x2F;获得列描述器</span><br><span class="line">        ColumnFamilyDescriptor cfd &#x3D; cdb.build();</span><br><span class="line">        &#x2F;&#x2F;添加列族</span><br><span class="line">        tdb.setColumnFamily(cfd);</span><br><span class="line">        &#x2F;&#x2F;获得表描述器</span><br><span class="line">        TableDescriptor td &#x3D; tdb.build();</span><br><span class="line">        &#x2F;&#x2F;创建表</span><br><span class="line">        admin.createTable(td);</span><br><span class="line">    &#125;else &#123;</span><br><span class="line">        System.out.println(&quot;表已存在&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;&#x2F;关闭连接</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="HBase-表添加数据"><a href="#HBase-表添加数据" class="headerlink" title="HBase 表添加数据"></a>HBase 表添加数据</h4><p>通过put api来添加数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line"> * 添加数据（多个rowKey，多个列族）</span><br><span class="line"> * @throws Exception</span><br><span class="line"> *&#x2F;</span><br><span class="line">public static void insertMany() throws Exception&#123;</span><br><span class="line">    Table table &#x3D; conn.getTable(TableName.valueOf(&quot;test&quot;));</span><br><span class="line">    List&lt;Put&gt; puts &#x3D; new ArrayList&lt;Put&gt;();</span><br><span class="line">    Put put1 &#x3D; new Put(Bytes.toBytes(&quot;rowKey1&quot;));</span><br><span class="line">    put1.addColumn(Bytes.toBytes(&quot;user&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(&quot;wd&quot;));</span><br><span class="line"></span><br><span class="line">    Put put2 &#x3D; new Put(Bytes.toBytes(&quot;rowKey2&quot;));</span><br><span class="line">    put2.addColumn(Bytes.toBytes(&quot;user&quot;), Bytes.toBytes(&quot;age&quot;), Bytes.toBytes(&quot;25&quot;));</span><br><span class="line"></span><br><span class="line">    Put put3 &#x3D; new Put(Bytes.toBytes(&quot;rowKey3&quot;));</span><br><span class="line">    put3.addColumn(Bytes.toBytes(&quot;user&quot;), Bytes.toBytes(&quot;weight&quot;), Bytes.toBytes(&quot;60kg&quot;));</span><br><span class="line"></span><br><span class="line">    Put put4 &#x3D; new Put(Bytes.toBytes(&quot;rowKey4&quot;));</span><br><span class="line">    put4.addColumn(Bytes.toBytes(&quot;user&quot;), Bytes.toBytes(&quot;sex&quot;), Bytes.toBytes(&quot;男&quot;));</span><br><span class="line"></span><br><span class="line">    puts.add(put1);</span><br><span class="line">    puts.add(put2);</span><br><span class="line">    puts.add(put3);</span><br><span class="line">    puts.add(put4);</span><br><span class="line">    table.put(puts);</span><br><span class="line">    table.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="删除-HBase-的列簇或列"><a href="#删除-HBase-的列簇或列" class="headerlink" title="删除 HBase 的列簇或列"></a>删除 HBase 的列簇或列</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line"> * 根据rowKey删除一行数据、或者删除某一行的某个列簇，或者某一行某个列簇某列</span><br><span class="line"> * @param tableName</span><br><span class="line"> * @param rowKey</span><br><span class="line"> * @throws Exception</span><br><span class="line"> *&#x2F;</span><br><span class="line">public static void deleteData(TableName tableName, String rowKey, String rowKey, String columnFamily, String columnName) throws Exception&#123;</span><br><span class="line">    Table table &#x3D; conn.getTable(tableName);</span><br><span class="line">    Delete delete &#x3D; new Delete(Bytes.toBytes(rowKey));</span><br><span class="line">    &#x2F;&#x2F;①根据rowKey删除一行数据</span><br><span class="line">    table.delete(delete);</span><br><span class="line">    </span><br><span class="line">    &#x2F;&#x2F;②删除某一行的某一个列簇内容</span><br><span class="line">    delete.addFamily(Bytes.toBytes(columnFamily));</span><br><span class="line">    </span><br><span class="line">    &#x2F;&#x2F;③删除某一行某个列簇某列的值</span><br><span class="line">    delete.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(columnName));</span><br><span class="line">    table.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="更新-HBase-表的列"><a href="#更新-HBase-表的列" class="headerlink" title="更新 HBase 表的列"></a>更新 HBase 表的列</h4><p>使用Put api直接替换掉即可</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line"> * 根据RowKey , 列簇， 列名修改值</span><br><span class="line"> * @param tableName</span><br><span class="line"> * @param rowKey</span><br><span class="line"> * @param columnFamily</span><br><span class="line"> * @param columnName</span><br><span class="line"> * @param columnValue</span><br><span class="line"> * @throws Exception</span><br><span class="line"> *&#x2F;</span><br><span class="line">public static void updateData(TableName tableName, String rowKey, String columnFamily, String columnName, String columnValue) throws Exception&#123;</span><br><span class="line">    Table table &#x3D; conn.getTable(tableName);</span><br><span class="line">    Put put1 &#x3D; new Put(Bytes.toBytes(rowKey));</span><br><span class="line">    put1.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(columnName), Bytes.toBytes(columnValue));</span><br><span class="line">    table.put(put1);</span><br><span class="line">    table.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="HBase-查询"><a href="#HBase-查询" class="headerlink" title="HBase 查询"></a>HBase 查询</h4><p>HBase查询分为get、scan、scan和filter结合。filter过滤器又分为RowFilter（rowKey过滤器）、SingleColumnValueFilter（列值过滤器）、ColumnPrefixFilter（列名前缀过滤器）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line"> * 根据rowKey查询数据</span><br><span class="line"> * @param tableName</span><br><span class="line"> * @param rowKey</span><br><span class="line"> * @throws Exception</span><br><span class="line"> *&#x2F;</span><br><span class="line">public static void getResult(TableName tableName, String rowKey) throws Exception&#123;</span><br><span class="line">    Table table &#x3D; conn.getTable(tableName);</span><br><span class="line">    &#x2F;&#x2F;获得一行</span><br><span class="line">    Get get &#x3D; new Get(Bytes.toBytes(rowKey));</span><br><span class="line">    Result set &#x3D; table.get(get);</span><br><span class="line">    Cell[] cells &#x3D; set.rawCells();</span><br><span class="line">    for (Cell cell: cells)&#123;</span><br><span class="line">        System.out.println(Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength()) + &quot;::&quot; +</span><br><span class="line">        Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));</span><br><span class="line">    &#125;</span><br><span class="line">    table.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;过滤器 LESS &lt;  LESS_OR_EQUAL &lt;&#x3D;   EQUAL &#x3D;   NOT_EQUAL &lt;&gt;   GREATER_OR_EQUAL &gt;&#x3D;   GREATER &gt;   NO_OP 排除所有</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * @param tableName</span><br><span class="line"> * @throws Exception</span><br><span class="line"> *&#x2F;</span><br><span class="line">public static void scanTable(TableName tableName) throws Exception&#123;</span><br><span class="line">    Table table &#x3D; conn.getTable(tableName);</span><br><span class="line">    </span><br><span class="line">    &#x2F;&#x2F;①全表扫描</span><br><span class="line">    Scan scan1 &#x3D; new Scan();</span><br><span class="line">    ResultScanner rscan1 &#x3D; table.getScanner(scan1);</span><br><span class="line">    </span><br><span class="line">    &#x2F;&#x2F;②rowKey过滤器</span><br><span class="line">    Scan scan2 &#x3D; new Scan();</span><br><span class="line">    &#x2F;&#x2F;str$ 末尾匹配，相当于sql中的 %str  ^str开头匹配，相当于sql中的str%</span><br><span class="line">    RowFilter filter &#x3D; new RowFilter(CompareOperator.EQUAL, new RegexStringComparator(&quot;Key1$&quot;));</span><br><span class="line">    scan2.setFilter(filter);</span><br><span class="line">    ResultScanner rscan2 &#x3D; table.getScanner(scan2);</span><br><span class="line">    </span><br><span class="line">    &#x2F;&#x2F;③列值过滤器</span><br><span class="line">    Scan scan3 &#x3D; new Scan();</span><br><span class="line">    &#x2F;&#x2F;下列参数分别为列族，列名，比较符号，值</span><br><span class="line">    SingleColumnValueFilter filter3 &#x3D; new SingleColumnValueFilter(Bytes.toBytes(&quot;author&quot;), Bytes.toBytes(&quot;name&quot;),</span><br><span class="line">               CompareOperator.EQUAL, Bytes.toBytes(&quot;spark&quot;));</span><br><span class="line">    scan3.setFilter(filter3);</span><br><span class="line">    ResultScanner rscan3 &#x3D; table.getScanner(scan3);</span><br><span class="line">    </span><br><span class="line">    &#x2F;&#x2F;列名前缀过滤器</span><br><span class="line">    Scan scan4 &#x3D; new Scan();</span><br><span class="line">    ColumnPrefixFilter filter4 &#x3D; new ColumnPrefixFilter(Bytes.toBytes(&quot;name&quot;));</span><br><span class="line">    scan4.setFilter(filter4);</span><br><span class="line">    ResultScanner rscan4 &#x3D; table.getScanner(scan4);</span><br><span class="line">    </span><br><span class="line">    &#x2F;&#x2F;过滤器集合</span><br><span class="line">    Scan scan5 &#x3D; new Scan();</span><br><span class="line">    FilterList list &#x3D; new FilterList(FilterList.Operator.MUST_PASS_ALL);</span><br><span class="line">    SingleColumnValueFilter filter51 &#x3D; new SingleColumnValueFilter(Bytes.toBytes(&quot;author&quot;), Bytes.toBytes(&quot;name&quot;),</span><br><span class="line">              CompareOperator.EQUAL, Bytes.toBytes(&quot;spark&quot;));</span><br><span class="line">    ColumnPrefixFilter filter52 &#x3D; new ColumnPrefixFilter(Bytes.toBytes(&quot;name&quot;));</span><br><span class="line">    list.addFilter(filter51);</span><br><span class="line">    list.addFilter(filter52);</span><br><span class="line">    scan5.setFilter(list);</span><br><span class="line">    ResultScanner rscan5 &#x3D; table.getScanner(scan5);</span><br><span class="line">    </span><br><span class="line">    for (Result rs : rscan)&#123;</span><br><span class="line">        String rowKey &#x3D; Bytes.toString(rs.getRow());</span><br><span class="line">        System.out.println(&quot;row key :&quot; + rowKey);</span><br><span class="line">        Cell[] cells &#x3D; rs.rawCells();</span><br><span class="line">        for (Cell cell: cells)&#123;</span><br><span class="line">            System.out.println(Bytes.toString(cell.getFamilyArray(), cell.getFamilyOffset(), cell.getFamilyLength()) + &quot;::&quot;</span><br><span class="line">                    + Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength()) + &quot;::&quot;</span><br><span class="line">                    + Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(&quot;-------------------------------------------&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="闲扯"><a href="#闲扯" class="headerlink" title="闲扯"></a>闲扯</h4><p>现工作使用的是HBase2.1版本的集群，Java 客户端连接 HBase 与之前不一样了</p>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>HBase In-Memory Compaction</title>
    <url>/2019/12/04/HBase-In-Memory-Compaction/</url>
    <content><![CDATA[<p>HBase2.0 新特性之 In-Memory Compaction</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>In-Memory Compaction 是 HBase2.0 中的重要特性之一，通过在内存中引入LSM结构，减少多余数据，实现降低flush频率和减小写放大的效果。</p>
<h4 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h4><p>建议先阅读范欣欣老师的<a href="https://mp.weixin.qq.com/s/LPhfQmx0lhRayA_KQWD0-g" target="_blank" rel="noopener">《HBase原理 &#124; HBase内存管理之MemStore进化论》</a>，对 MemStore 发展有一定的理解</p>
<p>HBase2.0中相关代码以及社区的讨论<a href="https://issues.apache.org/jira/browse/HBASE-14918" target="_blank" rel="noopener">HBASE-14918</a>、<a href="https://blogs.apache.org/hbase/entry/accordion-hbase-breathes-with-in" target="_blank" rel="noopener">博客</a>，介绍In-Memory Compaction的使用和实现原理。</p>
<p>再最后阅读下<a href="https://www.cnblogs.com/hbase-community/p/8879871.html" target="_blank" rel="noopener">HBase2.0新特性之In-Memory Compaction</a></p>
<p>2.0中，是默认开启In-Memory Compaction的，策略为basic</p>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>CentOs 内网无网络安装软件</title>
    <url>/2019/12/04/Centos-no-network-yum/</url>
    <content><![CDATA[<p>在内网无网络的情况下处理 yum 安装</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>为了处理内网服务器无网络安装的情况，避免一个个去下载rpm包，一个个安装，使用到的是yum离线安装方法</p>
<h4 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h4><p>在具备外网访问的服务器上启用yum缓存，这样本地就不会删除安装包及其依赖包了</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim &#x2F;etc&#x2F;yum.conf</span><br><span class="line">修改keepcache&#x3D;1，开启yum缓存</span><br></pre></td></tr></table></figure>

<p>外网服务器yum下载安装包（会自动下载所需的依赖包）</p>
<p>下载后的目录在/var/cache/yum/x86_64/{?}/ 目录下，{?}得看服务器是centos6还是centos7，我这里是7</p>
<p>软件一般是在/var/cache/yum/x86_64/7/base/packages</p>
<p><img src="/images/blog/2019-12-04-1.png" alt></p>
<p>说明下，我这猜 base 目录也好，extras 也好，Graph 也好，我猜这几个目录是根据 repo 的源不同，创建的目录，Graph 是我自建的一个图库yum源，<br>之前我还yum 安装过nginx，当时还有个nginx 的目录</p>
<p><img src="/images/blog/2019-12-04-2.png" alt></p>
<p>这里以 expect 为例</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 下载 expect</span><br><span class="line"># --downloadonly表示只下载不安装</span><br><span class="line">yum -y install --downloadonly expect</span><br></pre></td></tr></table></figure>

<p>将 packages 下的所有 rpm 包复制移动到内网服务器</p>
<p>如果内网服务器没有/var/cache/yum/x86_64/7目录，先执行yum clean packages就可以了</p>
<p>执行安装命令, 指定从缓存处安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># -C 只从缓存安装</span><br><span class="line">yum -y -C install unixODBC-devel</span><br></pre></td></tr></table></figure>

<p>关闭外网服务器的yum缓存和清理安装包</p>
<ol>
<li><p>关闭YUM缓存：修改keepcache=0</p>
</li>
<li><p>清除YUM缓存：yum clean headers 和 yum clean packages</p>
</li>
</ol>
<p>yum 会把下载的软件包和header存储在cache中，而不会自动删除。假如觉得他们占用了磁盘空间，能够使用yum clean指令进行清除，更精确 的用法是yum clean headers清除header，yum clean packages清除下载的rpm包，yum clean all 一股脑儿端</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/weixin_42683408/article/details/87627444" target="_blank" rel="noopener">centos内网安装软件</a></li>
<li><a href="https://www.cnblogs.com/clicli/p/6371118.html" target="_blank" rel="noopener">YUM 安装及清理</a></li>
</ul>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>Keepalived 配置 Nginx  高可用</title>
    <url>/2019/12/02/keepalived-nginx/</url>
    <content><![CDATA[<p>Keepalived 配置 Nginx  高可用</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>通过Keepalived服务vip漂移的方式配置Nginx的高可用；当vip所在节点的Nginx服务宕掉后，会将vip漂移到另外一个节点。而在配置Nginx的配置时，分别在两个节点配置Nginx服务，然后两个Nginx服务配置相同的负载均衡等配置；在配置连接时不使用两个Nginx节点的ip，而是使用vip; 以这样的方式实现Nginx的高可用</p>
<h4 id="Nginx-安装"><a href="#Nginx-安装" class="headerlink" title="Nginx 安装"></a>Nginx 安装</h4><p>在两个节点同时安装Nginx, 此处展示一个节点</p>
<p>默认情况Centos7中无Nginx的源，yum 安装方式参考自宋兴柱-Sindrol 的[<a href="https://www.cnblogs.com/songxingzhu/p/8568432.html" target="_blank" rel="noopener">CentOS7中使用yum安装Nginx的方法</a></p>
<p>Nginx官网提供了Centos的源地址。因此可以如下执行命令添加源 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rpm -Uvh http:&#x2F;&#x2F;nginx.org&#x2F;packages&#x2F;centos&#x2F;7&#x2F;noarch&#x2F;RPMS&#x2F;nginx-release-centos-7-0.el7.ngx.noarch.rpm</span><br></pre></td></tr></table></figure>

<p>安装Nginx </p>
<p>通过<code>yum search nginx</code>看看是否已经添加源成功。如果成功则执行<code>yum -y install nginx</code>命令安装 Nginx</p>
<p>此处修改端口为8089，默认端口为80、配置文件默认在<code>/etc/nginx/conf.d/default.conf</code> </p>
<p><img src="/images/blog/2019-12-02-5.png" alt></p>
<p>为什么是修改<code>default.conf</code>文件呢？从<code>/etc/nginx/nginx.conf</code>看到是指向了``/etc/nginx/conf.d`目录下</p>
<p><img src="/images/blog/2019-12-02-6.png" alt></p>
<p>启动Nginx并设置开机自动运行 </p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl start nginx</span><br><span class="line">systemctl enable nginx</span><br><span class="line">systemctl status nginx</span><br></pre></td></tr></table></figure>

<p>启动成功 </p>
<p><img src="/images/blog/2019-12-02-7.png" alt></p>
<p><img src="/images/blog/2019-12-02-8.png" alt></p>
<h4 id="安装-Keepalived"><a href="#安装-Keepalived" class="headerlink" title="安装 Keepalived"></a>安装 Keepalived</h4><p>两个节点都安装 <code>yum -y install keepalived</code> </p>
<p>添加到开机自启动 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl enable keepalived</span><br></pre></td></tr></table></figure>

<h4 id="配置-HA"><a href="#配置-HA" class="headerlink" title="配置 HA"></a>配置 HA</h4><p>两个节点同时配置 </p>
<p>编辑脚本/etc/keepalived/nginx_check.sh，脚本内容如下(nginx _check.sh) </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">A&#x3D;&#96;ps -C nginx --no-header | wc -l&#96;</span><br><span class="line">if [ $A -eq 0 ]; then</span><br><span class="line">    echo &#96;date&#96;&#39;:  nginx is not healthy, try to killall keepalived&#39; &gt;&gt; &#x2F;etc&#x2F;keepalived&#x2F;keepalived.log</span><br><span class="line">    killall keepalived</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>增加执行权限 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chmod +x &#x2F;etc&#x2F;keepalived&#x2F;nginx_check.sh</span><br></pre></td></tr></table></figure>

<p>修改配置文件/etc/keepalived/keepalived.conf</p>
<p>master 配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">! Configuration File for keepalived</span><br><span class="line"></span><br><span class="line">global_defs &#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_script check_nginx &#123;</span><br><span class="line">    script &quot;&#x2F;etc&#x2F;keepalived&#x2F;nginx_check.sh&quot;</span><br><span class="line">    interval 2 # 每2秒检测一次nginx的运行状态</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state MASTER # 状态，主节点为MASTER，备份节点为BACKUP</span><br><span class="line">    interface ens33 # 绑定VIP的网络接口，通过ifconfig查看自己的网络接口</span><br><span class="line">    virtual_router_id 51 # 虚拟路由的ID号,两个节点设置必须一样,可选IP最后一段使用,相同的VRID为一个组,他将决定多播的MAC地址</span><br><span class="line">    priority 120 # 节点优先级，值范围0～254，MASTER要比BACKUP高，最好高51以上</span><br><span class="line">    advert_int 1 # 组播信息发送时间间隔，两个节点必须设置一样，默认为1秒</span><br><span class="line">    # 设置验证信息，两个节点必须一致</span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_type PASS</span><br><span class="line">        auth_pass 1111</span><br><span class="line">    &#125;</span><br><span class="line">    # 虚拟IP，两个节点设置必须一样。可以设置多个，一行写一个</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        192.168.1.225</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    track_script &#123;</span><br><span class="line">        check_nginx # nginx存活状态检测脚本</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>slave 配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">! Configuration File for keepalived</span><br><span class="line"></span><br><span class="line">global_defs &#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_script check_nginx &#123;</span><br><span class="line">    script &quot;&#x2F;etc&#x2F;keepalived&#x2F;nginx_check.sh&quot;</span><br><span class="line">    interval 2 </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state BACKUP</span><br><span class="line">    interface ens33</span><br><span class="line">    virtual_router_id 51</span><br><span class="line">    priority 50</span><br><span class="line">    advert_int 1</span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_type PASS</span><br><span class="line">        auth_pass 1111</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        192.168.1.225</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    track_script &#123;</span><br><span class="line">        check_nginx</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>启动Keepalived服务 <code>systemctl start keepalived</code></p>
<p>vip已绑定成功 </p>
<p><img src="/images/blog/2019-12-02-9.png" alt></p>
<h4 id="验证高可用"><a href="#验证高可用" class="headerlink" title="验证高可用"></a>验证高可用</h4><p>不管是keepalived宕机还是nginx宕机</p>
<p>都会切换到BACKUP节点上</p>
<p><img src="/images/blog/2019-12-02-10.png" alt></p>
<h4 id="非抢占模式"><a href="#非抢占模式" class="headerlink" title="非抢占模式"></a>非抢占模式</h4><p>keepalived的HA分为抢占模式和非抢占模式，抢占模式即MASTER从故障中恢复后，会将VIP从BACKUP节点中抢占过来。非抢占模式即MASTER恢复后不抢占BACKUP升级为MASTER后的VIP。 </p>
<p>之前的配置都是抢占模式的，如果想配置非抢占模式只改了两个地方：<br>1&gt;  在vrrp_instance块下两个节点各增加了nopreempt指令，表示不争抢vip<br>2&gt;  节点的state都为BACKUP</p>
<p>两个keepalived节点都启动后，默认都是BACKUP状态，双方在发送组播信息后，会根据优先级来选举一个MASTER出来。由于两者都配置了nopreempt，所以MASTER从故障中恢复后，不会抢占vip。这样会避免VIP切换可能造成的服务延迟。 </p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/JRMTtSYBctsnFtFqIbog6A" target="_blank" rel="noopener">0684-如何配置Nginx高可用</a></li>
<li><a href="https://www.cnblogs.com/yaboya/p/9107442.html" target="_blank" rel="noopener">7.Nginx_Keepalived高可用配置</a></li>
<li><a href="https://www.cnblogs.com/lichunyang321/p/8889326.html" target="_blank" rel="noopener">Keepalived+Nginx实现高可用（HA）</a></li>
<li><a href="https://blog.csdn.net/YLD10/article/details/80242487" target="_blank" rel="noopener">修改 nginx 的默认端口</a></li>
</ul>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>磁盘阵列 Raid</title>
    <url>/2019/11/28/Linux-RAID/</url>
    <content><![CDATA[<p>Redundant Arrays of independent Disks,RAID, 廉价冗余（独立）磁盘阵列</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>RAID 是一种把多块独立的物理硬盘按不同的方式组合起来形成一个硬盘组（逻辑硬盘），提供比单个硬盘更高的存储性能和数据备份技术。</p>
<p>RAID技术，可以实现把多个磁盘组合在一起作为一个逻辑卷提供磁盘跨越功能；可以把数据分成多个数据块（Block）并行写入/读出多个磁盘以提高访问磁盘的速度；可以通过镜像或校验操作提供容错能力。具体的功能以不同的RAID组合实现。</p>
<p>在用户看来，RAID组成的磁盘组就像是一个硬盘，可以对它进行分区、格式化等操作。RAID的存储速度比单个硬盘高很多，并且可以提供自动数据备份，提供良好的容错能力。</p>
<p>其底层是使用了条带化技术，RAID 条带（strip）是把连续的数据分割成相同大小的数据块，把每段数据分别写入到阵列中的不同磁盘上的方法。</p>
<h4 id="级别"><a href="#级别" class="headerlink" title="级别"></a>级别</h4><h5 id="JBOD"><a href="#JBOD" class="headerlink" title="JBOD"></a>JBOD</h5><p>JBOD （ Just a Bunch Of Disks ）不是标准的 RAID 等级，它通常用来表示一个没有控制软件提供协调控制的磁盘集合。 JBOD 将多个物理磁盘串联起来，提供一个巨大的逻辑磁盘。 JBOD （如图 1 ）的数据存放机制是由第一块磁盘开始按顺序往后存储，当前磁盘存储空间用完后，再依次往后面的磁盘存储数据。 JBOD 存储性能完全等同于单块磁盘，而且也不提供数据安全保护。它只是简单提供一种扩展存储空间的机制， JBOD 可用存储容量等于所有成员磁盘的存储空间之和。目前 JBOD 常指磁盘柜，而不论其是否提供 RAID 功能。</p>
<h5 id="RAID0"><a href="#RAID0" class="headerlink" title="RAID0"></a>RAID0</h5><p>数据在从内存缓冲区写入磁盘时，根据磁盘数量将数据分成N份，这些数据同时并发写入N块磁盘，使得数据整体写入速度是一块磁盘的N倍，读取的时候也一样，因此RAID0具有极快的数据读写速度。但是RAID0不做数据备份，N块磁盘中只要有一块损坏，数据完整性就被破坏，所有磁盘的数据都会损坏。</p>
<h5 id="RAID1"><a href="#RAID1" class="headerlink" title="RAID1"></a>RAID1</h5><p>数据在写入磁盘时，将一份数据同时写入两块磁盘，这样任何一块磁盘损坏都不会导致数据丢失，只能 2 块盘，盘的大小可以不一样，以小的为准。</p>
<p>插入一块新磁盘就可以通过复制数据的方式自动修复，具有极高的可靠性。10G + 10G 只有 10G 有效数据，另一个做备份。它有 100% 的冗余，缺点是浪费资源，成本高。</p>
<h5 id="RAID10"><a href="#RAID10" class="headerlink" title="RAID10"></a>RAID10</h5><p>结合 RAID0 和 RAID1 两种方案，将所有磁盘平均分成两份大磁盘，数据同时在两份大磁盘写入，相当于 RAID1，但是在每一份大磁盘里面有 N/2 块磁盘，利用 RAID0 技术并发读写，既提高可靠性又改善性能，不过 RAID10 的磁盘利用率较低，有一半的磁盘用来写备份数据。</p>
<h5 id="RAID3"><a href="#RAID3" class="headerlink" title="RAID3"></a>RAID3</h5><p>一般情况下，一台服务器上不会出现同时损坏两块磁盘的情况，在只损坏一块磁盘的情况下，如果能利用其它磁盘的数据恢复损坏磁盘的数据，就能在保证可靠性和性能的同时，大幅提升磁盘利用率。</p>
<p>在数据写入磁盘的时候，将数据分成N-1份，并发写入N-1块磁盘，并在第N块磁盘记录校验数据，任何一块磁盘损坏（包括校验数据磁盘），都可以利用其它N-1块磁盘的数据修复。</p>
<p>但是在数据修改较多的场景中，任何磁盘修改数据都会导致第N块磁盘重写校验数据，频繁写入的后果是第N块磁盘比其它磁盘容易损坏，需要频繁更换，所以RAID3很少在实践中使用。</p>
<h5 id="RAID5"><a href="#RAID5" class="headerlink" title="RAID5"></a>RAID5</h5><p>相比RAID3，更多被使用的方案是RAID5。</p>
<p>RAID5和RAID3很相似，但是校验数据不是写入第N块磁盘，而是螺旋式地写入所有磁盘中。这样校验数据的修改也被平均到所有磁盘上，避免RAID3频繁写坏一块磁盘的情况。</p>
<h5 id="RAID6"><a href="#RAID6" class="headerlink" title="RAID6"></a>RAID6</h5><p>如果数据需要很高的可靠性，在出现同时损坏两块磁盘的情况下（或者运维管理水平比较落后，坏了一块磁盘但是迟迟没有更换，导致又坏了一块磁盘），仍然需要修复数据，这时候可以使用RAID6。</p>
<p>RAID6 和 RAID5 类似，但是数据只写入N-2块磁盘，并螺旋式地在两块磁盘中写入校验信息（使用不同算法生成）。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>在相同磁盘数目（N）的情况下，各种RAID技术的比较如下表所示</p>
<p><img src="/images/blog/2019-12-01-1.png" alt></p>
<p>RAID技术有硬件实现，比如专用的RAID卡或者主板直接支持，也可以通过软件实现，在操作系统层面将多块磁盘组成RAID，在逻辑视作一个访问目录。RAID 技术在传统关系数据库及文件系统中应用比较广泛，是改善计算机存储特性的重要手段。</p>
<p>RAID 技术只是在单台服务器的多块磁盘上组成阵列，大数据需要更大规模的存储空间和访问速度。将 RAID 技术原理应用到分布式服务器集群上，就形成了 Hadoop 分布式文件系统 HDFS 的架构思想。</p>
<p>RAID5 一般使用 XOR 编码，因为只需要容忍单个磁盘故障，而 RAID6 使用 Reed-Solomon 和两个奇偶校验块来容忍最多两个磁盘故障。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/9WRXz9E_H_R7d9tYf5Ih4w" target="_blank" rel="noopener">一文详解大规模数据计算处理原理及操作重点</a></li>
<li><a href="https://cloud.tencent.com/developer/article/1363388" target="_blank" rel="noopener">什么是HDFS的纠删码</a></li>
<li><a href="https://www.cnblogs.com/oneasdf/p/9367152.html" target="_blank" rel="noopener">RAID技术超详细讲解</a></li>
</ul>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>HBase 写 WAL 先还是 Memestore ？</title>
    <url>/2019/11/27/HBase-WAL-Memestore/</url>
    <content><![CDATA[<p>HBase 先写 WAL 先还是 Memestore ？</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>第一次遇到这个问题时是刚学HBase时，阅读范欣欣老师的<a href="http://hbasefly.com/2016/03/23/hbase_writer/" target="_blank" rel="noopener">《HBase － 数据写入流程解析》</a>文章的评论看到的。</p>
<p><img src="/images/blog/2019-11-27-1.png" alt></p>
<p>后续又跟同事讨论过这个问题。在这里记录下理清楚的思路。</p>
<h4 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h4><p>在<a href="https://mp.weixin.qq.com/s/AORh1vI3L5e7B3MAI8zizg" target="_blank" rel="noopener">《一条数据的HBase之旅，简明HBase入门教程-Write全流程》</a>找到了相应解释。</p>
<ul>
<li><p>在0.94版本之前，Region中的写入顺序是先写WAL再写MemStore，这与WAL的定义也相符。</p>
</li>
<li><p>但在0.94版本中，将这两者的顺序颠倒了，当时颠倒的初衷，是为了使得行锁能够在WAL sync之前先释放，从而可以提升针对单行数据的更新性能。详细问题单，请参考HBASE-4528</p>
</li>
<li><p>在2.0版本中，这一行为又被改回去了，原因在于修改了行锁机制以后(下面内容将讲到)，发现了一些性能下降，而HBASE-4528中的优化却无法再发挥作用，详情请参考HBASE-15158。改动之后的逻辑也更简洁了。</p>
</li>
</ul>
<p>这里说2.x版本又改回去了的原因只是因为之前性能下降，但是没有说为什么性能下降，改回去怎么提升了性能。我还是一脸懵逼。带着打破砂锅问到底的心态，我去翻了翻<a href="https://issues.apache.org/jira/browse/HBASE-15158" target="_blank" rel="noopener">HBASE-15158</a>。</p>
<p><img src="/images/blog/2019-11-27-2.png" alt></p>
<p>issues 给的解释原文:  The old ordering was put in place because it got better performance in a time when WAL was different and before row locks were read/write (<a href="https://issues.apache.org/jira/browse/HBASE-12751" target="_blank" rel="noopener"><del>HBASE-12751</del></a>). </p>
<p>渣英语，翻译过来旧方式(即先写WAL再写MemStore)之所以被采用，是因为它在WAL不同的时候以及在读/写行锁之前获得了更好的性能。</p>
<p>官方有做相应的性能测试<a href="https://issues.apache.org/jira/browse/HBASE-15046" target="_blank" rel="noopener">HBASE-15046</a></p>
<p>通读下来，我理解是采用旧方式提升了 CAS 接口的吞吐，同时性能测试上显示没有降低，反而提高。</p>
<p>为什么提升了CAS吞吐？</p>
<p>先来看看 CAS 接口概述，<a href="https://mp.weixin.qq.com/s/WbCl8J1e5-IItD-l9rFQ_w" target="_blank" rel="noopener">《HBase客户端避坑指南》</a>的第2部分有介绍，自行阅读。</p>
<p>就是因为 branch-1 的 HBase 写入操作的顺序导致 CAS 接口是 Region 级别的串行执行，如果不串行执行(即 CAS 运行步骤第 2 步)，客户端B可以在客户端A释放行锁，但没有推进 MVCC 的情况下，拿到相同行，然后更新。客户端认为更新两次，但是服务端只执行了一次。所以因为这个设计，所以强制 CAS 是 Region 级别的串行，就算更新不同行也是串行的，极度影响性能。</p>
<p>HBase 2.x 调整了顺序。这样对同一个 Region 内的不同行可以并行执行 CAS，大大提高了 Region 内的 CAS 吞吐。</p>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>Linux 系统硬链接和软链接</title>
    <url>/2019/11/27/Hard-Link-Symbolic-Link/</url>
    <content><![CDATA[<p>硬链接和软链接</p>
<hr>
<h4 id="转载出处"><a href="#转载出处" class="headerlink" title="转载出处"></a>转载出处</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">作者：songguojun</span><br><span class="line">链接：https:&#x2F;&#x2F;www.cnblogs.com&#x2F;songgj&#x2F;p&#x2F;9115954.html</span><br><span class="line">来源：博客园</span><br></pre></td></tr></table></figure>

<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>在 Linux 系统中有种文件是链接文件，可以为解决文件的共享使用。链接的方式可以分为两种，一种是硬链接（Hard Link），另一种是软链接或者也称为符号链接（Symbolic Link）</p>
<h4 id="硬链接"><a href="#硬链接" class="headerlink" title="硬链接"></a>硬链接</h4><p>硬链接是指通过索引节点来进行链接。在Linux的文件系统中，保存在磁盘分区中的文件不管是什么类型都会给它分配一个编号，这个编号被称为索引节点编号号(Inode Index)或者Inode，它是文件或者目录在一个文件系统中的唯一标识，文件的实际数据放置在数据区域（data block），它存储着文件重要参数信息，也就是元数据 （metadata），比如创建时间、修改时间、文件大小、属主、归属的用户组、读写权限、数据所在block号等</p>
<p><img src="/images/blog/2019-11-27-3.png" alt></p>
<p>在Linux系统中，多个文件名指向同一索引节点(Inode)是正常且允许的。一般这种链接就称为硬链接。硬链接的作用之一是允许一个文件拥有多个有效路径名，这样用户就可以建立硬链接到重要的文件，以防止“误删”源数据(很多硬件，如netapp存储中的快照功能就应用了这个原理，增加一个快照就多了一个硬链接)。不过硬链接只能在同一文件系统中的文件之间进行链接，不能对目录进行创建。之所以文件建立了硬链接就会防止数据误删，是因为文件系统的原理是，只要文件的索引节点还有一个以上的链接（仅删除了该文件的指向），只删除其中一个链接并不影响索引节点本身和其他的链接（数据的实体并未删除），只有当最后一个链接被删除后，此时如果有新数据要存储到磁盘上，被删除的文件的数据块及目录的链接才会被释放，空间被新数据暂用覆盖。</p>
<h4 id="软链接"><a href="#软链接" class="headerlink" title="软链接"></a>软链接</h4><p>软链接（也叫符号链接），类似于windows系统中的快捷方式，与硬链接不同，软链接就是一个普通文件，只是数据块内容有点特殊，文件用户数据块中存放的内容是另一文件的路径名的指向，通过这个方式可以快速定位到软连接所指向的源文件实体。软链接可对文件或目录创建。</p>
<p>软链接作用:   </p>
<ul>
<li>便于文件的管理，比如把一个复杂路径下的文件链接到一个简单路径下方便用户访问。  </li>
<li>节省空间解决空间不足问题，某个文件文件系统空间已经用完了，但是现在必须在该文件系统下创建一个新的目录并存储大量的文件，那么可以把另一个剩余空间较多的文件系统中的目录链接到该文件系统中。   </li>
</ul>
<p>删除软链接并不影响被指向的文件，但若被指向的原文件被删除，则相关软连接就变成了死链接。</p>
<h4 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h4><p><strong>软链接</strong></p>
<ul>
<li>软链接是存放另一个文件的路径的形式存在</li>
<li>软链接可以跨文件系统，硬链接不可以</li>
<li>软链接可以对一个不存在的文件名进行链接，硬链接必须要有源文件</li>
<li>软链接可以对目录进行链接</li>
</ul>
<p><strong>硬链接</strong></p>
<ul>
<li>硬链接，以文件副本的形式存在。但不占用实际空间</li>
<li>不允许给目录创建硬链接</li>
<li>硬链接只有在同一个文件系统中才能创建</li>
<li>删除其中一个硬链接文件并不影响其他有相同 inode 号的文件</li>
</ul>
<p>不论是硬链接或软链接都不会将原本的档案复制一份，只会占用非常少量的磁碟空间。</p>
<h4 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h4><p>linux系统可以用ln命令来创建链接文件。</p>
<p>ln命令格式： </p>
<p>ln [参数] [源文件或目录] [目标文件或目录]</p>
<p>软链接 （符号链接） <code>ln -s source  target</code><br>硬链接 （实体链接） <code>ln source  target</code></p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>CDH 如何给 Hadoop 集群划分角色</title>
    <url>/2019/11/26/CDH-Assign-Roles/</url>
    <content><![CDATA[<p>介绍由Cloudera Manager管理的CDH集群的角色划分</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>请自行参阅 Fayson 的文章<a href="https://mp.weixin.qq.com/s/KcTAAo4nrMw-xKCZiChK_w" target="_blank" rel="noopener">如何给Hadoop集群划分角色</a></p>
]]></content>
      <categories>
        <category>CDH</category>
      </categories>
  </entry>
  <entry>
    <title>HBase 场景</title>
    <url>/2019/11/26/HBase-scene/</url>
    <content><![CDATA[<p>HBase 场景梳理</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>运维 HBase 很久了，记录下HBase的场景</p>
<h4 id="梳理"><a href="#梳理" class="headerlink" title="梳理"></a>梳理</h4><p><img src="/images/blog/2019-11-26-1.png" alt></p>
<ul>
<li>对象存储：我们知道不少的头条类、新闻类的的新闻、网页、图片存储在HBase之中，一些病毒公司的病毒库也是存储在HBase之中</li>
<li>时序数据：HBase之上有OpenTSDB模块，可以满足时序类场景的需求</li>
<li>推荐画像：特别是用户的画像，是一个比较大的稀疏矩阵，蚂蚁的风控就是构建在HBase之上</li>
<li>时空数据：主要是轨迹、气象网格之类，滴滴打车的轨迹数据主要存在HBase之中，另外在技术所有大一点的数据量的车联网企业，数据都是存在HBase之中</li>
<li>CubeDB OLAP：Kylin一个cube分析工具，底层的数据就是存储在HBase之中，不少客户自己基于离线计算构建cube存储在hbase之中，满足在线报表查询的需求</li>
<li>消息/订单：在电信领域、银行领域，不少的订单查询底层的存储，另外不少通信、消息同步的应用构建在HBase之上</li>
<li>Feeds流：典型的应用就是xx朋友圈类似的应用</li>
<li>NewSQL：之上有Phoenix的插件，可以满足二级索引、SQL的需求，对接传统数据需要SQL非事务的需求</li>
</ul>
<p>参自: <a href="https://mp.weixin.qq.com/s/A3_qPtuDOxUtsk67D7qFNQ" target="_blank" rel="noopener">再谈HBase八大应用场景</a></p>
<p>与 Flink 的结合</p>
<p>Flink 是一个偏向于PaaS层的技术，而客户无法直接通过使用Flink来解决自己的场景化需求，在Flink的前面需要像Kafka这样的工具实现数据的导入，在其后面还需要一些像HBase这样的存储工具帮助实现数据的存储。</p>
<p><a href="https://mp.weixin.qq.com/s/3a_7_XLvJ6sLxu_jILdfpg" target="_blank" rel="noopener">Flink+HBase场景化解决方案</a></p>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>HDFS 数据平衡</title>
    <url>/2019/11/20/DataNode-Balancer-DiskBalancer/</url>
    <content><![CDATA[<p>节点间平衡与节点内平衡</p>
<hr>
<h4 id="转载出处"><a href="#转载出处" class="headerlink" title="转载出处"></a>转载出处</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">作者：LittleMagic</span><br><span class="line">链接：https:&#x2F;&#x2F;www.jianshu.com&#x2F;p&#x2F;5c6459fbe9ee</span><br><span class="line">来源：简书</span><br></pre></td></tr></table></figure>

<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>HDFS容易发生数据不平衡的问题。这其中包括各个DataNode之间存储的数据量差异，以及一个DataNode内部各磁盘之间存储的数据量差异。</p>
<p>HDFS专门提供了对应的机制来解决。</p>
<h4 id="节点间平衡-Balancer"><a href="#节点间平衡-Balancer" class="headerlink" title="节点间平衡 Balancer"></a>节点间平衡 Balancer</h4><p>CDH在HDFS中提供了Balancer角色，使我们可以免于用命令行执行start-balancer.sh来手动配置。Cloudera Manager里与Balancer有关的配置项有以下这些。</p>
<ul>
<li>Balancing Threshold：Balancer平衡的阈值。平衡过程结束后，所有节点的磁盘占用率与集群的平均占用率之差必须小于threshold（按百分比计）。默认值是10，我们设成了5。</li>
<li>Rebalancing Policy：计算平衡度的策略，有DataNode和BlockPool两种。前者是按节点级别来算，后者是按块池级别来算。后者只有对HDFS Federation才有效，所以我们选前者。</li>
<li>Included/Excluded Hosts：分别用来指定参与平衡的节点和被排除的节点。这样可以先人为判断数据分布情况，然后只让我们认为需要平衡的节点来操作。</li>
<li>dfs.balancer.moverThreads/dispatcherThreads：分别表示移动数据的线程池大小，和调度数据移动方案的线程池大小，默认值1000和200。</li>
<li>dfs.datanode.balance.max.concurrent.moves：表示能够同时移动的块（英文说法叫in-flight）数量，默认值50。</li>
<li>dfs.balancer.max-size-to-move：表示在Balancer的一次迭代（下面会提到）中，一个DataNode的最大数据交换量，默认值10G。</li>
<li>另外，还有一个出现在DataNode参数但又与平衡相关的：dfs.datanode.balance.bandwidthPerSec，即每个节点可以用来做平衡的最大带宽，默认1MB/s。这个值在多数情况下是偏小的，可以适当增大，如10甚至20。千万注意不能挤占太多带宽，以保证正常业务的运行。</li>
</ul>
<p>CDH Balancer的用法很简单，只需要设定好上述参数，再点击Actions→Rebalance菜单项，就会自动开始平衡了。</p>
<p><img src="/images/blog/2019-11-20-1.png" alt></p>
<p>那么Balancer内部是如何执行的呢？Balancer类的源码位于org.apache.hadoop.hdfs.server.balancer包中，限于篇幅，就不贴出来了，只简单叙述一下。<br>Balancer是迭代执行的，也就是说每次平衡都只移动一定量的数据，然后检查集群是否符合平衡的标准。迭代的大致步骤是：</p>
<p>根据平衡度策略，计算出集群使用量均值，并与平衡阈值作比较，将节点按使用量从高到低划分为4类。<br>根据划分的4类节点，确认出需要平衡的源节点与目标节点对，并计算每对节点之间的数据交换量。<br>构造Dispatcher（这是与Balancer在同一个包中的类），初始化mover和dispatcher线程池。mover用来移动块，dispatcher用来调度节点对。<br>对每个节点上的块，确认它是否可以成为一个好的候选块。如果可以，那么它就会被移动到目标节点上去。</p>
<p>从宏观上看，就是这张图。</p>
<p><img src="/images/blog/2019-11-20-2.png" alt></p>
<h4 id="节点内平衡-DiskBalancer"><a href="#节点内平衡-DiskBalancer" class="headerlink" title="节点内平衡 DiskBalancer"></a>节点内平衡 DiskBalancer</h4><p><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSDiskbalancer.html" target="_blank" rel="noopener">官网文档</a></p>
<p>如果想要解决节点内多块磁盘数据不均衡的现象，就要借助DiskBalancer。在CDH 5.8.2+版本中，可以通过在CM中配置进行开启。</p>
<p>在 HDFS 配置项中找到“DataNode Advanced Configuration Snippet (Safety Valve) for hdfs-site.xml”，中文为“hdfs-site.xml的HDFS服务⾼级配置代码段（安全阀）”，加入</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.disk.balancer.enabled&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.disk.balancer.max.disk.throughputInMBperSec&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;50&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.disk.balancer.plan.threshold.percent&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;2&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.disk.balancer.block.tolerance.percent&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;5&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>

<ul>
<li>dfs.disk.balancer.max.disk.throughputInMBperSec：指定磁盘间平衡时占用的最大磁盘带宽，默认值10MB/s。在不影响读写性能的情况下可以适当调大。</li>
<li>dfs.disk.balancer.plan.threshold.percent：各盘之间数据平衡的阈值。DiskBalancer中采用一种叫volume data density（卷数据密度）的度量来确定占用率的偏差值，该值越大，表明磁盘间的数据越不均衡。平衡过程结束后，每个盘的卷数据密度与平均密度之差必须小于threshold（按百分比计）。默认值是10，我们设成了5。</li>
<li>dfs.disk.balancer.block.tolerance.percent：在每次移动块的过程中，移动块的数量与理想平衡状态之间的偏差容忍值（按百分比计）。一般也设成5。</li>
</ul>
<p>DiskBalancer的运行流程与Balancer类似，不过对象由节点变成了磁盘。它分为Discover、Plan与Execute三个阶段，分别是计算磁盘不平衡度、生成平衡计划与执行平衡计划。关于它的设计细节，可以参考 JIRA 中的 <a href="https://issues.apache.org/jira/browse/HDFS-1312" target="_blank" rel="noopener">HDFS-1312</a>。</p>
<p>配置完成后，重启 DataNode，然后 SSH 到该节点上，手动执行即可</p>
<p>生成平衡计划（hadoop1为主机名）<br><code>hdfs diskbalancer -plan hadoop1</code></p>
<p>执行平衡计划<br><code>hdfs diskbalancer -execute /system/diskbalancer/hadoop1.plan.json</code></p>
<p>查看执行状态<br><code>hdfs diskbalancer -query hadoop1</code></p>
<blockquote>
<p><strong>什么时候手动或调度执行？</strong><br>a.新盘加入<br>b.监控服务器的磁盘剩余空间 小于阈值 10%，发邮件预警 手动执行</p>
</blockquote>
<p>多个磁盘作为 DN 数据目录时，是加在 <code>dfs.datanode.data.dir</code> 这个参数中，默认英文逗号(comma-delimited)分隔</p>
<p>为什么datanode在生产上挂载多个物理的磁盘目录? 比如：<br>/data01 disk1<br>/data02 disk2<br>/data03 disk3   </p>
<p>为了高效率写 ，高效率读，多个个磁盘并发读写</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.cnblogs.com/cuishuai/p/10026905.html" target="_blank" rel="noopener">HDFS集群数据不均衡处理</a></li>
</ul>
<ul>
<li><a href="https://blog.csdn.net/nituoge/article/details/103447284" target="_blank" rel="noopener">Hadoop副本放置策略+读写流程+磁盘均衡+安全模式+hdfs dfs命令</a></li>
<li><a href="https://blog.csdn.net/weixin_42868638/article/details/89451703" target="_blank" rel="noopener">Hadoop3.2.0 HDFS磁盘平衡器</a></li>
</ul>
]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title>Shell 整数比较符号</title>
    <url>/2019/11/19/Shell-Compare/</url>
    <content><![CDATA[<p>Shell 脚本中大于，大于等于，小于，小于等于、不等于的表示方法</p>
<hr>
<h4 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h4><p>记住性对应的英文很容易记住</p>
<ul>
<li>大于 -gt (greater than)  </li>
<li>小于 -lt (less than)   </li>
<li>大于或等于 -ge (greater than or equal)  </li>
<li>小于或等于 -le (less than or equal)  </li>
<li>不相等 -ne （not equal）</li>
<li>相等 -eq (equal)</li>
</ul>
<p>使用方法 <code>if [&quot;$a&quot; -eq &quot;$b&quot; ]</code></p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/zhuguiqin1/article/details/79159912" target="_blank" rel="noopener">Shell 整数比较，以及各种符号</a></li>
<li><a href="https://blog.csdn.net/weixin_37998647/article/details/79718821" target="_blank" rel="noopener">shell脚本中大于，大于等于，小于，小于等于、不等于的表示方法</a></li>
</ul>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>DataNode 心跳机制</title>
    <url>/2019/11/18/hdfs-datanode-heartbeat/</url>
    <content><![CDATA[<p>DataNode 心跳机制的作用</p>
<hr>
<h4 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h4><p>DataNode 心跳机制的作用</p>
<ul>
<li>register: 当DataNode启动的时候，DataNode需要将自身的一些信息(hostname, version等)告诉NameNode，NameNode经过check后使其成为集群中的一员，然后信息维护在NetworkTopology中</li>
<li>block report: 将block的信息汇报给NameNode,使得NameNode可以维护数据块和数据节点之间的映射关系</li>
<li>定期的send heartbeat<ul>
<li>告诉NameNode我还活着，我的存储空间还有多少等信息</li>
<li>执行NameNode通过heartbeat传过来的指令，比如删除数据块</li>
</ul>
</li>
</ul>
<p>以上第1和第2个动作都是在DataNode启动的时候发生的，register的步骤主要功能是使得这个DataNode成为HDFS集群中的成员，DataNode注册成功后，DataNode会将它管理的所有的数据块信息，通过blockReport方法上报到NameNode，帮助NameNode建立HDFS文件数据块到DataNode的映射关系，这一步操作完成后，DataNode才正式算启动完成，可以对外提供服务了。</p>
<p>由于NameNode和DataNode之间存在主从关系，DataNode需要每隔3s发送心跳到NameNode，如果NameNode长时间收不到DataNode节点的心跳信息，那么NameNode会认为DataNode已经失效。NameNode如果有一些需要DataNode配合的动作，则会通过心跳返回给DataNode，心跳返回值是一个DataNodeCommand数组，它是一系列NameNode的指令，这样DataNode就可以根据指令完成指定的动作，比如HDFS文件的删除。</p>
<p>心跳间隔默认3s, 可以在 <code>hdfs-default.xml</code> 配置文件中修改</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.heartbeat.interval&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;3&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;description&gt;Determines datanode heartbeat interval in seconds.&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>

<h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><h5 id="DN-宕机"><a href="#DN-宕机" class="headerlink" title="DN 宕机"></a>DN 宕机</h5><p>如何知道 DataNode 已经死掉？</p>
<p>通过心跳机制来判断死亡，如果 DN 默认3s时间间隔没有给namenode发送心跳，NN 并不会直接断定 DN 死了。<br>NN 会给 DN 为10次的机会，如果 DN 在30秒的时间还没有向 NN 发送心跳，NN 就暂时的，记住是暂时的认为 DN 死亡了。<br>于是 NN 就会每隔5分钟向 DN 发送一次检查，如果发送了两次依旧还没有收到 DN 的消息，那么 NN 就判定这个 DN 节点挂掉了。</p>
<p>HDFS 默认的超时时间为 10分钟+30秒</p>
<p>计算公式为:<br>timeout  = 2 * heartbeat.recheck.interval + 10 * dfs.heartbeat.interval</p>
<p>默认的<code>heartbeat.recheck.interval</code>大小为5000毫秒，即5分钟，<code>dfs.heartbeat.interval</code>默认的大小为3秒</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;heartbeat.recheck.interval&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;5000&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>

<h5 id="文件删除"><a href="#文件删除" class="headerlink" title="文件删除"></a>文件删除</h5><p><img src="/images/blog/2019-11-18-1.png" alt></p>
<p>流程图其实也是很简单，步骤如下</p>
<ol>
<li>客户端创建一个FileSystem实例</li>
<li>调用FileSystem的delete方法，这个时候会向NameNode发起删除文件的请求，这个时候在NameNode中会删除对应的文件的元数据，并将这个文件标记为删除，但是这个文件对应的数据块并不会删除</li>
<li>当需要删除的文件对应的数据块所在的DataNode向NaneNode发了心跳后，NameNode将需要删除这个文件对应数据块的指令通过心跳返回给DataNode，DataNode收到指令后才会真正的删除数据块</li>
</ol>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.cnblogs.com/xd502djj/p/4645298.html" target="_blank" rel="noopener">hadoop入门之设置datanode的心跳时间的方法</a></li>
<li><a href="https://blog.csdn.net/aohuang8877/article/details/101116081" target="_blank" rel="noopener">HDFS中DataNode的心跳机制</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/75479796" target="_blank" rel="noopener">hadoop的四大机制</a></li>
</ul>
]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title>使用Cloudera Manager在线为集群减容</title>
    <url>/2019/11/17/CDH-reduce-host/</url>
    <content><![CDATA[<p>使用 Cloudera Manager 在线为集群减容</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>公司节约成本，需要对 CDH 集群进行减容</p>
<p>减容自然针对非核心服务所在的节点进行下线操作，记录一下下线操作 </p>
<h4 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h4><p>经过调研使用 Cloudera Manager 在线减容集群。具体过程可以参见 Fayson 的文章—<a href="https://mp.weixin.qq.com/s/wj2tCJu_uaGWoDSYswlmQg" target="_blank" rel="noopener">如何使用Cloudera Manager在线为集群减容</a></p>
<p>这里选择的是 节点正常下线 方法，在正常下线时勾选了“解除授权角色”和“跳过管理角色”功能，则在进行节点下线的时候会将该节点的数据复制到集群的其它节点上，在数据全部复制成功后才会将该节点从集群中删除。这样就不会发生数据丢失的情况</p>
<p>最后的最后把 Host 移除 CM 之后，要登录到移除的机器上，停止 <code>cloudera-scm-agent</code>, 不然又监听到 agent 心跳又将其加入回 CM</p>
<p>当然，Apache 版的 Hadoop 集群也有办法。我没尝试过，不知道是否要重启，可以移步到下方文章进行了解</p>
<ul>
<li>invincine的<a href="https://www.jianshu.com/p/c6bbcf422396" target="_blank" rel="noopener">Hadoop集群下线datanode</a></li>
<li><a href="https://x.medemede.cn/archives/1549249276517" target="_blank" rel="noopener">服役和退役数据节点</a></li>
</ul>
]]></content>
      <categories>
        <category>CDH</category>
      </categories>
  </entry>
  <entry>
    <title>HDFS 纠删码</title>
    <url>/2019/11/14/HDFS-Erasure-Encoding/</url>
    <content><![CDATA[<p>纠删码可以将HDFS的存储开销降低约50%，同时与三分本策略一样，还可以保证数据的可用性</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>测试环境上搭建了一套 CDH 6.3.1 集群，发现有个隐患，说需要9个DN，ec 什么来着。</p>
<p><img src="/images/blog/2020-02-24-1.png" alt></p>
<p>之前从没遇过，带着好奇心我对其进行搜索查询后，了解到这是 Hadoop3 新特性纠删码，打开了新的技术世界。</p>
<p>个人写的只是理解性，感兴趣的小伙伴自行阅读下方链接: </p>
<ul>
<li><a href="https://cloud.tencent.com/developer/article/1363388" target="_blank" rel="noopener">什么是HDFS的纠删码</a></li>
<li><a href="https://cloud.tencent.com/developer/article/1363393" target="_blank" rel="noopener">如何在CDH6.0中使用纠删码</a></li>
</ul>
<h4 id="多副本和纠删码"><a href="#多副本和纠删码" class="headerlink" title="多副本和纠删码"></a>多副本和纠删码</h4><p>多副本策略即将数据存储多个副本（一般是三副本，比如HDFS），当某个副本丢失时，可以通过其他副本复制回来。三副本的磁盘利用率为1/3。</p>
<p>纠删码技术主要是通过纠删码算法将原始的数据进行编码得到校验数据块来达到冗余，并将数据块和校验块一并存储起来，以达到容错的目的。磁盘利用率为 n/(n+m)。    </p>
<h4 id="EC-和-RAID"><a href="#EC-和-RAID" class="headerlink" title="EC 和 RAID"></a>EC 和 RAID</h4><p>Erasurecoding 纠删码技术简称 EC，是一种数据保护技术。</p>
<p>最简单的 EC 实现可以基于异或操作(XOR)，Reed-SolomonCodes 也是 EC 编码中的一种。Reed-Solomon Codes 缩写为 RS 码</p>
<p>RAID5 一般使用 XOR 编码，因为只需要容忍单个磁盘故障，而 RAID6 使用 Reed-Solomon 和两个奇偶校验块来容忍最多两个磁盘故障。</p>
<h4 id="Reed-Solomon-RS-码"><a href="#Reed-Solomon-RS-码" class="headerlink" title="Reed-Solomon(RS) 码"></a>Reed-Solomon(RS) 码</h4><p>Reed-Solomon（RS）码是存储系统较为常用的一种纠删码，它有两个参数 n 和 m，记为 RS(n,m)。n 代表原始数据块个数。m 代表校验块个数。具体原理参阅下方文章</p>
<ul>
<li><a href="https://www.jianshu.com/p/4abf65ad03af" target="_blank" rel="noopener">纠删码（Erasure Code）浅析</a></li>
<li><a href="https://mp.weixin.qq.com/s/aOX4sfTCrRp4agPxrM4txA" target="_blank" rel="noopener">纠删码(Erasure Code)的数学原理竟然这么简单</a></li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>工程实践中，一般对于热数据还是会使用多副本策略来冗余，冷数据使用纠删码。</p>
<p>纠删码和RAID技术看起来是有些类似，但与RAID5、RAID6不同的是纠删码从功能上来看最大的区分特点是校验和数据的比例按N+M可调整，并且校验块数量不再受限于2个(RAID最多2个，比如RAID6)，纠删码的M可以是3、4甚至更多。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://cloud.tencent.com/developer/article/1363388" target="_blank" rel="noopener">什么是HDFS的纠删码</a></li>
<li><a href="https://cloud.tencent.com/developer/article/1363393" target="_blank" rel="noopener">如何在CDH6.0中使用纠删码</a></li>
<li><a href="https://www.jianshu.com/p/4abf65ad03af" target="_blank" rel="noopener">纠删码（Erasure Code）浅析</a></li>
<li><a href="https://mp.weixin.qq.com/s/aOX4sfTCrRp4agPxrM4txA" target="_blank" rel="noopener">纠删码(Erasure Code)的数学原理竟然这么简单</a></li>
<li><a href="https://blog.csdn.net/BtB5e6Nsu1g511Eg5XEg/article/details/82321298" target="_blank" rel="noopener">纠删码(Erasure Code)的数学原理竟然这么简单(转)</a></li>
</ul>
]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title>HDFS 安全模式</title>
    <url>/2019/11/13/hdfs-SafeMode/</url>
    <content><![CDATA[<p>Hadoop 安全模式 Safe mode</p>
<hr>
<h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><p><strong>什么是 HDFS 安全模式？</strong></p>
<p>安全模式是 HDFS 所处的一种特殊状态，在这种状态下，文件系统只接受读数据请求，<br>而不接受删除、修改等变更请求。在NameNode主节点启动时，HDFS首先进入安全模式，<br>DataNode在启动的时候会向namenode汇报可用的block等状态，当整个系统达到安全标准时，<br>HDFS自动离开安全模式。如果HDFS出于安全模式下，则文件block不能进行任何的副本复制操作，<br>因此达到最小的副本数量要求是基于datanode启动时的状态来判定的，启动时不会再做任何复制</p>
<p>在安全模式期间，Hadoop主要做这些事情</p>
<ol>
<li><p>NameNode 从磁盘读取 FsImage 和 EditLog 的信息，将 EditLog 中的所有事务应用到 FsImage 的内存中，并将这新版本的数据刷新到磁盘上的新 FsImage中</p>
</li>
<li><p>接收集群中的 DataNode 块报告</p>
</li>
</ol>
<h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><p>系统什么时候才离开安全模式，需要满足哪些条件？当收到来自datanode的状态报告后，namenode根据配置，确定<br>1）可用的block占总数的比例<br>2）可用的数据节点数量符合要求之后，离开安全模式。</p>
<p>如果有必要，也可以通过命令强制离开安全模式</p>
<p>与安全模式相关的主要配置在 hdfs-site.xml 文件中</p>
<ul>
<li><p>dfs.namenode.replication.min: 最小副本级别，成功执行写操作所需要创建的最小副本数目，默认为1.</p>
</li>
<li><p>dfs.namenode.safemode.threshold-pct: 指定达到最小副本数的数据块的百分比，当实际比例超过该配置后，才能离开安全模式（但是还需要其他条件也满足）。默认为0.999f，也就是说符合最小副本数要求的block占比超过99.9%时，并且其他条件也满足才能离开安全模式。如果为小于等于0，则不会等待任何副本达到要求即可离开。如果大于1，则永远处于安全模式。</p>
</li>
<li><p>dfs.namenode.safemode.min.datanodes: 离开安全模式的最小可用（alive）datanode数量要求，默认为0.也就是即使所有datanode都不可用，仍然可以离开安全模式。</p>
</li>
<li><p>dfs.namenode.safemode.extension: 当集群都达到要求之后，如果在该参数配置的时间段之后依然能满足要求，此时 NameNode 会自动退出 Safemode 状态。单位为毫秒，默认为30000，即30秒.也就是当满足条件并且能够维持30秒之后，离开安全模式。 这个配置主要是对集群的稳定程度做进一步的确认。避免达到要求后马上又不符合安全标准。</p>
</li>
</ul>
<p>总结一下，要离开安全模式，需要满足以下条件: </p>
<p>1）达到副本数量要求的block比例满足要求<br>2）可用的datanode节点数满足配置的数量要求<br>3） 1、2 两个条件满足后维持的时间达到配置的要求   </p>
<h4 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h4><p>Hadoop 提供脚本用于对安全模式进行操作，主要命令为<code>hadoop dfsadmin -safemode &lt;command&gt;</code></p>
<p>command 的可用取值如下</p>
<table>
<thead>
<tr>
<th>command</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>get</td>
<td>查看当前状态</td>
</tr>
<tr>
<td>enter</td>
<td>进入安全模式</td>
</tr>
<tr>
<td>leave</td>
<td>离开安全模式</td>
</tr>
<tr>
<td>wait</td>
<td>一直等待直到安全模式结束</td>
</tr>
<tr>
<td>forceExit</td>
<td>强制退出安全模式</td>
</tr>
</tbody></table>
<p>需要知道，当Resource is low时，只有Active NameNode才会进入到安全模式（HDFS-2914）</p>
<h4 id="事故"><a href="#事故" class="headerlink" title="事故"></a>事故</h4><p>如果 NN 长时间处于安全模式，那么多半是因为 HDFS 的数据损坏的太多</p>
<p><a href="https://lihuimintu.github.io/2019/03/14/hdfs-block-fault/" target="_blank" rel="noopener">HDFS 块损坏</a></p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/bingduanlbd/article/details/51900512" target="_blank" rel="noopener">HDFS安全模式详解</a></li>
<li><a href="https://www.iteblog.com/archives/977.html" target="_blank" rel="noopener">Hadoop安全模式详解及配置</a></li>
<li><a href="https://segmentfault.com/a/1190000020167593" target="_blank" rel="noopener">Hadoop安全模式Safe mode</a></li>
</ul>
]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title>元数据逆向修复原理</title>
    <url>/2019/11/13/HBase-fix-meta/</url>
    <content><![CDATA[<p>HBase运维基础之元数据逆向修复原理</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>大好的晚上时光肯定是用来学习的。</p>
<h4 id="心得"><a href="#心得" class="headerlink" title="心得"></a>心得</h4><p>回顾<a href="https://mp.weixin.qq.com/s/yt4X2tDQrLx35NsviRHbPg" target="_blank" rel="noopener">HBase运维基础——元数据逆向修复原理</a>读了一遍，第一次读看的云里雾里的，<br>一年后再读果然相对容易多了。</p>
<p>hbck 工具历来都是 HBase 线上运维大杀器，修复的过程得谨慎避免异常</p>
<p>不过在 hbase2.0+ 以后 hbck 的所有修复功能全都不支持, 执行 <code>hbase hbck -help</code> 可见下图</p>
<p><img src="/images/blog/2019-12-20-2.png" alt></p>
<p>OfflineMetaRepair 工具未使用过，怀疑 2.x 估计不支持了，看源码验证了下</p>
<p><img src="/images/blog/2019-11-13-3.png" alt></p>
<p>科学上网后发现 hbase2.0+ 提供了一个叫 hbck2 工具，文章中的问题其实可以用 hbck2 解决的，亲测可以可用<a href="https://lihuimintu.github.io/2019/12/16/hbase-hbck2/" target="_blank" rel="noopener">CDH-HBase 使用 HBCK2 运维</a></p>
<ul>
<li><a href="https://yq.aliyun.com/articles/738176" target="_blank" rel="noopener">HBase 2.0.0 META 数据修复工具</a></li>
<li><a href="https://www.jianshu.com/p/19bb8f7ff38c" target="_blank" rel="noopener">HBase 2.0 META 数据修复</a></li>
</ul>
<p>在 HBCK2 中找到 <a href="https://github.com/apache/hbase-operator-tools/blob/master/hbase-hbck2/src/main/java/org/apache/hbase/hbck1/OfflineMetaRepair.java" target="_blank" rel="noopener">OfflineMetaRepair 工具</a>，原来是将其添加到 hbase-operator-tools 中</p>
<h4 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h4><p>在 hbase-operator-tools 中有关于 hbase:meta 表重建的介绍</p>
<ul>
<li>Online hbase:meta rebuild recipe</li>
<li>Detailed rebuild recipe</li>
</ul>
<p>更多参阅<a href="https://github.com/apache/hbase-operator-tools/tree/master/hbase-hbck2#missing-regions-in-hbasemeta-regiontable-restorerebuild" target="_blank" rel="noopener">Missing Regions in hbase:meta region/table restore/rebuild</a> 到 <a href="https://github.com/apache/hbase-operator-tools/tree/master/hbase-hbck2#detailed-rebuild-recipe" target="_blank" rel="noopener">Detailed rebuild recipe</a> 部分</p>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>HBase 集成 hbtop</title>
    <url>/2019/11/12/HBase-hbtop/</url>
    <content><![CDATA[<p>HBase 2.1.0-cdh6.3.1 体验下 hbtop 新功能</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>HBase 2.1.0-cdh6.3.1 想体验下 hbtop 新功能。 <a href="https://blogs.apache.org/hbase/entry/introduction-hbtop-a-real-time" target="_blank" rel="noopener">hbtop</a> 的介绍，从介绍可以看到目前 hbtop 不支持 HBase 1.x </p>
<h4 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h4><p>从 github 上下载 <a href="https://github.com/cloudera/hbase/tree/cdh6.3.1" target="_blank" rel="noopener">HBase 2.1.0-cdh6.3.1</a>源码, <a href="https://github.com/apache/hbase/releases/tag/rel%2F2.1.7" target="_blank" rel="noopener">Apache HBase 2.1.7</a>源码</p>
<p>把 Apache HBase 的 hbase-hbtop moudle 拷贝到 CDH HBase 里。</p>
<p>在 CDH HBase 的 pom.xml 添加</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;module&gt;hbase-hbtop&lt;&#x2F;module&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;artifactId&gt;hbase-hbtop&lt;&#x2F;artifactId&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.hbase&lt;&#x2F;groupId&gt;</span><br><span class="line">  &lt;version&gt;$&#123;project.version&#125;&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/2019-11-12-1.png" alt></p>
<p><img src="/images/blog/2019-11-12-2.png" alt></p>
<p><img src="/images/blog/2019-11-12-3.png" alt></p>
<p>修改 maven setting 文件, 记得修改 localRepository 为你本地的仓库地址</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;!--</span><br><span class="line">Licensed to the Apache Software Foundation (ASF) under one</span><br><span class="line">or more contributor license agreements.  See the NOTICE file</span><br><span class="line">distributed with this work for additional information</span><br><span class="line">regarding copyright ownership.  The ASF licenses this file</span><br><span class="line">to you under the Apache License, Version 2.0 (the</span><br><span class="line">&quot;License&quot;); you may not use this file except in compliance</span><br><span class="line">with the License.  You may obtain a copy of the License at</span><br><span class="line"></span><br><span class="line">    http:&#x2F;&#x2F;www.apache.org&#x2F;licenses&#x2F;LICENSE-2.0</span><br><span class="line"></span><br><span class="line">Unless required by applicable law or agreed to in writing,</span><br><span class="line">software distributed under the License is distributed on an</span><br><span class="line">&quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY</span><br><span class="line">KIND, either express or implied.  See the License for the</span><br><span class="line">specific language governing permissions and limitations</span><br><span class="line">under the License.</span><br><span class="line">--&gt;</span><br><span class="line"></span><br><span class="line">&lt;!--</span><br><span class="line"> | This is the configuration file for Maven. It can be specified at two levels:</span><br><span class="line"> |</span><br><span class="line"> |  1. User Level. This settings.xml file provides configuration for a single user,</span><br><span class="line"> |                 and is normally provided in $&#123;user.home&#125;&#x2F;.m2&#x2F;settings.xml.</span><br><span class="line"> |</span><br><span class="line"> |                 NOTE: This location can be overridden with the CLI option:</span><br><span class="line"> |</span><br><span class="line"> |                 -s &#x2F;path&#x2F;to&#x2F;user&#x2F;settings.xml</span><br><span class="line"> |</span><br><span class="line"> |  2. Global Level. This settings.xml file provides configuration for all Maven</span><br><span class="line"> |                 users on a machine (assuming they&#39;re all using the same Maven</span><br><span class="line"> |                 installation). It&#39;s normally provided in</span><br><span class="line"> |                 $&#123;maven.home&#125;&#x2F;conf&#x2F;settings.xml.</span><br><span class="line"> |</span><br><span class="line"> |                 NOTE: This location can be overridden with the CLI option:</span><br><span class="line"> |</span><br><span class="line"> |                 -gs &#x2F;path&#x2F;to&#x2F;global&#x2F;settings.xml</span><br><span class="line"> |</span><br><span class="line"> | The sections in this sample file are intended to give you a running start at</span><br><span class="line"> | getting the most out of your Maven installation. Where appropriate, the default</span><br><span class="line"> | values (values used when the setting is not specified) are provided.</span><br><span class="line"> |</span><br><span class="line"> |--&gt;</span><br><span class="line">&lt;settings xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;SETTINGS&#x2F;1.0.0&quot;</span><br><span class="line">          xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">          xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;SETTINGS&#x2F;1.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;settings-1.0.0.xsd&quot;&gt;</span><br><span class="line">  &lt;!-- localRepository</span><br><span class="line">   | The path to the local repository maven will use to store artifacts.</span><br><span class="line">   |</span><br><span class="line">   | Default: $&#123;user.home&#125;&#x2F;.m2&#x2F;repository</span><br><span class="line">  &lt;localRepository&gt;&#x2F;path&#x2F;to&#x2F;local&#x2F;repo&lt;&#x2F;localRepository&gt;</span><br><span class="line">  --&gt;</span><br><span class="line">  &lt;localRepository&gt;&#x2F;Users&#x2F;tu&#x2F;.m2&#x2F;repository&lt;&#x2F;localRepository&gt;</span><br><span class="line">  &lt;!-- interactiveMode</span><br><span class="line">   | This will determine whether maven prompts you when it needs input. If set to false,</span><br><span class="line">   | maven will use a sensible default value, perhaps based on some other setting, for</span><br><span class="line">   | the parameter in question.</span><br><span class="line">   |</span><br><span class="line">   | Default: true</span><br><span class="line">  &lt;interactiveMode&gt;true&lt;&#x2F;interactiveMode&gt;</span><br><span class="line">  --&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- offline</span><br><span class="line">   | Determines whether maven should attempt to connect to the network when executing a build.</span><br><span class="line">   | This will have an effect on artifact downloads, artifact deployment, and others.</span><br><span class="line">   |</span><br><span class="line">   | Default: false</span><br><span class="line">  &lt;offline&gt;false&lt;&#x2F;offline&gt;</span><br><span class="line">  --&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- pluginGroups</span><br><span class="line">   | This is a list of additional group identifiers that will be searched when resolving plugins by their prefix, i.e.</span><br><span class="line">   | when invoking a command line like &quot;mvn prefix:goal&quot;. Maven will automatically add the group identifiers</span><br><span class="line">   | &quot;org.apache.maven.plugins&quot; and &quot;org.codehaus.mojo&quot; if these are not already contained in the list.</span><br><span class="line">   |--&gt;</span><br><span class="line">  &lt;pluginGroups&gt;</span><br><span class="line">    &lt;!-- pluginGroup</span><br><span class="line">     | Specifies a further group identifier to use for plugin lookup.</span><br><span class="line">    &lt;pluginGroup&gt;com.your.plugins&lt;&#x2F;pluginGroup&gt;</span><br><span class="line">    --&gt;</span><br><span class="line">  &lt;&#x2F;pluginGroups&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- proxies</span><br><span class="line">   | This is a list of proxies which can be used on this machine to connect to the network.</span><br><span class="line">   | Unless otherwise specified (by system property or command-line switch), the first proxy</span><br><span class="line">   | specification in this list marked as active will be used.</span><br><span class="line">   |--&gt;</span><br><span class="line">  &lt;proxies&gt;</span><br><span class="line">    &lt;!-- proxy</span><br><span class="line">     | Specification for one proxy, to be used in connecting to the network.</span><br><span class="line">     |</span><br><span class="line">    &lt;proxy&gt;</span><br><span class="line">      &lt;id&gt;optional&lt;&#x2F;id&gt;</span><br><span class="line">      &lt;active&gt;true&lt;&#x2F;active&gt;</span><br><span class="line">      &lt;protocol&gt;http&lt;&#x2F;protocol&gt;</span><br><span class="line">      &lt;username&gt;proxyuser&lt;&#x2F;username&gt;</span><br><span class="line">      &lt;password&gt;proxypass&lt;&#x2F;password&gt;</span><br><span class="line">      &lt;host&gt;proxy.host.net&lt;&#x2F;host&gt;</span><br><span class="line">      &lt;port&gt;80&lt;&#x2F;port&gt;</span><br><span class="line">      &lt;nonProxyHosts&gt;local.net|some.host.com&lt;&#x2F;nonProxyHosts&gt;</span><br><span class="line">    &lt;&#x2F;proxy&gt;</span><br><span class="line">    --&gt;</span><br><span class="line">  &lt;&#x2F;proxies&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- servers</span><br><span class="line">   | This is a list of authentication profiles, keyed by the server-id used within the system.</span><br><span class="line">   | Authentication profiles can be used whenever maven must make a connection to a remote server.</span><br><span class="line">   |--&gt;</span><br><span class="line">  &lt;servers&gt;</span><br><span class="line">    &lt;!-- server</span><br><span class="line">     | Specifies the authentication information to use when connecting to a particular server, identified by</span><br><span class="line">     | a unique name within the system (referred to by the &#39;id&#39; attribute below).</span><br><span class="line">     |</span><br><span class="line">     | NOTE: You should either specify username&#x2F;password OR privateKey&#x2F;passphrase, since these pairings are</span><br><span class="line">     |       used together.</span><br><span class="line">     |</span><br><span class="line">    &lt;server&gt;</span><br><span class="line">      &lt;id&gt;deploymentRepo&lt;&#x2F;id&gt;</span><br><span class="line">      &lt;username&gt;repouser&lt;&#x2F;username&gt;</span><br><span class="line">      &lt;password&gt;repopwd&lt;&#x2F;password&gt;</span><br><span class="line">    &lt;&#x2F;server&gt;</span><br><span class="line">    --&gt;</span><br><span class="line">    &lt;!-- server config for deploy --&gt;</span><br><span class="line">    &lt;server&gt;</span><br><span class="line">      &lt;id&gt;releases&lt;&#x2F;id&gt;</span><br><span class="line">      &lt;username&gt;deployment&lt;&#x2F;username&gt;</span><br><span class="line">      &lt;password&gt;2016deployBQS&lt;&#x2F;password&gt;</span><br><span class="line">    &lt;&#x2F;server&gt;</span><br><span class="line">    &lt;server&gt;</span><br><span class="line">      &lt;id&gt;snapshots&lt;&#x2F;id&gt;</span><br><span class="line">      &lt;username&gt;deployment&lt;&#x2F;username&gt;</span><br><span class="line">      &lt;password&gt;2016deployBQS&lt;&#x2F;password&gt;</span><br><span class="line">    &lt;&#x2F;server&gt;</span><br><span class="line">    &lt;server&gt;</span><br><span class="line">      &lt;id&gt;thirdparty&lt;&#x2F;id&gt;</span><br><span class="line">      &lt;username&gt;deployment&lt;&#x2F;username&gt;</span><br><span class="line">      &lt;password&gt;2016deployBQS&lt;&#x2F;password&gt;</span><br><span class="line">    &lt;&#x2F;server&gt;</span><br><span class="line">    &lt;!-- server config for developer --&gt;</span><br><span class="line">    &lt;server&gt;</span><br><span class="line">      &lt;id&gt;repository&lt;&#x2F;id&gt;</span><br><span class="line">      &lt;username&gt;deployment&lt;&#x2F;username&gt;</span><br><span class="line">      &lt;password&gt;2016deployBQS&lt;&#x2F;password&gt;</span><br><span class="line">    &lt;&#x2F;server&gt;</span><br><span class="line">    &lt;server&gt;</span><br><span class="line">      &lt;id&gt;localnexus&lt;&#x2F;id&gt;</span><br><span class="line">      &lt;username&gt;deployment&lt;&#x2F;username&gt;</span><br><span class="line">      &lt;password&gt;2016deployBQS&lt;&#x2F;password&gt;</span><br><span class="line">    &lt;&#x2F;server&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- Another sample, using keys to authenticate.</span><br><span class="line">    &lt;server&gt;</span><br><span class="line">      &lt;id&gt;siteServer&lt;&#x2F;id&gt;</span><br><span class="line">      &lt;privateKey&gt;&#x2F;path&#x2F;to&#x2F;private&#x2F;key&lt;&#x2F;privateKey&gt;</span><br><span class="line">      &lt;passphrase&gt;optional; leave empty if not used.&lt;&#x2F;passphrase&gt;</span><br><span class="line">    &lt;&#x2F;server&gt;</span><br><span class="line">    --&gt;</span><br><span class="line">  &lt;&#x2F;servers&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- mirrors</span><br><span class="line">   | This is a list of mirrors to be used in downloading artifacts from remote repositories.</span><br><span class="line">   |</span><br><span class="line">   | It works like this: a POM may declare a repository to use in resolving certain artifacts.</span><br><span class="line">   | However, this repository may have problems with heavy traffic at times, so people have mirrored</span><br><span class="line">   | it to several places.</span><br><span class="line">   |</span><br><span class="line">   | That repository definition will have a unique id, so we can create a mirror reference for that</span><br><span class="line">   | repository, to be used as an alternate download site. The mirror site will be the preferred</span><br><span class="line">   | server for that repository.</span><br><span class="line">   |--&gt;</span><br><span class="line">  &lt;mirrors&gt;</span><br><span class="line">    &lt;!-- mirror</span><br><span class="line">     | Specifies a repository mirror site to use instead of a given repository. The repository that</span><br><span class="line">     | this mirror serves has an ID that matches the mirrorOf element of this mirror. IDs are used</span><br><span class="line">     | for inheritance and direct lookup purposes, and must be unique across the set of mirrors.</span><br><span class="line">     |</span><br><span class="line">    &lt;mirror&gt;</span><br><span class="line">      &lt;id&gt;mirrorId&lt;&#x2F;id&gt;</span><br><span class="line">      &lt;mirrorOf&gt;repositoryId&lt;&#x2F;mirrorOf&gt;</span><br><span class="line">      &lt;name&gt;Human Readable Name for this Mirror.&lt;&#x2F;name&gt;</span><br><span class="line">      &lt;url&gt;http:&#x2F;&#x2F;my.repository.com&#x2F;repo&#x2F;path&lt;&#x2F;url&gt;</span><br><span class="line">    &lt;&#x2F;mirror&gt;</span><br><span class="line">     --&gt;</span><br><span class="line">  &lt;&#x2F;mirrors&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- profiles</span><br><span class="line">   | This is a list of profiles which can be activated in a variety of ways, and which can modify</span><br><span class="line">   | the build process. Profiles provided in the settings.xml are intended to provide local machine-</span><br><span class="line">   | specific paths and repository locations which allow the build to work in the local environment.</span><br><span class="line">   |</span><br><span class="line">   | For example, if you have an integration testing plugin - like cactus - that needs to know where</span><br><span class="line">   | your Tomcat instance is installed, you can provide a variable here such that the variable is</span><br><span class="line">   | dereferenced during the build process to configure the cactus plugin.</span><br><span class="line">   |</span><br><span class="line">   | As noted above, profiles can be activated in a variety of ways. One way - the activeProfiles</span><br><span class="line">   | section of this document (settings.xml) - will be discussed later. Another way essentially</span><br><span class="line">   | relies on the detection of a system property, either matching a particular value for the property,</span><br><span class="line">   | or merely testing its existence. Profiles can also be activated by JDK version prefix, where a</span><br><span class="line">   | value of &#39;1.4&#39; might activate a profile when the build is executed on a JDK version of &#39;1.4.2_07&#39;.</span><br><span class="line">   | Finally, the list of active profiles can be specified directly from the command line.</span><br><span class="line">   |</span><br><span class="line">   | NOTE: For profiles defined in the settings.xml, you are restricted to specifying only artifact</span><br><span class="line">   |       repositories, plugin repositories, and free-form properties to be used as configuration</span><br><span class="line">   |       variables for plugins in the POM.</span><br><span class="line">   |</span><br><span class="line">   |--&gt;</span><br><span class="line">  &lt;profiles&gt;</span><br><span class="line">    &lt;!-- profile</span><br><span class="line">     | Specifies a set of introductions to the build process, to be activated using one or more of the</span><br><span class="line">     | mechanisms described above. For inheritance purposes, and to activate profiles via &lt;activatedProfiles&#x2F;&gt;</span><br><span class="line">     | or the command line, profiles have to have an ID that is unique.</span><br><span class="line">     |</span><br><span class="line">     | An encouraged best practice for profile identification is to use a consistent naming convention</span><br><span class="line">     | for profiles, such as &#39;env-dev&#39;, &#39;env-test&#39;, &#39;env-production&#39;, &#39;user-jdcasey&#39;, &#39;user-brett&#39;, etc.</span><br><span class="line">     | This will make it more intuitive to understand what the set of introduced profiles is attempting</span><br><span class="line">     | to accomplish, particularly when you only have a list of profile id&#39;s for debug.</span><br><span class="line">     |</span><br><span class="line">     | This profile example uses the JDK version to trigger activation, and provides a JDK-specific repo.</span><br><span class="line">    &lt;profile&gt;</span><br><span class="line">      &lt;id&gt;jdk-1.4&lt;&#x2F;id&gt;</span><br><span class="line"></span><br><span class="line">      &lt;activation&gt;</span><br><span class="line">        &lt;jdk&gt;1.4&lt;&#x2F;jdk&gt;</span><br><span class="line">      &lt;&#x2F;activation&gt;</span><br><span class="line"></span><br><span class="line">      &lt;repositories&gt;</span><br><span class="line">        &lt;repository&gt;</span><br><span class="line">          &lt;id&gt;jdk14&lt;&#x2F;id&gt;</span><br><span class="line">          &lt;name&gt;Repository for JDK 1.4 builds&lt;&#x2F;name&gt;</span><br><span class="line">          &lt;url&gt;http:&#x2F;&#x2F;www.myhost.com&#x2F;maven&#x2F;jdk14&lt;&#x2F;url&gt;</span><br><span class="line">          &lt;layout&gt;default&lt;&#x2F;layout&gt;</span><br><span class="line">          &lt;snapshotPolicy&gt;always&lt;&#x2F;snapshotPolicy&gt;</span><br><span class="line">        &lt;&#x2F;repository&gt;</span><br><span class="line">      &lt;&#x2F;repositories&gt;</span><br><span class="line">    &lt;&#x2F;profile&gt;</span><br><span class="line">    --&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--</span><br><span class="line">     | Here is another profile, activated by the system property &#39;target-env&#39; with a value of &#39;dev&#39;,</span><br><span class="line">     | which provides a specific path to the Tomcat instance. To use this, your plugin configuration</span><br><span class="line">     | might hypothetically look like:</span><br><span class="line">     |</span><br><span class="line">     | ...</span><br><span class="line">     | &lt;plugin&gt;</span><br><span class="line">     |   &lt;groupId&gt;org.myco.myplugins&lt;&#x2F;groupId&gt;</span><br><span class="line">     |   &lt;artifactId&gt;myplugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">     |</span><br><span class="line">     |   &lt;configuration&gt;</span><br><span class="line">     |     &lt;tomcatLocation&gt;$&#123;tomcatPath&#125;&lt;&#x2F;tomcatLocation&gt;</span><br><span class="line">     |   &lt;&#x2F;configuration&gt;</span><br><span class="line">     | &lt;&#x2F;plugin&gt;</span><br><span class="line">     | ...</span><br><span class="line">     |</span><br><span class="line">     | NOTE: If you just wanted to inject this configuration whenever someone set &#39;target-env&#39; to</span><br><span class="line">     |       anything, you could just leave off the &lt;value&#x2F;&gt; inside the activation-property.</span><br><span class="line">     |</span><br><span class="line">    &lt;profile&gt;</span><br><span class="line">      &lt;id&gt;env-dev&lt;&#x2F;id&gt;</span><br><span class="line"></span><br><span class="line">      &lt;activation&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">          &lt;name&gt;target-env&lt;&#x2F;name&gt;</span><br><span class="line">          &lt;value&gt;dev&lt;&#x2F;value&gt;</span><br><span class="line">        &lt;&#x2F;property&gt;</span><br><span class="line">      &lt;&#x2F;activation&gt;</span><br><span class="line"></span><br><span class="line">      &lt;properties&gt;</span><br><span class="line">        &lt;tomcatPath&gt;&#x2F;path&#x2F;to&#x2F;tomcat&#x2F;instance&lt;&#x2F;tomcatPath&gt;</span><br><span class="line">      &lt;&#x2F;properties&gt;</span><br><span class="line">    &lt;&#x2F;profile&gt;</span><br><span class="line">    --&gt;</span><br><span class="line">	&lt;profile&gt;</span><br><span class="line">		&lt;id&gt;aliyun&lt;&#x2F;id&gt;</span><br><span class="line">		&lt;repositories&gt;</span><br><span class="line">			&lt;repository&gt;</span><br><span class="line">				&lt;id&gt;myRepository1_1&lt;&#x2F;id&gt;</span><br><span class="line">				&lt;url&gt;http:&#x2F;&#x2F;maven.aliyun.com&#x2F;nexus&#x2F;content&#x2F;groups&#x2F;public&#x2F;&lt;&#x2F;url&gt;</span><br><span class="line">				&lt;releases&gt;</span><br><span class="line">					&lt;enabled&gt;true&lt;&#x2F;enabled&gt;</span><br><span class="line">				&lt;&#x2F;releases&gt;</span><br><span class="line">				&lt;snapshots&gt;</span><br><span class="line">					&lt;enabled&gt;true&lt;&#x2F;enabled&gt;</span><br><span class="line">					&lt;updatePolicy&gt;always&lt;&#x2F;updatePolicy&gt;</span><br><span class="line">				&lt;&#x2F;snapshots&gt;</span><br><span class="line">			&lt;&#x2F;repository&gt;</span><br><span class="line">		&lt;&#x2F;repositories&gt;</span><br><span class="line">	&lt;&#x2F;profile&gt;</span><br><span class="line">	&lt;profile&gt;</span><br><span class="line">		&lt;id&gt;cloudera-release&lt;&#x2F;id&gt;</span><br><span class="line">		&lt;repositories&gt;</span><br><span class="line">			&lt;repository&gt;</span><br><span class="line">				&lt;id&gt;cloudera-lib&lt;&#x2F;id&gt;</span><br><span class="line">				&lt;name&gt;cloudera repository&lt;&#x2F;name&gt;</span><br><span class="line">				&lt;url&gt;https:&#x2F;&#x2F;repository.cloudera.com&#x2F;artifactory&#x2F;libs-release-local&lt;&#x2F;url&gt;</span><br><span class="line">				&lt;releases&gt;</span><br><span class="line">					&lt;enabled&gt;true&lt;&#x2F;enabled&gt;</span><br><span class="line">				&lt;&#x2F;releases&gt;</span><br><span class="line">				&lt;snapshots&gt;</span><br><span class="line">					&lt;enabled&gt;true&lt;&#x2F;enabled&gt;</span><br><span class="line">					&lt;updatePolicy&gt;always&lt;&#x2F;updatePolicy&gt;</span><br><span class="line">				&lt;&#x2F;snapshots&gt;</span><br><span class="line">			&lt;&#x2F;repository&gt;</span><br><span class="line">		&lt;&#x2F;repositories&gt;</span><br><span class="line">	&lt;&#x2F;profile&gt;</span><br><span class="line">	&lt;profile&gt;</span><br><span class="line">		&lt;id&gt;cloudera-lib&lt;&#x2F;id&gt;</span><br><span class="line">		&lt;repositories&gt;</span><br><span class="line">			&lt;repository&gt;</span><br><span class="line">				&lt;id&gt;cloudera-lib&lt;&#x2F;id&gt;</span><br><span class="line">				&lt;name&gt;cloudera repository&lt;&#x2F;name&gt;</span><br><span class="line">				&lt;url&gt;https:&#x2F;&#x2F;repository.cloudera.com&#x2F;artifactory&#x2F;cloudera-repos&lt;&#x2F;url&gt;</span><br><span class="line">				&lt;releases&gt;</span><br><span class="line">					&lt;enabled&gt;true&lt;&#x2F;enabled&gt;</span><br><span class="line">				&lt;&#x2F;releases&gt;</span><br><span class="line">				&lt;snapshots&gt;</span><br><span class="line">					&lt;enabled&gt;true&lt;&#x2F;enabled&gt;</span><br><span class="line">					&lt;updatePolicy&gt;always&lt;&#x2F;updatePolicy&gt;</span><br><span class="line">				&lt;&#x2F;snapshots&gt;</span><br><span class="line">			&lt;&#x2F;repository&gt;</span><br><span class="line">		&lt;&#x2F;repositories&gt;</span><br><span class="line">	&lt;&#x2F;profile&gt;</span><br><span class="line">  &lt;&#x2F;profiles&gt;</span><br><span class="line"></span><br><span class="line">  &lt;activeProfiles&gt;</span><br><span class="line">   &lt;!-- &lt;activeProfile&gt;mynexus&lt;&#x2F;activeProfile&gt; --&gt;</span><br><span class="line">    &lt;activeProfile&gt;aliyun&lt;&#x2F;activeProfile&gt;</span><br><span class="line">    &lt;activeProfile&gt;cloudera-release&lt;&#x2F;activeProfile&gt;</span><br><span class="line">    &lt;activeProfile&gt;cloudera-lib&lt;&#x2F;activeProfile&gt;</span><br><span class="line">  &lt;&#x2F;activeProfiles&gt;</span><br><span class="line">  &lt;!-- activeProfiles</span><br><span class="line">   | List of profiles that are active for all builds.</span><br><span class="line">   |</span><br><span class="line">  &lt;activeProfiles&gt;</span><br><span class="line">    &lt;activeProfile&gt;alwaysActiveProfile&lt;&#x2F;activeProfile&gt;</span><br><span class="line">    &lt;activeProfile&gt;anotherAlwaysActiveProfile&lt;&#x2F;activeProfile&gt;</span><br><span class="line">  &lt;&#x2F;activeProfiles&gt;</span><br><span class="line">  --&gt;</span><br><span class="line">&lt;&#x2F;settings&gt;</span><br></pre></td></tr></table></figure>

<p>maven 加载依赖过程很长。等待着把。我花了7个小时才弄完。。。</p>
<p>依赖弄完后，pom 有些无关紧要的错误，忽略就好</p>
<p>进入 hbase-hbtop 项目下执行 maven 打包命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;Users&#x2F;tu&#x2F;IdeaProjects&#x2F;hbase-cdh6.3.1-release&#x2F;hbase-hbtop</span><br><span class="line">mvn clean package -DskipTests&#x3D;true</span><br></pre></td></tr></table></figure>

<p>当然也可以用 IDEA 来打包</p>
<p><img src="/images/blog/2019-11-12-4.png" alt></p>
<p>进入 hbase-hbtop 项目 target 目录下可以看到已经打好的 jar 包</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[11:16:25] tu target (master) $ ls</span><br><span class="line">classes</span><br><span class="line">generated-sources</span><br><span class="line">generated-test-sources</span><br><span class="line">hbase-hbtop-2.1.0-cdh6.3.1-sources.jar</span><br><span class="line">hbase-hbtop-2.1.0-cdh6.3.1-test-sources.jar</span><br><span class="line">hbase-hbtop-2.1.0-cdh6.3.1-tests.jar</span><br><span class="line">hbase-hbtop-2.1.0-cdh6.3.1.jar</span><br><span class="line">maven-archiver</span><br><span class="line">maven-shared-archive-resources</span><br><span class="line">test-classes</span><br></pre></td></tr></table></figure>

<p>现在可以尝试部署到HBase集群上了，为了验证，先只在一台机器上做，成功后再每台机器上重复，当然，只在一台机器上部署也行，每次排查问题时都到这台<br>机器上操作即可</p>
<p>将 <code>hbase-hbtop-2.1.0-cdh6.3.1.jar</code> 拷贝到 <code>/opt/cloudera/parcels/CDH/lib/hbase/lib</code> 目录下</p>
<p><img src="/images/blog/2019-11-12-5.png" alt></p>
<p>修改 <code>bin/hbase</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">elif [ &quot;$COMMAND&quot; &#x3D; &quot;hbtop&quot; ] ; then</span><br><span class="line">  CLASS&#x3D;&#39;org.apache.hadoop.hbase.hbtop.HBTop&#39;</span><br><span class="line">  if [ -n &quot;$&#123;shaded_jar&#125;&quot; ] ; then</span><br><span class="line">    for f in &quot;$&#123;HBASE_HOME&#125;&quot;&#x2F;lib&#x2F;hbase-hbtop*.jar; do</span><br><span class="line">      if [ -f &quot;$&#123;f&#125;&quot; ]; then</span><br><span class="line">        CLASSPATH&#x3D;&quot;$&#123;CLASSPATH&#125;:$&#123;f&#125;&quot;</span><br><span class="line">        break</span><br><span class="line">      fi  </span><br><span class="line">    done</span><br><span class="line">    for f in &quot;$&#123;HBASE_HOME&#125;&quot;&#x2F;lib&#x2F;commons-lang3*.jar; do</span><br><span class="line">      if [ -f &quot;$&#123;f&#125;&quot; ]; then</span><br><span class="line">        CLASSPATH&#x3D;&quot;$&#123;CLASSPATH&#125;:$&#123;f&#125;&quot;</span><br><span class="line">        break</span><br><span class="line">      fi  </span><br><span class="line">    done</span><br><span class="line">  fi  </span><br><span class="line"></span><br><span class="line">  HBASE_OPTS&#x3D;&quot;$&#123;HBASE_OPTS&#125; -Dlog4j.configuration&#x3D;file:$&#123;HBASE_HOME&#125;&#x2F;conf&#x2F;log4j-hbtop.properties&quot;</span><br></pre></td></tr></table></figure>

<p>添加位置如图所示629行～646行</p>
<p><img src="/images/blog/2019-11-12-6.png" alt></p>
<p>接着在conf目录下添加一个<code>log4j-hbtop.properties</code> 文件，用来设置 hbtop 的日志级别</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Licensed to the Apache Software Foundation (ASF) under one</span><br><span class="line"># or more contributor license agreements.  See the NOTICE file</span><br><span class="line"># distributed with this work for additional information</span><br><span class="line"># regarding copyright ownership.  The ASF licenses this file</span><br><span class="line"># to you under the Apache License, Version 2.0 (the</span><br><span class="line"># &quot;License&quot;); you may not use this file except in compliance</span><br><span class="line"># with the License.  You may obtain a copy of the License at</span><br><span class="line">#</span><br><span class="line">#     http:&#x2F;&#x2F;www.apache.org&#x2F;licenses&#x2F;LICENSE-2.0</span><br><span class="line">#</span><br><span class="line"># Unless required by applicable law or agreed to in writing, software</span><br><span class="line"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line"># See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line"></span><br><span class="line">log4j.rootLogger&#x3D;WARN,console</span><br><span class="line">log4j.threshold&#x3D;WARN</span><br><span class="line"></span><br><span class="line"># console</span><br><span class="line">log4j.appender.console&#x3D;org.apache.log4j.ConsoleAppender</span><br><span class="line">log4j.appender.console.target&#x3D;System.err</span><br><span class="line">log4j.appender.console.layout&#x3D;org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.console.layout.ConversionPattern&#x3D;%d&#123;ISO8601&#125; %-5p [%t] %c&#123;2&#125;: %m%n</span><br><span class="line"></span><br><span class="line"># ZooKeeper will still put stuff at WARN</span><br><span class="line">log4j.logger.org.apache.zookeeper&#x3D;ERROR</span><br></pre></td></tr></table></figure>

<p>在终端启动hbtop， <code>hbase hbtop</code></p>
<p><img src="/images/blog/2019-11-12-7.png" alt></p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blogs.apache.org/hbase/entry/introduction-hbtop-a-real-time" target="_blank" rel="noopener">hbtop blog</a></li>
</ul>
]]></content>
      <categories>
        <category>CDH</category>
      </categories>
  </entry>
  <entry>
    <title>Ganglia</title>
    <url>/2019/11/07/ganglia/</url>
    <content><![CDATA[<p>Ganglia 易于扩展的监控系统</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>Ganglia，它是一个易于扩展的监控系统。使用它可以实时查看 Linux 服务器和集群（图形化展示）中的各项性能指标</p>
<p>Ganglia 能够让你以集群（按服务器组）和网格（按地理位置）的方式更好地组织服务器</p>
<p>可以创建一个包含所有远程主机的网格，然后将那些机器按照其它标准分组成小的集合</p>
<p>此外， Ganglia 的 web 页面对移动设备进行过优化，也允许你导出 csv 和 .json 格式的数据</p>
<h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>启用 EPEL 仓库 ，然后安装 Ganglia 和相关工具</p>
<p>主节点</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum -y install epel-release</span><br><span class="line"></span><br><span class="line">yum -y install ganglia-gmetad ganglia-gmond ganglia-web</span><br></pre></td></tr></table></figure>

<p>监控节点</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum -y install epel-release</span><br><span class="line"></span><br><span class="line">yum -y install ganglia-gmond</span><br></pre></td></tr></table></figure>

<p><code>ganglia-gmetad</code>一个守护进程，用来收集被监控主机的数据</p>
<p>被监控主机与主节点主机都要安装<code>ganglia-gmond</code></p>
<p><code>ganglia-web</code>提供 Web 前端，用于显示监控系统的历史数据和图形</p>
<h4 id="基本认证"><a href="#基本认证" class="headerlink" title="基本认证"></a>基本认证</h4><p>使用 Apache 提供的基本认证功能，为 Ganglia Web 界面（/usr/share/ganglia）配置身份认证</p>
<p>为完成这个目标，我们需要创建一个用户名并设定一个密码，以访问被 Apache 保护的资源</p>
<p>在本例中，我们先来创建一个叫 adminganglia 的用户名，然后给它分配一个密码，它将被储存在 /etc/httpd/auth.basic（可以随意选择另一个目录 和/或 文件名， 只要 Apache 对此有读取权限就可以。）</p>
<p>htpasswd -c /etc/httpd/auth.basic adminganglia</p>
<p>给 adminganglia 输入两次密码完成密码设置</p>
<p>修改配置文件 <code>/etc/httpd/conf.d/ganglia.conf</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Alias &#x2F;ganglia &#x2F;usr&#x2F;share&#x2F;ganglia</span><br><span class="line">&lt;Location &#x2F;ganglia&gt;</span><br><span class="line">AuthType basic</span><br><span class="line">AuthName &quot;Ganglia web UI&quot;</span><br><span class="line">AuthBasicProvider file</span><br><span class="line">AuthUserFile &quot;&#x2F;etc&#x2F;httpd&#x2F;auth.basic&quot;</span><br><span class="line">Require user adminganglia</span><br><span class="line">&lt;&#x2F;Location&gt;</span><br></pre></td></tr></table></figure>

<h4 id="修改-gmetad-conf"><a href="#修改-gmetad-conf" class="headerlink" title="修改 gmetad.conf"></a>修改 gmetad.conf</h4><p>编辑 <code>/etc/ganglia/gmetad.conf</code></p>
<p>首先，使用 <code>gridname</code> 指令来为网格设置一个描述性名称</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">gridname &quot;Home office&quot;</span><br></pre></td></tr></table></figure>

<p>然后，使用 data_source 指令，后面跟集群名（服务器组）、轮询时间间隔（秒）、主节点主机和被监控节点的 IP 地址</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">data_source &quot;Labs&quot; 60 192.168.0.29:8649 192.168.0.30 192.168.0.31</span><br></pre></td></tr></table></figure>

<h4 id="修改-gmond-conf"><a href="#修改-gmond-conf" class="headerlink" title="修改 gmond.conf"></a>修改 gmond.conf</h4><p>编辑 <code>/etc/ganglia/gmond.conf</code></p>
<p>确保集群的配置类似下面</p>
<p>指定集群名字</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cluster &#123;</span><br><span class="line">  name &#x3D; &quot;Labs&quot; # gmetad.conf 中的  data_source 指令的名字</span><br><span class="line">  owner &#x3D; &quot;unspecified&quot;</span><br><span class="line">  latlong &#x3D; &quot;unspecified&quot;</span><br><span class="line">  url &#x3D; &quot;unspecified&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>每个节点写自己的主机名</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">host &#123;</span><br><span class="line">  location &#x3D; &quot;region-30&quot; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在 <code>udp_send_channel</code> 中，注释掉 <code>mcast_join</code>，并指向 gmetad 主机</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">udp_send_channel &#123;</span><br><span class="line">  #mcast_join &#x3D; 239.2.11.71</span><br><span class="line">  host &#x3D; 192.168.0.29  # 每个节点都指向gmetad主机</span><br><span class="line">  port &#x3D; 8649</span><br><span class="line">  ttl &#x3D; 1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在 <code>udp_recv_channel</code> 中，注释掉 <code>mcast_join</code> 和 <code>bind</code> 部分</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">udp_recv_channel &#123;</span><br><span class="line">  # mcast_join &#x3D; 239.2.11.71 ## comment out</span><br><span class="line">  port &#x3D; 8649</span><br><span class="line">  # bind &#x3D; 239.2.11.71 ## comment out</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br></pre></td></tr></table></figure>

<h4 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h4><p>启动 Apache、gmetad、gmond，并确保它们启用了“开机启动”。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl start httpd gmetad gmond</span><br><span class="line">ystemctl enable httpd gmetad httpd</span><br></pre></td></tr></table></figure>

<p>至此，我们现在能够打开 Ganglia 的 Web 页面 <a href="http://192.168.0.29/ganglia" target="_blank" rel="noopener">http://192.168.0.29/ganglia</a> 并用步骤 2 中设置的凭证登录</p>
<h4 id="收集-Hadoop-集群-metric"><a href="#收集-Hadoop-集群-metric" class="headerlink" title="收集 Hadoop 集群 metric"></a>收集 Hadoop 集群 metric</h4><p>因为我用的是 CDH 版本的Hadoop。所以这里以CDH为例，Apache 版的网上好多。自行搜索</p>
<p><img src="/images/blog/2019-11-07-1.png" alt></p>
<p>根绝<a href="https://docs.cloudera.com/documentation/enterprise/5-9-x/topics/cm_mc_metrics_ganglia.html" target="_blank" rel="noopener">官方文档</a>可以知道</p>
<p>主页-&gt;配置-&gt;高级配置代码段-&gt;搜索Metrics-&gt;Hadoop 度量 2 高级配置代码段（安全阀）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">*.sink.ganglia.class&#x3D;org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31 </span><br><span class="line">*.period&#x3D;10</span><br><span class="line">hbase.sink.ganglia.servers&#x3D;192.168.1.240:8649</span><br><span class="line">datanode.sink.ganglia.servers&#x3D;192.168.1.240:8649</span><br><span class="line">namenode.sink.ganglia.servers&#x3D;192.168.1.240:8649</span><br><span class="line">*.sink.ganglia.retry_socket_interval&#x3D;60000</span><br><span class="line">*.sink.ganglia.socket_connection_retries&#x3D;10</span><br><span class="line">*.source.filter.class&#x3D;org.apache.hadoop.metrics2.filter.GlobFilter</span><br><span class="line">*.record.filter.class&#x3D;$&#123;*.source.filter.class&#125;</span><br><span class="line">*.metric.filter.class&#x3D;$&#123;*.source.filter.class&#125;</span><br><span class="line">nodemanager.sink.ganglia.record.filter.exclude&#x3D;ContainerResource*</span><br><span class="line">secondarynamenode.sink.ganglia.servers&#x3D;192.168.1.240:8649</span><br><span class="line">nodemanager.sink.ganglia.servers&#x3D;192.168.1.240:8649</span><br><span class="line">resourcemanager.sink.ganglia.servers&#x3D;192.168.1.240:8649</span><br><span class="line">jobhistoryserver.sink.ganglia.servers&#x3D;192.168.1.240:8649</span><br></pre></td></tr></table></figure>

<p>把 IP 改成你自己的 gmetad IP，然后重启</p>
<p>选择某个Node具体观察，可以看到已经收集到了HDFS和YARN的度量数据</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://linux.cn/article-8161-1.html" target="_blank" rel="noopener">使用 Ganglia 对 Linux 网格和集群服务器进行实时监控</a></li>
<li><a href="https://www.centos.bz/2017/07/ganglia-monitor-hadoop-nagios-notice/" target="_blank" rel="noopener">Ganglia监控Hadoop集群 使用Nagios发送告警邮件</a></li>
<li><a href="https://docs.cloudera.com/documentation/enterprise/5-9-x/topics/cm_mc_metrics_ganglia.html" target="_blank" rel="noopener">Exposing Hadoop Metrics to Ganglia</a></li>
<li><a href="https://www.dazhuanlan.com/2019/08/21/5d5d10f64d4ed/" target="_blank" rel="noopener">Ganglia部署实战</a></li>
</ul>
]]></content>
      <categories>
        <category>Tools</category>
      </categories>
  </entry>
  <entry>
    <title>CDH 使用原生 Apache Spark</title>
    <url>/2019/11/05/Spark/</url>
    <content><![CDATA[<p>CDH 使用原生 Apache Spark</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>需要使用 Spark 2.x 版本</p>
<p>在 Spark 官网中下载的 Spark 包中是有 spark-submit、spark-shell、spark-sql，但是在 Cloudera 版 Spark 中是没有spark-sql的</p>
<p><img src="/images/blog/2020-01-02-4.png" alt></p>
<h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>官网下载原生 Apache Spark 2.x 版本，直接解压即可</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar -zxvf spark-2.4.4-bin-hadoop2.7.tgz -C &#x2F;opt</span><br><span class="line">mv spark-2.4.4-bin-hadoop2.7 spark</span><br></pre></td></tr></table></figure>

<p>设置环境变量 HADOOP_HOME、HADOOP_CONF_DIR、YARN_CONF_DIR、SPARK_HOME</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim &#x2F;etc&#x2F;profile</span><br><span class="line">export HADOOP_HOME&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.3.1-1.cdh6.3.1.p0.1470567&#x2F;lib&#x2F;hadoop</span><br><span class="line">export HADOOP_CONF_DIR&#x3D;&#x2F;etc&#x2F;hadoop&#x2F;conf</span><br><span class="line">export YARN_CONF_DIR&#x3D;&#x2F;etc&#x2F;hadoop&#x2F;conf</span><br><span class="line">export SPARK_HOME&#x3D;&#x2F;opt&#x2F;spark</span><br><span class="line">export PATH&#x3D;$SPARK_HOME&#x2F;bin:$PATH</span><br></pre></td></tr></table></figure>

<p>支持 Hive </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp -r &#x2F;etc&#x2F;hive&#x2F;conf $SPARK_HOME&#x2F;conf</span><br></pre></td></tr></table></figure>

<p>上传本地的 Spark jar 包到 HDFS 上</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop fs -mkdir -p &#x2F;user&#x2F;spark&#x2F;jars</span><br><span class="line">hadoop fs -put $&#123;SPARK_HOME&#125;&#x2F;jars&#x2F;* &#x2F;user&#x2F;spark&#x2F;jars</span><br><span class="line"></span><br><span class="line"># 在spark-default.conf添加一行，如果HDFS 是 HA 的话，请填 nameserver 地址</span><br><span class="line"># spark.yarn.jars                    hdfs:&#x2F;&#x2F;nameserver1&#x2F;user&#x2F;spark&#x2F;jars&#x2F;*</span><br><span class="line">spark.yarn.jars                    hdfs:&#x2F;&#x2F;master-240:8020&#x2F;user&#x2F;spark&#x2F;jars&#x2F;*</span><br></pre></td></tr></table></figure>

<p>解决 native 告警</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim &#x2F;etc&#x2F;profile</span><br><span class="line">export JAVA_LIBRARY_PATH&#x3D;$HADOOP_HOME&#x2F;lib&#x2F;native&#x2F;</span><br><span class="line"></span><br><span class="line">vim $SPARK_HOME&#x2F;conf&#x2F;spark-env.sh</span><br><span class="line">export LD_LIBRARY_PATH&#x3D;$JAVA_LIBRARY_PATH</span><br></pre></td></tr></table></figure>

<h4 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h4><p>spark-sql</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark-sql \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--num-executors 10</span><br></pre></td></tr></table></figure>

<p>spark-shell</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark-shell \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--num-executors 10</span><br></pre></td></tr></table></figure>

<p>spark-submit</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">&#x2F;opt&#x2F;spark&#x2F;examples&#x2F;jars&#x2F;spark-examples_2.11-2.4.4.jar 10</span><br></pre></td></tr></table></figure>

<h4 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h4><p>如果不知道 HADOOP_HOME 路径</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop checknative -a</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/2019-11-05-3.png" alt></p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/wsjwoods/article/details/90108220" target="_blank" rel="noopener">WARN spark.yarn.jars falling back to uploading</a></li>
<li><a href="https://www.cnblogs.com/zzhuang/p/10825542.html" target="_blank" rel="noopener">CDH 5.12.0 中使用 spark 2.4.2</a></li>
<li><a href="https://www.cnblogs.com/tijun/p/7562282.html" target="_blank" rel="noopener">运行Spark-shell，解决Unable to load native-hadoop library for your platform</a></li>
<li><a href="http://lxw1234.com/archives/2015/07/416.htm" target="_blank" rel="noopener">Spark On Yarn：提交Spark应用程序到Yarn</a></li>
<li><a href="http://lxw1234.com/archives/2015/08/448.htm" target="_blank" rel="noopener">在Yarn上运行spark-shell和spark-sql命令行</a></li>
<li><a href="https://blog.csdn.net/tianjun2012/article/details/75039807" target="_blank" rel="noopener">cdh5.9添加sparksql cli直接操作hive</a></li>
</ul>
]]></content>
      <categories>
        <category>CDH</category>
      </categories>
  </entry>
  <entry>
    <title>yum update操作导致系统版本升级</title>
    <url>/2019/11/04/yum-upgrade-update/</url>
    <content><![CDATA[<p>CentOS 下的 yum upgrade和 yum update区别</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>虚拟机初始化时是 CentOs7.6 的版本，在搭建CDH的时候查看系统版本突然发现变成 7.7</p>
<p>这之间发生了些奇怪的事</p>
<h4 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h4><p>原来是跟yum update有关</p>
<blockquote>
<ul>
<li>yum update: 升级所有包同时也升级软件和系统内核</li>
<li>yum upgrade: 只升级所有包，不升级软件和系统内核</li>
</ul>
</blockquote>
<h4 id="测试样例"><a href="#测试样例" class="headerlink" title="测试样例"></a>测试样例</h4><p>升级前</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">系统版本： centos5.5</span><br><span class="line">内核版本： 2.6.18-194.el5</span><br></pre></td></tr></table></figure>

<p>yum upgrade 升级后，只升级所有包，不升级软件和系统内核</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">系统版本： centos5.7</span><br><span class="line">内核版本： 2.6.18-194.el5</span><br></pre></td></tr></table></figure>

<p>yum update 升级后，升级所有包同时也升级软件和系统内核</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">系统版本： centos5.7</span><br><span class="line">内核版本： 2.6.18-238.el5</span><br></pre></td></tr></table></figure>

<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>生产环境对软件版本和内核版本要求非常精确，别没事有事随便的进行yum update操作！</p>
<p>CentOS下的yum upgrade和yum update区别，没事别乱用，和Ubuntu的update不一样！</p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>JVM 堆内存 heap 详解</title>
    <url>/2019/11/04/Java-heap/</url>
    <content><![CDATA[<p>JAVA堆内存管理是影响性能主要因素之一</p>
<hr>
<h4 id="堆划分"><a href="#堆划分" class="headerlink" title="堆划分"></a>堆划分</h4><p><img src="/images/blog/2019-11-05-1.png" alt></p>
<ul>
<li>JVM内存划分为堆内存和非堆内存，堆内存分为年轻代（Young Generation）、老年代（Old Generation），非堆内存就一个永久代（Permanent Generation）</li>
<li>年轻代又分为 Eden 和 Survivor 区。Survivor区由 FromSpace 和 ToSpace 组成。Eden区占大容量，Survivor 两个区占小容量，默认比例是8:1:1</li>
<li>堆内存用途: 存放的是对象，垃圾收集器就是收集这些对象，然后根据GC算法回收</li>
<li>非堆内存用途: 永久代，也称为方法区，存储程序运行时长期存活的对象，比如类的元数据、方法、常量、属性等</li>
</ul>
<blockquote>
<p>年轻代（New）: 年轻代用来存放JVM刚分配的Java对象<br>年老代（Tenured）: 年轻代中经过垃圾回收没有回收掉的对象将被Copy到年老代<br>永久代（Perm）: 永久代存放Class、Method元信息，其大小跟项目的规模、类、方法的量有关，一般设置为128M就足够，设置原则是预留30%的空间</p>
</blockquote>
<p><strong>在JDK1.8版本废弃了永久代，替代的是元空间（MetaSpace），元空间与永久代上类似，都是方法区的实现，他们最大区别是：元空间并不在JVM中，而是使用本地内存。</strong></p>
<p>永久代有两个参数</p>
<ul>
<li>-XX:PermSize: 永久代初始大小</li>
<li>-XX:MaxPermSize: 永久代最大允许大小</li>
</ul>
<p>元空间也有两个参数: </p>
<ul>
<li>-XX:MetaspaceSize: 初始化元空间大小，控制发生GC阈值</li>
<li>-XX:MaxMetaspaceSize: 限制元空间大小上限，防止异常占用过多物理内存</li>
</ul>
<h4 id="堆内存溢出"><a href="#堆内存溢出" class="headerlink" title="堆内存溢出"></a>堆内存溢出</h4><p>在年轻代中经过GC后还存活的对象会被复制到老年代中。当老年代空间不足时，JVM会对老年代进行完全的垃圾回收（Full GC）</p>
<p>如果GC后，还是无法存放从Survivor区复制过来的对象，就会出现OOM（Out of Memory）</p>
<p>OOM（Out of Memory）异常常见有以下几个原因</p>
<ul>
<li>老年代内存不足: java.lang.OutOfMemoryError:Javaheapspace</li>
<li>永久代内存不足: java.lang.OutOfMemoryError:PermGenspace</li>
<li>代码bug，占用内存无法及时回收</li>
</ul>
<p>OOM在这几个内存区都有可能出现，实际遇到OOM时，能根据异常信息定位到哪个区的内存溢出</p>
<p>可以通过添加个参数-XX:+HeapDumpOnOutMemoryError，让虚拟机在出现内存溢出异常时Dump出当前的内存堆转储快照以便事后分析。 -XX:HeapDumpPath=${目录} 表示生成DUMP文件的路径，也可以指定文件名称，例-XX:HeapDumpPath=${目录}/java_heapdump.hprof</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.cnblogs.com/UncleWang001/articles/10422289.html" target="_blank" rel="noopener">JVM堆内存(heap)详解</a></li>
<li><a href="https://juejin.im/post/5b8d2a5551882542ba1ddcf8" target="_blank" rel="noopener">JVM 系列文章之 Full GC 和 Minor GC</a></li>
</ul>
]]></content>
      <categories>
        <category>JVM</category>
      </categories>
  </entry>
  <entry>
    <title>CDH 6.3.1 安装</title>
    <url>/2019/11/04/CDH-6.3.1/</url>
    <content><![CDATA[<p>CDH 6.3.1 安装</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>CDH6集群的完全离线部署</p>
<h4 id="环境要求"><a href="#环境要求" class="headerlink" title="环境要求"></a>环境要求</h4><p>安装之前查阅下<a href="https://docs.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_requirements_supported_versions.html" target="_blank" rel="noopener">官方文档</a>的要求</p>
<p><img src="/images/blog/2019-11-04-1.png" alt></p>
<p>个人关注 操作系统、数据库、JDK 三个点</p>
<p><strong>操作系统</strong></p>
<p>选用 <a href="http://mirrors.sohu.com/centos/7.5.1804/isos/x86_64/CentOS-7-x86_64-DVD-1804.iso" target="_blank" rel="noopener">CentOS 7.5</a></p>
<p>Cloudera Manager和CDH内置嵌入式PostgreSQL数据库，用于非生产环境。生产环境不支持PostgreSQL数据库，必须为集群配置专用外部数据库。</p>
<p>注意:</p>
<ul>
<li>数据库需要使用UTF8编码。对于MySQL和MariaDB，必须使用UTF8编码，而不是utf8mb4</li>
<li>对于MySQL5.7，必须要额外安装MySQL-shared-compat或者MySQL-shared包</li>
</ul>
<p><strong>Java要求</strong></p>
<p>支持Oracle 64位JDK8，不支持JDK7，JDK9</p>
<h4 id="安装之前"><a href="#安装之前" class="headerlink" title="安装之前"></a>安装之前</h4><h5 id="Host"><a href="#Host" class="headerlink" title="Host"></a>Host</h5><p><img src="/images/blog/2019-08-02-5.png" alt></p>
<p>主机YZ-JDB-106-38-13的名称中有大写字符。在这种情况下，通过Kerberos进行的身份验证将无法正常工作。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 一定要注意 hostname 不能包含下划线</span><br><span class="line"># 设置唯一主机名</span><br><span class="line">  </span><br><span class="line"># 修改hostname</span><br><span class="line">hostname yz-100-73-18-59  # 直接修改命令</span><br><span class="line">hostname                  # 查看修改后的hostname</span><br><span class="line">  </span><br><span class="line"># 修改网络配置文件</span><br><span class="line">vim &#x2F;etc&#x2F;sysconfig&#x2F;network</span><br><span class="line">HOSTNAME&#x3D;yz-JDB-106-35-3</span><br><span class="line">  </span><br><span class="line"># 修改hosts文件</span><br><span class="line"># 把集群所有的ip与对应的hostname</span><br><span class="line">vim &#x2F;etc&#x2F;hosts     </span><br><span class="line">100.73.41.20 yz-100-73-41-20</span><br><span class="line">100.73.41.21 yz-100-73-41-21</span><br><span class="line">100.73.41.22 yz-100-73-41-22</span><br></pre></td></tr></table></figure>

<h5 id="ssh免密"><a href="#ssh免密" class="headerlink" title="ssh免密"></a>ssh免密</h5><p>手动操作如下的代码块所示。 参考自<a href="https://www.cnblogs.com/keitsi/p/5653520.html" target="_blank" rel="noopener">CentOS 配置集群机器之间SSH免密码登录</a></p>
<p><img src="/images/blog/2020-02-08-1.png" alt></p>
<blockquote>
<p>上图就是SSh免密登陆原理图，从上图可以看出，SSH免密登陆的前提是使用<code>ssh-keygen -t RSA</code>生成公私秘钥对，<br>然后通过<code>ssh-copy-id -i ~/.ssh/id_rsa.pub user@host</code>命令将公钥分发至远程主机。<br>接下来的每次免密登陆步骤如下: </p>
<ol>
<li>用户使用ssh user@host命令对远程主机发起登陆；</li>
<li>远程主机对用户返回一个随机串；</li>
<li>用户所在主机使用私钥对这个随机串进行加密，并将加密的随机串返回至远程主机；</li>
<li>远程主机使用分发过来的公钥对加密随机串进行解密；</li>
<li>如果解密成功，就证明用户的登陆信息是正确的，则允许登陆；否则反之。<br>通过<code>ssh-copy-id user@host</code>命令分发的公钥都会被保存至远程主机的<code>~/.ssh/authorized_keys</code>文件中。</li>
</ol>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 集群密码登录</span><br><span class="line"># 集群中的每台主机上执行下面命令，一路回车，可生成本机的 rsa 类型的密钥</span><br><span class="line">ssh-keygen -t rsa</span><br><span class="line"># 执行完之后在 ~&#x2F;.ssh&#x2F; 目录下会生成一个保存有公钥的文件：id_rsa.pub</span><br><span class="line">   </span><br><span class="line"># 把公钥写入 authorized_keys 文件</span><br><span class="line"># 把自己的公钥拷贝到集群中的Master机</span><br><span class="line">ssh-copy-id root@HadoopMaster</span><br><span class="line">   </span><br><span class="line"># 最终Master机上生成~&#x2F;.ssh&#x2F;authorized_keys文件   该文件保存所有机器的公钥</span><br><span class="line"># 把~&#x2F;.ssh&#x2F;authorized_keys 拷贝到集群的其他机器上</span><br><span class="line">scp ~&#x2F;.ssh&#x2F;authorized_keys root@HadoopSlave1:~&#x2F;.ssh&#x2F;</span><br><span class="line">scp ~&#x2F;.ssh&#x2F;authorized_keys root@HadoopSlave2:~&#x2F;.ssh&#x2F;</span><br></pre></td></tr></table></figure>

<h5 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看防火墙</span><br><span class="line">systemctl status firewalld</span><br><span class="line"> </span><br><span class="line"># 关闭防火墙</span><br><span class="line">systemctl stop firewalld</span><br><span class="line"> </span><br><span class="line"># 开机禁用</span><br><span class="line">systemctl disable firewalld</span><br></pre></td></tr></table></figure>

<h5 id="禁用-SELinux"><a href="#禁用-SELinux" class="headerlink" title="禁用 SELinux"></a>禁用 SELinux</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 关闭SELINUX</span><br><span class="line"># 临时生效</span><br><span class="line">setenforce 0</span><br><span class="line"> </span><br><span class="line"># 永久生效</span><br><span class="line">vim &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line">SELINUX&#x3D;disabled</span><br></pre></td></tr></table></figure>

<h5 id="启用-NTP"><a href="#启用-NTP" class="headerlink" title="启用 NTP"></a>启用 NTP</h5><p>集群中所有节点的时间必须要保持同步，可以选择集群中其中一个节点作为ntp服务端，其余节点作为客户端</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 停止影响的服务</span><br><span class="line">systemctl stop chronyd</span><br><span class="line">systemctl disable chronyd</span><br><span class="line"></span><br><span class="line"># 安装ntp软件</span><br><span class="line">yum -y install ntp</span><br><span class="line"></span><br><span class="line"># 配置主节点ntp配置文件</span><br><span class="line">vim &#x2F;etc&#x2F;ntp.conf</span><br><span class="line"></span><br><span class="line">driftfile &#x2F;var&#x2F;lib&#x2F;ntp&#x2F;drift</span><br><span class="line">restrict 127.0.0.1</span><br><span class="line">restrict -6 ::1</span><br><span class="line">restrict default nomodify notrap</span><br><span class="line">server ntp1.aliyun.com prefer</span><br><span class="line">minpoll 6</span><br><span class="line">includefile &#x2F;etc&#x2F;ntp&#x2F;crypto&#x2F;pw</span><br><span class="line">keys &#x2F;etc&#x2F;ntp&#x2F;keys </span><br><span class="line"></span><br><span class="line"># 配置其他节点ntp文件，同样修改ntp.conf文件</span><br><span class="line">driftfile &#x2F;var&#x2F;lib&#x2F;ntp&#x2F;drift</span><br><span class="line">restrict 127.0.0.1</span><br><span class="line">restrict -6 ::1</span><br><span class="line">restrict default kod nomodify notrap nopeer noquery</span><br><span class="line">restrict -6 default kod nomodify notrap nopeer noquery</span><br><span class="line">#这里是主节点的主机名或者ip</span><br><span class="line">server cdh-master.test.com</span><br><span class="line">minpoll 6</span><br><span class="line">includefile &#x2F;etc&#x2F;ntp&#x2F;crypto&#x2F;pw</span><br><span class="line">keys &#x2F;etc&#x2F;ntp&#x2F;keys</span><br><span class="line"> </span><br><span class="line"># 启动ntp服务</span><br><span class="line">systemctl start ntpd</span><br><span class="line"></span><br><span class="line"># 查看ntp服务</span><br><span class="line">systemctl status ntpd</span><br><span class="line"></span><br><span class="line"># 在其它节点上手动同步主节点的时间，换成你自己的主节点IP</span><br><span class="line">ntpdate -u 192.168.0.201</span><br><span class="line"></span><br><span class="line"># 配置开机自启动</span><br><span class="line">systemctl enable ntpd</span><br><span class="line"></span><br><span class="line"># 确认是否同步成功</span><br><span class="line">ntpstat (若网络没问题，过一会synchronised to NTP server ...)</span><br><span class="line">ntpq -p  (若网络没问题，过一会会看到有一行开头会有个星号)</span><br></pre></td></tr></table></figure>

<p>NTP服务端重启后，客户机要等5分钟再与其进行时间同步，否则会提示“no server suitable for synchronization found”错误。</p>
<h5 id="虚拟内存参数设置"><a href="#虚拟内存参数设置" class="headerlink" title="虚拟内存参数设置"></a>虚拟内存参数设置</h5><p>需要调整的参数为vm.swappiness，是一个0-100的值，用于控制应用数据从物理内存到磁盘上的虚拟内存的交换，值越高，交换越积极，值越小，交换的次数越少</p>
<p>在大多数Linux系统上，该参数默认值为60，这并不适用于Hadoop集群，因为即使有足够的内存，也可能会进行进程的交换，从而可能会影响Hadoop的性能和稳定性</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看当前该项参数值</span><br><span class="line">cat &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;swappiness</span><br><span class="line"></span><br><span class="line"># 修改该项参数值（临时生效）</span><br><span class="line">sysctl -w vm.swappiness&#x3D;0</span><br><span class="line"></span><br><span class="line"># （重启后永久生效）</span><br><span class="line">echo vm.swappiness &#x3D; 0 &gt;&gt; &#x2F;etc&#x2F;sysctl.conf</span><br></pre></td></tr></table></figure>

<h5 id="禁用-tuned-服务"><a href="#禁用-tuned-服务" class="headerlink" title="禁用 tuned 服务"></a>禁用 tuned 服务</h5><p>如果您的群集主机正在运行RHEL / CentOS 7.x，请通过运行以下命令禁用”tuned”服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 确保tuned服务已开启</span><br><span class="line">systemctl start tuned</span><br><span class="line"></span><br><span class="line"># 关闭tuned服务</span><br><span class="line">tuned-adm off</span><br><span class="line"></span><br><span class="line"># 确保没有已激活的配置</span><br><span class="line">tuned-adm list</span><br><span class="line">如果输出内容中包含No current active profile表示关闭成功</span><br><span class="line"></span><br><span class="line"># 关闭并且禁用tuned服务</span><br><span class="line">systemctl stop tuned</span><br><span class="line">systemctl disable tuned</span><br></pre></td></tr></table></figure>

<p>官方文档<a href="https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/cdh_admin_performance.html#cdh_performance__disable-tuned" target="_blank" rel="noopener">Disable the tuned Service</a></p>
<h5 id="禁用-THP"><a href="#禁用-THP" class="headerlink" title="禁用 THP"></a>禁用 THP</h5><p>大多数Linux平台都包含一个名为transparent hugepages的功能，该功能可能会严重降低Hadoop集群的性能</p>
<p>在root权限下，使用以下命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 检查THP是否启用</span><br><span class="line">cat &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled</span><br><span class="line">cat &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;defrag</span><br><span class="line"></span><br><span class="line">输出[always] never意味着THP已启用，always [never]意味着THP未启用</span><br></pre></td></tr></table></figure>

<p>该图就表示THP已启用</p>
<p><img src="/images/blog/2019-11-13-1.png" alt></p>
<p>要禁用透明大页面，请在所有群集主机上执行以下步骤</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 编辑文件，最下面添加两行配置，重启服务器生效</span><br><span class="line">vim &#x2F;etc&#x2F;rc.d&#x2F;rc.local</span><br><span class="line">echo never &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled</span><br><span class="line">echo never &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;defrag</span><br><span class="line"></span><br><span class="line"># 赋予&#x2F;etc&#x2F;rc.d&#x2F;rc.local文件可执行权限</span><br><span class="line">chmod +x &#x2F;etc&#x2F;rc.d&#x2F;rc.local</span><br><span class="line"></span><br><span class="line"># 修改GRUB配置, 仅RHEL&#x2F;CentOS 7.x需要进行该项操作</span><br><span class="line"># 在GRUB_CMDLINE_LINUX项目后面添加一个参数</span><br><span class="line">vim &#x2F;etc&#x2F;default&#x2F;grub </span><br><span class="line">transparent_hugepage&#x3D;never</span><br><span class="line"></span><br><span class="line"># 执行命令</span><br><span class="line">grub2-mkconfig -o &#x2F;boot&#x2F;grub2&#x2F;grub.cfg</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/2019-11-13-2.png" alt></p>
<p>还可以使用以下命令即刻生效禁用透明的大页面, 但请记住，重新启动后它将失效</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo never &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled</span><br><span class="line">echo never &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;defrag</span><br></pre></td></tr></table></figure>

<p>官方文档介绍<a href="https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/cdh_admin_performance.html#cdh_performance__section_hw3_sdf_jq" target="_blank" rel="noopener">Disabling Transparent Hugepages (THP)</a></p>
<h5 id="安装-mysql5-7"><a href="#安装-mysql5-7" class="headerlink" title="安装 mysql5.7"></a>安装 mysql5.7</h5><blockquote>
<p>CDH 对 MySQL 有一些限制，具体如下</p>
<ol>
<li>MySQL 默认的 datadir 目录是<code>/var/lib/mysql</code>，要确保该目录存在的分区有足够的空间。</li>
<li>如果 MySQL 启用了 GTID 复制，会导致 Cloudera Manager 安装失败。</li>
<li>数据库需要使用UTF8编码。对于MySQL和MariaDB，必须使用UTF8编码，而不是utf8mb4</li>
<li>对于MySQL5.7，必须要额外安装 MySQL-shared-compat 或者 MySQL-shared 包，因为 Cloudera Manager Agent 包的安装依赖这两个</li>
</ol>
</blockquote>
<p>只需要在集群中某一个节点上安装即可</p>
<p>我这选用<a href="https://mirrors.tuna.tsinghua.edu.cn/mysql/downloads/MySQL-5.7/" target="_blank" rel="noopener">MySQL 5.7.25</a></p>
<p><img src="/images/blog/2019-12-02-1.png" alt></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 首先卸载操作系统可能会自带的mariadb-libs</span><br><span class="line">yum -y remove mariadb-libs</span><br><span class="line"></span><br><span class="line">解压mysql rpm-bundle tar包</span><br><span class="line">tar -zxvf mysql-5.7.25-1.el7.x86_64.rpm-bundle.tar</span><br><span class="line"></span><br><span class="line"># 开始安装mysql, 一定要按照下面的顺序来安装，否则会安装不成功</span><br><span class="line">rpm -ivh mysql-community-common-5.7.25-1.el7.x86_64.rpm</span><br><span class="line">rpm -ivh mysql-community-libs-5.7.25-1.el7.x86_64.rpm</span><br><span class="line">rpm -ivh mysql-community-client-5.7.25-1.el7.x86_64.rpm</span><br><span class="line">rpm -ivh mysql-community-server-5.7.25-1.el7.x86_64.rpm</span><br><span class="line">rpm -ivh mysql-community-libs-compat-5.7.25-1.el7.x86_64.rpm #（安装Cloudera Manager6需要）</span><br><span class="line"></span><br><span class="line"># 启动mysql服务</span><br><span class="line">systemctl start mysqld</span><br><span class="line"># 如果无法启动，则需要修改mysql数据目录所有者：chown -R mysql:mysql &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;</span><br><span class="line"></span><br><span class="line"># 查看root用户初始密码</span><br><span class="line">grep password &#x2F;var&#x2F;log&#x2F;mysqld.log</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/2019-12-02-2.png" alt></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 登录mysql修改root密码</span><br><span class="line">mysql -uroot -p</span><br><span class="line"># 如果密码复杂度不够，则会禁止修改，默认密码规则为：包含数字、大小写字母、特殊字符，同时还有长度要求</span><br><span class="line"># 可以通过修改全局参数来解决，但是还是要求密码长度至少为8位</span><br><span class="line">mysql&gt; set global validate_password_policy&#x3D;0;</span><br><span class="line">mysql&gt; set password &#x3D; password(&#39;12345678&#39;);</span><br><span class="line"></span><br><span class="line"># 设置远程登录权限</span><br><span class="line">mysql&gt; grant all privileges on *.* to &#39;root&#39;@&#39;%&#39; identified by &#39;12345678&#39;;</span><br><span class="line">mysql&gt; flush privileges;</span><br><span class="line"></span><br><span class="line"># 修改mysql数据库默认编码</span><br><span class="line">查看原数据库编码：mysql&gt; SHOW VARIABLES LIKE &#39;char%&#39;;可以看到数据库和服务端的编码都还不是utf8：</span><br></pre></td></tr></table></figure>
<p><img src="/images/blog/2019-12-02-3.png" alt></p>
<p>编辑/etc/my.cnf文件，在[mysqld]下面添加一行character-set-server=utf8</p>
<p>重启mysql服务：systemctl restart mysqld，再次登录数据库查看编码，修改成功</p>
<p><img src="/images/blog/2019-12-02-3.png" alt></p>
<p>这里我贴出我的 my.cnf</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line">datadir&#x3D;&#x2F;var&#x2F;lib&#x2F;mysql</span><br><span class="line">socket&#x3D;&#x2F;var&#x2F;lib&#x2F;mysql&#x2F;mysql.sock</span><br><span class="line">transaction-isolation &#x3D; READ-COMMITTED</span><br><span class="line"># Disabling symbolic-links is recommended to prevent assorted security risks;</span><br><span class="line"># to do so, uncomment this line:</span><br><span class="line">symbolic-links &#x3D; 0</span><br><span class="line">key_buffer_size &#x3D; 32M</span><br><span class="line">max_allowed_packet &#x3D; 32M</span><br><span class="line">thread_stack &#x3D; 256K</span><br><span class="line">thread_cache_size &#x3D; 64</span><br><span class="line">query_cache_limit &#x3D; 8M</span><br><span class="line">query_cache_size &#x3D; 64M</span><br><span class="line">query_cache_type &#x3D; 1</span><br><span class="line">max_connections &#x3D; 550</span><br><span class="line">character-set-server &#x3D; utf8</span><br><span class="line">#expire_logs_days &#x3D; 10</span><br><span class="line">#max_binlog_size &#x3D; 100M</span><br><span class="line">#log_bin should be on a disk with enough free space.</span><br><span class="line">#Replace &#39;&#x2F;var&#x2F;lib&#x2F;mysql&#x2F;mysql_binary_log&#39; with an appropriate path for your</span><br><span class="line">#system and chown the specified folder to the mysql user.</span><br><span class="line">log_bin&#x3D;&#x2F;var&#x2F;lib&#x2F;mysql&#x2F;mysql_binary_log</span><br><span class="line">#In later versions of MySQL, if you enable the binary log and do not set</span><br><span class="line">#a server_id, MySQL will not start. The server_id must be unique within</span><br><span class="line">#the replicating group.</span><br><span class="line">server_id&#x3D;1</span><br><span class="line">binlog_format &#x3D; mixed</span><br><span class="line">read_buffer_size &#x3D; 2M</span><br><span class="line">read_rnd_buffer_size &#x3D; 16M</span><br><span class="line">sort_buffer_size &#x3D; 8M</span><br><span class="line">join_buffer_size &#x3D; 8M</span><br><span class="line"># InnoDB settings</span><br><span class="line">innodb_file_per_table &#x3D; 1</span><br><span class="line">innodb_flush_log_at_trx_commit  &#x3D; 2</span><br><span class="line">innodb_log_buffer_size &#x3D; 64M</span><br><span class="line">innodb_buffer_pool_size &#x3D; 4G</span><br><span class="line">innodb_thread_concurrency &#x3D; 8</span><br><span class="line">innodb_flush_method &#x3D; O_DIRECT</span><br><span class="line">innodb_log_file_size &#x3D; 512M</span><br><span class="line">[mysqld_safe]</span><br><span class="line">log-error&#x3D;&#x2F;var&#x2F;log&#x2F;mysqld.log</span><br><span class="line">pid-file&#x3D;&#x2F;var&#x2F;run&#x2F;mysqld&#x2F;mysqld.pid</span><br><span class="line">sql_mode&#x3D;STRICT_ALL_TABLES</span><br></pre></td></tr></table></figure>

<h4 id="安装-CM"><a href="#安装-CM" class="headerlink" title="安装 CM"></a>安装 CM</h4><h5 id="文件下载"><a href="#文件下载" class="headerlink" title="文件下载"></a>文件下载</h5><p>准备 CM 和 CDH 文件</p>
<p>CM6.3.1</p>
<ul>
<li><a href="https://archive.cloudera.com/cm6/6.3.1/repo-as-tarball/cm6.3.1-redhat7.tar.gz" target="_blank" rel="noopener">https://archive.cloudera.com/cm6/6.3.1/repo-as-tarball/cm6.3.1-redhat7.tar.gz</a></li>
</ul>
<p>CDH6.3.1</p>
<ul>
<li><a href="https://archive.cloudera.com/cdh6/6.3.1/parcels/CDH-6.3.1-1.cdh6.3.1.p0.1470567-el7.parcel" target="_blank" rel="noopener">https://archive.cloudera.com/cdh6/6.3.1/parcels/CDH-6.3.1-1.cdh6.3.1.p0.1470567-el7.parcel</a></li>
<li><a href="https://archive.cloudera.com/cdh6/6.3.1/parcels/CDH-6.3.1-1.cdh6.3.1.p0.1470567-el7.parcel.sha1" target="_blank" rel="noopener">https://archive.cloudera.com/cdh6/6.3.1/parcels/CDH-6.3.1-1.cdh6.3.1.p0.1470567-el7.parcel.sha1</a></li>
<li><a href="https://archive.cloudera.com/cdh6/6.3.1/parcels/manifest.json" target="_blank" rel="noopener">https://archive.cloudera.com/cdh6/6.3.1/parcels/manifest.json</a></li>
</ul>
<p>ASC文件</p>
<ul>
<li><a href="https://archive.cloudera.com/cm6/6.3.1/allkeys.asc" target="_blank" rel="noopener">https://archive.cloudera.com/cm6/6.3.1/allkeys.asc</a></li>
</ul>
<p>repo 文件</p>
<ul>
<li><a href="https://archive.cloudera.com/cm6/6.3.1/redhat7/yum/cloudera-manager.repo" target="_blank" rel="noopener">https://archive.cloudera.com/cm6/6.3.1/redhat7/yum/cloudera-manager.repo</a></li>
</ul>
<p>同时还需要下载一个repo 文件、asc文件，同样保存到 Cloudera Manager 目录下</p>
<p>Cloudera Manager 目录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cm6.3.1</span><br><span class="line">├── allkeys.asc</span><br><span class="line">├── cloudera-manager.repo</span><br><span class="line">├── repodata</span><br><span class="line">│   ├── 3662f97de72fd44c017bb0e25cee3bc9398108c8efb745def12130a69df2ecb2-filelists.sqlite.bz2</span><br><span class="line">│   ├── 43f3725f730ee7522712039982aa4befadae4db968c8d780c8eb15ae9872cd4d-primary.xml.gz</span><br><span class="line">│   ├── 49e4d60647407a36819f1d8ed901258a13361749b742e3be9065025ad31feb8e-filelists.xml.gz</span><br><span class="line">│   ├── 8afda99b921fd1538dd06355952719652654fc06b6cd14515437bda28376c03d-other.sqlite.bz2</span><br><span class="line">│   ├── b9300879675bdbc300436c1131a910a535b8b5a5dc6f38e956d51769b6771a96-primary.sqlite.bz2</span><br><span class="line">│   ├── e28836e19e07f71480c4dad0f7a87a804dc93970ec5277ad95614e8ffcff0d58-other.xml.gz</span><br><span class="line">│   ├── repomd.xml</span><br><span class="line">│   ├── repomd.xml.asc</span><br><span class="line">│   └── repomd.xml.key</span><br><span class="line">├── RPM-GPG-KEY-cloudera</span><br><span class="line">├── RPMS</span><br><span class="line">│   ├── noarch</span><br><span class="line">│   └── x86_64</span><br><span class="line">│       ├── cloudera-manager-agent-6.3.1-1466458.el7.x86_64.rpm</span><br><span class="line">│       ├── cloudera-manager-daemons-6.3.1-1466458.el7.x86_64.rpm</span><br><span class="line">│       ├── cloudera-manager-server-6.3.1-1466458.el7.x86_64.rpm</span><br><span class="line">│       ├── cloudera-manager-server-db-2-6.3.1-1466458.el7.x86_64.rpm</span><br><span class="line">│       ├── enterprise-debuginfo-6.3.1-1466458.el7.x86_64.rpm</span><br><span class="line">│       └── oracle-j2sdk1.8-1.8.0+update181-1.x86_64.rpm</span><br><span class="line">└── SRPMS</span><br></pre></td></tr></table></figure>

<p>sha文件得重命名下，去掉末尾的数字</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mv CDH-6.3.1-1.cdh6.3.1.p0.1470567-el7.parcel.sha1 &#x2F;var&#x2F;www&#x2F;html&#x2F;cdh6_parcel&#x2F;CDH-6.3.1-1.cdh6.3.1.p0.1470567-el7.parcel.sha</span><br></pre></td></tr></table></figure>

<p>CDH 目录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cdh6.3.1</span><br><span class="line">├── CDH-6.3.1-1.cdh6.3.1.p0.1470567-el7.parcel</span><br><span class="line">├── CDH-6.3.1-1.cdh6.3.1.p0.1470567-el7.parcel.sha</span><br><span class="line">└── manifest.json</span><br></pre></td></tr></table></figure>

<h5 id="搭建-yum-库"><a href="#搭建-yum-库" class="headerlink" title="搭建 yum 库"></a>搭建 yum 库</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 安装 httpd</span><br><span class="line">yum -y install httpd</span><br><span class="line"></span><br><span class="line"># 启动httpd服务并设置开机自启动</span><br><span class="line">systemctl start httpd</span><br><span class="line">systemctl enable httpd</span><br></pre></td></tr></table></figure>

<p>将 Cloudera Manager 目录、CDH 目录移动到 httpd 的 html 目录下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mv cm6.3.1  &#x2F;var&#x2F;www&#x2F;html</span><br><span class="line">mv cdh6.3.1 &#x2F;var&#x2F;www&#x2F;html</span><br></pre></td></tr></table></figure>

<p>修改 repo 文件, {host}改为对应IP</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[cloudera-manager]</span><br><span class="line">name&#x3D;Cloudera Manager 6.3.1</span><br><span class="line">baseurl&#x3D;http:&#x2F;&#x2F;&#123;host&#125;&#x2F;cm6.3.1</span><br><span class="line">gpgcheck&#x3D;0</span><br><span class="line">enabled&#x3D;1</span><br></pre></td></tr></table></figure>

<p>复制 repo 文件到 yum 文件夹</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp cloudera-manager.repo &#x2F;etc&#x2F;yum.repos.d</span><br></pre></td></tr></table></figure>

<p>启动httpd服务并准备好目录之后浏览器打开 http://{host}/cm6.3.1 即可看到</p>
<p><img src="/images/blog/2019-11-04-2.png" alt></p>
<p>清除yum缓存，查看机器是否识别</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum clean all </span><br><span class="line">yum list | grep cloudera</span><br></pre></td></tr></table></figure>

<h5 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h5><p>在CM Server节点上操作</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum -y install cloudera-manager-server</span><br></pre></td></tr></table></figure>

<h5 id="jdbc-驱动"><a href="#jdbc-驱动" class="headerlink" title="jdbc 驱动"></a>jdbc 驱动</h5><p>下载<a href="https://cdn.mysql.com//Downloads/Connector-J/mysql-connector-java-5.1.47.tar.gz" target="_blank" rel="noopener">mysql-connector-java-5.1.47.tar.gz</a>包</p>
<p>解压出mysql-connector-java-5.1.47-bin.jar文件</p>
<p>将 mysql-connector-java-5.1.47-bin.jar 文件上传至 CM Server节点上的 /usr/share/java/ 目录下并重命名为mysql-connector-java.jar（如果/usr/share/java/目录不存在，需要手动创建）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar zxvf mysql-connector-java-5.1.47.tar.gz</span><br><span class="line">mkdir -p &#x2F;usr&#x2F;share&#x2F;java&#x2F;</span><br><span class="line">cp mysql-connector-java-5.1.47-bin.jar &#x2F;usr&#x2F;share&#x2F;java&#x2F;mysql-connector-java.jar</span><br></pre></td></tr></table></figure>

<h5 id="创建CDH所需要的数据库"><a href="#创建CDH所需要的数据库" class="headerlink" title="创建CDH所需要的数据库"></a>创建CDH所需要的数据库</h5><p>根据所需要安装的服务参照下表创建对应的数据库以及数据库用户，数据库必须使用utf8编码，创建数据库时要记录好用户名及对应密码</p>
<p><img src="/images/blog/2019-11-04-3.png" alt></p>
<p>我这里就创建 cm 所需要的数据库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE DATABASE scm DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</span><br><span class="line"></span><br><span class="line">GRANT ALL ON scm.* TO &#39;scm&#39;@&#39;%&#39; IDENTIFIED BY &#39;scm&#39;;</span><br><span class="line"></span><br><span class="line"># 查看授权是否正确</span><br><span class="line">SHOW GRANTS FOR &#39;scm&#39;@&#39;%&#39;;</span><br><span class="line"></span><br><span class="line"># 查看数据库和服务端的编码是不是utf8</span><br><span class="line">SHOW VARIABLES LIKE &#39;char%&#39;;</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/2019-11-04-4.png" alt></p>
<p><img src="/images/blog/2019-11-05-2.png" alt></p>
<h5 id="设置数据库"><a href="#设置数据库" class="headerlink" title="设置数据库"></a>设置数据库</h5><p>Cloudera Manager Server包含一个配置数据库的脚本</p>
<blockquote>
<p>传参格式 scm_prepare_database.sh &lt;db.type&gt; &lt;db.name&gt; &lt;db.user&gt;</p>
</blockquote>
<p><strong>mysql数据库与CM Server是同一台主机</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;opt&#x2F;cloudera&#x2F;cm&#x2F;schema&#x2F;scm_prepare_database.sh mysql scm scm</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/2019-11-04-5.png" alt></p>
<p><strong>mysql数据库与CM Server不在同一台主机上</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;opt&#x2F;cloudera&#x2F;cm&#x2F;schema&#x2F;scm_prepare_database.sh mysql -h &lt;mysql-host-ip&gt; --scm-host &lt;cm-server-ip&gt; scm scm</span><br></pre></td></tr></table></figure>

<p>在 CM 服务所在服务器，可以看到<code>/etc/cloudera-scm-server/db.properties</code>文件有相应数据</p>
<p><img src="/images/blog/2020-02-19-1.png" alt></p>
<h4 id="安装CDH节点"><a href="#安装CDH节点" class="headerlink" title="安装CDH节点"></a>安装CDH节点</h4><h5 id="启动-Server-服务"><a href="#启动-Server-服务" class="headerlink" title="启动 Server 服务"></a>启动 Server 服务</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl start cloudera-scm-server</span><br></pre></td></tr></table></figure>

<p>然后等待Cloudera Manager Server启动，可能需要稍等一会儿</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看 server 端口、可能需要稍等一会儿</span><br><span class="line">netstat -anlp | grep 7180</span><br><span class="line"></span><br><span class="line"># 查看 server 服务状态，active 说明正常</span><br><span class="line">systemctl status cloudera-scm-server</span><br></pre></td></tr></table></figure>

<p>如果启动报错，通过 /var/log/cloudera-scm-server/cloudera-scm-server.log 日志文件去查看状态</p>
<h5 id="访问-WEB-界面"><a href="#访问-WEB-界面" class="headerlink" title="访问 WEB 界面"></a>访问 WEB 界面</h5><p>打开浏览器，访问地址：http://<server_host>:7180，默认账号和密码都为 admin</server_host></p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.staroon.dev/2018/12/01/CDH6Install/" target="_blank" rel="noopener">CentOS7下完全离线安装CDH6集群</a></li>
<li><a href="https://www.jianshu.com/p/33e8246c31fb" target="_blank" rel="noopener">CDH 6.3.1 rpm+http安装部署</a></li>
<li><a href="https://www.jellythink.com/archives/555" target="_blank" rel="noopener">详解SSH原理</a></li>
<li><a href="https://www.cndba.cn/dave/article/3374" target="_blank" rel="noopener">CDH 依赖的MySQL 数据库安装配置说明</a></li>
</ul>
]]></content>
      <categories>
        <category>CDH</category>
      </categories>
  </entry>
  <entry>
    <title>CMS 原理</title>
    <url>/2019/10/29/CMS/</url>
    <content><![CDATA[<p>CMS(Concurrent Mark and Swee 并发-标记-清除)</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>CMS(Concurrent Mark and Swee 并发-标记-清除) 是一款基于并发、使用标记清除算法的垃圾回收算法，只针对老年代进行垃圾回收。</p>
<p>CMS 主要适合场景是对响应时间的重要性需求大于对吞吐量的要求</p>
<p>CMS 收集器工作时，尽可能让GC线程和用户线程并发执行，以达到降低STW时间的目的</p>
<p>CMS 目标是尽量减少应用的暂停时间，减少full gc发生的几率，利用和应用程序线程并发的垃圾回收线程来标记清除年老代</p>
<h5 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h5><p>GC过程短暂停，适合对时延要求较高的服务，用户线程不允许长时间的停顿</p>
<h5 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h5><p>服务长时间运行，造成严重的内存碎片化</p>
<h4 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h4><h5 id="可达性分析算法"><a href="#可达性分析算法" class="headerlink" title="可达性分析算法"></a>可达性分析算法</h5><p>用于判断对象是否存活，基本思想是通过一系列称为“GC Root”的对象作为起点（常见的GC Root有系统类加载器、栈中的对象、处于激活状态的线程等），基于对象引用关系，从GC Roots开始向下搜索，所走过的路径称为引用链，当一个对象到GC Root没有任何引用链相连，证明对象不再存活</p>
<h5 id="Stop-The-World"><a href="#Stop-The-World" class="headerlink" title="Stop The World"></a>Stop The World</h5><p>GC过程中分析对象引用关系，为了保证分析结果的准确性，需要通过停顿所有Java执行线程，保证引用关系不再动态变化，该停顿事件称为Stop The World(STW)</p>
<h5 id="Safepoint"><a href="#Safepoint" class="headerlink" title="Safepoint"></a>Safepoint</h5><p>代码执行过程中的一些特殊位置，当线程执行到这些位置的时候，说明虚拟机当前的状态是安全的，如果有需要GC，线程可以在这个位置暂停。HotSpot采用主动中断的方式，让执行线程在运行期轮询是否需要暂停的标志，若需要则中断挂起</p>
<h4 id="实现机制"><a href="#实现机制" class="headerlink" title="实现机制"></a>实现机制</h4><p>CMS分为两种模式，background和foreground，background采用concurrent remark模式，可以和用户进程并行，而foreground则必须要stop the world（STW）。周期性的CMS采用的是background的方式，而主动的GC则采用foreground方式。</p>
<p>根据GC的触发机制分为：周期性Old GC（被动）和主动Old GC、个人理解，实在不知道怎么分才好。</p>
<h5 id="Background-Collect"><a href="#Background-Collect" class="headerlink" title="Background Collect"></a>Background Collect</h5><p>周期性Old GC，执行的逻辑也叫Background Collect，对老年代进行回收，在GC日志中比较常见，由后台线程ConcurrentMarkSweepThread循环判断（默认2s）是否需要触发。</p>
<p><img src="/images/blog/2019-10-29-4.png" alt></p>
<p>触发条件</p>
<ol>
<li><p>如果没有设置-XX:+UseCMSInitiatingOccupancyOnly，虚拟机会根据收集的数据决定是否触发（建议线上环境带上这个参数，不然会加大问题排查的难度）</p>
</li>
<li><p>老年代使用率达到阈值 CMSInitiatingOccupancyFraction，默认92%</p>
</li>
<li><p>永久代的使用率达到阈值 CMSInitiatingPermOccupancyFraction，默认92%，前提是开启 CMSClassUnloadingEnabled</p>
</li>
<li><p>新生代的晋升担保失败(晋升担保失败触发Full GC)</p>
</li>
</ol>
<h6 id="新生代垃圾回收"><a href="#新生代垃圾回收" class="headerlink" title="新生代垃圾回收"></a>新生代垃圾回收</h6><p>能与CMS搭配使用的新生代垃圾收集器有Serial收集器和ParNew收集器。这2个收集器都采用标记复制算法，都会触发STW事件，停止所有的应用线程。</p>
<p>不同之处在于，Serial是单线程执行，ParNew是多线程执行</p>
<p><img src="/images/blog/2019-10-29-1.png" alt></p>
<h6 id="老年代垃圾回收"><a href="#老年代垃圾回收" class="headerlink" title="老年代垃圾回收"></a>老年代垃圾回收</h6><p><strong>GC 状态</strong></p>
<p>当触发 cms gc 对老年代进行垃圾收集时，算法中会使用 <code>_collectorState</code> 变量记录执行状态，整个周期分成以下几个状态</p>
<p><img src="/images/blog/2019-10-29-3.png" alt></p>
<ul>
<li>Idling: 一次 cms gc 生命周期的初始化状态。</li>
<li>InitialMarking: 根据 gc roots，标记出直接可达的活跃对象，这个过程需要stw的。</li>
<li>Marking: 根据 InitialMarking 阶段标记出的活跃对象，并发迭代遍历所有的活跃对象，这个过程可以和用户线程并发执行。</li>
<li>Precleaning: 并发预清理。</li>
<li>AbortablePreclean: 因为某些原因终止预清理。</li>
<li>FinalMarking: 由于marking阶段是和用户线程并发执行的，该过程中可能有用户线程修改某些活跃对象的字段，指向了一个非标记过的对象，在这个阶段需要重新标记出这些遗漏的对象，防止在下一阶段被清理掉，这个过程也是需要stw的。</li>
<li>Sweeping: 并发清理掉未标记的对象。</li>
<li>Resizing: 如果有需要，重新调整堆大小。</li>
<li>Resetting: 重置数据，为下一次的 cms gc 做准备。</li>
</ul>
<p>各阶段过程自行阅读：<a href="https://www.jianshu.com/p/2a1b2f17d3e4" target="_blank" rel="noopener">《图解CMS垃圾回收机制，你值得拥有》</a></p>
<h5 id="Foreground-Collect"><a href="#Foreground-Collect" class="headerlink" title="Foreground Collect"></a>Foreground Collect</h5><p>主动的GC则采用foreground方式。这里有两种情况，是否进行 Compact，如果需要就执行 MSC 算法(即Serial Old或Parallel Old)，否则就是 Foreground Collect</p>
<p>怎么理解上面那句话呢？往下看</p>
<p>主动GC开始时，需要判断本次GC是否要对老年代的空间进行Compact，即是否要压缩（因为长时间的周期性GC会造成大量的碎片空间）</p>
<p>判断逻辑实现如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">*should_compact &#x3D;</span><br><span class="line">    UseCMSCompactAtFullCollection &amp;&amp;</span><br><span class="line">    ((_full_gcs_since_conc_gc &gt;&#x3D; CMSFullGCsBeforeCompaction) ||</span><br><span class="line">     GCCause::is_user_requested_gc(gch-&gt;gc_cause()) ||</span><br><span class="line">     gch-&gt;incremental_collection_will_fail(true &#x2F;* consult_young *&#x2F;));</span><br></pre></td></tr></table></figure>

<p>在三种情况下会进行压缩:</p>
<ol>
<li><p>其中参数UseCMSCompactAtFullCollection(默认true)和 CMSFullGCsBeforeCompaction(默认0)，所以默认每次的主动GC都会对老年代的内存空间进行压缩，就是把对象移动到内存的最左边</p>
</li>
<li><p>执行了System.gc()，前提是没有参数ExplicitGCInvokesConcurrent，也会进行压缩</p>
</li>
<li><p>Young GC 过程中发生 promotion failure</p>
</li>
</ol>
<p>带压缩动作的算法，称为MSC，标记-清理-压缩，采用单线程，全暂停的方式进行垃圾收集，暂停时间很长很长</p>
<p>那不带压缩动作的算法是什么样的呢？</p>
<p>不带压缩动作的执行逻辑叫Foreground Collect，整个过程相对周期性Old GC来说，少了 Precleaning 和 AbortablePreclean 两个阶段，其它过程都差不多。但是它整个过程都是STW的</p>
<p>最后从源码可以看出，除了达到一定次数之外，如果用户调用了System.gc()以及发生了promotion failed，也会进行一次压缩。同时也可以看出，foreground不一定会采用压缩，所以那些说foreground就是mark swap compact（msc）的是不对的。</p>
<h5 id="MSC"><a href="#MSC" class="headerlink" title="MSC"></a>MSC</h5><p>MSC的全称是Mark Sweep Compact，即标记-清理-压缩，MSC是一种算法，请注意Compact，即它会压缩整理堆，这一点很重要。</p>
<p>MSC算法采用Serial Old或Parallel Old这些垃圾收集器，这些收集器采用标记-整理算法的GC方式</p>
<p>这是foreground CMS在特定情况下才会采用的一种垃圾回收算法。</p>
<p>这里讲一下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-XX:+UseCMSCompactAtFullCollection </span><br><span class="line">-XX:CMSFullGCsBeforeCompaction&#x3D;0</span><br></pre></td></tr></table></figure>

<p>配置-XX:+UseCMSCompactAtFullCollection（默认）前提下，如果CMSFullGCsBeforeCompaction=0，那么每次foreground CMS后都会采用MSC算法压缩堆内存；如果CMSFullGCsBeforeCompaction=3，那么每3次foreground CMS后才会有1次采用MSC算法压缩堆内存。</p>
<h4 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h4><h5 id="最终标记阶段停顿时间过长问题"><a href="#最终标记阶段停顿时间过长问题" class="headerlink" title="最终标记阶段停顿时间过长问题"></a>最终标记阶段停顿时间过长问题</h5><p>CMS的GC停顿时间约80%都在最终标记阶段(Final Remark)，若该阶段停顿时间过长，常见原因是新生代对老年代的无效引用，在上一阶段的并发可取消预清理阶段中，执行阈值时间内未完成循环，来不及触发Young GC，清理这些无效引用</p>
<p>通过添加参数：-XX:+CMSScavengeBeforeRemark。在执行最终操作之前先触发Young GC，从而减少新生代对老年代的无效引用，降低最终标记阶段的停顿，但如果在上个阶段(并发可取消的预清理)已触发Young GC，也会重复触发Young GC</p>
<h5 id="并发模式失败-concurrent-mode-failure-amp-晋升失败-promotion-failed-问题"><a href="#并发模式失败-concurrent-mode-failure-amp-晋升失败-promotion-failed-问题" class="headerlink" title="并发模式失败(concurrent mode failure) &amp; 晋升失败(promotion failed)问题"></a>并发模式失败(concurrent mode failure) &amp; 晋升失败(promotion failed)问题</h5><p>如果触发了主动Old GC，这时周期性Old GC正在执行，那么会夺过周期性Old GC的执行权（同一个时刻只能有一种在Old GC在运行），并记录 concurrent mode failure 或者 concurrent mode interrupted</p>
<p><img src="/images/blog/2019-11-06-1.png" alt></p>
<p><img src="/images/blog/2019-11-06-2.png" alt></p>
<p>这两个图看得特别费力，最后还是弄懂了。应该是下面这个理解</p>
<p><code>promotion failed</code>直接意思就是晋升到老年代，然而有时候会出现一些意外，导致对象晋升失败，这就是promotion failed。</p>
<p>什么情况下会发生晋升失败呢？从空间分配担保可以知道有两种情况:</p>
<ul>
<li>是基于历史统计数据，比如统计数据显示历史上平均每次晋升对象的大小是av_promo，而当前老年代的空间并不足以存放av_promo大小的对象，则GC线程认为实际晋升的过程中可能会发生失败，晚失败不如早点失败，于是GC线程告诉堆发生了一次GC失败事件便终止了这次回收动作。</li>
<li>是历史统计数据显示这次老年代有足够的空间来支持对象晋升，但是 Minor GC 后，幸存空间容纳不了剩余对象（超过动态年龄的对象），将剩余对象要放入老年代，老年代有碎片或者不能容纳这些对象，所以对象晋升失败，于是GC线程同样会告诉堆发生了一次垃圾回收失败事件</li>
</ul>
<p>发生 promotion failed 会有什么后果？就是紧接着触发concurrent mode failure，其意思为并发模式失败</p>
<p><code>concurrent mode failure</code>在执行 CMS GC 的过程中预留的内存空间不足以保存对象，这会导致 Concurrent Mode Failure 失败，这时会启用 Serial Old 收集器来重新进行老年代的收集。为什么CMS GC会有对象进入老年代？<br>因为存在可能业务线程会将大对象放入老年代，而此时老年代空间不足；或者 promotion failed 导致的对象晋升，而此时老年代空间不足或老年代空间足够，但存在大量碎片。 </p>
<p>当出现<code>concurrent mode failure</code>的现象时，就意味着此时JVM将继续采用 Stop-The-World 的方式来进行 Full GC，Full GC 会启用 Serial Old 收集器， Serial Old 收集器是单线程收集器，这样就会导致 STW 更长</p>
<p><code>concurrent mode interrupted</code> 是由于外部因素触发了 full gc，比如执行了System.gc()</p>
<p>并发模式失败和晋升失败都会导致长时间的停顿，常见解决思路如下</p>
<ul>
<li>降低触发CMS GC的阈值，即参数-XX:CMSInitiatingOccupancyFraction的值，让CMS GC尽早执行，以保证有足够的空间</li>
<li>增加CMS线程数，即参数-XX:ConcGCThreads，</li>
<li>增大老年代空间</li>
<li>让对象尽量在新生代回收，避免进入老年代</li>
</ul>
<h5 id="内存碎片问题"><a href="#内存碎片问题" class="headerlink" title="内存碎片问题"></a>内存碎片问题</h5><p>通常CMS的GC过程基于标记清除算法，不带压缩动作，导致越来越多的内存碎片需要压缩</p>
<p>可通过参数CMSFullGCsBeforeCompaction的值，设置多少次foreground CMS触发一次MSC</p>
<h4 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h4><p>周期性的CMS（background）只会回收老年代，除了周期性的进行GC之外，还有一些紧急情况，需要主动触发GC（foreground），主动触发的GC会连带一次Minor GC，所以也称为Full GC。主动触发的GC是会暂停所有用户现场的，俗称stop the world（STW）。</p>
<p>在那些没有实现CMS的老虚拟机或者没有开启CMS的虚拟机中，每一次Old GC都是Full GC，且会STW，当然因为除了CMS之外，其他的老年代回收算法都是采用标记-整理的方式，所以肯定也是压缩的，。</p>
<p>如果正在进行CMS回收，又触发了一次Full GC，则Full GC会抢占回收执行机会，停止CMS，采用Serial Old或Parallel Old这些采用标记-整理算法的GC方式（CMS采用的是标记-清理算法）进行Full GC</p>
<p>在 JDK 9 中 CMS GC 被废弃后，现有应用程序的最佳处理方法是什么？感兴趣可以参阅<a href="https://mp.weixin.qq.com/s/i-MDolxTNGDWRB9Z_pq8rg" target="_blank" rel="noopener">CMS 被废弃了，该怎么办呢？</a></p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.jianshu.com/p/2a1b2f17d3e4" target="_blank" rel="noopener">图解CMS垃圾回收机制，你值得拥有</a></li>
<li><a href="https://www.cnblogs.com/JMLiu/p/8482437.html" target="_blank" rel="noopener">触发Full GC的时机</a></li>
<li><a href="https://www.jianshu.com/p/be5389ca93f7" target="_blank" rel="noopener">CMS几种GC模式解读-感谢你假笨的指正</a></li>
<li><a href="https://www.jianshu.com/p/55670407fdb9" target="_blank" rel="noopener">关于CMS垃圾收集算法的一些疑惑</a></li>
<li><a href="https://www.cnblogs.com/ggjucheng/p/3977612.html" target="_blank" rel="noopener">JVM GC算法 CMS 详解(转)</a></li>
<li><a href="https://juejin.im/post/5b6b986c6fb9a04fd1603f4a" target="_blank" rel="noopener">老大难的GC原理及调优，这下全说清楚了</a></li>
<li><a href="https://www.cnblogs.com/hongdada/p/10277782.html" target="_blank" rel="noopener">CMS GC启动参数优化配置</a></li>
<li><a href="https://toutiao.io/posts/3ogeka/preview" target="_blank" rel="noopener">聊一聊 GC 中的 promotion failed</a></li>
<li><a href="https://my.oschina.net/hosee/blog/674181" target="_blank" rel="noopener">使用CMS垃圾收集器产生的问题和解决方案</a></li>
<li><a href="https://mp.weixin.qq.com/s/PE1N_tb99gA3hoPe8vuZvw" target="_blank" rel="noopener">看完这篇垃圾回收，你也能做面试官</a></li>
</ul>
]]></content>
      <categories>
        <category>JVM</category>
      </categories>
  </entry>
  <entry>
    <title>HDFS 文件读写流程</title>
    <url>/2019/10/24/HDFS-RW/</url>
    <content><![CDATA[<p>HDFS 文件读写流程 </p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>HDFS 读写流程都搞不明白还混个球啊</p>
<h4 id="读文件的流程"><a href="#读文件的流程" class="headerlink" title="读文件的流程"></a>读文件的流程</h4><p><img src="/images/blog/2019-10-24-3.png" alt></p>
<p>如图所示，读文件的流程主要包括以下6个步骤：</p>
<ol>
<li>打开分布式文件：调用分布式文件 DistributedFileSystem.open( ) 方法；</li>
<li>寻址请求：从 NameNode 处得到 DataNode 的地址，DistributedFileSystem使用 RPC 方式调用了NameNode，NameNode 返回存有该副本的DataNode 地址，DistributedFileSystem 返回了一个输入流对象（FSDataInputStream），该对象封装了输入流 DFSInputStream；</li>
<li>连接到DataNode：调用输入流 FSDataInputStream.read( ) 方法从而让DFSInputStream 连接到 DataNodes；</li>
<li>从 DataNode 中获取数据：通过循环调用 read( ) 方法，从而将数据从 DataNode 传输到客户端；</li>
<li>读取另外的 DataNode 直到完成：到达块的末端时候，输入流 DFSInputStream 关闭与 DataNode 连接， 寻找下一个 DataNode；</li>
<li>完成读取，关闭连接：即调用输入流 FSDataInputStream.close( )；</li>
</ol>
<h4 id="写文件流程"><a href="#写文件流程" class="headerlink" title="写文件流程"></a>写文件流程</h4><p><img src="/images/blog/2019-10-24-4.png" alt></p>
<ol>
<li>发送创建文件请求：调用分布式文件系统 DistributedFileSystem.create( )方法；</li>
<li>NameNode 创建文件记录：分布式文件系统 DistributedFileSystem 发送 RPC 请求给 NameNode，NameNode 检查权限后创建一条记录，DistributedFileSystem 返回输出流 FSDataOutputStream，封装了输出流 DFSOutputDtream；</li>
<li>客户端写入数据：输出流 DFSOutputDtream 将数据分成一个个的数据包，并写入内部队列。DataStreamer 根据 DataNode 列表来要求 NameNode 分配适合的新块来存储数据备份。 一组 DataNode 构成管线(管线的 DataNode 之间使用 Socket 流式通信)；</li>
<li>使用管线传输数据：DataStreamer 将数据包流式传输到管线第一个DataNode，第一个 DataNode 再传到第二个DataNode，直到完成；</li>
<li>确认队列：DataNode 收到数据后发送确认，管线的 DataNode 所有的确认组成一个确认队列。所有 DataNode 都确认，管线数据包删除；</li>
<li>关闭：客户端对数据量调用 close( ) 方法。将剩余所有数据写入DataNode管线，联系NameNode并且发送文件写入完成信息之前等待确认；</li>
<li>NameNode确认</li>
</ol>
<p>故障处理：若过程中发生故障，则先关闭管线，将ackQueue中的packet添加回dataQueue，确保数据包不漏。从管线中移除故障 DataNode，从 NameNode 中重新申请 DataNode 与原有的没有发生故障的 DataNode 组成新的管线。<br>重建管线时会将已经发送成功的packet从之前正常的 DataNode 上发送到新增加的 DataNode 上。接着生成新的 stamp 标识，并将该标识传送给 NameNode，<br>然后客户端跟管线构建连接后使用 stamp 标识更新 block，这样发生故障的 DataNode 上的错误 block 会在节点恢复正常后被 NameNode 删除。</p>
<p>在数据的读取过程中难免碰到网络故障，脏数据，DataNode 失效等问题，这些问题 HDFS 在设计的时候都早已考虑到了。下面来介绍一下数据损坏处理流程</p>
<ul>
<li>当 DataNode 读取 block 的时候，它会计算 checksum。</li>
<li>如果计算后的 checksum，与 block 创建时值不一样，说明该 block 已经损坏。</li>
<li>Client 读取其它 DataNode上的 block。</li>
<li>NameNode 标记该块已经损坏，然后复制 block 达到预期设置的文件备份数 。</li>
<li>DataNode 在其文件创建后验证其 checksum。</li>
</ul>
<p><em>写过程中的三层buffer</em></p>
<p>写过程中会以chunk、packet及packet queue三个粒度做三层缓存；</p>
<p>首先，当数据流入DFSOutputStream时，DFSOutputStream内会有一个chunk大小的buf，当数据写满这个buf（或遇到强制flush），会计算checksum值，然后填塞进packet；<br>当一个chunk填塞进入packet后，仍然不会立即发送，而是累积到一个packet填满后，将这个packet放入dataqueue队列；<br>进入dataqueue队列的packet会被另一线程按序取出发送到datanode；（注：生产者消费者模型，阻塞生产者的条件是dataqueue与ackqueue之和超过一个block的packet上限）</p>
<p><img src="/images/blog/2019-10-24-5.png" alt></p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/35934452" target="_blank" rel="noopener">HDFS-文件读写流程</a></li>
<li><a href="http://bigdatadecode.club/HDFS%20write%E8%A7%A3%E6%9E%90.html" target="_blank" rel="noopener">HDFS write解析</a></li>
</ul>
]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title>Java内存分析工具 jmap</title>
    <url>/2019/10/24/jmap/</url>
    <content><![CDATA[<p>使用 jmap 分析JVM内存状态</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><h4 id="heap"><a href="#heap" class="headerlink" title="heap"></a>heap</h4><p>查看整个JVM内存状态 </p>
<p>打印 heap 的概要信息，GC使用的算法，heap 的配置和使用情况，可以用此来判断内存目前的使用情况以及垃圾回收情况。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; jmap -heap 1234</span><br><span class="line"></span><br><span class="line">Attaching to process ID 13497, please wait...</span><br><span class="line">Debugger attached successfully.</span><br><span class="line">Server compiler detected.</span><br><span class="line">JVM version is 25.201-b09</span><br><span class="line"></span><br><span class="line">using thread-local object allocation.</span><br><span class="line">Parallel GC with 2 thread(s)</span><br><span class="line"></span><br><span class="line">Heap Configuration:</span><br><span class="line">   MinHeapFreeRatio         &#x3D; 0</span><br><span class="line">   MaxHeapFreeRatio         &#x3D; 100</span><br><span class="line">   MaxHeapSize              &#x3D; 1048576000 (1000.0MB)</span><br><span class="line">   NewSize                  &#x3D; 87031808 (83.0MB)</span><br><span class="line">   MaxNewSize               &#x3D; 349175808 (333.0MB)</span><br><span class="line">   OldSize                  &#x3D; 175112192 (167.0MB)</span><br><span class="line">   NewRatio                 &#x3D; 2</span><br><span class="line">   SurvivorRatio            &#x3D; 8</span><br><span class="line">   MetaspaceSize            &#x3D; 21807104 (20.796875MB)</span><br><span class="line">   CompressedClassSpaceSize &#x3D; 1073741824 (1024.0MB)</span><br><span class="line">   MaxMetaspaceSize         &#x3D; 17592186044415 MB</span><br><span class="line">   G1HeapRegionSize         &#x3D; 0 (0.0MB)</span><br><span class="line"></span><br><span class="line">Heap Usage:</span><br><span class="line">PS Young Generation</span><br><span class="line">Eden Space:</span><br><span class="line">   capacity &#x3D; 35127296 (33.5MB)</span><br><span class="line">   used     &#x3D; 31401816 (29.947105407714844MB)</span><br><span class="line">   free     &#x3D; 3725480 (3.5528945922851562MB)</span><br><span class="line">   89.39434450064132% used</span><br><span class="line">From Space:</span><br><span class="line">   capacity &#x3D; 524288 (0.5MB)</span><br><span class="line">   used     &#x3D; 131072 (0.125MB)</span><br><span class="line">   free     &#x3D; 393216 (0.375MB)</span><br><span class="line">   25.0% used</span><br><span class="line">To Space:</span><br><span class="line">   capacity &#x3D; 524288 (0.5MB)</span><br><span class="line">   used     &#x3D; 0 (0.0MB)</span><br><span class="line">   free     &#x3D; 524288 (0.5MB)</span><br><span class="line">   0.0% used</span><br><span class="line">PS Old Generation</span><br><span class="line">   capacity &#x3D; 92798976 (88.5MB)</span><br><span class="line">   used     &#x3D; 11140584 (10.624488830566406MB)</span><br><span class="line">   free     &#x3D; 81658392 (77.8755111694336MB)</span><br><span class="line">   12.005072124933791% used</span><br><span class="line"></span><br><span class="line">12155 interned Strings occupying 1063376 bytes.</span><br></pre></td></tr></table></figure>

<h4 id="histo"><a href="#histo" class="headerlink" title="histo"></a>histo</h4><p>查看JVM堆中对象详细占用情况</p>
<p>打印堆的对象统计，包括对象数、内存大小等。jmap -histo:live 这个命令执行，JVM会先触发gc，然后再统计信息。</p>
<p>列对应信息为编号id、实例个数、所有实例大小、类名</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; jmap -histo 783</span><br><span class="line"></span><br><span class="line"> num     #instances         #bytes  class name</span><br><span class="line">----------------------------------------------</span><br><span class="line">   1:        286511        9516456  [Ljava.lang.Object;</span><br><span class="line">   2:        113692        9458248  [C</span><br><span class="line">   3:         31510        6249248  [B</span><br><span class="line">   4:        132529        4240928  java.io.ObjectStreamClass$WeakClassKey</span><br><span class="line">   5:         95868        3834720  java.util.TreeMap$Entry</span><br><span class="line">   6:         14097        3740048  [I</span><br><span class="line">   7:         60312        1447488  java.lang.String</span><br><span class="line">   8:         25273        1213104  java.util.HashMap</span><br><span class="line">   9:         46425        1114200  java.lang.Long</span><br><span class="line">  10:         10056        1045824  java.io.ObjectStreamClass</span><br><span class="line">  11:         10429         500592  java.util.TreeMap</span><br><span class="line">  12:          7332         470824  [Ljava.util.Hashtable$Entry;</span><br><span class="line">  13:         14637         468384  java.util.Vector</span><br><span class="line">  14:          4609         442464  java.lang.management.ThreadInfo</span><br><span class="line">  15:         12704         406528  java.util.TreeMap$KeyIterator</span><br><span class="line">  16:         15719         377256  java.io.SerialCallbackContext</span><br><span class="line">  17:          7325         351600  java.util.Hashtable</span><br></pre></td></tr></table></figure>

<h4 id="dump"><a href="#dump" class="headerlink" title="dump"></a>dump</h4><p>dump堆到文件，format指定输出格式，live指明是活着的对象，file指定文件名</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; jmap -dump:live,format&#x3D;b,file&#x3D;&#x2F;lihm.hprof 129665</span><br></pre></td></tr></table></figure>

<blockquote>
<p>这个命令执行，JVM会将整个heap的信息dump写入到一个文件，heap如果比较大的话，就会导致这个过程比较耗时，并且执行的过程中为了保证dump的信息是可靠的，所以会暂停应用， 线上系统慎用。</p>
</blockquote>
<p>lihm.hprof 为二进制文件，<code>less</code> 命令查看不了，得用分析工具分析，比如jhat、MAT</p>
<p>jhat 查看</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; jhat -J-Xmx1024M lihm.hprof</span><br><span class="line">Reading from lihm.txt...</span><br><span class="line">Dump file created Thu Oct 24 16:26:47 CST 2019</span><br><span class="line">Snapshot read, resolving...</span><br><span class="line">Resolving 104381 objects...</span><br><span class="line">Chasing references, expect 20 dots....................</span><br><span class="line">Eliminating duplicate references....................</span><br><span class="line">Snapshot resolved.</span><br><span class="line">Started HTTP server on port 7000</span><br><span class="line">Server is ready.</span><br></pre></td></tr></table></figure>

<p>访问7000端口, 可以统计实例总数、占用大小</p>
<p><img src="/images/blog/2019-10-24-1.png" alt></p>
<p><img src="/images/blog/2019-10-24-2.png" alt></p>
<p>MAT工具的下载和安装</p>
<blockquote>
<p>MAT(Memory Analyzer Tool)工具是eclipse的一个插件(MAT也可以单独使用)，使用起来非常方便，尤其是在分析大内存的dump文件时，可以非常直观的看到各个对象在堆空间中所占用的内存大小、类实例数量、对象引用关系、利用OQL对象查询，以及可以很方便的找出对象GC Roots的相关信息，当然最吸引人的还是能够快速为开发人员生成内存泄露报表，方便定位问题和分析问题。</p>
</blockquote>
<p>MAT工具的<a href="http://www.eclipse.org/mat/downloads.php" target="_blank" rel="noopener">下载地址</a>, 具体参考<a href="https://blog.csdn.net/Jin_Kwok/article/details/80326088" target="_blank" rel="noopener">Java内存分析工具MAT(Memory Analyzer Tool)安装使用实例</a></p>
<h4 id="F"><a href="#F" class="headerlink" title="-F"></a>-F</h4><p>强制模式。如果指定的pid没有响应，请使用jmap -dump或jmap -histo选项。此模式下，不支持live子选项。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.cnblogs.com/lujiango/p/9002270.html" target="_blank" rel="noopener">Java内存分析工具jmap</a></li>
<li><a href="https://blog.csdn.net/xidiancoder/article/details/70948569" target="_blank" rel="noopener">使用jmap dump 分析JVM内存状态</a></li>
<li><a href="https://www.cnblogs.com/luao/p/10622220.html" target="_blank" rel="noopener">通过jstack与jmap分析一次线上故障</a></li>
<li><a href="https://www.jianshu.com/p/067216f780e4" target="_blank" rel="noopener">jmap - JVM堆内存分析</a></li>
<li><a href="https://blog.csdn.net/Jin_Kwok/article/details/80326088" target="_blank" rel="noopener">Java内存分析工具MAT(Memory Analyzer Tool)安装使用实例</a></li>
</ul>
]]></content>
      <categories>
        <category>JVM</category>
      </categories>
  </entry>
  <entry>
    <title>双向链表的插入及删除</title>
    <url>/2019/10/06/Linked-List/</url>
    <content><![CDATA[<p>双向链表插入、删除操作</p>
<hr>
<h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><p>单向链表中每个结点增加一个指向其前驱的指针域prior。这样形成的链表中就有两条方向不同的链，可称之为双（向）链表（DoubleLinked List）。</p>
<p><img src="/images/blog/2019-10-06-1.png" alt></p>
<h4 id="插入"><a href="#插入" class="headerlink" title="插入"></a>插入</h4><p><img src="/images/blog/2019-10-06-2.png" alt></p>
<p>第一步：首先找到插入位置，节点 s 将插入到节点 p 之前<br>第二步：将节点 s 的前驱指向节点 p 的前驱，即 s-&gt;prior = p-&gt;prior;<br>第三步：将节点 p 的前驱的后继指向节点 s 即 p-&gt;prior-&gt;next = s;<br>第四步：将节点 s 的后继指向节点 p 即 s-&gt;next = p;<br>第五步：将节点 p 的前驱指向节点 s 即 p-&gt;prior = s;  </p>
<h4 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h4><p><img src="/images/blog/2019-10-06-3.png" alt></p>
<p>第一步：找到即将被删除的节点 p<br>第二步：将 p 的前驱的后继指向 p 的后继，即 p-&gt;prior-&gt;next = p-&gt;next;<br>第三步：将 p 的后继的前驱指向 p 的前驱，即 p-&gt;next-&gt;prior = p-&gt;prior;<br>第四步：删除节点 p 即 delete p;  </p>
<p><strong>删除如果碰到尾删除怎么办？</strong></p>
<p>在第三步时加个 p-&gt;next 是否为空的判断。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">转载出处： &lt;https:&#x2F;&#x2F;blog.csdn.net&#x2F;lisayh&#x2F;article&#x2F;details&#x2F;79216796&gt;</span><br></pre></td></tr></table></figure>

<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/lisayh/article/details/79216796" target="_blank" rel="noopener">双向链表的插入及删除图解</a></li>
</ul>
]]></content>
      <categories>
        <category>DataStructure</category>
      </categories>
  </entry>
  <entry>
    <title>Java GC 原理</title>
    <url>/2019/09/29/Java-GC/</url>
    <content><![CDATA[<p>介绍 GC 基础原理和理论</p>
<hr>
<h4 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h4><p>大多数情况下对 Java 程序进行 GC 调优, 主要关注两个目标: 响应速度、吞吐量</p>
<ul>
<li><p><strong>响应速度(Responsiveness)</strong> 响应速度指程序或系统对一个请求的响应有多迅速。比如，用户订单查询响应时间，对响应速度要求很高的系统，较大的停顿时间是不可接受的。调优的重点是在短的时间内快速响应</p>
</li>
<li><p><strong>吞吐量(Throughput)</strong> 吞吐量关注在一个特定时间段内应用系统的最大工作量，例如每小时批处理系统能完成的任务数量，在吞吐量方面优化的系统，较长的GC停顿时间也是可以接受的，因为高吞吐量应用更关心的是如何尽可能快地完成整个任务，不考虑快速响应用户请求</p>
</li>
</ul>
<p>GC调优中，GC导致的应用暂停时间影响系统响应速度，GC处理线程的CPU使用率影响系统吞吐量</p>
<h4 id="GC-垃圾回收算法"><a href="#GC-垃圾回收算法" class="headerlink" title="GC 垃圾回收算法"></a>GC 垃圾回收算法</h4><h5 id="标记-清除算法-Mark-Sweep"><a href="#标记-清除算法-Mark-Sweep" class="headerlink" title="标记-清除算法(Mark-Sweep)"></a>标记-清除算法(Mark-Sweep)</h5><p>最基础的垃圾回收算法，之所以说它是最基础的是因为它最容易实现，思想也是最简单的。标记-清除算法分为两个阶段：标记阶段和清除阶段。标记阶段的任务是标记出所有需要被回收的对象，清除阶段就是回收被标记的对象所占用的空间。</p>
<p>标记-清除算法采用从根集合（GC Roots）进行扫描，对存活的对象进行标记，标记完毕后，再扫描整个空间中未被标记的对象，进行回收，如下图所示。标记-清除算法不需要进行对象的移动，只需对不存活的对象进行处理，在存活对象比较多的情况下极为高效，但由于标记-清除算法直接回收不存活的对象，因此会造成内存碎片。</p>
<h5 id="复制算法-Copying"><a href="#复制算法-Copying" class="headerlink" title="复制算法(Copying)"></a>复制算法(Copying)</h5><p>为了解决Mark-Sweep算法的缺陷，Copying算法就被提了出来。它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用的内存空间一次清理掉，这样一来就不容易出现内存碎片的问题。</p>
<p>这种算法虽然实现简单，运行高效且不容易产生内存碎片，但是却对内存空间的使用做出了高昂的代价，因为能够使用的内存缩减到原来的一半。</p>
<p>Copying算法的效率跟存活对象的数目多少有很大的关系，如果存活对象很多，那么Copying算法的效率将会大大降低。</p>
<h5 id="标记-整理算法-Mark-compact"><a href="#标记-整理算法-Mark-compact" class="headerlink" title="标记-整理算法(Mark-compact)"></a>标记-整理算法(Mark-compact)</h5><p>标记-整理算法采用标记-清除算法一样的方式进行对象的标记，但在清除时不同，在回收不存活的对象占用的空间后，会将所有的存活对象往左端空闲空间移动，并更新对应的指针<strong>。标记-整理算法是在标记-清除算法的基础上，又进行了对象的移动</strong>，因此成本更高，但是却解决了内存碎片的问题。</p>
<h5 id="分代收集算法-Generational-Collection（分代收集）算法"><a href="#分代收集算法-Generational-Collection（分代收集）算法" class="headerlink" title="分代收集算法 Generational Collection（分代收集）算法"></a>分代收集算法 Generational Collection（分代收集）算法</h5><p>分代收集算法就是目前虚拟机使用的回收算法，它解决了标记整理不适用于老年代的问题，将内存分为各个年代，在不同年代使用不同的算法，从而使用最合适的算法，新生代存活率低，可以使用复制算法。而老年代对象存活率高，没有额外空间对它进行分配担保，所以使用标记整理算法。具体请看下面介绍</p>
<h4 id="GC-分代收集算法"><a href="#GC-分代收集算法" class="headerlink" title="GC 分代收集算法"></a>GC 分代收集算法</h4><p>将 Java 的堆内存逻辑上分成两块: 新生代、老年代，针对不同存活周期、不同大小的对象采取不同的垃圾回收策略</p>
<p>“分代回收”是基于这样一个事实: 对象的生命周期不同，所以针对不同生命周期的对象可以采取不同的回收方式，以便提高回收效率。</p>
<p><img src="/images/blog/2019-09-29-1.png" alt></p>
<p><strong>新生代（Young Generation）</strong></p>
<p>新生代又叫年轻代，大多数对象在新生代中被创建，很多对象的生命周期很短。每次新生代的垃圾回收（又称Young GC、Minor GC、YGC）后只有少量对象存活，所以使用复制算法，只需少量的复制操作成本就可以完成回收</p>
<p>新生代内又分三个区：一个Eden区，两个Survivor区(S0、S1，又称From Survivor、To Survivor)，大部分对象在Eden区中生成。   </p>
<p>两个Surviver区分别称为“From”区和“To”区。对象在Eden区创建，经过一次Yong GC后，还存活的对象将会被复制到Surviver区的“From”区，此时“To”区是空的。到了下一次GC的时候，Eden区还存活的对象会直接移动到Surviver区的“To”区，而“Form”区的对象有两个去处，“From”区的对象会根据经过的GC次数计算年龄，如果年龄到达了阈值（默认15），则会被移动到老年代中，否则就复制到“To”区，此时“From”区变成了空的，然后“From”区和“To”区进行角色互换，到下一次进行GC时，还是有一块空的“To”区，用来存放从eden区和“From”区移动过来的对象。</p>
<p><strong>老年代（Old Generation）</strong></p>
<p>在新生代中经历了N次垃圾回收后仍然存活的对象，就会被放到老年代，该区域中对象存活率高。老年代的垃圾回收通常使用“标记-整理”算法</p>
<h4 id="GC-事件分类"><a href="#GC-事件分类" class="headerlink" title="GC 事件分类"></a>GC 事件分类</h4><p>根据垃圾收集回收的区域不同，垃圾收集主要通常分为Young GC、Old GC、Full GC、Mixed GC</p>
<p><strong>Young GC</strong></p>
<p>新生代内存的垃圾收集事件称为Young GC(又称Minor GC)，当JVM无法为新对象分配在新生代内存空间时总会触发 Young GC，比如 Eden 区占满时。新对象分配频率越高, Young GC 的频率就越高<br>Young GC 每次都会引起全线停顿(Stop-The-World)，暂停所有的应用线程，停顿时间相对老年代GC的造成的停顿，几乎可以忽略不计</p>
<p><strong>Old GC 、Full GC、Mixed GC</strong></p>
<p>Old GC，只清理老年代空间的GC事件，只有CMS的并发收集是这个模式</p>
<p>Full GC，清理整个堆的GC事件，包括新生代、老年代、元空间等</p>
<p>Mixed GC，清理整个新生代以及部分老年代的GC，只有G1有这个模式</p>
<h4 id="GC日志分析"><a href="#GC日志分析" class="headerlink" title="GC日志分析"></a>GC日志分析</h4><p>GC日志是一个很重要的工具，它准确记录了每一次的GC的执行时间和执行结果，通过分析GC日志可以调优堆设置和GC设置，或者改进应用程序的对象分配模式</p>
<p>可以查阅<a href="https://lihuimintu.github.io/2019/06/24/GC-Log-Detailed/" target="_blank" rel="noopener">GC 日志详解</a></p>
<h4 id="内存分配策略"><a href="#内存分配策略" class="headerlink" title="内存分配策略"></a>内存分配策略</h4><p>Java提供的自动内存管理，可以归结为解决了对象的内存分配和回收的问题，前面已经介绍了内存回收，下面介绍几条最普遍的内存分配策略</p>
<p><img src="/images/blog/2020-02-10-1.png" alt></p>
<ul>
<li><p><strong>对象优先在Eden区分配</strong><br>大多数情况下，对象在先新生代Eden区中分配。当Eden区没有足够空间进行分配时，虚拟机将发起一次Young GC</p>
</li>
<li><p><strong>大对象之间进入老年代</strong><br>JVM提供了一个对象大小阈值参数(-XX:PretenureSizeThreshold，默认值为0，代表不管多大都是先在Eden中分配内存)，大于参数设置的阈值值的对象直接在老年代分配，这样可以避免对象在Eden及两个Survivor直接发生大内存复制</p>
</li>
<li><p><strong>长期存活的对象将进入老年代</strong><br>对象每经历一次垃圾回收，且没被回收掉，它的年龄就增加1，大于年龄阈值参数(-XX:MaxTenuringThreshold，默认15)的对象，将晋升到老年代中</p>
</li>
<li><p><strong>空间分配担保</strong><br>当进行Young GC之前，JVM需要预估老年代是否能够容纳Young GC后新生代晋升到老年代的存活对象，以确定是否需要提前触发GC回收老年代空间，基于空间分配担保策略来计算</p>
</li>
</ul>
<p>continueSize: 老年代最大可用连续空间</p>
<p><img src="/images/blog/2019-09-29-4.png" alt></p>
<blockquote>
<p>在发生 MinorGC 之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象的总空间，如果大于，那么Minor GC 可以确保是安全的，如果不大于，<br>那么会继续检查老年代最大可用连续空间是否大于历次晋升到老年代对象的平均大小，如果大于则进行 Young GC，否则可能进行一次 Full GC。<br>Young GC后晋升对象能被老年代容纳下，则担保成功，否则担保失败，出现”promotion failed”错误，需要 Full GC</p>
</blockquote>
<p>动态年龄判定:<br>新生代对象的年龄可能没达到阈值(MaxTenuringThreshold参数指定)就晋升老年代，如果Young GC之后，新生代存活对象年龄从小到大的累加和总和大于任一Survivor空间的一半，<strong>此时S0或者S1区即将容纳不了存活的新生代对象</strong>，年龄大于或等于该年龄的对象就可以直接进入老年代，无须等到MaxTenuringThreshold中要求的年龄</p>
<h4 id="Safe-Point"><a href="#Safe-Point" class="headerlink" title="Safe Point"></a>Safe Point</h4><p>先提一下STW(Stop The World), STW 即在 GC（Minor GC 或 Full GC）期间，只有垃圾回收器线程在工作，其他工作线程则被挂起，这能造成挺大的性能开销。</p>
<p><img src="/images/blog/2020-02-10-2.png" alt></p>
<p>由于 Full GC（或Minor GC） 会影响性能，所以我们要在一个合适的时间点发起 GC，这个时间点被称为 <strong>Safe Point</strong>，这个时间点的选定既不能太少以让 GC 时间太长导致程序过长时间卡顿，也不能过于频繁以至于过分增大运行时的负荷。</p>
<p>一般当线程在这个时间点上状态是可以确定的，如确定 GC Root 的信息等，可以使 JVM 开始安全地 GC。Safe Point 主要指的是以下特定位置：</p>
<ul>
<li>循环的末尾</li>
<li>方法返回前</li>
<li>调用方法的 call 之后</li>
<li>抛出异常的位置</li>
</ul>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/u014493323/article/details/82921740" target="_blank" rel="noopener">jvm误区–动态对象年龄判定</a></li>
<li><a href="https://juejin.im/post/5b6b986c6fb9a04fd1603f4a?utm_source=gold_browser_extension" target="_blank" rel="noopener">老大难的GC原理及调优，这下全说清楚了</a></li>
<li><a href="https://www.jianshu.com/p/90d5ea71b330" target="_blank" rel="noopener">jvm 对象分配及回收</a></li>
<li><a href="https://mp.weixin.qq.com/s/PE1N_tb99gA3hoPe8vuZvw" target="_blank" rel="noopener">看完这篇垃圾回收，你也能做面试官</a></li>
</ul>
]]></content>
      <categories>
        <category>JVM</category>
      </categories>
  </entry>
  <entry>
    <title>HDFS checkpoint</title>
    <url>/2019/09/20/HDFS-HA-checkpoint/</url>
    <content><![CDATA[<p>Hadoop 2.0 HA 的 checkpoint 过程</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>HDFS 将文件系统的元数据信息存放在 fsimage 和一系列的 edits 文件中。</p>
<p>在启动 HDFS 集群时，系统会先加载 fsimage，然后逐个执行所有Edits文件中的每一条操作，来获取完整的文件系统元数据。</p>
<h4 id="文件"><a href="#文件" class="headerlink" title="文件"></a>文件</h4><p>HDFS 的存储元数据是由 fsimage 和 edits 文件组成。fsimage 存放上次 checkpoint 生成的文件系统元数据，edits 存放文件系统操作日志。checkpoint的过程，就是合并 fsimage 和 edits 文件，然后生成最新的 fsimage 的过程。</p>
<p>fsimage文件: fsimage 里保存的是 HDFS 文件系统的元数据信息。每次 checkpoint 的时候生成一个新的 fsimage 文件，fsimage 文件同步保存在 active namenode 上和 standby namenode 上。是在 standby namenode 上生成并上传到 active namenode 上的。</p>
<p>edits文件: active namenode 会及时把 HDFS 的修改信息（创建，修改，删除等）写入到本地目录，和 journalnode 上的 edits 文件，每一个操作以一条数据的形式存放。edits文件默认每2分钟产生一个。正在写入的Edits文件以 edits_inprogress_* 格式存在。</p>
<p><img src="/images/blog/2019-09-20-1.png" alt></p>
<h4 id="checkpoint-过程"><a href="#checkpoint-过程" class="headerlink" title="checkpoint 过程"></a>checkpoint 过程</h4><p>开启HA的HDFS，有 active 和 standby namenode 两个 namenode 节点。他们的内存中保存了一样的集群元数据信息。</p>
<p>因为 standby namenode 已经将集群状态存储在内存中了，所以创建检查点checkpoint的过程只需要从内存中生成新的fsimage。</p>
<p>这里 standby namenode 称为SbNN，active namenode 称为ANN</p>
<p><img src="/images/blog/2020-02-18-1.png" alt></p>
<ol>
<li><p>SbNN查看是否满足创建检查点的条件</p>
<ul>
<li>距离上次checkpoint的时间间隔 &gt;= ${dfs.namenode.checkpoint.period}</li>
<li>edits中的事务条数达到 ${dfs.namenode.checkpoint.txns} 限制</li>
</ul>
</li>
<li><p>SbNN将内存中当前的状态保存成一个新的文件，命名为fsimage.ckpt_txid。其中txid是最后一个edit中的最后一条事务的ID（transaction ID）。然后为该fsimage文件创建一个MD5文件，并将fsimage文件重命名为fsimage_txid。</p>
</li>
<li><p>SbNN向ANN发送一条HTTP GET请求。请求中包含了SbNN的域名，端口以及新fsimage的txid。</p>
</li>
<li><p>ANN收到请求后，用获取到的信息反过来向SbNN再发送一条HTTP GET请求，获取新的fsimage文件。这个新的fsimage文件传输到ANN上后，也是先命名为fsimage.ckpt_txid，并为它创建一个MD5文件。然后再改名为fsimage_txid。fsimage过程完成。</p>
</li>
</ol>
<h4 id="checkpoint-相关配置"><a href="#checkpoint-相关配置" class="headerlink" title="checkpoint 相关配置"></a>checkpoint 相关配置</h4><p><strong>dfs.namenode.checkpoint.period</strong></p>
<p>两次检查点创建之间的固定时间间隔，默认3600，即1小时</p>
<p><strong>dfs.namenode.checkpoint.txns</strong></p>
<p>未检查的事务数量。若没检查事务数达到这个值，也触发一次checkpoint，1,000,000</p>
<p><strong>dfs.namenode.checkpoint.check.period</strong></p>
<p>standby namenode检查是否满足建立checkpoint的条件的检查周期。默认60，即每1min检查一次</p>
<p><strong>dfs.namenode.num.checkpoints.retained</strong></p>
<p>在namenode上保存的fsimage的数目，超出的会被删除。默认保存2个</p>
<p><strong>dfs.namenode.num.checkpoints.retained</strong></p>
<p>最多能保存的edits文件个数，默认为1,000,000. 官方解释是为防止standby namenode宕机导致edits文件堆积的情况，设置的限制</p>
<p><strong>dfs.ha.tail-edits.period</strong></p>
<p>standby namenode每隔多长时间去检测新的Edits文件。默认60s，只检测完成了的Edits， 不检测inprogress的文件。(不是很明白)</p>
<h4 id="手动-checkpoint"><a href="#手动-checkpoint" class="headerlink" title="手动 checkpoint"></a>手动 checkpoint</h4><p>首先，checkpoint 之前要先进入安全模式。进入安全模式后，执行 saveNamespace 命令，他会把 a-nn 的 fsimage 与 大于 fsimage txid 的 editlog（包括 finalized 与 in_progress 的）合并成新的 fsimage 并落盘，然后新生成一个 editlog。</p>
<p>执行以下命令手动保存一次 HDFS 的检查点</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@ip-172-31-4-109 ~]# sudo -u hdfs hdfs dfsadmin -safemode enter</span><br><span class="line">Safe mode is ON in ip-172-31-4-109.ap-southeast-1.compute.internal&#x2F;172.31.4.109:8020</span><br><span class="line">Safe mode is ON in ip-172-31-1-163.ap-southeast-1.compute.internal&#x2F;172.31.1.163:8020</span><br><span class="line">[root@ip-172-31-4-109 ~]# sudo -u hdfs hdfs dfsadmin -saveNamespace</span><br><span class="line">Save namespace successful for ip-172-31-4-109.ap-southeast-1.compute.internal&#x2F;172.31.4.109:8020</span><br><span class="line">Save namespace successful for ip-172-31-1-163.ap-southeast-1.compute.internal&#x2F;172.31.1.163:8020</span><br><span class="line">[root@ip-172-31-4-109 ~]# sudo -u hdfs hdfs dfsadmin -safemode leave</span><br><span class="line">Safe mode is OFF in ip-172-31-4-109.ap-southeast-1.compute.internal&#x2F;172.31.4.109:8020</span><br><span class="line">Safe mode is OFF in ip-172-31-1-163.ap-southeast-1.compute.internal&#x2F;172.31.1.163:8020</span><br><span class="line">[root@ip-172-31-4-109 ~]#</span><br></pre></td></tr></table></figure>

<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.cloudera.com/a-guide-to-checkpointing-in-hadoop/" target="_blank" rel="noopener">A Guide to Checkpointing in Hadoop</a></li>
<li><a href="https://blog.csdn.net/Amber_amber/article/details/47003589" target="_blank" rel="noopener">Hadoop2.0 HA的checkpoint过程</a></li>
<li><a href="https://www.cnblogs.com/jiangxiaoxian/p/9719117.html" target="_blank" rel="noopener">hdfs standby namenode checkpoint 的一些参数</a></li>
<li><a href="https://cloud.tencent.com/developer/article/1378671" target="_blank" rel="noopener">0482-HDFS上一次检查点异常分析</a></li>
<li><a href="https://blog.csdn.net/qq_41733481/article/details/103767518" target="_blank" rel="noopener">手动触发Hadoop checkpoint</a></li>
</ul>
]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title>HBase 误删表恢复</title>
    <url>/2019/09/17/HBase-wrong-deletion-table/</url>
    <content><![CDATA[<p>HBase 误删表</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>HBase 的数据主要存储在分布式文件系统 HFile 和 HLog 两类文件中。Compaction操作会将合并完的不用的小 HFile 移动到 archive 文件夹。WAL 文件在数据完全 flush 到 HFile 中时便会过期，<br>被移动到oldWALs文件夹中。</p>
<p>HMaster 上的定时线程 HFileCleaner/LogCleaner 周期性扫描 archive 目录和 oldWALs 目录, 判断目录下的HFile或者WAL是否可以被删除，如果可以,就直接删除文件。</p>
<h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><p>关于 HFile 文件和 HLog 文件的过期时间，其中涉及到两个参数</p>
<p><strong>hbase.master.logcleaner.ttl</strong></p>
<p>HLog在 oldWAL 目录中生存的最长时间，过期则被 Master 的线程清理，默认是600000（ms）</p>
<p><img src="/images/blog/2019-09-17-2.png" alt></p>
<p><strong>hbase.master.hfilecleaner.plugins</strong></p>
<p>HFile的清理插件列表，逗号分隔，被HFileService调用，可以自定义，默认org.apache.hadoop.hbase.master.cleaner.TimeToLiveHFileCleaner。</p>
<p><img src="/images/blog/2019-09-17-3.png" alt></p>
<p>官方文档解释</p>
<p><img src="/images/blog/2019-09-17-4.png" alt></p>
<p>在类org.apache.hadoop.hbase.master.cleaner.TimeToLiveHFileCleaner中，可以看到如下的设置</p>
<p>默认 HFile 的失效时间是5分钟。由于一般的hadoop平台默认都没有对该参数的设置，可以在配置选项中添加对hbase.master.hfilecleaner.ttl的设置。</p>
<p><img src="/images/blog/2019-09-17-5.png" alt></p>
<p>实际在测试的过程中，删除一个hbase表，在archive文件夹中，会立即发现删除表的所有region数据（不包含regioninfo、tabledesc等元数据文件），等待不到6分钟所有数据消失，说明所有数据生命周期结束，被删除。</p>
<h4 id="恢复"><a href="#恢复" class="headerlink" title="恢复"></a>恢复</h4><p>删除表步骤 distable + drop。当然 truncate 清除表数据也可以通过这种方式恢复</p>
<h5 id="抢救数据"><a href="#抢救数据" class="headerlink" title="抢救数据"></a>抢救数据</h5><p>保证在删除表之后的5分钟之内将 HDFS 目录 /hbase/archive/ 文件夹下的表 region 数据拷贝到 /tmp 下。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop fs -cp &#x2F;hbase&#x2F;archive&#x2F;data&#x2F;default&#x2F;&#123;tableName&#125;  &#x2F;tmp</span><br></pre></td></tr></table></figure>

<h5 id="建表"><a href="#建表" class="headerlink" title="建表"></a>建表</h5><p>新建同名和同列族的表</p>
<p>注意: <strong>请提供表结构，如果表结构未提供，将很难恢复</strong></p>
<h5 id="拷贝"><a href="#拷贝" class="headerlink" title="拷贝"></a>拷贝</h5><p>将抢救下来的 region 数据拷贝到 hbase 表对应的目录下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop fs -cp &#x2F;tmp&#x2F;data&#x2F;default&#x2F;test&#x2F;* &#x2F;hbase&#x2F;data&#x2F;default&#x2F;test</span><br></pre></td></tr></table></figure>

<h5 id="元数据修复"><a href="#元数据修复" class="headerlink" title="元数据修复"></a>元数据修复</h5><p>HDFS 里有对应的 Region 数据了。想当然是想到用-fixMeta修复</p>
<p>-fixMeta: 主要修复.regioninfo文件和hbase:meta元数据表的不一致。修复的原则是以HDFS文件为准：如果region在HDFS上存在，但在hbase.meta表中不存在，就会在hbase:meta表中添加一条记录。反之如果在HDFS上不存在，而在hbase:meta表中存在，就会将hbase:meta表中对应的记录删除。</p>
<p>但是因为缺少 regioninfo 信息，不能直接用 -fixMeta 修复。所以得先修复下 regioninfo、tableinfo</p>
<p>-fixHdfsOrphans: 尝试修复hdfs中没有.regioninfo文件的region目录</p>
<p>-fixTableOrphans: 尝试修复hdfs中没有.tableinfo文件的table目录（只支持在线模式）</p>
<p>尝试先用fixHdfsOrphans,fixTableOrphans,fixMeta的顺序进行修复，失败。</p>
<p>最后用-repair修复，但是内部的执行顺序可能不对，执行一遍失败，多执行几遍，成功。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo -u hbase hbase hbck -repair &#123;tableName&#125;</span><br></pre></td></tr></table></figure>

<h5 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h5><p>通过 HBase Shell 验证</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>-repair 属于高危修复命令。要慎用，我指定 tableName 是避免 repair 影响到其他表</p>
<p>这里是说删表恢复。误删表数据也有恢复方法。<a href="https://mp.weixin.qq.com/s/XBwan9ZOCrOM565Cdw6Z0A" target="_blank" rel="noopener">误删HBase数据如何抢救？</a></p>
<p>感兴趣自己研读</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/chaolovejia/article/details/48265335" target="_blank" rel="noopener">HDFS和Hbase误删数据恢复</a></li>
<li><a href="https://www.cnblogs.com/yingjie2222/p/6377359.html" target="_blank" rel="noopener">HBase之disable+drop删除表疑点解惑</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>HMaster 由于 MasterProcWals 过多导致启动选主超时</title>
    <url>/2019/09/17/HBase-Master-Selection-timeout/</url>
    <content><![CDATA[<p>由 MasterProcWals 状态日志过多导致的HBase Master重启失败问题</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>由于 Kylin 业务迁移后， Master 日志中仍然报出 Kylin 相关表的合并日志，因此对 HBase 进行滚动重启，发现重启主 Master 后发生选主超时</p>
<h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><p>CM 页面显示未发现有活动的 Master 并有红色告警提示</p>
<p>但在 zookeeper 中 get /hbase/master 发现 100.106.37.5 的 master 选主已经成功</p>
<p>查看对应日志发现一直刷如下信息</p>
<p><img src="/images/blog/2019-09-17-6.png" alt></p>
<p>去 HDFS 查看该路径下有接近 20000 个这样的日志文件，且单副本达到了300余G的大小</p>
<p>同时根据已知 Master 进入活动状态需要读取并实例化所有正在运行的程序当前记录在 /hbase/MasterProcWALs/ 目录下对应的文件。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MasterProcWAL: HMaster记录管理操作，比如解决冲突的服务器，表创建和其它DDLs等操作到它的WAL文件中，</span><br><span class="line">这个WALs存储在MasterProcWALs目录下，它不像RegionServer的WALs，HMaster的WAL也支持弹性操作，</span><br><span class="line">就是如果Master服务器挂了，其它的Master接管的时候继续操作这个文件。</span><br></pre></td></tr></table></figure>

<p>因此推断是由于 MasterProcWA 日志文件太大太多，回放时间超过了 Master 选主时间</p>
<p>等2万个日志文件刷完以后，Master 报如下错误</p>
<p><img src="/images/blog/2019-09-18-1.png" alt></p>
<p>再过 900000ms 这么长时间继续报该错误</p>
<h4 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h4><p>根据日志和<a href="https://cloud.tencent.com/developer/article/1349438" target="_blank" rel="noopener">由MasterProcWals状态日志过多导致的HBase Master重启失败问题</a>猜测<br>可能是一个Bug</p>
<p>move /hbase/MasterProcWALs 目录下的所有文件，重启 HBase Master 解决问题。</p>
<p>重启 HBase 观察日志发现已经正常刷 Region 信息日志</p>
<p><img src="/images/blog/2019-09-18-2.png" alt></p>
<p>去 HBase Shell 中操作，报错ERROR：master is initializing，please wait（在这之前，没有移动 MasterProcWALs 日志 选主超时的时候，此时报错为ERRAOR：master server not running yet），证明已经重启成功，大概五分钟之后，Master 选主成功，CM页面告消失，HBase Shell 能够正常操作</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>从这次事故以后 MasterProcWals 状态日志就没有过多的情况了。</p>
<p>据 Fayson 大神说该问题主要和HBase某个分支的实现方式有关，据说已经重新设计了该实现方式，新的实现方式能够避免该问题，将在CDH 6中应用。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://cloud.tencent.com/developer/article/1349438" target="_blank" rel="noopener">由MasterProcWals状态日志过多导致的HBase Master重启失败问题</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>HMaster 定期清理 archive</title>
    <url>/2019/09/16/HMaster-clean-archive/</url>
    <content><![CDATA[<p>HFileCleaner 定期清理 archive 下的文件</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>表一旦删除，刚开始是可以在 archive 中看到删除表的数据文件，但是等待一段时间后 archive 中的数据就会被彻底删除，再也无法找回。</p>
<p>这是因为 master 上会启动一个定期清理 archive 中垃圾文件的线程（HFileCleaner），定期会对这些被删除的垃圾文件进行清理。</p>
<h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><p>定时清理任务的插件设置会从 hbase.master.hfilecleaner.plugins 配置里加载所有 BaseHFileCleanerDelegate。 只有所有 delegate 都同意才能被删除。</p>
<p><img src="/images/blog/2019-09-17-1.png" alt></p>
<p>就是通过 HFileLinkCleaner，SnapshotHFileCleaner，TimeToLiveHFileCleaner 这三种规则的约束来清理archive中的数据</p>
<h5 id="HFileLinkCleaner"><a href="#HFileLinkCleaner" class="headerlink" title="HFileLinkCleaner"></a>HFileLinkCleaner</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line"> * HFileLink cleaner that determines if a hfile should be deleted.</span><br><span class="line"> * HFiles can be deleted only if there&#39;re no links to them.</span><br><span class="line"> *</span><br><span class="line"> * When a HFileLink is created a back reference file is created in:</span><br><span class="line"> *      &#x2F;hbase&#x2F;archive&#x2F;table&#x2F;region&#x2F;cf&#x2F;.links-hfile&#x2F;ref-region.ref-table</span><br><span class="line"> * To check if the hfile can be deleted the back references folder must be empty.</span><br><span class="line"> *&#x2F;</span><br></pre></td></tr></table></figure>

<p>如果对archive中的文件的引用不存在了，则可以删除</p>
<p>问题: /hbase/archive/table/region/cf/.links-hfile/ref-region.ref-table 从何而来？<br>回答: HBase 做 clone_snapshot 的时候。有兴趣可以自看<a href="http://hbasefly.com/2017/09/17/hbase-snapshot/" target="_blank" rel="noopener">HBase原理 – 分布式系统中snapshot是怎么玩的？</a></p>
<h5 id="SnapshotHFileCleaner"><a href="#SnapshotHFileCleaner" class="headerlink" title="SnapshotHFileCleaner"></a>SnapshotHFileCleaner</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line"> * Implementation of a file cleaner that checks if a hfile is still used by snapshots of HBase</span><br><span class="line"> * tables.</span><br><span class="line"> *&#x2F;</span><br></pre></td></tr></table></figure>

<p>未被 snapshots 引用的文件，可以删除</p>
<p>问题: snapshot 为嘛会引用到 archive 中的文件？<br>回答: 表做了snapshot后，此时该表的元数据以及相关的link文件都存储在snapshot中, 该表发生 compact 操作前会将原始表移动到 archive 目录下再执行 compact。对于表删除操作，正常情况也会将删除表数据移动到archive目录下），这样snapshot对应的元数据就不会失去意义，只不过原始数据不再存在于数据目录下，而是移动到了archive目录下。</p>
<h5 id="TimeToLiveHFileCleaner"><a href="#TimeToLiveHFileCleaner" class="headerlink" title="TimeToLiveHFileCleaner"></a>TimeToLiveHFileCleaner</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line"> * HFile cleaner that uses the timestamp of the hfile to determine if it should be deleted. By</span><br><span class="line"> * default they are allowed to live for &#123;@value #DEFAULT_TTL&#125;</span><br><span class="line"> *&#x2F;</span><br></pre></td></tr></table></figure>

<p>默认清理时间超过5分钟的HFile</p>
<h4 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h4><p>在查看公司 HBase 集群 archive 文件夹发现有 2018 年的数据未清理。。。</p>
<p>按所学知识正常的情况下超过5分钟的HFile就会被清理。</p>
<p>继续翻阅文件发现了 links-hfile 文件。因此推断是因为有 clone 的表还依赖着。</p>
<p>links-hfile 文件的消失是在 clone 表做 Compact 之后会消失。但是因为 clone 表已经一直没有写入，所以 region 没有做 majorCompat, 后台线程周期性检查也会因为 needsCompaction() 方法去判断没有足够多的文件触发了 Compaction</p>
<p>只能手动触发 majorCompact，一旦手动触发，HBase 会不做很多自动化检查，直接执行合并。</p>
<p>等待一段时间回看发现 archive 文件的2018年数据已清理</p>
<p>整个过程忘记留图了。。。导致只能后续口诉加回忆。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.jianshu.com/p/f82aafd7b381" target="_blank" rel="noopener">HMaster 功能之定期清理archive</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>Flink Watermarks</title>
    <url>/2019/09/01/Flink-Watermark/</url>
    <content><![CDATA[<p>初学Flink，对Watermarks的一些理解和感悟</p>
<hr>
<h4 id="重要的概念"><a href="#重要的概念" class="headerlink" title="重要的概念"></a>重要的概念</h4><p>Window: Window是处理无界流的关键，Windows将流拆分为一个个有限大小的buckets，可以可以在每一个buckets中进行计算</p>
<p>start_time,end_time: 当Window时时间窗口的时候，每个window都会有一个开始时间和结束时间（前开后闭），这个时间是系统时间</p>
<p>event-time: 事件发生时间，是事件发生所在设备的当地时间，比如一个点击事件的时间发生时间，是用户点击操作所在的手机或电脑的时间</p>
<p>Watermarks: 可以把他理解为一个水位线，这个Watermarks在不断的变化，一旦Watermarks大于了某个window的end_time，就会触发此window的计算，Watermarks就是用来触发window计算的</p>
<h4 id="处理乱序的数据流"><a href="#处理乱序的数据流" class="headerlink" title="处理乱序的数据流"></a>处理乱序的数据流</h4><p>什么是乱序呢？可以理解为数据到达的顺序和他的event-time排序不一致。导致这的原因有很多，比如延迟，消息积压，重试等等</p>
<p>因为Watermarks是用来触发window窗口计算的，我们可以根据事件的event-time，计算出Watermarks，并且设置一些延迟，给迟到的数据一些机会。</p>
<p>可以阅读<a href="https://blog.csdn.net/a6822342/article/details/78064815" target="_blank" rel="noopener">Flink事件时间处理和水印</a>有个认识</p>
<h4 id="生成-Timestamp-和-Watermark"><a href="#生成-Timestamp-和-Watermark" class="headerlink" title="生成 Timestamp 和 Watermark"></a>生成 Timestamp 和 Watermark</h4><p>请仔细阅读<a href="https://www.jianshu.com/p/8c4a1861e49f" target="_blank" rel="noopener">Flink生成Timestamp和Watermark</a>对应<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/event_timestamps_watermarks.html" target="_blank" rel="noopener">官网</a></p>
<h4 id="Kafka-consumer-时间戳提取-水位生成"><a href="#Kafka-consumer-时间戳提取-水位生成" class="headerlink" title="Kafka consumer 时间戳提取/水位生成"></a>Kafka consumer 时间戳提取/水位生成</h4><p>自定义时间戳提取器/水位生成器，具体方法参见这里，然后按照下面的方式传递给consumer</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Properties properties &#x3D; new Properties();</span><br><span class="line">properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);</span><br><span class="line">&#x2F;&#x2F; only required for Kafka 0.8</span><br><span class="line">properties.setProperty(&quot;zookeeper.connect&quot;, &quot;localhost:2181&quot;);</span><br><span class="line">properties.setProperty(&quot;group.id&quot;, &quot;test&quot;);</span><br><span class="line"> </span><br><span class="line">FlinkKafkaConsumer08&lt;String&gt; myConsumer &#x3D;</span><br><span class="line">    new FlinkKafkaConsumer08&lt;&gt;(&quot;topic&quot;, new SimpleStringSchema(), properties);</span><br><span class="line">myConsumer.assignTimestampsAndWatermarks(new CustomWatermarkEmitter());</span><br><span class="line"> </span><br><span class="line">DataStream&lt;String&gt; stream &#x3D; env</span><br><span class="line">    .addSource(myConsumer)</span><br><span class="line">    .print();</span><br></pre></td></tr></table></figure>

<p>CustomWatermarkEmitter 为自定义的时间戳提取器/水位生成器, 具体方法参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/event_timestamp_extractors.html" target="_blank" rel="noopener">这里</a></p>
<p>在内部，Flink 会为每个Kafka分区都执行一个对应的assigner实例。一旦指定了这样的assigner，对于每条Kafka中的消息，extractTimestamp(T element, long previousElementTimestamp)方法会被调用来给消息分配时间戳，而getCurrentWatermark()方法（定时生成水位）或checkAndGetNextWatermark(T lastElement, long extractedTimestamp)方法(基于特定条件)会被调用以确定是否发送新的水位值。</p>
<p>请仔细阅读<a href="https://www.jianshu.com/p/8c4a1861e49f" target="_blank" rel="noopener">Flink生成Timestamp和Watermark</a>对应<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/event_timestamps_watermarks.html" target="_blank" rel="noopener">官网</a></p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://cloud.tencent.com/developer/article/1406134" target="_blank" rel="noopener">Apache-Flink深度解析-DataStream-Connectors之Kafka</a></li>
</ul>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
  </entry>
  <entry>
    <title>Flink connector</title>
    <url>/2019/09/01/Flink-Connector/</url>
    <content><![CDATA[<p>Connector 的作用就相当于一个连接器，连接 Flink 计算引擎跟外界存储系统。</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>Flink 是新一代流批统一的计算引擎，它需要从不同的第三方存储引擎中把数据读过来，进行处理，然后再写出到另外的存储引擎中。Connector 的作用就相当于一个连接器，连接 Flink 计算引擎跟外界存储系统。</p>
<p>Flink 里有以下几种方式，当然也不限于这几种方式可以跟外界进行数据交换</p>
<ul>
<li>Flink 里面预定义了一些 source 和 sink</li>
<li>Flink 内部也提供了一些 Boundled connectors</li>
<li>使用第三方 Apache Bahir 项目中提供的连接器</li>
<li>通过异步 IO 方式</li>
</ul>
<h4 id="预定义的-source-和-sink"><a href="#预定义的-source-和-sink" class="headerlink" title="预定义的 source 和 sink"></a>预定义的 source 和 sink</h4><p>Flink 里预定义了一部分 source 和 sink。</p>
<p><img src="/images/blog/2019-09-01-1.png" alt></p>
<p><strong>基于文件的 source 和 sink</strong></p>
<p>如果要从文本文件中读取数据，可以直接使用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">env.readTextFile(path)</span><br></pre></td></tr></table></figure>

<p>就可以以文本的形式读取该文件中的内容。当然也可以使用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">env.readFile(fileInputFormat, path)</span><br></pre></td></tr></table></figure>

<p>根据指定的 fileInputFormat 格式读取文件中的内容</p>
<p>如果数据在 Flink 内进行了一系列的计算，想把结果写出到文件里，也可以直接使用内部预定义的一些 sink，<br>比如将结果已文本或 csv 格式写出到文件中，可以使用 DataStream 的 writeAsText(path) 和<br>writeAsCsv(path)</p>
<p><strong>基于 Socket 的 Source 和 Sink</strong></p>
<p>提供 Socket 的 host name 及 port，可以直接用 StreamExecutionEnvironment 预定的接口<br>socketTextStream 创建基于 Socket 的 source，从该 socket 中以文本的形式读取数据。<br>当然如果想把结果写出到另外一个 Socket，也可以直接调用 DataStream writeToSocket。</p>
<p><strong>基于内存 Collections、Iterators 的 Source</strong></p>
<p>可以直接基于内存中的集合或者迭代器，调用 StreamExecutionEnvironment fromCollection、fromElements 构建相应的 source。结果数据也可以直接 print、printToError 的方式写出到标准输出或标准错误。</p>
<p>详细也可以参考 Flink 源码中提供的一些相对应的 Examples 来查看异常预定义 source 和 sink 的使用方法，例如 WordCount、SocketWindowWordCount。</p>
<h4 id="Bundled-Connectors"><a href="#Bundled-Connectors" class="headerlink" title="Bundled Connectors"></a>Bundled Connectors</h4><p>Flink 里已经提供了一些绑定的 Connector，例如 kafka source 和 sink，Es sink等。读写 kafka、es、rabbitMQ 时可以直接使用相应 connector 的 api 即可。第二部分会详细介绍生产环境中最常用的 kafka connector。</p>
<p>虽然该部分是 Flink 项目源代码里的一部分，但是真正意义上不算作 Flink 引擎相关逻辑，并且该部分没有打包在二进制的发布包里面。所以在提交 Job 时候需要注意， job 代码 jar 包中一定要将相应的 connetor 相关类打包进去，否则在提交作业时就会失败，提示找不到相应的类，或初始化某些类异常。</p>
<p><img src="/images/blog/2019-09-01-2.png" alt></p>
<h4 id="Apache-Bahir-中的连接器"><a href="#Apache-Bahir-中的连接器" class="headerlink" title="Apache Bahir 中的连接器"></a>Apache Bahir 中的连接器</h4><p>Apache Bahir 最初是从 Apache Spark 中独立出来项目提供，以提供不限于 Spark 相关的扩展/插件、连接器和其他可插入组件的实现。通过提供多样化的流连接器（streaming connectors）和 SQL 数据源扩展分析平台的覆盖面。如有需要写到 flume、redis 的需求的话，可以使用该项目提供的 connector。</p>
<p><img src="/images/blog/2019-09-01-3.png" alt></p>
<h4 id="Async-I-O"><a href="#Async-I-O" class="headerlink" title="Async I/O"></a>Async I/O</h4><p>流计算中经常需要与外部存储系统交互，比如需要关联 MySQL 中的某个表。一般来说，如果用同步 I/O 的方式，会造成系统中出现大的等待时间，影响吞吐和延迟。为了解决这个问题，异步 I/O 可以并发处理多个请求，提高吞吐，减少延迟。</p>
<p>Async 的原理可参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/stream/asyncio.html" target="_blank" rel="noopener">官方文档</a></p>
<p><img src="/images/blog/2019-09-01-4.png" alt></p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/p-YKnKnEnLbRfW7dfKxrcw" target="_blank" rel="noopener">如何正确使用 Flink Connector？</a></li>
<li><a href="https://github.com/flink-china/flink-training-course" target="_blank" rel="noopener">Flink 中文视频课程</a></li>
</ul>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
  </entry>
  <entry>
    <title>Flink kafka connector</title>
    <url>/2019/09/01/Flink-Kafka-Connector/</url>
    <content><![CDATA[<p>生产环境中最常用到的 Flink kafka connector</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>生产环境中最常用到的 Flink kafka connector</p>
<p>一是 Flink kafka Consumer，一个是 Flink kafka Producer。</p>
<p>首先看一个例子来串联下 Flink kafka connector。代码逻辑里主要是从 kafka 里读数据，然后做简单的处理，再写回到 kafka 中。</p>
<p><img src="/images/blog/2019-09-01-5.png" alt></p>
<p>分别用红框框出如何构造一个 Source sink Function。Flink 提供了现成的构造FlinkKafkaConsumer、Producer 的接口，可以直接使用。这里需要注意，因为 kafka 有多个版本，多个版本之间的接口协议会不同。Flink 针对不同版本的 kafka 有相应的版本的 Consumer 和 Producer。例如：针对 08、09、10、11 版本，Flink 对应的 consumer 分别是 FlinkKafkaConsumer 08、09、010、011，producer 也是。</p>
<h4 id="Consumer"><a href="#Consumer" class="headerlink" title="Consumer"></a>Consumer</h4><h5 id="反序列化数据"><a href="#反序列化数据" class="headerlink" title="反序列化数据"></a>反序列化数据</h5><p>Kafka 中数据都是以二进制 byte 形式存储的。读到 Flink 系统中之后，需要将二进制数据转化为具体的 java、scala 对象。</p>
<p>具体需要实现一个 schema 类，定义如何序列化和反序列数据。</p>
<p>反序列化时需要实现 DeserializationSchema 接口，并重写 deserialize(byte[] message) 函数</p>
<p>如果是反序列化 kafka 中 kv 的数据时，需要实现 KeyedDeserializationSchema 接口，并重写 deserialize(byte[] messageKey, byte[] message, String topic, int partition, long offset) 函数。</p>
<p>如果想自己实现 Schema ，可以参看<a href="https://juejin.im/post/5c8fd4dd5188252d92095995#heading-8" target="_blank" rel="noopener">Apache-Flink深度解析-DataStream-Connectors</a>之Kafka Simple ETL 部分</p>
<p>Flink 中也提供了一些常用的序列化反序列化的 schema 类。<a href="https://lihuimintu.github.io/2019/08/16/Flink-Kafka-deserialization" target="_blank" rel="noopener">Flink-Kafka 内置 Schemas</a></p>
<p>例如，SimpleStringSchema，按字符串方式进行序列化、反序列化。<br>TypeInformationSerializationSchema，它可根据 Flink 的 TypeInformation 信息来推断出需要选择的 schema。<br>JsonDeserializationSchema 使用 jackson 反序列化 json 格式消息，并返回 ObjectNode，可以使用 .get(“property”) 方法来访问相应字段。</p>
<h5 id="消费起始位置设置"><a href="#消费起始位置设置" class="headerlink" title="消费起始位置设置"></a>消费起始位置设置</h5><p>设置作业从 kafka 消费数据最开始的起始位置，这一部分 Flink 也提供了非常好的封装。在构造好的 FlinkKafkaConsumer 类后面调用如下相应函数，设置合适的起始位置。</p>
<ul>
<li><p>setStartFromGroupOffsets，也是默认的策略，<br>从 group offset 位置读取数据，group offset 指的是 kafka broker<br>端记录的某个 group 的最后一次的消费位置。但是 kafka broker 端没有该 group 信息，或者 group offset 无效的话，<br>将会根据 kafka 的参数”auto.offset.reset”的设置来决定从哪个位置开始消费，”auto.offset.reset” 默认为 largest。。</p>
</li>
<li><p>setStartFromEarliest，从 kafka 最早的位置开始读取。</p>
</li>
<li><p>setStartFromLatest，从 kafka 最新的位置开始读取。</p>
</li>
<li><p>setStartFromTimestamp(long)，从时间戳大于或等于指定时间戳的位置开始读取。Kafka 时戳，是指 kafka 为每条消息增加另一个时戳。该时戳可以表示消息在 proudcer 端生成时的时间、或进入到 kafka broker 时的时间。</p>
</li>
<li><p>setStartFromSpecificOffsets，从指定分区的 offset 位置开始读取，如指定的 offsets 中不存某个分区，该分区从 group offset 位置开始读取。此时需要用户给定一个具体的分区、offset 的集合。</p>
</li>
</ul>
<p><img src="/images/blog/2019-09-01-6.png" alt></p>
<p>需要注意的是，因为 Flink 框架有容错机制，如果作业故障，如果作业开启 checkpoint，会从上一次 checkpoint 状态开始恢复。或者在停止作业的时候主动做 savepoint，启动作业时从 savepoint 开始恢复。这两种情况下恢复作业时，作业消费起始位置是从之前保存的状态中恢复，与上面提到跟 kafka 这些单独的配置无关。</p>
<p><strong>如果该作业是从 checkpoint 或 savepoint 中恢复，则所有设置初始 offset 的函数均将失效，初始 offset 将从 checkpoint 中恢复。</strong></p>
<h5 id="topic-和-partition-动态发现"><a href="#topic-和-partition-动态发现" class="headerlink" title="topic 和 partition 动态发现"></a>topic 和 partition 动态发现</h5><p>实际的生产环境中可能有这样一些需求，比如场景一，有一个 Flink 作业需要将五份数据聚合到一起，五份数据对应五个 kafka topic，随着业务增长，新增一类数据，同时新增了一个 kafka topic，如何在不重启作业的情况下作业自动感知新的 topic。场景二，作业从一个固定的 kafka topic 读数据，开始该 topic 有 10 个 partition，但随着业务的增长数据量变大，需要对 kafka partition 个数进行扩容，由 10 个扩容到 20。该情况下如何在不重启作业情况下动态感知新扩容的 partition？</p>
<p>针对上面的两种场景，首先需要在构建 FlinkKafkaConsumer 时的 properties 中设置 flink.partition-discovery.interval-millis 参数为非负值，表示开启动态发现的开关，以及设置的时间间隔。此时 FlinkKafkaConsumer 内部会启动一个单独的线程定期去 kafka 获取最新的 meta 信息。针对场景一，还需在构建 FlinkKafkaConsumer 时，topic 的描述可以传一个正则表达式描述的 pattern。每次获取最新 kafka meta 时获取正则匹配的最新 topic 列表。针对场景二，设置前面的动态发现参数，在定期获取 kafka 最新 meta 信息时会匹配新的 partition。为了保证数据的正确性，新发现的 partition 从最早的位置开始读取。</p>
<p><img src="/images/blog/2019-09-01-7.png" alt></p>
<h5 id="commit-offset-方式"><a href="#commit-offset-方式" class="headerlink" title="commit offset 方式"></a>commit offset 方式</h5><p>Flink kafka consumer commit offset 方式需要区分是否开启了 checkpoint。</p>
<p>如果 checkpoint 关闭，commit offset 要依赖于 kafka 客户端的 auto commit。需设置 enable.auto.commit，auto.commit.interval.ms 参数到 consumer properties，就会按固定的时间间隔定期 auto commit offset 到 kafka。</p>
<p>如果开启 checkpoint，这个时候作业消费的 offset 是 Flink 在 state 中自己管理和容错。此时提交 offset 到 kafka，一般都是作为外部进度的监控，想实时知道作业消费的位置和 lag 情况。此时需要 setCommitOffsetsOnCheckpoints 为 true 来设置当 checkpoint 成功时提交 offset 到 kafka。此时 commit offset 的间隔就取决于 checkpoint 的间隔，所以此时从 kafka 一侧看到的 lag 可能并非完全实时，如果 checkpoint 间隔比较长 lag 曲线可能会是一个锯齿状。</p>
<p><img src="/images/blog/2019-09-01-8.png" alt></p>
<h5 id="Timestamp-Extraction-Watermark-生成"><a href="#Timestamp-Extraction-Watermark-生成" class="headerlink" title="Timestamp Extraction/Watermark 生成"></a>Timestamp Extraction/Watermark 生成</h5><p>Flink 作业内使用 EventTime 属性时，需要指定从消息中提取时戳和生成水位的函数。</p>
<p>FlinkKakfaConsumer 构造的 source 后直接调用 assignTimestampsAndWatermarks 函数设置水位生成器的好处是此时是每个 partition 一个 watermark assigner</p>
<p>如下图。source 生成的时戳为多个 partition 时戳对齐后的最小时戳。此时在一个 source 读取多个 partition，并且 partition 之间数据时戳有一定差距的情况下，因为在 source 端 watermark 在 partition 级别有对齐，不会导致数据读取较慢 partition 数据丢失。</p>
<h4 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q &amp; A"></a>Q &amp; A</h4><p><strong>在 Flink consumer 的并行度的设置：是对应 topic 的 partitions 个数吗？要是有多个主题数据源，并行度是设置成总体的 partitions 数吗？</strong></p>
<p>这个并不是绝对的，跟 topic 的数据量也有关，如果数据量不大，也可以设置小于 partitions 个数的并发数。但不要设置并发数大于 partitions 总数，因为这种情况下某些并发因为分配不到 partition 导致没有数据处理。</p>
<p><strong>如果 checkpoint 时间过长，offset 未提交到 kafka，此时节点宕机了，重启之后的重复消费如何保证呢？</strong></p>
<p>首先开启 checkpoint 时 offset 是 Flink 通过状态 state 管理和恢复的，并不是从 kafka 的 offset 位置恢复。在 checkpoint 机制下，作业从最近一次 checkpoint 恢复，本身是会回放部分历史数据，导致部分数据重复消费，Flink 引擎仅保证计算状态的精准一次，要想做到端到端精准一次需要依赖一些幂等的存储系统或者事务操作。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/p-YKnKnEnLbRfW7dfKxrcw" target="_blank" rel="noopener">如何正确使用 Flink Connector？</a></li>
<li><a href="https://github.com/flink-china/flink-training-course" target="_blank" rel="noopener">Flink 中文视频课程</a></li>
<li><a href="https://www.jianshu.com/p/f9d447a3c48f" target="_blank" rel="noopener">Flink Kafka Connector 详解</a></li>
<li><a href="https://cloud.tencent.com/developer/article/1406134" target="_blank" rel="noopener">Apache-Flink深度解析-DataStream-Connectors之Kafka</a></li>
</ul>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
  </entry>
  <entry>
    <title>Linux CPU</title>
    <url>/2019/08/29/Linux-CPU/</url>
    <content><![CDATA[<p>CPU是用来运行各种程序，做各种计算的，一旦CPU处于高负荷状态，容易引起服务响应速度变慢</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>CPU（Central Processing Unit）是计算机系统的运算和控制核心，是信息处理、程序运行的最终执行单元，相当于系统的“大脑”。当 CPU 过于繁忙，就像“人脑”并发处理过多的事情，会降低做事的效率，严重时甚至会导致崩溃“宕机”。因此，理解 CPU 的工作原理，合理控制负载，是保障系统稳定持续运行的重要手段。</p>
<h4 id="CPU-的物理核与逻辑核"><a href="#CPU-的物理核与逻辑核" class="headerlink" title="CPU 的物理核与逻辑核"></a>CPU 的物理核与逻辑核</h4><p>一台机器可能包含多块 CPU 芯片，多个 CPU 之间通过系统总线通信。</p>
<p>一块 CPU 芯片可能包含多个物理核，每个物理核都是一个实打实的运算核心（包括运算器、存储器等）。</p>
<p>超线程（Hyper-Threading）技术可以让一个物理核在单位时间内同时处理两个线程，变成两个逻辑核。但它不会拥有传统单核 2 倍的处理能力，也不可能提供完整的并行处理能力。</p>
<p><img src="/images/blog/2020-04-18-1.png" alt></p>
<h4 id="查询-CPU-信息"><a href="#查询-CPU-信息" class="headerlink" title="查询 CPU 信息"></a>查询 CPU 信息</h4><p>在 Linux 系统下，可以从 /proc/cpuinfo 文件中读取 CPU 信息</p>
<p><img src="/images/blog/2020-04-18-2.png" alt></p>
<p><strong>查看 CPU 个数</strong></p>
<p><code>grep &#39;physical id&#39; /proc/cpuinfo | sort -u | wc -l</code></p>
<p><strong>查看 CPU 物理核数</strong></p>
<p><code>grep &#39;core id&#39; /proc/cpuinfo | sort -u | wc -l</code></p>
<p><strong>查看 CPU 逻辑核数</strong></p>
<p><code>grep &#39;processor&#39; /proc/cpuinfo | sort -u | wc -l</code></p>
<p>逻辑CPU=物理CPU个数×每颗核数</p>
<h4 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h4><ul>
<li><a href="https://www.infoq.cn/article/5jjIdOPx12RWWvGX_H9J" target="_blank" rel="noopener">如何迅速分析出系统 CPU 的瓶颈在哪里？</a></li>
<li><a href="https://www.jellythink.com/archives/421" target="_blank" rel="noopener">Linux top命令详解</a></li>
<li><a href="https://www.jellythink.com/archives/419" target="_blank" rel="noopener">Linux vmstat命令详解</a></li>
</ul>
<p><strong>CPU 使用率</strong></p>
<p>CPU 使用率就是 CPU 非空闲态运行的时间占比，它反映了 CPU 的繁忙程度。比如，单核 CPU 1s 内非空闲态运行时间为 0.8s，那么它的 CPU 使用率就是 80%；双核 CPU 1s 内非空闲态运行时间分别为 0.4s 和 0.6s，那么，总体 CPU 使用率就是 (0.4s + 0.6s) / (1s * 2) = 50%，其中 2 表示 CPU 核数，多核 CPU 同理。</p>
<p>在 Linux 系统下，使用 top 命令查看 CPU 使用情况，可以得到如下信息</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Cpu(s): 0.2%us, 0.1%sy, 0.0%ni, 77.5%id, 2.1%wa, 0.0%hi, 0.0%si, 20.0%st</span><br></pre></td></tr></table></figure>

<ul>
<li>us(user)：表示 CPU 在用户态运行的时间百分比，通常用户态 CPU 高表示有应用程序比较繁忙。典型的用户态程序包括：数据库、Web 服务器等。</li>
<li>sy(sys)：表示 CPU 在内核态运行的时间百分比（不包括中断），通常内核态 CPU 越低越好，否则表示系统存在某些瓶颈。</li>
<li>ni(nice)：表示用 nice 修正进程优先级的用户态进程执行的 CPU 时间。nice 是一个进程优先级的修正值，如果进程通过它修改了优先级，则会单独统计 CPU 开销。</li>
<li>id(idle)：表示 CPU 处于空闲态的时间占比，此时，CPU 会执行一个特定的虚拟进程，名为 System Idle Process。</li>
<li>wa(iowait)：表示 CPU 在等待 I/O 操作完成所花费的时间，通常该指标越低越好，否则表示 I/O 存在瓶颈，可以用 iostat 等命令做进一步分析。</li>
<li>hi(hardirq)：表示 CPU 处理硬中断所花费的时间。硬中断是由外设硬件（如键盘控制器、硬件传感器等）发出的，需要有中断控制器参与，特点是快速执行。</li>
<li>si(softirq)：表示 CPU 处理软中断所花费的时间。软中断是由软件程序（如网络收发、定时调度等）发出的中断信号，特点是延迟执行。</li>
<li>st(steal)：表示 CPU 被其他虚拟机占用的时间，仅出现在多虚拟机场景。如果该指标过高，可以检查下宿主机或其他虚拟机是否异常。</li>
</ul>
<p>由于 CPU 有多种非空闲态，因此，CPU 使用率计算公式可以总结为：CPU 使用率 = (1 - 空闲态运行时间/总运行时间) * 100%。</p>
<p>根据经验法则， 建议生产系统的 CPU 总使用率不要超过 70%。</p>
<p><strong>平均负载（Load Average）</strong></p>
<p>平均负载（Load Average）是指单位时间内，系统处于可运行状态（Running / Runnable） 和 不可中断态 的平均进程数，也就是 平均活跃进程数。</p>
<p>可运行态进程包括正在使用 CPU 或者等待 CPU 的进程；</p>
<p>不可中断态进程是指处于内核态关键流程中的进程，并且该流程不可被打断。比如当进程向磁盘写数据时，如果被打断，就可能出现磁盘数据与进程数据不一致。不可中断态，本质上是系统对进程和硬件设备的一种保护机制。</p>
<p>在 Linux 系统下，使用 top 命令查看平均负载，可以得到如下信息</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">load average: 1.09, 1.12, 1.52</span><br></pre></td></tr></table></figure>

<p>这 3 个数字分别表示 1分钟、5分钟、15分钟内系统的平均负载。该值越小，表示系统工作量越少，负荷越低；反之负荷越高。</p>
<p>根据经验法则，平均负载值大于CPU内核数*0.7，就需要引起关注。至于为什么可以阅读<a href="https://blog.csdn.net/zwldx/article/details/82812704" target="_blank" rel="noopener">Linux系统平均负载3个数字的含义</a></p>
<blockquote>
<p>平均负载为多少更合理？  </p>
<p>理想情况下，每个 CPU 应该满负荷工作，并且没有等待进程，此时，平均负载 = CPU 逻辑核数。</p>
<p>但是，在实际生产系统中，不建议系统满负荷运行。通用的经验法则是：平均负载 = 0.7 * CPU 逻辑核数。</p>
<ul>
<li>当平均负载持续大于 0.7 * CPU 逻辑核数，就需要开始调查原因，防止系统恶化；</li>
<li>当平均负载持续大于 1.0 * CPU 逻辑核数，必须寻找解决办法，降低平均负载；</li>
<li>当平均负载持续大于 5.0 * CPU 逻辑核数，表明系统已出现严重问题，长时间未响应，或者接近死机。</li>
</ul>
<p>除了关注平均负载值本身，我们也应关注平均负载的变化趋势，这包含两层含义。<br>一是 load1、load5、load15 之间的变化趋势；二是历史的变化趋势。</p>
<ul>
<li>当 load1、load5、load15 三个值非常接近，表明短期内系统负载比较平稳。此时，应该将其与昨天或上周同时段的历史负载进行比对，观察是否有显著上升。</li>
<li>当 load1 远小于 load5 或 load15 时，表明系统最近 1 分钟的负载在降低，而过去 5 分钟或 15 分钟的平均负载却很高。</li>
<li>当 load1 远大于 load5 或 load15 时，表明系统负载在急剧升高，如果不是临时性抖动，而是持续升高，特别是当 load5 都已超过 0.7 * CPU 逻辑核数 时，应调查原因，降低系统负载。</li>
</ul>
</blockquote>
<p><strong>平均负载与CPU使用率的关系与区别</strong></p>
<p>CPU 使用率是单位时间内 CPU 繁忙程度的统计。而平均负载不仅包括正在使用 CPU 的进程，还包括等待 CPU 或 I/O 的进程。</p>
<p>因此，两者不能等同，有两种常见的场景如下所述: </p>
<ul>
<li>CPU 密集型应用，大量进程在等待或使用 CPU，此时 CPU 使用率与平均负载呈正相关状态。</li>
<li>I/O 密集型应用，大量进程在等待 I/O，此时平均负载会升高，但 CPU 使用率不一定很高</li>
</ul>
<p>为了更深入的理解 CPU 使用率与平均负载的关系，举一个例子：假设现在有一个电话亭，有 4 个人在等待打电话，电话亭同一时刻只能容纳 1 个人打电话，只有拿起电话筒才算是真正使用。</p>
<p>那么 CPU 使用率就是拿起电话筒的时间占比，它只取决于在电话亭里的人的行为，与平均负载没有非常直接的关系。而平均负载是指在电话亭里的人加上排队的总人数，如下图所示：</p>
<p><img src="/images/blog/2020-04-18-3.png" alt></p>
<p><strong>CPU使用率超过100%</strong></p>
<p>有时候使用top命令发现CPU占用率竟超过100%。</p>
<p>这跟 CPU 数量有关，top命令是按CPU总使用率来显示的，4核理论上最高可达400%</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/guotao15285007494/article/details/84135713" target="_blank" rel="noopener">Linux CPU使用率超过100%的原因</a></li>
<li><a href="https://www.cnblogs.com/shengs/p/5148284.html" target="_blank" rel="noopener">性能分析Linux服务器CPU利用率</a></li>
<li><a href="https://mp.weixin.qq.com/s/3I5kkNIGKkjQFazwvS4GIQ" target="_blank" rel="noopener">如何正确理解 CPU 使用率和平均负载的关系？看完你就知道了</a></li>
</ul>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>Linux 内存</title>
    <url>/2019/08/28/Linux-Memory/</url>
    <content><![CDATA[<p>内存是评判服务器的一个非常重要的指标。内存的多少，可能会直接影响着服务器的整体性能。</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>内存是评判服务器的一个非常重要的指标。内存的多少，可能会直接影响着服务器的整体性能。阅读<a href="https://www.jellythink.com/archives/462" target="_blank" rel="noopener">Linux性能监测：内存篇</a></p>
<h4 id="物理内存和虚拟内存"><a href="#物理内存和虚拟内存" class="headerlink" title="物理内存和虚拟内存"></a>物理内存和虚拟内存</h4><p>说到内存，我们都会说到物理内存和虚拟内存的</p>
<p>物理内存就是真实的硬件设备，也就是咱们的内存条</p>
<p>虚拟内存（Virtual Memory）是把计算机的内存空间扩展到硬盘，物理内存（RAM）和硬盘的一部分空间（SWAP）组合在一起作为虚拟内存为计算机提供了一个连贯的虚拟内存空间</p>
<p>好处是我们拥有的内存“变多了”，可以运行更多、更大的程序，坏处是把部分硬盘当内存用整体性能受到影响，硬盘读写速度要比内存慢几个数量级，并且RAM和SWAP之间的交换增加了系统的负担</p>
<p>需要记住的一点是Linux会在物理内存不足时，使用交换分区的虚拟内存</p>
<p>关于虚拟内存更多跳转<a href="https://www.cnblogs.com/yitianyouyitian/p/9603390.html" target="_blank" rel="noopener">内存参数</a></p>
<p>关于物理内存和虚拟内存的释放参阅<a href="https://www.cnblogs.com/bymo/p/7569281.html" target="_blank" rel="noopener">Linux 释放物理内存和虚拟内存</a></p>
<p>物理内存释放有坑。。。。 <a href="http://www.51niux.com/?id=76" target="_blank" rel="noopener">Linux 清除内存中的cache</a> 内核2.6的版本执行上述的操作都没问题的，但是到了内核3系列，就不能执行echo 0 &gt;/proc/sys/vm/drop_caches的操作了，这是一个坑，重启才能改回去</p>
<h4 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h4><p>请跳转阅读<a href="https://www.cnblogs.com/operationhome/p/10362776.html" target="_blank" rel="noopener">centos6 free 和 centos 7的free 的差异与对比</a></p>
<p>free命令可以显示Linux系统中空闲的、已用的物理内存及swap内存，及被内核使用的buffer</p>
<p>free命令习惯上有以下几种形式</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">free -k # 以KB为单位显示内存使用情况</span><br><span class="line">free -m # 以MB为单位显示内存使用情况</span><br><span class="line">free -g # 以GB为单位显示内存使用情况</span><br><span class="line">free -h # 以人类友好的方式显示内存使用情况</span><br></pre></td></tr></table></figure>

<p>输入free -m时，系统就会输出以下内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@massive-dataset-new-002 ~]# free -g</span><br><span class="line">             total       used       free     shared    buffers     cached</span><br><span class="line">Mem:            61         19         41          0          0         10</span><br><span class="line">-&#x2F;+ buffers&#x2F;cache:          8         52</span><br><span class="line">Swap:            4          0          4</span><br></pre></td></tr></table></figure>

<p>输出解释<br>total: 内存总数，物理内存总数<br>used: 已经使用的内存数<br>free: 空闲的内存数<br>shared: 多个进程共享的内存总额<br>buffers: 缓冲内存数<br>cached: 缓存内存数<br>- buffers/cached: 应用使用内存数<br>+ buffers/cached: 应用可用内存数<br>Swap: 交换分区，虚拟内存  </p>
<p>通过free命令查看机器空闲内存时，会发现free的值很小。<br>这主要是因为，在Linux系统中有这么一种思想，内存不用白不用，因此它尽可能的cache和buffer一些数据，以方便下次使用。<br>但实际上这些内存也是可以立刻拿来使用</p>
<p>在使用free命令时，都是需要重点关注 <code>- buffers/cached</code>和 + <code>buffers/cached</code>。</p>
<p>- buffers/cached，即used - buffers/cached，表示应用程序实际使用的内存<br>+ buffers/cached，即free + buffers/cached，表示理论上都可以被使用的内存    </p>
<p>可见-buffers/cache反映的是被程序实实在在吃掉的内存，而+buffers/cache反映的是可以挪用的内存总数。</p>
<h4 id="cache、buffer"><a href="#cache、buffer" class="headerlink" title="cache、buffer"></a>cache、buffer</h4><p>buffer和cache是两个在计算机技术中被用滥的名词，放在不通语境下会有不同的意义</p>
<p>在Linux的内存管理中，这里的buffer指Linux内存的：Buffer cache。这里的cache指Linux内存中的：Page cache。翻译成中文可以叫做缓冲区缓存和页面缓存</p>
<p><strong>cached</strong></p>
<p>当程序启动的时候，Linux内核首先检查CPU的缓存和物理内存，如果数据已经在内存里就忽略，如果数据不在内存里就引起一个缺页中断（Page Fault），然后从硬盘读取缺页，并把缺页缓存到物理内存里。</p>
<p>缺页中断可分为主缺页中断（Major Page Fault）和次缺页中断（Minor Page Fault），要从磁盘读取数据而产生的中断是主缺页中断；数据已经被读入内存并被缓存起来，但是没有向MMU注册的情况，操作系统只需要在MMU中注册相关页对应的物理地址即可，从内存缓存区中而不是直接从硬盘中读取数据而产生的中断是次缺页中断。</p>
<p>内存缓存区起到了预读硬盘的作用，内核先在物理内存里寻找缺页，没有的话产生次缺页中断从内存缓存里找，如果还没有发现的话就从硬盘读取。</p>
<p>这里说的内存缓存区即为cached，主要用于提升读取相关页面的效率，内核会将经常访问的页面放到cached中，这样就提高了页面的命中率，cached越大，命中率就越高，就减少了I/O读请求</p>
<p>如果 cache 的值很大，说明cache住的文件数很多。如果频繁访问到的文件都能被cache住，那么磁盘的读IO 必会非常小</p>
<p>所以cached是用来给文件做缓冲。cached直接用来记忆我们打开的文件</p>
<p><strong>buffers</strong></p>
<p>Buffer cache则主要是设计用来在系统对块设备进行读写的时候，对块进行数据缓存的系统来使用</p>
<p>buffers 主要用于I/O写，将应用程序的多次零碎写事件，集中到一个缓冲区(即buffers)，然后再一次性写入磁盘中，这样就提高了写磁盘的效率</p>
<blockquote>
<p>来自马哥Linux运维对Buffer和Cache解读</p>
<p>Cache（缓存）位于CPU与内存之间的临时存储器，缓存容量比内存小的多但交换速度比内存要快得多。Cache通过缓存文件数据块，解决CPU运算速度与内存读写速度不匹配的矛盾，提高CPU和内存之间的数据交换速度。Cache缓存越大，CPU处理速度越快。</p>
<p>Buffer（缓冲）高速缓冲存储器，通过缓存磁盘（I/O设备）数据块，加快对磁盘上数据的访问，减少I/O，提高内存和硬盘（或其他I/O设备）之间的数据交换速度。Buffer是即将要被写入磁盘的，而Cache是被从磁盘中读出来的。</p>
</blockquote>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>Linux IO</title>
    <url>/2019/08/28/Linux-IO/</url>
    <content><![CDATA[<p>I/O相关的应用通常用来处理大量数据，需要大量内存和存储，频繁I/O操作读、写数据</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>IO 是运维过程中比较关注的点。阅读<a href="https://www.jellythink.com/archives/451" target="_blank" rel="noopener">Linux性能监测：IO篇</a></p>
<h4 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h4><p>说到实际的，那就要说说实际运维工作中，如何发现IO性能瓶颈</p>
<p>iostat。通过这个神器，就可以监视具体的磁盘性能了。</p>
<p><code>iostat -x 1 20</code> 每一秒显示一次详细信息，共显示20次。   -x 显示详细信息 / -d 显示磁盘使用情况</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[user1@Test_Server ~]$ iostat -x</span><br><span class="line">Linux 3.10.0-693.2.2.el7.x86_64 (jellythink)    01&#x2F;05&#x2F;2019      _x86_64_        (1 CPU)</span><br><span class="line"></span><br><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">           1.83    0.00    0.31    0.09    0.00   97.77</span><br><span class="line"></span><br><span class="line">Device:         rrqm&#x2F;s   wrqm&#x2F;s     r&#x2F;s     w&#x2F;s    rkB&#x2F;s    wkB&#x2F;s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">vda               0.03     0.78    0.24    1.38    12.64    20.67    41.01     0.02   10.98   55.50    3.17   0.71   0.12</span><br></pre></td></tr></table></figure>

<p>具体解释详细请阅读<a href="https://www.jellythink.com/archives/438" target="_blank" rel="noopener">Linux iostat命令详解</a></p>
<p>关注性能监控指标</p>
<ul>
<li>%iowait: 如果该值较高，表示磁盘存在I/O瓶颈</li>
<li>await: 一般地，系统I/O响应时间应该低于5ms，如果大于10ms就比较大了。另一方面如果 svctm 比较接近 await，说明 I/O 几乎没有等待时间；如果 await 远大于 svctm，说明I/O 队列太长，io响应太慢，则需要进行必要优化。</li>
<li>%util: 一秒中有百分之多少的时间用于I/O操作，即被IO消耗的CPU百分比，一般地，如果该参数是100%表示设备已经接近满负荷运行了</li>
<li>avgqu-sz: 如果avgqu-sz比较大，也表示有当量io在等待。如果单块磁盘的队列长度持续超过2，一般认为该磁盘存在I/O性能问题。</li>
</ul>
<p>但是，在实际运维工作中，都希望找到是哪个进程消耗了IO，所以终极目的是找到这个进程ID。可是通过 iostat 都没法达到目的。<br>所以，神器pidstat命令就登场了，通过这个命令，就可以知道是谁在后台偷用IO了。</p>
<p>pidstat 是 sysstat 工具的一个命令，如果服务器上没有现成的，不要着急。CentOS 使用<code>yum install sysstat</code> 安装</p>
<p>使用 -d 选项，可以查看进程IO的统计信息</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@massive-dataset-new-002 ~]# pidstat -d 1</span><br><span class="line">Linux 2.6.32-431.el6.x86_64 (massive-dataset-new-002)   08&#x2F;28&#x2F;2019      _x86_64_        (8 CPU)</span><br><span class="line"></span><br><span class="line">03:28:50 PM       PID   kB_rd&#x2F;s   kB_wr&#x2F;s kB_ccwr&#x2F;s  Command</span><br><span class="line">03:28:51 PM       438      0.00      3.96      0.00  jbd2&#x2F;dm-1-8</span><br><span class="line">03:28:51 PM     21406      0.00    126.73      0.00  mysqld</span><br><span class="line"></span><br><span class="line">03:28:51 PM       PID   kB_rd&#x2F;s   kB_wr&#x2F;s kB_ccwr&#x2F;s  Command</span><br><span class="line">03:28:52 PM       438      0.00      4.00      0.00  jbd2&#x2F;dm-1-8</span><br><span class="line">03:28:52 PM       969      0.00      4.00      0.00  flush-253:1</span><br><span class="line">03:28:52 PM     14200      0.00      4.00      0.00  java</span><br><span class="line">03:28:52 PM     21406      0.00      8.00      0.00  mysqld</span><br><span class="line">03:28:52 PM     24608      0.00      8.00      0.00  java</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kB_rd&#x2F;s: 每秒进程从磁盘读取的数据量(以kB为单位)</span><br><span class="line">kB_wr&#x2F;s: 每秒进程向磁盘写的数据量(以kB为单位)</span><br><span class="line">Command: 拉起进程对应的命令</span><br></pre></td></tr></table></figure>

<p>具体解释详细请阅读<a href="https://www.jellythink.com/archives/444" target="_blank" rel="noopener">Linux pidstat命令详解</a></p>
<p>现在定位到进程级别了，很多时候，需要知道这个进程到底打开了哪些文件，这个进程到底和哪些进程关联，这个时候就不得不提到lsof命令了。</p>
<p><code>lsof -p 20711</code> 列出指定进程打开的文件列表</p>
<p>具体解释详细请阅读<a href="https://www.jellythink.com/archives/449" target="_blank" rel="noopener">Linux lsof命令详解</a></p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>排查问题所使用的工具大而全的知识一时半会记不全。要多练习多思考</p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>Linux 网络</title>
    <url>/2019/08/28/Linux-Network/</url>
    <content><![CDATA[<p>Linux 网络</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>网络的监测是所有Linux子系统里面最复杂的，有太多的因素在里面，比如：延迟、阻塞、冲突、丢包等，更糟的是与Linux主机相连的路由器、交换机、无线信号都会影响到整体网络并且很难判断是因为Linux网络子系统的问题还是别的设备的问题，增加了监测和判断的复杂度。</p>
<p>网络问题是什么，是不通，还是慢？</p>
<ol>
<li>如果是网络不通，要定位具体的问题，一般是不断尝试排除不可能故障的地方，最终定位问题根源。一般需要查看</li>
</ol>
<p>　　　　是否接入到链路</p>
<p>　　　　是否启用了相应的网卡</p>
<p>　　　　本地网络是否连接</p>
<p>　　　　DNS故障</p>
<p>　　　　能否路由到目标主机</p>
<p>　　　　远程端口是否开放</p>
<ol start="2">
<li>如果是网络速度慢，一般有以下几个方式定位问题源：</li>
</ol>
<p>　　　　DNS是否是问题的源头</p>
<p>　　　　查看路由过程中哪些节点是瓶颈</p>
<p>　　　　查看带宽的使用情况</p>
<h4 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h4><h5 id="是否接入到链路"><a href="#是否接入到链路" class="headerlink" title="是否接入到链路"></a>是否接入到链路</h5><p>使用 ethtool 查看 em1 的物理连接</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@hahaha ~]# ethtool em1</span><br><span class="line">Settings for em1:</span><br><span class="line">        Supported ports: [ FIBRE ]</span><br><span class="line">        Supported link modes:   1000baseT&#x2F;Full </span><br><span class="line">                                10000baseT&#x2F;Full </span><br><span class="line">        Supported pause frame use: No</span><br><span class="line">        Supports auto-negotiation: Yes</span><br><span class="line">        Advertised link modes:  1000baseT&#x2F;Full </span><br><span class="line">                                10000baseT&#x2F;Full </span><br><span class="line">        Advertised pause frame use: No</span><br><span class="line">        Advertised auto-negotiation: Yes</span><br><span class="line">        Speed: Unknown!</span><br><span class="line">        Duplex: Unknown! (255)</span><br><span class="line">        Port: FIBRE</span><br><span class="line">        PHYAD: 0</span><br><span class="line">        Transceiver: external</span><br><span class="line">        Auto-negotiation: on</span><br><span class="line">        Supports Wake-on: umbg</span><br><span class="line">        Wake-on: g</span><br><span class="line">        Current message level: 0x00000007 (7)</span><br><span class="line">                               drv probe link</span><br><span class="line">        Link detected: no</span><br></pre></td></tr></table></figure>

<p>Speed 显示当前网卡的速度。但是不知道为啥，我司机器是Unknown。。</p>
<h5 id="是否启用了相应的网卡"><a href="#是否启用了相应的网卡" class="headerlink" title="是否启用了相应的网卡"></a>是否启用了相应的网卡</h5><p><code>ip addr show</code> 显示网卡及配置的地址信息</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 16436 qdisc noqueue state UNKNOWN </span><br><span class="line">    link&#x2F;loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1&#x2F;8 scope host lo</span><br><span class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000</span><br><span class="line">    link&#x2F;ether 06:50:32:00:02:18 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 100.73.12.12&#x2F;24 brd 100.73.12.255 scope global eth0</span><br></pre></td></tr></table></figure>

<p>首先这个系统有两个接口：lo和eth0，lo是环回接口，而我们重点关注的则是eth0这个普通网络接口</p>
<p>state UP: 网络接口已启用。具体详细的请阅读<a href="https://www.jellythink.com/archives/469" target="_blank" rel="noopener">Linux ip命令详解</a></p>
<h5 id="是否正确设置路由"><a href="#是否正确设置路由" class="headerlink" title="是否正确设置路由"></a>是否正确设置路由</h5><p><code>route  -n</code>查看路由状态</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@massive-dataset-new-002 ~]# route  -n</span><br><span class="line">Kernel IP routing table</span><br><span class="line">Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span><br><span class="line">100.73.12.0     0.0.0.0         255.255.255.0   U     0      0        0 eth0</span><br><span class="line">0.0.0.0         100.73.12.254   0.0.0.0         UG    0      0        0 eth0</span><br></pre></td></tr></table></figure>

<p>通过route命令查看内核路由，检验具体的网卡是否连接到目标网路的路由，之后就可以尝试ping 网关，排查与网关之间的连接。</p>
<p>如果无法ping通网关，可能是网关限制了ICMP数据包，或者交换机设置的问题。</p>
<p><strong>关于网关介绍</strong></p>
<blockquote>
<p>网络连接到另一个网络的“关口”</p>
</blockquote>
<p>如果网络A中的主机发现数据包的目的主机不在本地网络中，就把数据包转发给它自己的网关，再由网关转发给网络B的网关，网络B的网关再转发给网络B的某个主机。</p>
<p>比如有网络A和网络B，网络A的IP地址范围为 “192.168.1.1<del>192. 168.1.254”，子网掩码为255.255.255.0；网络B的IP地址范围为“192.168.2.1</del>192.168.2.254”，子网掩码为255.255.255.0。在没有路由器的情况下，两个网络之间是不能进行TCP/IP通信的，即使是两个网络连接在同一台交换机（或集线器）上， TCP/IP协议也会根据子网掩码（255.255.255.0）判定两个网络中的主机处在不同的网络里。而要实现这两个网络之间的通信，则必须通过网关。</p>
<h5 id="DNS工作状况"><a href="#DNS工作状况" class="headerlink" title="DNS工作状况"></a>DNS工作状况</h5><p>通常很多网络问题是DNS故障或配置不当造成的</p>
<p>使用nslookup命令查看DNS解析</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@massive-dataset-new-002 ~]# nslookup baidu.com</span><br><span class="line">Server:         100.73.18.5</span><br><span class="line">Address:        100.73.18.5#53</span><br><span class="line"></span><br><span class="line">Non-authoritative answer:</span><br><span class="line">Name:   baidu.com</span><br><span class="line">Address: 39.156.69.79</span><br><span class="line">Name:   baidu.com</span><br><span class="line">Address: 220.181.38.148</span><br></pre></td></tr></table></figure>

<p>这里的DNS服务器 100.73.18.5 位于当前局域网内，nslookup的结果显示DNS工作正常。</p>
<p>如果这里nslookup命令无法解析目标域名，则很有可能是DNS配置不当</p>
<p>查看 dns 配置文件当然最简单的方法就是查看dns配置文件，/etc/resolv.conf文件里面的 nameserver 地址</p>
<h5 id="是否可以正常路由到远程主机"><a href="#是否可以正常路由到远程主机" class="headerlink" title="是否可以正常路由到远程主机"></a>是否可以正常路由到远程主机</h5><p>互谅网是通过大量路由器中继连接起来的，网络的访问就是在这些节点间一跳一跳最终到达目的地，想要查看网络连接，最直接最常用的命令是ping，ping得通，说明路由工作正常，但是如果ping不通，traceroute命令可以查看从当前主机到目标主机的全部“跳”的过程。traceroute和ping命令都是使用ICMP协议包。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@massive-dataset-new-002 ~]#  traceroute www.baidu.com</span><br><span class="line">traceroute to www.baidu.com (103.235.46.39), 30 hops max, 60 byte packets</span><br><span class="line"> 1  100.73.12.254 (100.73.12.254)  2.124 ms  2.670 ms  2.388 ms</span><br><span class="line"> 2  100.73.255.252 (100.73.255.252)  0.107 ms  0.092 ms  0.089 ms</span><br><span class="line"> 3  114.113.234.233 (114.113.234.233)  7.929 ms  8.298 ms  8.290 ms</span><br><span class="line"> 4  172.16.0.4 (172.16.0.4)  13.154 ms  14.051 ms  13.626 ms</span><br><span class="line"> 5  192.168.77.9 (192.168.77.9)  9.923 ms  9.330 ms  9.901 ms</span><br><span class="line"> 6  61.49.40.97 (61.49.40.97)  2.216 ms  2.319 ms  1.595 ms</span><br><span class="line"> 7  * * *</span><br><span class="line"> 8  219.232.11.73 (219.232.11.73)  2.156 ms 219.232.11.153 (219.232.11.153)  3.377 ms 219.232.11.233 (219.232.11.233)  2.410 ms</span><br><span class="line"> 9  202.96.12.21 (202.96.12.21)  3.088 ms 123.126.0.101 (123.126.0.101)  3.232 ms 125.33.186.85 (125.33.186.85)  3.251 ms</span><br><span class="line">10  219.158.5.146 (219.158.5.146)  9.310 ms 219.158.4.170 (219.158.4.170)  10.142 ms 219.158.5.158 (219.158.5.158)  8.519 ms</span><br><span class="line">11  219.158.3.138 (219.158.3.138)  10.748 ms  11.408 ms 219.158.16.66 (219.158.16.66)  8.184 ms</span><br><span class="line">12  219.158.96.26 (219.158.96.26)  152.988 ms 219.158.98.18 (219.158.98.18)  154.549 ms  154.532 ms</span><br><span class="line">13  219.158.40.190 (219.158.40.190)  170.641 ms 219.158.33.58 (219.158.33.58)  190.226 ms 219.158.40.190 (219.158.40.190)  170.384 ms</span><br><span class="line">14  if-ae-8-2.tcore1.sv1-santa-clara.as6453.net (66.110.59.9)  190.528 ms  192.138 ms  189.755 ms</span><br><span class="line">15  if-ae-0-2.tcore2.sv1-santa-clara.as6453.net (63.243.251.2)  170.883 ms  168.308 ms  168.036 ms</span><br><span class="line">16  209.58.86.30 (209.58.86.30)  168.931 ms *  192.311 ms</span><br><span class="line">17  * * *</span><br><span class="line">18  103.235.45.0 (103.235.45.0)  308.516 ms  329.374 ms  331.152 ms</span><br><span class="line">19  * * *</span><br><span class="line">20  * * *</span><br><span class="line">21  * * *^C</span><br></pre></td></tr></table></figure>

<p>traceroute可以查看网络中继在哪里中断或者网络延时情况，“*”是因为网络不通或者某个网关限制了ICMP协议包。</p>
<h5 id="远程主机是否开放端口"><a href="#远程主机是否开放端口" class="headerlink" title="远程主机是否开放端口"></a>远程主机是否开放端口</h5><p>telnet 命令是检查端口开放情况的利器</p>
<p>telnet IP PORT，可以查看指定远程主机是否开放目标端口</p>
<p>但是telnet 命令的功能非常有限，当防火墙存在时，就不能很好地显示结果，所以telnet无法连接包含两种可能：1是端口确实没有开放，2是防火墙过滤了连接。</p>
<h5 id="本机查看监听端口"><a href="#本机查看监听端口" class="headerlink" title="本机查看监听端口"></a>本机查看监听端口</h5><p>如果要在本地查看某个端口是否开放，可以使用如下命令</p>
<p><code>netstat -lnp | grep PORT</code> 查看本地指定端口的监听情况</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@massive-dataset-new-002 ~]# netstat -anlp | grep 13307</span><br><span class="line">tcp        0      0 0.0.0.0:13307               0.0.0.0:*                   LISTEN      18368&#x2F;mysqld        </span><br><span class="line">tcp        0      0 100.73.12.12:13307          100.73.53.250:46370         ESTABLISHED 18368&#x2F;mysqld        </span><br><span class="line">tcp        0      0 100.73.12.12:13307          100.73.53.250:51380         ESTABLISHED 18368&#x2F;mysqld        </span><br><span class="line">tcp        0      0 100.73.12.12:13307          100.73.53.250:33466         ESTABLISHED 18368&#x2F;mysqld        </span><br><span class="line">tcp        0      0 100.73.12.12:13307          100.73.53.250:48615         ESTABLISHED 18368&#x2F;mysqld        </span><br><span class="line">tcp        0      0 100.73.12.12:13307          100.73.53.250:44921         ESTABLISHED 18368&#x2F;mysqld        </span><br><span class="line">tcp        0      0 100.73.12.12:13307          100.73.53.250:35016         ESTABLISHED 18368&#x2F;mysqld</span><br></pre></td></tr></table></figure>

<p>其中第一列是套接字通信协议，第2列和第3列显示的是接收和发送队列，第4列是主机监听的本地地址，反映了该套接字监听的网络；第6列显示当前套接字的状态，最后一列显示打开端口的进程。</p>
<p>查看当前活动端口监听的网络，如果netstat找不到指定的端口，说明没有进程在监听指定端口。</p>
<h5 id="网络较慢的排查"><a href="#网络较慢的排查" class="headerlink" title="网络较慢的排查"></a>网络较慢的排查</h5><p>iftop命令类似于top命令，查看哪些网络连接占用的带宽较多</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># CentOS上安装所需依赖包</span><br><span class="line">yum install flex byacc  libpcap ncurses ncurses-devel libpcap-devel</span><br><span class="line">yum install iftop</span><br></pre></td></tr></table></figure>

<p>该命令按照带宽占用高低排序，可以确定那些占用带宽的网络连接</p>
<p><img src="/images/blog/2019-08-28-2.png" alt></p>
<p>最上方的一行刻度是整个网络的带宽比例，下面第1列是源IP，第2列是目标IP，箭头表示了二者之间是否在传输数据，以及传输的方向。最后三列分别是2s、10s、40s时两个主机之间的数据传输速率。</p>
<p>最下方的TX、RX分别代表发送、接收数据的统计，TOTAL则是数据传输总量。</p>
<p>使用 -n 选项直接显示连接的IP，否则看到的则是解析成域名后的结果。<br>-i 选项可以指定要查看的网卡，默认情况下，iftop会显示自己找到的第一个网卡<br>在进入iftop的非交互界面后，按 p 键可以打开或关闭显示端口，按 s 键可以显示或隐藏源主机，而按 d 键则可以显示或隐藏目标主机。</p>
<p>虽然iftop报告每个连接所使用的带宽，但它无法报告参与某个套按字连接的进程名称/编号（ID）。</p>
<p><strong>tcpdump</strong></p>
<p>当一切排查手段都无济于事时仍然不能找到网络速度慢、丢包严重等原因时，往往祭出杀手锏——抓包。抓包的最佳手段是在通信的双方同时抓取，这样可以同时检验发出的数据包和收到的数据包，tcpdump是常用的抓包工具。</p>
<p>具体自行查阅<a href="https://www.jellythink.com/archives/478" target="_blank" rel="noopener">Linux tcpdump命令详解</a></p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.cnblogs.com/Security-Darren/p/4700387.html" target="_blank" rel="noopener">Linux系统排查4——网络篇</a></li>
<li><a href="https://www.jianshu.com/p/be05dc99c447" target="_blank" rel="noopener">网关（Gateway）</a></li>
<li><a href="https://www.cnblogs.com/cjm123/p/8513939.html" target="_blank" rel="noopener">默认网关是多少</a></li>
</ul>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>HBase Region 数量&amp;大小</title>
    <url>/2019/08/20/HBase-Region-Number/</url>
    <content><![CDATA[<p>HBase Region 数量&amp;大小</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>准备迁移表，评估下目标HBase集群是否能容纳下该表。表在原集群有192个 Region，因此我想评估下该目标集群是否足够容纳</p>
<h4 id="Region-数量"><a href="#Region-数量" class="headerlink" title="Region 数量"></a>Region 数量</h4><p>通常较少的 Region 数量可使群集运行的更加平稳，官方指出每个RegionServer大约 100 个regions的时候效果最好</p>
<ul>
<li><p>HBase的一个特性MSLAB，它有助于防止堆内存的碎片化，减轻垃圾回收Full GC的问题，默认是开启的。但是每个MemStore需要2MB（一个列簇对应一个写缓存MemStore）。<br>所以如果每个Region有2个family列簇，总有1000个region，就算不存储数据也要3.95G内存空间</p>
</li>
<li><p>如果很多region，它们中MemStore也过多，内存大小触发RegionServer级别限制导致flush，就会对用户请求产生较大的影响，可能阻塞该Region Server上的更新操作</p>
</li>
<li><p>HMaster 要花大量的时间来分配和移动 Region，且过多 Region 会增加 ZooKeeper 的负担</p>
</li>
<li><p>从 HBase 读入数据进行处理的 mapreduce 程序，过多 Region 会产生太多 Map 任务数量，默认情况下由涉及的 Region 数量决定</p>
</li>
</ul>
<p>如果一个 HRegion 中 Memstore 过多，而且大部分都频繁写入数据，每次flush的开销必然会很大，因此也建议在进行表设计的时候尽量减少ColumnFamily的个数。每个region都有自己的MemStore，当大小达到了上限(hbase.hregion.memstore.flush.size，默认128MB)，会触发Memstore刷新</p>
<p><strong>计算集群 Region 数量的公式</strong></p>
<p><code>((RS Xmx) * hbase.regionserver.global.memstore.size) / (hbase.hregion.memstore.flush.size * (# column families))</code></p>
<p>假设一个RS有16GB内存，那么16384*0.4/128m 等于51个活跃的region。</p>
<p>如果写很重的场景下，可以适当调高 hbase.regionserver.global.memstore.size，这样可以容纳更多的 Region 数量</p>
<p>建议分配合理的 Region 数量，根据写请求量的情况，一般 20-200 个之间，可以提高集群稳定性，排除很多不确定的因素，提升读写性能。</p>
<p>监控Region Server中所有 MemStore 的大小总和是否达到了上限(hbase.regionserver.global.memstore.upperLimit ＊ hbase_heapsize，默认 40%的JVM内存使用量)，超过可能会导致不良后果，如服务器反应迟钝或compact风暴</p>
<h4 id="Region-大小"><a href="#Region-大小" class="headerlink" title="Region 大小"></a>Region 大小</h4><p>HBase中数据一开始会写入memstore，满128MB（看配置）以后，会flush到disk上而成为storefile。当storefile数量超过触发因子时（可以配置），会启动compaction过程将它们合并为一个storefile。对集群的性能有一定影响。而当合并后的storefile大于max.filesize，会触发分割动作，将它切分成两个region。</p>
<ul>
<li><p>当hbase.hregion.max.filesize比较小时，触发split的机率更大，系统的整体访问服务会出现不稳定现象。</p>
</li>
<li><p>当hbase.hregion.max.filesize比较大时，由于长期得不到split，因此同一个region内发生多次compaction的机会增加了。这样会降低系统的性能、稳定性，因此平均吞吐量会受到一些影响而下降。</p>
</li>
</ul>
<p>hbase.hregion.max.filesize 不宜过大或过小，经过实战，生产高并发运行下，关闭某些重要场景的HBase表的major_compact！在非高峰期的时候再去调用major_compact，这样可以减少split的同时，显著提供集群的性能，吞吐量、非常有用。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/0tGNpmBRHbI673TwxIC2NA" target="_blank" rel="noopener">HBase最佳实践之Region数量&amp;大小</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>使用 Flink 时遇到的问题</title>
    <url>/2019/08/20/Flink-Problem/</url>
    <content><![CDATA[<p>使用Flink时遇到的问题（不断更新中）</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>作为公司第一个吃 Flink 这支螃蟹的人肯定免不了踩坑，因此通过文字记录的方式记录踩坑经历</p>
<h4 id="Could-not-build-the-program-from-JAR-file"><a href="#Could-not-build-the-program-from-JAR-file" class="headerlink" title="Could not build the program from JAR file."></a>Could not build the program from JAR file.</h4><p>用 Flink1.8 提交 jar 包到 Yarn 集群运行时报错</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Could not build the program from JAR file.</span><br><span class="line"></span><br><span class="line">Use the help option (-h or --help) to get help on the command.</span><br></pre></td></tr></table></figure>

<p>这是需要配置 Hadoop Classpaths。 <a href="https://ci.apache.org/projects/flink/flink-docs-stable/ops/deployment/hadoop.html#configuring-flink-with-hadoop-classpaths" target="_blank" rel="noopener">Configuring Flink with Hadoop Classpaths</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export HADOOP_CLASSPATH&#x3D;&#96;hadoop classpath&#96;</span><br></pre></td></tr></table></figure>

<h4 id="UnsupportedClassVersionError"><a href="#UnsupportedClassVersionError" class="headerlink" title="UnsupportedClassVersionError"></a>UnsupportedClassVersionError</h4><p>用 Flink1.8 提交 jar 包到 Yarn 集群运行时报 UnsupportedClassVersionError 错误</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.UnsupportedClassVersionError: org&#x2F;apache&#x2F;flink&#x2F;client&#x2F;cli&#x2F;CliFrontend : Unsupported major.minor version 52.0</span><br><span class="line">        at java.lang.ClassLoader.defineClass1(Native Method)</span><br><span class="line">        at java.lang.ClassLoader.defineClass(ClassLoader.java:800)</span><br><span class="line">        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)</span><br><span class="line">        at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)</span><br><span class="line">        at java.net.URLClassLoader.access$100(URLClassLoader.java:71)</span><br><span class="line">        at java.net.URLClassLoader$1.run(URLClassLoader.java:361)</span><br><span class="line">        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)</span><br><span class="line">        at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)</span><br><span class="line">        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)</span><br><span class="line">        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)</span><br><span class="line">        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)</span><br><span class="line">        at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:482)</span><br></pre></td></tr></table></figure>

<p>Java 版本不对, Flink1.8 需要 JDK1.8 以上。修改环境变量 JAVA_HOME</p>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
  </entry>
  <entry>
    <title>Kafka server 配置参数</title>
    <url>/2019/08/19/Kafka-config/</url>
    <content><![CDATA[<p>Kafka server 配置参数</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>Kafka 在不修改 server 配置参数的情况下，可以以默认的参数配置启动</p>
<p>但是在生产环境下需要修改部分参数以更好的提供服务</p>
<h4 id="properties"><a href="#properties" class="headerlink" title="properties"></a>properties</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># broker的名字</span><br><span class="line">broker.id&#x3D;1</span><br><span class="line">num.network.threads&#x3D;3</span><br><span class="line">num.io.threads&#x3D;8</span><br><span class="line"># kafka存放数据的路径</span><br><span class="line">log.dirs&#x3D;&#x2F;data&#x2F;kafka&#x2F;kafka-logs</span><br><span class="line"># Zookeeper 连接</span><br><span class="line">zookeeper.connect&#x3D;</span><br><span class="line">host.name&#x3D;</span><br><span class="line">advertised.host.name&#x3D;</span><br><span class="line">listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;:9092</span><br><span class="line"># 客户端连接的端口</span><br><span class="line">port&#x3D;9092</span><br><span class="line"># 可以接收的消息最大尺寸</span><br><span class="line">message.max.bytes&#x3D;10240000</span><br><span class="line">socket.send.buffer.bytes&#x3D;20480000</span><br><span class="line">socket.receive.buffer.bytes&#x3D;20480000</span><br><span class="line">socket.request.max.bytes&#x3D;1048576000</span><br><span class="line">num.partitions&#x3D;3</span><br><span class="line">num.recovery.threads.per.data.dir&#x3D;1</span><br><span class="line">log.retention.hours&#x3D;168</span><br><span class="line">log.segment.bytes&#x3D;1073741824</span><br><span class="line">log.flush.interval.messages&#x3D;10000</span><br><span class="line">log.flush.interval.ms&#x3D;60000</span><br><span class="line">log.retention.check.interval.ms&#x3D;300000</span><br><span class="line">log.cleaner.enable&#x3D;true</span><br><span class="line">log.cleanup.policy&#x3D;delete</span><br><span class="line">zookeeper.connection.timeout.ms&#x3D;6000</span><br><span class="line">auto.create.topics.enable&#x3D;false</span><br><span class="line">delete.topic.enable&#x3D;true</span><br><span class="line"># server用来处理网络请求的网络线程数目</span><br><span class="line">num.network.threads&#x3D;32</span><br><span class="line"># server用来处理请求的I&#x2F;O线程的数目</span><br><span class="line">num.io.threads&#x3D;64</span><br><span class="line">num.replica.fetchers&#x3D;8</span><br><span class="line">auto.leader.rebalance.enable&#x3D;true</span><br><span class="line"># 在网络线程停止读取新请求之前，可以排队等待I&#x2F;O线程处理的最大请求个数</span><br><span class="line">queued.max.requests&#x3D;5000</span><br><span class="line">replica.fetch.max.bytes&#x3D;10240000</span><br><span class="line">controller.socket.timeout.ms&#x3D;30000</span><br><span class="line">controlled.shutdown.enable&#x3D;true</span><br></pre></td></tr></table></figure>

<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.cnblogs.com/gxc2015/p/9835837.html" target="_blank" rel="noopener">kafka配置参数详解</a></li>
</ul>
]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
  </entry>
  <entry>
    <title>kafka offset 机制</title>
    <url>/2019/08/18/Kafka-offset/</url>
    <content><![CDATA[<p>kafka offset 机制</p>
<hr>
<h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><p>什么是 offset？</p>
<p>offset 是 consumer position，Topic 的每个 Partition 都有各自的 offset</p>
<p>消费者需要自己保留一个 offset，从 kafka 获取消息时，只拉去当前 offset 以后的消息</p>
<p>Kafka 的 scala/java 版的 client 已经实现了这部分的逻辑</p>
<p>之前 offset 保存到 zookeeper 上，broker 存放 offset 是 kafka 从 0.9 版本开始，提供的新的消费方式原因是zookeeper来存放，还是有许多弊端，不方便灵活控制，效率不高</p>
<h4 id="offset-记录位置"><a href="#offset-记录位置" class="headerlink" title="offset 记录位置"></a>offset 记录位置</h4><p>kafka 消费者在会保存其消费的进度，也就是offset，存储的位置根据选用的 kafka api 不同而不同。具体可以参看<a href="https://blog.csdn.net/u013063153/article/details/78122088" target="_blank" rel="noopener">kafka 消费者offset记录位置和方式</a></p>
<h4 id="offset-更新方式"><a href="#offset-更新方式" class="headerlink" title="offset 更新方式"></a>offset 更新方式</h4><ul>
<li><p>自动提交，设置 enable.auto.commit=true，更新的频率根据参数【auto.commit.interval.ms】来定。这种方式也被称为【at most once】，fetch 到消息后就可以更新offset，无论是否消费成功</p>
</li>
<li><p>手动提交，设置 enable.auto.commit=false，这种方式称为【at least once】。fetch 到消息后，等消费完成再调用方法【consumer.commitSync()】，手动更新offset；如果消费失败，则 offset 也不会更新，此条消息会被重复消费一次</p>
</li>
</ul>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/u012129558/article/details/80075270" target="_blank" rel="noopener">Kafka的offset管理</a></li>
<li><a href="https://blog.csdn.net/u013063153/article/details/78122088" target="_blank" rel="noopener">kafka 消费者offset记录位置和方式</a></li>
<li><a href="https://blog.csdn.net/camel84/article/details/82433075" target="_blank" rel="noopener">kafka系列-进阶篇之消息和offset存储</a></li>
<li><a href="https://www.cnblogs.com/FG123/p/10091599.html" target="_blank" rel="noopener">Kafka提交offset机制</a></li>
</ul>
]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
  </entry>
  <entry>
    <title>ulimit 未生效</title>
    <url>/2019/08/16/ulimit-Invalid/</url>
    <content><![CDATA[<p>ulimit 未生效</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>在安装启动 elasticsearch 时遇些问题。</p>
<p><code>max file descriptors [65535] for elasticsearch process is too low, increase to at least [65536]</code></p>
<p>根据搜索到的资料<a href="https://www.jianshu.com/p/2285f1f8ec21" target="_blank" rel="noopener">elasticsearch启动常见的几个报错</a>，我尝试切换到root用户，编辑limits.conf添加如下内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim &#x2F;etc&#x2F;security&#x2F;limits.conf</span><br><span class="line">* soft nofile 128000</span><br><span class="line">* hard nofile 128000</span><br></pre></td></tr></table></figure>

<p>然后又切回普通账号来启动ES，发现还是报错。。(ES6.x之后为了安全只能普通账号启动)</p>
<p>Orz，又开启了排查问题之路</p>
<h4 id="排查"><a href="#排查" class="headerlink" title="排查"></a>排查</h4><p>在 root 账号下执行 <code>ulimit -n</code> 输出 128000</p>
<p>但是我在 普通账号下执行 <code>ulimit -n</code> 输出 65535</p>
<p>额。说明普通账号未生效，百思不解。</p>
<p>尝试依靠搜索引擎寻求帮助，找到<a href="https://blog.csdn.net/weixin_34331102/article/details/92789331" target="_blank" rel="noopener">limits.conf不生效问题</a>也没解决问题</p>
<p>寻求 Sa 同学帮忙之后了解到这个<code>/etc/security/limits.d/90-nofile.conf</code>文件设置会覆盖全局设置。</p>
<p>将该文件进行修改</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim &#x2F;etc&#x2F;security&#x2F;limits.d&#x2F;90-nofile.conf</span><br><span class="line">* soft nofile 128000</span><br><span class="line">* hard nofile 128000</span><br></pre></td></tr></table></figure>

<p><code>ulimit -n</code> 再查看发现已修改</p>
<p>完结、撒花</p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>Flink-Kafka 内置 Schemas</title>
    <url>/2019/08/16/Flink-Kafka-deserialization/</url>
    <content><![CDATA[<p>Apache Flink 内部提供了内置的常用消息格式的 Schemas</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>Flink 消费 Kafka 时，要对消息进行格式化</p>
<h4 id="Schemas"><a href="#Schemas" class="headerlink" title="Schemas"></a>Schemas</h4><p>Flink 有提供内置的 Schemas</p>
<p>先看 Flink 中初始化 Kafka 数据源代码，其中传入服务器名和 Topic 名就可以了</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Properties props &#x3D; new Properties();</span><br><span class="line">props.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);</span><br><span class="line">FlinkKafkaConsumer010&lt;String&gt; consumer &#x3D; new FlinkKafkaConsumer010&lt;&gt;(</span><br><span class="line">    &quot;flink_test&quot;, new SimpleStringSchema(), props);</span><br><span class="line">DataStream&lt;String&gt; stream &#x3D; env.addSource(consumer);</span><br></pre></td></tr></table></figure>

<p>上方 SimpleStringSchema 即为 Schemas。如果需要使用其他的，替换掉该参数，并把相应数据类型进行修改</p>
<p>如果想自己实现 Schema ，可以参看<a href="https://juejin.im/post/5c8fd4dd5188252d92095995#heading-8" target="_blank" rel="noopener">Apache-Flink深度解析-DataStream-Connectors之Kafka</a> Simple ETL 部分，<br>与 SimpleStringSchema 是一样的效果，只是自己实现的 Schema</p>
<p>如果类找不到，请添加依赖</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;flink-avro&lt;&#x2F;artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.8.0&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>

<h5 id="SimpleStringSchema"><a href="#SimpleStringSchema" class="headerlink" title="SimpleStringSchema"></a>SimpleStringSchema</h5><p>SimpleStringSchema 把 message 反序列化为字符串。如果 message 有键, 则忽略键</p>
<h5 id="JSONDeserializationSchema"><a href="#JSONDeserializationSchema" class="headerlink" title="JSONDeserializationSchema"></a>JSONDeserializationSchema</h5><p>JSONDeserializationSchema 使用 jackson 将 message 反序列化为 json 格式的消息并返回 com.fasterxml.jackson.databind.node.ObjectNode 对象流。你可以使用 .get(“property”) 方法访问字段。再一次, 键被忽略。</p>
<h5 id="JSONKeyValueDeserializationSchema"><a href="#JSONKeyValueDeserializationSchema" class="headerlink" title="JSONKeyValueDeserializationSchema"></a>JSONKeyValueDeserializationSchema</h5><p>JSONKeyValueDeserializationSchema 与前一个非常类似，但处理带有json编码的键和值的消息</p>
<p>返回的 ObjectNode 包含如下字段:</p>
<p>key: 键中存在的所有字段<br>value: 所有的 message 字段<br>metadata（可选）: 暴露消息的 offset, partition 和 topic（将 true 传递给构造函数以获取元数据）</p>
<p>例如:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kafka-console-producer --broker-list localhost:9092 --topic json-topic \</span><br><span class="line">    --property parse.key&#x3D;true \</span><br><span class="line">    --property key.separator&#x3D;|</span><br><span class="line">&#123;&quot;keyField1&quot;: 1, &quot;keyField2&quot;: 2&#125; | &#123;&quot;valueField1&quot;: 1, &quot;valueField2&quot; : &#123;&quot;foo&quot;: &quot;bar&quot;&#125;</span><br></pre></td></tr></table></figure>

<p>会被解码为</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;key&quot;:&#123;&quot;keyField1&quot;:1,&quot;keyField2&quot;:2&#125;,</span><br><span class="line">    &quot;value&quot;:&#123;&quot;valueField1&quot;:1,&quot;valueField2&quot;:&#123;&quot;foo&quot;:&quot;bar&quot;&#125;&#125;,</span><br><span class="line">    &quot;metadata&quot;:&#123;</span><br><span class="line">        &quot;offset&quot;:43,</span><br><span class="line">        &quot;topic&quot;:&quot;json-topic&quot;,</span><br><span class="line">        &quot;partition&quot;:0</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如果消息本身就没有 key、metadata。JSONKeyValueDeserializationSchema 解析出来也是没有key、value</p>
<h5 id="AvroDeserializationSchema"><a href="#AvroDeserializationSchema" class="headerlink" title="AvroDeserializationSchema"></a>AvroDeserializationSchema</h5><p>使用静态提供的模式读取使用 Avro 格式序列化的数据</p>
<p>可以从 Avro 生成的类（AvroDeserializationSchema.forSpecific（…））推断出模式，或者它可以与 GenericRecords 一起使用手动提供的模式（使用AvroDeserializationSchema.forGeneric（…））</p>
<h5 id="TypeInformationSerializationSchema"><a href="#TypeInformationSerializationSchema" class="headerlink" title="TypeInformationSerializationSchema"></a>TypeInformationSerializationSchema</h5><p>TypeInformationSerializationSchema (and TypeInformationKeyValueSerializationSchema) 它基于Flink的TypeInformation创建模式。 如果数据由Flink写入和读取，这将非常有用</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>AvroDeserializationSchema 与 TypeInformationSerializationSchema </p>
<p>还没接触过，这里就暂时记录下，如果下次有用到再回来记录</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://juejin.im/post/5c8fd4dd5188252d92095995#heading-5" target="_blank" rel="noopener">Apache-Flink深度解析-DataStream-Connectors之Kafka</a></li>
<li><a href="https://ohmycloud.github.io/2019/05/15/apache-flink-built-in-deserialization-schemas-md/" target="_blank" rel="noopener">Flink-Kafka 内置的反序列化 schemas</a></li>
<li><a href="https://riptutorial.com/zh-CN/apache-flink/example/27995/%E5%86%85%E7%BD%AE%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96%E6%A8%A1%E5%BC%8F" target="_blank" rel="noopener">apache-flink内置反序列化模式</a></li>
<li><a href="https://stackoverflow.com/questions/39300183/flink-kafka-json-java-example" target="_blank" rel="noopener">Flink + Kafka + JSON - java example</a></li>
</ul>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
  </entry>
  <entry>
    <title>HBase 参数设置参考</title>
    <url>/2019/08/15/HBase-tuning/</url>
    <content><![CDATA[<p>HBase 参数设置参考</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>HBase 配置参数极其繁多，参数配置可能会影响到 HBase 性能问题，因此得好好总结下。</p>
<p>HBase 调优是个技术活。得结合多年生产经验加测试环境下性能测试得出。</p>
<ol>
<li><p>JVM垃圾回收优化</p>
</li>
<li><p>本地 memstore 分配缓存优化</p>
</li>
<li><p>Region 拆分优化</p>
</li>
<li><p>Region 合并优化</p>
</li>
<li><p>Region 预先加载优化</p>
</li>
<li><p>负载均衡优化</p>
</li>
<li><p>启用压缩，推荐snappy</p>
</li>
<li><p>进行预分区，从而避免自动 split，提高 HBase 响应速度</p>
</li>
<li><p>避免出现 region 热点现象，启动按照 table 级别进行 balance</p>
</li>
</ol>
<h4 id="GC"><a href="#GC" class="headerlink" title="GC"></a>GC</h4><p><a href="https://lihuimintu.github.io/2020/01/18/HBase-CMS/" target="_blank" rel="noopener">HBase CMS GC 配置参考</a></p>
<h4 id="RS-参数"><a href="#RS-参数" class="headerlink" title="RS 参数"></a>RS 参数</h4><p><strong>hbase.server.thread.wakefrequency</strong></p>
<p>该值默认是 10 秒，它影响着 Flush 和 Compaction</p>
<p>FlushHandler 从队列 flushQueue 取出需要刷新的请求，从队列里取请求超时时间是该参数</p>
<p>Compaction 后台定期线程检查也是该参数 </p>
<h4 id="BlockCache"><a href="#BlockCache" class="headerlink" title="BlockCache"></a>BlockCache</h4><p><strong>hfile.block.cache.size</strong><br>表示 BlockCache 占 hbase_heapsize 比例，参数默认 0.4<br>一个 RS 上有一个 BlockCache 和多个 MemStore，它们的大小之和建议小于 heapsize * 0.8，<br>否则 HBase 可能不能启动，因为仍然要留有一些内存保证其它任务的执行。</p>
<h4 id="flush-参数"><a href="#flush-参数" class="headerlink" title="flush 参数"></a>flush 参数</h4><p><strong>hbase.regionserver.global.memstore.size</strong><br>对应旧参数<code>hbase.regionserver.global.memstore.upperLimit</code><br>表示 RS 全局 MemStore 占 hbase_heapsize 最大占比，参数默认 0.4<br>一个 RS 上有一个 BlockCache 和N个 MemStore，它们的大小之和建议小于 heapsize * 0.8，<br>否则 HBase 可能不能启动，因为仍然要留有一些内存保证其它任务的执行。</p>
<p><strong>hbase.hregion.memstore.block.multiplier</strong></p>
<p>写请求达到 HRegion 后，HRegion 首先会加行锁，然后进行 checkResource 操作，在 checkResource 操作里主要检查 memstoreSize 是否大于 blockingMemStoreSize，其中 blockingMemStoreSize 由等于 memstoreFlushSize *<br>hbase.hregion.memstore.block.multiplier，该参数默认值是 4</p>
<p>hbase.hregion.memstore.block.multiplier 设置的太大在写入量大的时候很可能会导致机器内存耗尽而引发 OutofMem 错误</p>
<p><strong>hbase.hregion.memstore.flush.size</strong></p>
<p>上文提到的 memstoreFlushSize 大小由参数 hbase.hregion.memstore.flush.size 指定，该值默认值是 128M</p>
<p><strong>hbase.hstore.flusher.count</strong></p>
<p>MemStoreFlusher 的职责是接受 HRegion 的刷新请求并调度该请求，在 MemStoreFlusher 内部有一个 flushQueue 队列，消费此队列的是 FlushHandler，FlushHandler 的个数由参数 hbase.hstore.flusher.count 设定, 默认值是 2</p>
<p>如果线程个数较少，MemStore 刷新将排队。对于更多的线程个数，刷新将并行执行，从而增加 HDFS 负载。这也会导致更多的 compact。</p>
<p><strong>hbase.hstore.blockingStoreFiles</strong></p>
<p>在HFile的数量过多的时候会限制写请求的速度，如果过多就会阻塞 flush 而进行 compact 操作并阻塞一定时间后才进行 flush 操作</p>
<p>阻塞文件参数由参数 hbase.hstore.blockingStoreFiles 控制该值默认为 7，阻塞时间由参数 hbase.hstore.blockingWaitTime 控制，该值默认为 90000 毫秒</p>
<p><strong>hbase.hstore.blockingWaitTime</strong></p>
<p>值默认为 90000 毫秒，上文有提参数作用</p>
<h4 id="Split-参数"><a href="#Split-参数" class="headerlink" title="Split 参数"></a>Split 参数</h4><p><strong>hbase.regionserver.region.split.policy</strong></p>
<p>HBase 的分裂策略可以通过表的属性 SPLIT_POLICY 指定，也可以通过 hbase-site.xml 全局指派，参数为 hbase.regionserver.region.split.policy，默认为IncreasingToUpperBoundRegionSplitPolicy。</p>
<p><strong>hbase.regionserver.thread.split</strong></p>
<p>执行 split 的线程数，默认为 1</p>
<p><strong>hbase.regionserver.regionSplitLimit</strong></p>
<p>当前 regionserver 的 region 个数最大值，如果当前 regionserver 的 region 个数超过该值，那么将不会在进行 split 操作。默认值 1000</p>
<h4 id="Compaction-参数"><a href="#Compaction-参数" class="headerlink" title="Compaction 参数"></a>Compaction 参数</h4><p>Compaction 的主要目的</p>
<ul>
<li>将多个HFile 合并为较大HFile，从而提高查询性能</li>
<li>减少HFile 数量，减少小文件对 HDFS 影响</li>
<li>提高 Region 初始化速度</li>
</ul>
<p><strong>hbase.hstore.compaction.min</strong></p>
<p>当某个列族下的 HFile 文件数量超过这个值，则会触发 minor compaction 操作 默认是3，比较小，建议设置 10-15<br>设置过小会导致合并文件太频繁，特别是频繁 bulkload 或者数据量比较大的情况下 设置过大又会导致一个列族下面的 HFile 数量比较多，影响查询效率</p>
<p><strong>hbase.hstore.compaction.max</strong></p>
<p>一次最多可以合并多少个HFile，默认为 10 限制某个列族下面选择最多可选择多少个文件来进行合并<br>注意需要满足条件<code>hbase.hstore.compaction.max</code> &gt; <code>hbase.hstore.compaction.min</code></p>
<p><strong>hbase.hstore.compaction.max.size</strong></p>
<p>默认 Long 最大值，minor_compact 时 HFile 大小超过这个值则不会被选中合并<br>用来限制防止过大的 HFile 被选中合并，减少写放大以及提高合并速度</p>
<p><strong>hbase.hstore.compaction.min.size</strong></p>
<p>默认 memstore 大小，minor_compact 时 HFile 小于这个值，则一定会被选中<br>可用来优化尽量多的选择合并小的文件</p>
<p><strong>hbase.regionserver.thread.compaction.small</strong></p>
<p>默认1，每个RS的 minor compaction线程数，其实不是很准确，这个线程主要是看参与合并的 HFile 数据量 有可能 minor compaction 数据量较大会使用 compaction.large 提高线程可提高 HFile 合并效率</p>
<p><strong>hbase.regionserver.thread.compaction.large</strong></p>
<p>默认1，每个RS的 major compaction线程数，其实不是很准确，这个线程主要是看参与合并的 HFile 数据量 有可能 minor compaction 数据量较大会使用 compaction.large 提高线程可提高 HFile 合并效率</p>
<p><strong>hbase.hregion.majorcompaction</strong></p>
<p>默认604800000, 单位是毫秒，即7天。major compaction 间隔</p>
<p>设置为0即关闭hbase major compaction，改为业务低谷手动执行。<a href="https://lihuimintu.github.io/2019/06/09/HBase-timing-major-compaction/" target="_blank" rel="noopener">HBase 自动大合并脚本</a></p>
<p><strong>hbase.hregion.majorcompaction.jetter</strong></p>
<p>Major compaction 抖动参数，默认值0.5。这个参数是为了避免major compaction同时在各个regionserver上同时发生，避免此操作给集群带来很大压力。 这样节点major compaction就会在 + 或 - 两者乘积的时间范围内随机发生。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/uMDoSnsbcqznCvSQCW5_yA" target="_blank" rel="noopener">HBase调优 HBase Compaction参数调优</a></li>
<li><a href="https://cloud.tencent.com/developer/article/1005586" target="_blank" rel="noopener">Hbase Region Split compaction 过程分析以及调优</a></li>
<li><a href="https://blog.csdn.net/u011598442/article/details/90632702" target="_blank" rel="noopener">深入理解 HBase Compaction 机制</a></li>
<li><a href="https://www.cnblogs.com/hixiaowei/p/11936325.html" target="_blank" rel="noopener">hfile.block.cache.size - hbase调优</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>测试故障模拟</title>
    <url>/2019/08/14/fault-simulation/</url>
    <content><![CDATA[<p>针对一定场景实现故障现场进行模拟测试</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>与测试同事沟通时，看到一篇测试同学写的故障模拟文档。本着 SRE 精神，有时候自己也需要模拟下故障场景。<br>尤其是模拟应急故障演练时特别需要</p>
<h4 id="打满-CPU"><a href="#打满-CPU" class="headerlink" title="打满 CPU"></a>打满 CPU</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"> </span><br><span class="line">cat &lt;&lt; EOF &gt; &#x2F;tmp&#x2F;infiniteburn.sh</span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">while true;</span><br><span class="line">    do openssl speed;</span><br><span class="line">done</span><br><span class="line">EOF</span><br><span class="line"> </span><br><span class="line">for i in &#123;1..32&#125;</span><br><span class="line">do</span><br><span class="line">    nohup &#x2F;bin&#x2F;bash &#x2F;tmp&#x2F;infiniteburn.sh &amp;</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<p><code>openssl speed</code>是用来测试加密算法性能的，是一种CPU密集型计算，运行一个脚本只会打满一个CPU</p>
<p><code>for i in {1..32}</code>用来执行32次/tmp/infiniteburn.sh脚本，32是假设当前机器的内核个数不会超过32，若超过修改一下这个数值即可</p>
<h4 id="打满-IO"><a href="#打满-IO" class="headerlink" title="打满 IO"></a>打满 IO</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"> </span><br><span class="line">cat &lt;&lt; EOF &gt; &#x2F;tmp&#x2F;loopburnio.sh</span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">while true;</span><br><span class="line">do</span><br><span class="line">    dd if&#x3D;&#x2F;dev&#x2F;urandom of&#x3D;&#x2F;burn bs&#x3D;1M count&#x3D;1024 iflag&#x3D;fullblock</span><br><span class="line">done</span><br><span class="line">EOF</span><br><span class="line"> </span><br><span class="line">nohup &#x2F;bin&#x2F;bash &#x2F;tmp&#x2F;loopburnio.sh &amp;</span><br></pre></td></tr></table></figure>

<p>dd命令用于读取、转换并输出数据<br>dd可以从标准输入或文件中读取数据，根据指定的格式来转换数据再输出到文件、设备或标准输出</p>
<p><code>dd if=/dev/urandom of=/burn bs=1M count=1024 iflag=fullblock</code><br>这条命令的意思是采用dd工具模拟读写。if指定输入的文件名，of指定输出的文件名，bs同时设置读写块的大小为1M，count是指仅拷贝1024个块，块大小等于bs指定的字节数。iflag=fullblock表示堆积满block</p>
<h5 id="paping工具"><a href="#paping工具" class="headerlink" title="paping工具"></a>paping工具</h5><p>通过该工具可以查看模拟网络故障的效果如何</p>
<p>paping 可以在 Linux 平台上测试网络的连通性以及网络延迟等</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget http:&#x2F;&#x2F;www.updateweb.cn&#x2F;softwares&#x2F;paping_1.5.5_x86-64_linux.tar.gz</span><br><span class="line">tar -zvxf paping_1.5.5_x86_linux.tar.gz</span><br><span class="line">.&#x2F;paping -p 80 -c 5000 www.baidu.com</span><br></pre></td></tr></table></figure>

<p>如遇到<code>./paping: error while loading shared libraries: libstdc++.so.6: cannot open shared object file: No such file or directory</code><br>错误，安装对应库来解决<code>yum -y install libstdc++6</code> <code>yum -y install lib32stdc++6</code></p>
<p>使用方式</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Syntax: paping [options] destination</span><br><span class="line"> </span><br><span class="line">Options:</span><br><span class="line"> -?, --help     display usage</span><br><span class="line"> -p, --port N   set TCP port N (required)                       &#x2F;&#x2F; 指定被测试服务的TCP端口（必须）</span><br><span class="line">     --nocolor  Disable color output                            &#x2F;&#x2F; 屏蔽彩色输出</span><br><span class="line"> -t, --timeout  timeout in milliseconds (default 1000)          &#x2F;&#x2F; 指定超时时长，单位为毫秒，默认为1000</span><br><span class="line"> -c, --count N  set number of checks to N                       &#x2F;&#x2F; 指定测试次</span><br></pre></td></tr></table></figure>

<p>示例</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@100.73.12.12 miaold]#  .&#x2F;paping -p 80 -c 5 www.baidu.com  </span><br><span class="line">paping v1.5.5 - Copyright (c) 2011 Mike Lovell</span><br><span class="line"> </span><br><span class="line">Connecting to www.a.shifen.com [180.97.33.107] on TCP 80:</span><br><span class="line"> </span><br><span class="line">Connected to 180.97.33.107: time&#x3D;0.23ms protocol&#x3D;TCP port&#x3D;80</span><br><span class="line">Connected to 180.97.33.107: time&#x3D;0.24ms protocol&#x3D;TCP port&#x3D;80</span><br><span class="line">Connected to 180.97.33.107: time&#x3D;0.26ms protocol&#x3D;TCP port&#x3D;80</span><br><span class="line">Connected to 180.97.33.107: time&#x3D;0.24ms protocol&#x3D;TCP port&#x3D;80</span><br><span class="line">Connected to 180.97.33.107: time&#x3D;0.19ms protocol&#x3D;TCP port&#x3D;80</span><br><span class="line"> </span><br><span class="line">Connection statistics:</span><br><span class="line">        Attempted &#x3D; 5, Connected &#x3D; 5, Failed &#x3D; 0 (0.00%)</span><br><span class="line">Approximate connection times:</span><br><span class="line">        Minimum &#x3D; 0.19ms, Maximum &#x3D; 0.26ms, Average &#x3D; 0.23ms</span><br></pre></td></tr></table></figure>

<p>可以看到平均连接时间为0.23ms</p>
<h4 id="网络延迟"><a href="#网络延迟" class="headerlink" title="网络延迟"></a>网络延迟</h4><p>该命令将网卡eth0的传输设置为延迟300ms发送</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tc qdisc add dev eth0 root netem delay 300ms</span><br></pre></td></tr></table></figure>

<p>该命令将 eth0 网卡的传输设置为延迟 300ms ± 50ms (250 ~ 350 ms 之间的任意值)发送 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tc qdisc add dev eth0 root netem delay 300ms 50ms</span><br></pre></td></tr></table></figure>

<p>对应的删除命令为<code>tc qdisc del dev eth0 root netem xxxxxx</code></p>
<h4 id="网络中断"><a href="#网络中断" class="headerlink" title="网络中断"></a>网络中断</h4><p>该命令将 eth0 网卡的传输设置为随机产生 10% 的损坏的数据包</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tc qdisc add dev eth0 root netem corrupt 10%</span><br></pre></td></tr></table></figure>

<h4 id="网络丢包"><a href="#网络丢包" class="headerlink" title="网络丢包"></a>网络丢包</h4><p>该命令将 eth0 网卡的传输设置为随机丢掉7%的数据包, 成功率为25%；如果不加上后面的25%，那么一次就是随机丢掉7%的数据包</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tc qdisc add dev eth0 root netem loss 7% 25%</span><br></pre></td></tr></table></figure>

<p>机器假死、进程假死和线程假死时，其上游调用所接受到的状态分别为:</p>
<p>机器假死 - timeout<br>进程假死 - connection refused<br>线程假死 - timeout  </p>
<h4 id="模拟-timeout"><a href="#模拟-timeout" class="headerlink" title="模拟 timeout"></a>模拟 timeout</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">iptables -A OUTPUT -p tcp --dport 6666 -j DROP</span><br></pre></td></tr></table></figure>

<h4 id="模拟-connection-refused"><a href="#模拟-connection-refused" class="headerlink" title="模拟 connection refused"></a>模拟 connection refused</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">iptables -A OUTPUT -p tcp --dport 7777 -j REJECT</span><br></pre></td></tr></table></figure>

<p>对应的删除命令为</p>
<p>iptables -D xxxxxx</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/lym152898/article/details/83306993" target="_blank" rel="noopener">使用cat和EOF添加多行数据</a></li>
</ul>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>HBase 预分区</title>
    <url>/2019/08/09/HBase-Prepartition/</url>
    <content><![CDATA[<p>HBase 提供了预分区功能，即用户可以在创建表的时候对表按照一定的规则分区</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>HBase 存在热点问题。热点问题首选解决方法是 Rowkey 的散列与预分区设计</p>
<p>Rowkey 散列可以参看<a href="https://lihuimintu.github.io/2019/08/07/HBase-Rowkey/" target="_blank" rel="noopener">HBase Rowkey 设计指南</a></p>
<h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><p><strong>什么是预分区？</strong></p>
<p>初次接触HBase的玩家，在创建HBase表的时候，不指分区的数目，另外就是 rowkey 设计不合理，导致热点。</p>
<p>另外随着表越来越大，表将会进行split，分裂为2个分区。表在进行split的时候，会耗费大量的资源，频繁的分区对HBase的性能有巨大的影响。</p>
<p>HBase提供了预分区功能，即用户可以在创建表的时候对表按照一定的规则分区。</p>
<p><strong>预分区的目的是什么？</strong></p>
<p>减少由于 region split 带来的资源消耗。从而提高HBase的性能。</p>
<h4 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h4><h5 id="shell"><a href="#shell" class="headerlink" title="shell"></a>shell</h5><p>通过 HBase shell 来创建</p>
<p>在命令中指定分区的 Rowkey</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create &#39;t1&#39;, &#39;f1&#39;, SPLITS &#x3D;&gt; [&#39;10&#39;, &#39;20&#39;, &#39;30&#39;, &#39;40&#39;]</span><br><span class="line"></span><br><span class="line">create &#39;t1&#39;, &#123;NAME &#x3D;&gt;&#39;f1&#39;, TTL &#x3D;&gt; 180&#125;, SPLITS &#x3D;&gt; [&#39;10&#39;, &#39;20&#39;, &#39;30&#39;, &#39;40&#39;]</span><br><span class="line"></span><br><span class="line">create &#39;t1&#39;, &#123;NAME &#x3D;&gt;&#39;f1&#39;, TTL &#x3D;&gt; 180&#125;, &#123;NAME &#x3D;&gt; &#39;f2&#39;, TTL &#x3D;&gt; 240&#125;, SPLITS &#x3D;&gt; [&#39;10&#39;, &#39;20&#39;, &#39;30&#39;, &#39;40&#39;]</span><br></pre></td></tr></table></figure>

<p>还可以通过 HBase shell 指定文件来创建</p>
<p>在任意路径下创建一个保存分区 key 的文件，一行代表一个 Rowkey</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create &#39;t1&#39;, &#39;f1&#39;, SPLITS_FILE &#x3D;&gt; &#39;&#x2F;home&#x2F;hadmin&#x2F;hbase-1.3.1&#x2F;txt&#x2F;splits.txt&#39;</span><br><span class="line"></span><br><span class="line">create &#39;t1&#39;, &#123;NAME &#x3D;&gt;&#39;f1&#39;, TTL &#x3D;&gt; 180&#125;, SPLITS_FILE &#x3D;&gt; &#39;&#x2F;home&#x2F;hadmin&#x2F;hbase-1.3.1&#x2F;txt&#x2F;splits.txt&#39;</span><br><span class="line"></span><br><span class="line">create &#39;t1&#39;, &#123;NAME &#x3D;&gt;&#39;f1&#39;, TTL &#x3D;&gt; 180&#125;, &#123;NAME &#x3D;&gt; &#39;f2&#39;, TTL &#x3D;&gt; 240&#125;, SPLITS_FILE &#x3D;&gt; &#39;&#x2F;home&#x2F;hadmin&#x2F;hbase-1.3.1&#x2F;txt&#x2F;splits.txt&#39;</span><br></pre></td></tr></table></figure>

<p>通过 HBase 自带三种 pre-split 的算法，分别是 <code>HexStringSplit</code>、<code>DecimalStringSplit</code> 和 <code>UniformSplit</code></p>
<p>各种Split算法适用场景</p>
<ul>
<li>HexStringSplit: rowkey是十六进制的字符串作为前缀的。<a href="https://www.jianshu.com/p/d67a58eed1f1" target="_blank" rel="noopener">HexStringSplit预分区表</a></li>
<li>DecimalStringSplit: rowkey是10进制数字字符串作为前缀的</li>
<li>UniformSplit: rowkey前缀完全随机</li>
</ul>
<h5 id="api"><a href="#api" class="headerlink" title="api"></a>api</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">package api;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.hbase.HBaseConfiguration;</span><br><span class="line">import org.apache.hadoop.hbase.HColumnDescriptor;</span><br><span class="line">import org.apache.hadoop.hbase.HTableDescriptor;</span><br><span class="line">import org.apache.hadoop.hbase.TableName;</span><br><span class="line">import org.apache.hadoop.hbase.client.Admin;</span><br><span class="line">import org.apache.hadoop.hbase.client.Connection;</span><br><span class="line">import org.apache.hadoop.hbase.client.ConnectionFactory;</span><br><span class="line">import org.apache.hadoop.hbase.util.Bytes;</span><br><span class="line"></span><br><span class="line">public class create_table_sample2 &#123;</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        Configuration conf &#x3D; HBaseConfiguration.create();</span><br><span class="line">        conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;192.168.1.80,192.168.1.81,192.168.1.82&quot;);</span><br><span class="line">        Connection connection &#x3D; ConnectionFactory.createConnection(conf);</span><br><span class="line">        Admin admin &#x3D; connection.getAdmin();</span><br><span class="line"></span><br><span class="line">        TableName table_name &#x3D; TableName.valueOf(&quot;TEST1&quot;);</span><br><span class="line">        if (admin.tableExists(table_name)) &#123;</span><br><span class="line">            admin.disableTable(table_name);</span><br><span class="line">            admin.deleteTable(table_name);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        HTableDescriptor desc &#x3D; new HTableDescriptor(table_name);</span><br><span class="line">        HColumnDescriptor family1 &#x3D; new HColumnDescriptor(constants.COLUMN_FAMILY_DF.getBytes());</span><br><span class="line">        family1.setTimeToLive(3 * 60 * 60 * 24);     &#x2F;&#x2F;过期时间</span><br><span class="line">        family1.setMaxVersions(3);                   &#x2F;&#x2F;版本数</span><br><span class="line">        desc.addFamily(family1);</span><br><span class="line"></span><br><span class="line">        byte[][] splitKeys &#x3D; &#123;</span><br><span class="line">            Bytes.toBytes(&quot;row01&quot;),</span><br><span class="line">            Bytes.toBytes(&quot;row02&quot;)</span><br><span class="line">        &#125;;</span><br><span class="line"></span><br><span class="line">        admin.createTable(desc, splitKeys);</span><br><span class="line">        admin.close();</span><br><span class="line">        connection.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.cnblogs.com/quchunhui/p/7543385.html" target="_blank" rel="noopener">HBase预分区方法</a></li>
<li><a href="https://www.cnblogs.com/bdifn/p/3801737.html" target="_blank" rel="noopener">HBase Rowkey的散列与预分区设计</a></li>
<li><a href="https://help.aliyun.com/document_detail/71787.html?spm=a2c6h.13066369.0.0.7aec487crCTfhS" target="_blank" rel="noopener">预分区</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>Yarn 认识</title>
    <url>/2019/08/09/Yarn/</url>
    <content><![CDATA[<p>Apache Hadoop YARN 是一种新的 Hadoop 资源管理器，它是一个通用资源管理系统，可为上层应用提供统一的资源管理和调度</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>Yarn 在 Hadoop 生态圈中起到一个资源调度的作用，全文摘自以下链接拼凑出来</p>
<ul>
<li><a href="https://www.cnblogs.com/znicy/p/6714039.html" target="_blank" rel="noopener">YARN学习总结</a></li>
<li><a href="https://www.ibm.com/developerworks/cn/data/library/bd-yarn-intro/index.html" target="_blank" rel="noopener">YARN 简介</a></li>
<li><a href="https://blog.csdn.net/suifeng3051/article/details/49486927" target="_blank" rel="noopener">Hadoop Yarn详解</a></li>
</ul>
<h4 id="出身"><a href="#出身" class="headerlink" title="出身"></a>出身</h4><p>Hadoop1.x 中 JobTracker 具有两种不同的职责</p>
<ol>
<li>ResourceManagement 资源管理</li>
<li>JobScheduling/JobMonitoring 任务调度监控</li>
</ol>
<p>为单个进程安排大量职责会导致重大的可伸缩性问题，尤其是在较大的集群上，JobTracker 必须不断跟踪数千个 TaskTracker、数百个作业，以及数万个 map 和 reduce 任务。<br>相反，TaskTracker 通常近运行十来个任务，这些任务由勤勉的 JobTracker 分配给它们。</p>
<p>大型 Apache Hadoop 集群 (MRv1) 上繁忙的 JobTracker</p>
<p><img src="/images/blog/2020-01-03-1.png" alt></p>
<p>Hadoop2.x Yarn 的目标是将这两部分功能分开，也就是分别用两个进程来管理这两个任务</p>
<ol>
<li>ResourceManger</li>
<li>ApplicationMaster</li>
</ol>
<p>需要注意的是，在Yarn中把 job 的概念换成了 application</p>
<h4 id="组件"><a href="#组件" class="headerlink" title="组件"></a>组件</h4><ol>
<li>ResourceManager：Global（全局）的进程 </li>
<li>NodeManager：运行在每个节点上的进程</li>
<li>ApplicationMaster：Application-specific（应用级别）的进程</li>
<li><em>Scheduler：是ResourceManager的一个组件</em></li>
<li><em>Container：节点上一组CPU和内存资源</em></li>
</ol>
<p><img src="/images/blog/2020-01-03-2.png" alt></p>
<p><strong>Container</strong>是Yarn对计算机计算资源的抽象，它其实就是一组CPU和内存资源，所有的应用都会运行在Container中。<strong>ApplicationMaster</strong>是对运行在Yarn中某个应用的抽象，它其实就是某个类型应用的实例，ApplicationMaster是应用级别的，它的主要功能就是向 <strong>ResourceManager</strong>（全局的）申请计算资源（Containers）并且和 <strong>NodeManager</strong> 交互来执行和监控具体的task。<strong>Scheduler</strong>是ResourceManager专门进行资源管理的一个组件，负责分配NodeManager上的Container资源，NodeManager也会不断发送自己Container使用情况给ResourceManager</p>
<p>各组件详解翻阅开头推荐链接</p>
<h4 id="提交作业"><a href="#提交作业" class="headerlink" title="提交作业"></a>提交作业</h4><p>Application 在 Yarn 中的执行过程，整个执行过程可以总结为三步</p>
<ol>
<li>应用程序提交</li>
<li>启动应用的ApplicationMaster实例</li>
<li>ApplicationMaster实例管理应用程序的执行</li>
</ol>
<p><img src="/images/blog/2020-01-03-3.png" alt></p>
<ol>
<li><p>客户端程序向ResourceManager提交应用并请求一个ApplicationMaster实例</p>
</li>
<li><p>ResourceManager找到可以运行一个Container的NodeManager，并在这个Container中启动ApplicationMaster实例</p>
</li>
<li><p>ApplicationMaster向ResourceManager进行注册，注册之后客户端就可以查询ResourceManager获得自己ApplicationMaster的详细信息，以后就可以和自己的ApplicationMaster直接交互了</p>
</li>
<li><p>在平常的操作过程中，ApplicationMaster根据resource-request协议向ResourceManager发送resource-request请求</p>
</li>
<li><p>当Container被分配之后，ApplicationMaster请求被分配到的 NodeManager 启动Container ，启动成功后 ，ApplicationMaster 向NodeManager发送container-launch-specification信息来启动Container， container-launch-specification信息包含了能够让Container和ApplicationMaster交流所需要的资料</p>
</li>
<li><p>应用程序的代码在启动的Container中运行，并把运行的进度、状态等信息通过application-specific协议发送给ApplicationMaster</p>
</li>
<li><p>在应用程序运行期间，提交应用的客户端主动和ApplicationMaster交流获得应用的运行状态、进度更新等信息，交流的协议也是application-specific协议</p>
</li>
<li><p>一但应用程序执行完成并且所有相关工作也已经完成，ApplicationMaster向ResourceManager取消注册然后关闭，用到所有的Container也归还给系统</p>
</li>
</ol>
<p><strong>Resource Request 和 Container</strong></p>
<p>ApplicationMaster 资源请求以 <code>resource-request</code> 的形式发送给 ResourceManager 的 Scheduler，<br>Scheduler 在这个原始的 <code>resource-request</code> 中返回分配到的资源描述Container</p>
<p>包含的字段信息如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;resource-name, priority, resource-requirement, number-of-containers&gt;</span><br></pre></td></tr></table></figure>

<blockquote>
<ul>
<li>resource-name：资源名称，现阶段指的是资源所在的host和rack，后期可能还会支持虚拟机或者更复杂的网络结构</li>
<li>priority：资源的优先级</li>
<li>resource-requirement：资源的具体需求，现阶段指内存和cpu需求的数量</li>
<li>number-of-containers：满足需求的Container的集合</li>
</ul>
</blockquote>
<p>number-of-containers中的Containers就是ResourceManager给ApplicationMaster分配资源的结果。Container就是授权给应用程序可以使用某个节点机器上CPU和内存的数量。</p>
<p>ApplicationMaster在得到这些Containers后，还需要与分配Container所在机器上的NodeManager交互来启动Container并运行相关任务。当然Container的分配是需要认证的，以防止ApplicationMaster自己去请求集群资源。</p>
<p><img src="/images/blog/2020-01-03-4.png" alt></p>
<p>有些提到了还有位置字段。。。 这个看来有说法，有空再看看源码之类的确认下。。</p>
]]></content>
      <categories>
        <category>Yarn</category>
      </categories>
  </entry>
  <entry>
    <title>HBase Rowkey 设计指南</title>
    <url>/2019/08/07/HBase-Rowkey/</url>
    <content><![CDATA[<p>HBase 的 Rowkey 设计可以说是使用 HBase 最为重要的事情，直接影响到HBase的性能</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p><strong>RowKey 到底是什么？</strong></p>
<p>常说看一张 HBase 表设计的好不好，就看它的 RowKey 设计的好不好。可见 RowKey 在 HBase 中的地位。那么 RowKey 到底是什么？</p>
<p>RowKey 的特点如下:</p>
<ul>
<li><p>类似于 MySQL、Oracle中的主键，用于标示唯一的行</p>
</li>
<li><p>完全是由用户指定的一串不重复的字符串</p>
</li>
<li><p>HBase 中的数据永远是根据 Rowkey 的字典排序来排序的</p>
</li>
</ul>
<p><strong>RowKey的作用</strong></p>
<ul>
<li><p>读写数据时通过 RowKey 找到对应的 Region</p>
</li>
<li><p>MemStore 中的数据按 RowKey 字典顺序排序</p>
</li>
<li><p>HFile 中的数据按 RowKey 字典顺序排序</p>
</li>
</ul>
<h4 id="Rowkey-对查询的影响"><a href="#Rowkey-对查询的影响" class="headerlink" title="Rowkey 对查询的影响"></a>Rowkey 对查询的影响</h4><p>如果 RowKey 设计为 uid+phone+name</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 那么这种设计可以很好的支持以下的场景</span><br><span class="line">uid &#x3D; 111 AND phone &#x3D; 123 AND name &#x3D; iteblog</span><br><span class="line">uid &#x3D; 111 AND phone &#x3D; 123</span><br><span class="line">uid &#x3D; 111 AND phone &#x3D; 12?</span><br><span class="line">uid &#x3D; 111</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"># 难以支持的场景</span><br><span class="line">phone &#x3D; 123 AND name &#x3D; iteblog</span><br><span class="line">phone &#x3D; 123</span><br><span class="line">name &#x3D; iteblog</span><br></pre></td></tr></table></figure>

<p>难以支持的场景可以做索引表来支持。二级索引，或者把索引存在ES中</p>
<h4 id="Rowkey-对-Region-划分影响"><a href="#Rowkey-对-Region-划分影响" class="headerlink" title="Rowkey 对 Region 划分影响"></a>Rowkey 对 Region 划分影响</h4><p>HBase 表的数据是按照 Rowkey 来分散到不同 Region，不合理的 Rowkey 设计会导致热点问题。</p>
<p>热点问题是大量的 Client 直接访问集群的一个或极少数个节点，而集群中的其他节点却处于相对空闲状态。</p>
<p><img src="/images/blog/2019-08-07-1.png" alt></p>
<p>如上图，Region1 上的数据是 Region 2 的5倍，这样会导致 Region1 的访问频率比较高，进而影响这个 Region 所在机器的其他 Region。</p>
<h4 id="Rowkey-原则"><a href="#Rowkey-原则" class="headerlink" title="Rowkey 原则"></a>Rowkey 原则</h4><h5 id="长度原则"><a href="#长度原则" class="headerlink" title="长度原则"></a>长度原则</h5><p>Rowkey是一个二进制码流，最大长度为64KB（1024 * 64 = 65536字节，因为 row length 占2字节），Rowkey的长度被很多开发者建议说设计在10~100个字节，建议是越短越好，不要超过16个字节。</p>
<p>数据的持久化文件 HFile 中是按照 KeyValue 存储的，如果 Rowkey 过长比如100个字节，1000万列数据光 Rowkey 就要占用 100*1000万=10亿个字节，将近1G数据，这会极大影响 HFile 的存储效率；</p>
<p>MemStore将缓存部分数据到内存，如果 Rowkey 字段过长内存的有效利用率会降低，系统将无法缓存更多的数据，这会降低检索效率。因此 Rowkey 的字节长度越短越好。</p>
<p>目前操作系统是都是64位系统，内存8字节对齐。控制在16个字节，8字节的整数倍利用操作系统的最佳特性。</p>
<h5 id="散列原则"><a href="#散列原则" class="headerlink" title="散列原则"></a>散列原则</h5><p>HBase 中的行是按照 Rowkey 的字典顺序排序的，这种设计优化了 scan 操作，可以将相关的行以及会被一起读取的行存取在临近位置，便于scan。</p>
<p>然而糟糕的 Rowkey 设计是热点的源头。 热点发生在大量的 client 直接访问集群的一个或极少数个节点（访问可能是读，写或者其他操作）。</p>
<p>大量访问会使热点 Region 所在的单个机器超出自身承受能力，引起性能下降甚至 Region 不可用，这也会影响同一个 RS 上的其他 Region，<br>由于主机无法服务其他 Region 的请求。 设计良好的数据访问模式以使集群被充分，均衡的利用。</p>
<p>为了避免写热点，设计 Rowkey 使得数据应该被写入集群的多个 Region，而不是一个。</p>
<h5 id="唯一原则"><a href="#唯一原则" class="headerlink" title="唯一原则"></a>唯一原则</h5><p>必须在设计上保证其唯一性。</p>
<p>rowkey是按照字典顺序排序存储的，因此，设计rowkey的时候，要充分利用这个排序的特点，将经常读取的数据存储到一块，将最近可能会被访问的数据放到一块。</p>
<h4 id="RowKey-设计技巧"><a href="#RowKey-设计技巧" class="headerlink" title="RowKey 设计技巧"></a>RowKey 设计技巧</h4><h5 id="Salting-加盐"><a href="#Salting-加盐" class="headerlink" title="Salting(加盐)"></a>Salting(加盐)</h5><p>在 Rowkey 的前面增加随机数，具体就是给 Rowkey 分配一个随机前缀以使得它和之前的 Rowkey 的开头不同。</p>
<p>分配的前缀种类数量应该和你想使用数据分散到不同的 Region 的数量一致。加盐之后的 Rowkey 就会根据随机生成的前缀分散到各个 Region 上，以避免热点。</p>
<p>假如你有下列 Rowkey，你表中每一个 Region 对应字母表中每一个字母。 以 ‘a’ 开头是同一个 Region, ‘b’开头的是同一个 Region。在表中，所有以 ‘f’开头的都在同一个 Region， 它们的 Rowkey 像下面这样</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">foo0001</span><br><span class="line">foo0002</span><br><span class="line">foo0003</span><br><span class="line">foo0004</span><br></pre></td></tr></table></figure>

<p>现在，假如你需要将上面这个 Rowkey 分散到 4个 Region。你可以用4个不同的盐：’a’, ‘b’, ‘c’, ‘d’.在这个方案下，每一个字母前缀都会在不同的 Region 中。<br>加盐之后，你有了下面的 Rowkey:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a-foo0003</span><br><span class="line">b-foo0001</span><br><span class="line">c-foo0004</span><br><span class="line">d-foo0002</span><br></pre></td></tr></table></figure>

<p>所以，你可以向4个不同的 Region 写，理论上说，如果所有人都向同一个 Region 写的话，你将拥有之前4倍的吞吐量。</p>
<p>现在，如果再增加一行，它将随机分配 a,b,c,d 中的一个作为前缀，并以一个现有行作为尾部结束: </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a-foo0003</span><br><span class="line">b-foo0001</span><br><span class="line">c-foo0003</span><br><span class="line">c-foo0004</span><br><span class="line">d-foo0002</span><br></pre></td></tr></table></figure>

<p>因为分配是随机的，所以如果你想要以字典序取回数据，你需要做更多工作。加盐这种方式增加了写时的吞吐量，但是当读时有了额外代价。</p>
<h5 id="Hashing-哈希"><a href="#Hashing-哈希" class="headerlink" title="Hashing(哈希)"></a>Hashing(哈希)</h5><p>Hashing 的原理是计算 Rowkey 的 hash 值，然后取 hash 的部分字符串和原来的 Rowkey 进行拼接。</p>
<p>因为哈希会使同一行永远用一个前缀加盐。因此哈希既可以让负载分散到整个集群，又可以让读可以预测。使用确定的哈希可以让客户端重构完整的rowkey，可以使用get操作准确获取某一个行数据。</p>
<p>这里说的 hash 包含 MD5、sha1、sha256或sha512等算法，常见的是使用MD5算法。</p>
<p>比如我们有如下的 RowKey:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">foo0001</span><br><span class="line">foo0002</span><br><span class="line">foo0003</span><br><span class="line">foo0004</span><br></pre></td></tr></table></figure>

<p>使用 md5 计算这些 RowKey 的 hash 值，然后取前 6 位和原来的 RowKey 拼接得到新的 RowKey</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">95f18cfoo0001</span><br><span class="line">6ccc20foo0002</span><br><span class="line">b61d00foo0003</span><br><span class="line">1a7475foo0004</span><br></pre></td></tr></table></figure>

<p>优缺点: 可以一定程度打散整个数据集，但是不利于 Scan；比如我们使用 md5 算法，来计算Rowkey的md5值，然后截取前几位的字符串。subString(MD5(设备ID), 0, x) + 设备ID，其中x一般取5或6。</p>
<h5 id="Reversing-the-Key-反转键"><a href="#Reversing-the-Key-反转键" class="headerlink" title="Reversing the Key(反转键)"></a>Reversing the Key(反转键)</h5><p>第三种预防hotspotting的方法是反转一段固定长度或者可数的键，来让最常改变的部分（最低显著位， the least significant digit ）在第一位，这样有效地打乱了行键，但是却牺牲了行排序的属性。</p>
<p>如果先导字段本身会带来热点问题，但该字段尾部的信息却具备良好的随机性。此时，可以考虑将先导字段做反转处理，将尾部几位直接提前到前面，或者直接将整个字段完全反转。</p>
<p>例如以手机号为rowkey，可以将手机号反转后的字符串作为rowkey，这样的就避免了以手机号那样比较固定开头导致热点问题。</p>
<p>比如我们有一下手机号，并作为RowKey</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">13400001111</span><br><span class="line">13400002313</span><br><span class="line">13400001686</span><br><span class="line">13400000939</span><br></pre></td></tr></table></figure>

<p>完全反转</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">11110000431</span><br><span class="line">23130000431</span><br><span class="line">16860000431</span><br><span class="line">09390000431</span><br></pre></td></tr></table></figure>

<p>有时候遇到后缀相同的，前缀随机，可我们需要把后缀相同的放在一起。所以可以考虑反转</p>
<p>比如我们有以下 URL ，并作为 RowKey</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">flink.iteblog.com</span><br><span class="line">www.iteblog.com</span><br><span class="line">carbondata.iteblog.com</span><br><span class="line">def.iteblog.com</span><br></pre></td></tr></table></figure>

<p>这些 URL 其实属于同一个域名，但是由于前面不一样，导致数据不在一起存放。我们可以对其进行反转，如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">moc.golbeti.knilf</span><br><span class="line">moc.golbeti.www</span><br><span class="line">moc.golbeti.atadnobrac</span><br><span class="line">moc.golbeti.fed</span><br></pre></td></tr></table></figure>

<p>经过这个之后，这些 URL 的数据就可以放一起了</p>
<h4 id="RowKey-设计案例剖析"><a href="#RowKey-设计案例剖析" class="headerlink" title="RowKey 设计案例剖析"></a>RowKey 设计案例剖析</h4><h5 id="交易类表-Rowkey-设计"><a href="#交易类表-Rowkey-设计" class="headerlink" title="交易类表 Rowkey 设计"></a>交易类表 Rowkey 设计</h5><p>查询某个卖家某段时间内的交易记录<br>sellerId + timestamp + orderId</p>
<p>查询某个买家某段时间内的交易记录<br>buyerId + timestamp ＋orderId</p>
<p>根据订单号查询<br>orderNo</p>
<p>如果某个商家卖了很多商品，可以如下设计 Rowkey 实现快速搜索<br>salt + sellerId + timestamp 其中，salt 是随机数。<br>可以支持的场景：</p>
<p>全表 Scan</p>
<p>按照 sellerId 查询</p>
<p>按照 sellerId + timestamp 查询</p>
<h5 id="金融风控-Rowkey-设计"><a href="#金融风控-Rowkey-设计" class="headerlink" title="金融风控 Rowkey 设计"></a>金融风控 Rowkey 设计</h5><p>查询某个用户的用户画像数据</p>
<p>prefix + uid</p>
<p>prefix + idcard</p>
<p>prefix + tele</p>
<p>其中 prefix = substr(md5(uid),0 ,x)， x 取 5-6。uid、idcard以及 tele 分别表示用户唯一标识符、身份证、手机号码。</p>
<h5 id="车联网-Rowkey-设计"><a href="#车联网-Rowkey-设计" class="headerlink" title="车联网 Rowkey 设计"></a>车联网 Rowkey 设计</h5><p>查询某辆车在某个时间范围的交易记录<br>carId + timestamp</p>
<p>某批次的车太多，造成热点<br>prefix + carId + timestamp 其中 prefix = substr(md5(uid),0 ,x)</p>
<p>查询最近的数据<br>查询用户最新的操作记录或者查询用户某段时间的操作记录，RowKey 设计如下：<br>uid + Long.Max_Value - timestamp</p>
<h5 id="支持的场景"><a href="#支持的场景" class="headerlink" title="支持的场景"></a>支持的场景</h5><p>查询用户最新的操作记录<br>Scan [uid] startRow [uid][000000000000] stopRow [uid][Long.Max_Value - timestamp]</p>
<p>查询用户某段时间的操作记录<br>Scan [uid] startRow [uid][Long.Max_Value – startTime] stopRow [uid][Long.Max_Value - endTime]</p>
<h5 id="时间戳反转"><a href="#时间戳反转" class="headerlink" title="时间戳反转"></a>时间戳反转</h5><p>尽量避免直接使用 time 作为 Rowkey。</p>
<p>如果有需求是快速获取数据的最近版本，使用反转的时间戳作为 Rowkey 的一部分对这个问题十分有用，可以用 Long.Max_Value - timestamp 追加到 key 的末尾，</p>
<p>例如 [key][reverse_timestamp] , [key] 的最新值可以通过scan [key]获得[key]的第一条记录，因为HBase中rowkey是有序的，第一条记录是最后录入的数据</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>单单一张 HBase 数据表只能根据 Rowkey 来查询。多维度查询下是不够用的，<br>可以尝试二级索引。Phoenix、Solr 以及 ElasticSearch 都可以用于构建二级索引。</p>
<p><img src="/images/blog/2019-08-07-2.png" alt></p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://help.aliyun.com/document_detail/59035.html?spm=a2c4g.11186623.6.578.vRusED" target="_blank" rel="noopener">Rowkey设计</a></li>
<li><a href="https://mp.weixin.qq.com/s/IAOTaP8oxAEksuGJz4VeKA" target="_blank" rel="noopener">玩转HBase RowKey设计及案例(code)，你会吗？</a></li>
<li><a href="https://mp.weixin.qq.com/s/AORh1vI3L5e7B3MAI8zizg" target="_blank" rel="noopener">一条数据的HBase之旅，简明HBase入门教程-Write全流程</a></li>
<li><a href="https://mp.weixin.qq.com/s/7SulKTNNrkYkcNRJE4IyUw" target="_blank" rel="noopener">HBase实战 HBase Rowkey 设计指南</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>Maven 使用</title>
    <url>/2019/08/06/maven/</url>
    <content><![CDATA[<p>maven 是一种软件项目管理和理解的工具。基于项目对象模型的概念。</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>maven 一直当作导jar包的工具，在实际开发中遇到了maven基础知识不够的大坑</p>
<p>本篇大部分复制于茶轴的青春的文章<a href="https://mp.weixin.qq.com/s/hQ8ycjQvavxQoS9d9xFllA" target="_blank" rel="noopener">《一文带你彻底搞懂 Maven》</a></p>
<h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>maven是一种软件项目管理和理解的工具。基于项目对象模型的概念。</p>
<h4 id="定位"><a href="#定位" class="headerlink" title="定位"></a>定位</h4><p>多数博客或者视频都将maven定义为自动化构建工具。那什么是自动化构建工具呢？</p>
<p>首先来解释构建</p>
<ol>
<li>一个BS项目最终运行的并不是动态web工程本身，而是这个动态web工程“编译的结果”</li>
<li>将java源文件变成字节码，交给JVM去执行</li>
<li>编译</li>
<li>部署</li>
</ol>
<p>构建各个过程的步骤</p>
<blockquote>
<p>清理: 将以前编译得到的旧字节码删除掉</p>
<p>编译: 将java源代码变成字节码</p>
<p>测试: 执行test文件夹中的测试程序</p>
<p>报告: 显示测试程序执行的结果</p>
<p>打包: 动态Web工程打成war包，Java工程打成jar包</p>
<p>安装: Maven的特定概念—将打包得到的文件复制到”仓库”中指定的位置</p>
<p>部署: 将动态Web工程生成的war包复制到Servlet容器中指定的目录下,使其可以运行</p>
</blockquote>
<p>自动化构建，其实上述步骤，在 elipse 和 IDEA 中也可以完成，只不过没那么标准。</p>
<p>既然 IDE 已经可以完成这些工作了，那么还要 maven 干什么呢？</p>
<p>日常开发中，以下几个步骤是我们经常走的</p>
<p>编译、打包、部署、测试</p>
<p>这几个步骤是程式化的，没有太大的变数或者说根本就没有变数。程序员们很希望从这些重复的工作中脱身出来，将这些重复的工作交给工具去做。此时Maven的意义就体现出来了，它可以自动的从构建过程中的起点一直执行到终点。</p>
<h4 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h4><h5 id="POM"><a href="#POM" class="headerlink" title="POM"></a>POM</h5><p>POM: a project object model. 项目对象模型</p>
<p>学习 Maven 就是学习 pom.xml 文件中的配置</p>
<h5 id="坐标"><a href="#坐标" class="headerlink" title="坐标"></a>坐标</h5><p>使用如下三个向量在 Maven 的仓库中唯一的确定一个 Maven 工程</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[1] groupid：公司或组织的域名倒序+当前项目名称</span><br><span class="line">[2] artifactId：当前项目的模块名称</span><br><span class="line">[3] version：当前模块的版本</span><br><span class="line">    &lt;groupId&gt;com.atguigu.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;Hello&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;0.0.1-SNAPSHOT&lt;&#x2F;version&gt;</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/2019-10-23-1.png" alt></p>
<p>引入对象的依赖非常的简单，去Maven的中央仓库，输入jar包的名字，选择需要的版本，然后将复制仓库给出的依赖，粘贴在pom.xml文件中的<dependencies>标签下就可以了。</dependencies></p>
<h5 id="仓库"><a href="#仓库" class="headerlink" title="仓库"></a>仓库</h5><p>仓库可以理解为存放jar包的地方。</p>
<p>本地仓库: 当前电脑上的部署的仓库目录，为当前电脑上的所有Maven工程服务。默认在用户家目录下的 <code>~/.m2/repository</code></p>
<p>远程仓库: 私服，搭建在局域网环境中，为局域网范围内的所有Maven工程服务。</p>
<p>中央仓库: 架设在Internet上，为全世界范围内所有的Maven工程服务。</p>
<p>中央仓库镜像: 为了分担中央仓库的流量，提升用户的访问速度。如阿里云</p>
<h5 id="生命周期"><a href="#生命周期" class="headerlink" title="生命周期"></a>生命周期</h5><p>Maven 生命周期定义了各个构建环节的执行顺序，有了这个清单，Maven 就可以自动化的执行构建命令了。</p>
<p>Maven有三套互相独立的生命周期</p>
<ul>
<li>Clean Lifecycle 在进行真正的构建之前进行一些清理工作</li>
<li>Default Lifecycle 构建的核心部分，编译，测试，打包，安装，部署等等</li>
<li>Site Lifecycle 生成项目报告，站点，发布站点</li>
</ul>
<p>它们是相互独立的，你可以仅仅调用 clean 来清理工作目录，仅仅调用 site 来生成站点。</p>
<p>当然你也可以直接运行 mvn clean install site 运行所有这三套生命周期。</p>
<p>这个生成站点的意思就是: 在本地生成有关你项目的相关信息。具体没用过。。感兴趣的自看<a href="https://blog.csdn.net/liuc0317/article/details/11890175" target="_blank" rel="noopener">使用maven 插件site 生成站点</a></p>
<p>Clean生命周期</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1 pre-clean 执行一些需要在 clean 之前完成的工作</span><br><span class="line">2 clean 移除所有上一次构建生成的文件</span><br><span class="line">3 post-clean 执行一些需要在 clean 之后立刻完成的工作</span><br></pre></td></tr></table></figure>

<p>Site生命周期</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1 pre-site 执行一些需要在生成站点文档之前完成的工作</span><br><span class="line">2 site 生成项目的站点文档</span><br><span class="line">3 post-site 执行一些需要在生成站点文档之后完成的工作，并且为部署做准备</span><br><span class="line">4 site-deploy 将生成的站点文档部署到特定的服务器上</span><br><span class="line">这里经常用到的是 site 阶段和 site-deploy 阶段，用以生成和发布 Maven 站点，这可是 Maven相当强大的功能，</span><br><span class="line">Manager比较喜欢，文档及统计数据自动生成，很好看。</span><br></pre></td></tr></table></figure>

<p>Default 生命周期是 Maven 生命周期中最重要的一个，绝大部分工作都发生在这个生命周期中。这里，只解释一些比较重要和常用的阶段</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">validate</span><br><span class="line">generate-sources</span><br><span class="line">process-sources</span><br><span class="line">generate-resources</span><br><span class="line">process-resources          复制并处理资源文件，至目标目录，准备打包。</span><br><span class="line">compile                    编译项目的源代码。</span><br><span class="line">process-classes</span><br><span class="line">generate-test-sources   </span><br><span class="line">process-test-sources</span><br><span class="line">generate-test-resources</span><br><span class="line">process-test-resources          复制并处理资源文件，至目标测试目录。</span><br><span class="line">test-compile                    编译测试源代码。</span><br><span class="line">process-test-classes</span><br><span class="line">test                使用合适的单元测试框架运行测试。这些测试代码不会被打包或部署。</span><br><span class="line">prepare-package</span><br><span class="line">package          接受编译好的代码，打包成可发布的格式，如  JAR  。</span><br><span class="line">pre-integration-test</span><br><span class="line">integration-test</span><br><span class="line">post-integration-test</span><br><span class="line">verify</span><br><span class="line">install          将包安装至本地仓库，以让其它项目依赖。</span><br><span class="line">deploy          将最终的包复制到远程的仓库，以让其它开发人员与项目共享</span><br></pre></td></tr></table></figure>

<p>maven 生命周期（lifecycle）由各个阶段组成，每个阶段由 maven 的插件 plugin 来执行完成。当我们执行的 maven 命令需要用到某些插件时，Maven 的核心程序会首先到本地仓库中查找。</p>
<h5 id="依赖的范围"><a href="#依赖的范围" class="headerlink" title="依赖的范围"></a>依赖的范围</h5><p>经常可以看到 pom 配置 dependency 时出现 scope。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-runtime_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.5.2&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;scope&gt;test&lt;&#x2F;scope&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>

<p>提一下<code>&lt;scope&gt;</code>的含义</p>
<ol>
<li><p>compile: 默认值 他表示被依赖项目需要参与当前项目的编译，还有后续的测试，运行周期也参与其中，是一个比较强的依赖。打包的时候通常需要包含进去</p>
</li>
<li><p>test: 依赖项目仅仅参与测试相关的工作，包括测试代码的编译和执行，不会被打包，例如：junit</p>
</li>
<li><p>runtime: 表示被依赖项目无需参与项目的编译，不过后期的测试和运行周期需要其参与。与compile相比，跳过了编译而已。例如JDBC驱动，适用运行和测试阶段</p>
</li>
<li><p>provided: 打包的时候可以不用包进去，别的设施会提供。事实上该依赖理论上可以参与编译，测试，运行等周期。相当于compile，但是打包阶段做了exclude操作</p>
</li>
<li><p>system: 从参与度来说，和provided相同，不过被依赖项不会从maven仓库下载，而是从本地文件系统拿。需要添加systemPath的属性来定义路径</p>
</li>
</ol>
<p>踩坑现场 <a href="https://blog.csdn.net/lh11077/article/details/80143996" target="_blank" rel="noopener">IDEA 在使用Maven项目时，未加载 provided 范围的依赖包，导致启动时报错</a></p>
<p>明明导入了依赖也确认过没有jar冲突，死活说找不到java.lang.ClassNotFoundException，最后才发现是Run Application时，IDEA未加载 provided 范围的依赖包，导致启动时报错（eclipse里面好像默认会加载，所以在那边是能正确运行的）</p>
<p>主要关注 compile、test、provided</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">① complie</span><br><span class="line">对主程序是否有效：有效</span><br><span class="line">对测试程序是否有效: 有效</span><br><span class="line">是否参与打包: 参与</span><br><span class="line">是否参与部署: 参与</span><br><span class="line">典型例子: Spring-core</span><br><span class="line"></span><br><span class="line">② test</span><br><span class="line">对主程序是否有效: 无效</span><br><span class="line">对测试程序是否有效: 有效</span><br><span class="line">是否参与打包: 不参与</span><br><span class="line">是否参与部署: 不参与</span><br><span class="line">典型例子: junit</span><br><span class="line"></span><br><span class="line">③ provided</span><br><span class="line">对主程序是否有效: 有效</span><br><span class="line">对测试程序是否有效: 有效</span><br><span class="line">是否参与打包: 不参与</span><br><span class="line">是否参与部署: 不参与</span><br><span class="line">典型例子: servlet-api.jar</span><br></pre></td></tr></table></figure>

<h5 id="依赖的传递性"><a href="#依赖的传递性" class="headerlink" title="依赖的传递性"></a>依赖的传递性</h5><p>模块A依赖于模块B。模块B又依赖于C模块，则模块A内会有C模块。</p>
<p>但是有时候会存在jar冲突，导致我不想用C模块的依赖</p>
<p>可以这样排除依赖</p>
<p><img src="/images/blog/2019-10-23-2.png" alt></p>
<p>解决 jar 包冲突, 可以参考下面文章，个人用的最多的是 Maven Healper</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/72551146" target="_blank" rel="noopener">通过IDEA快速定位和排除依赖冲突</a></li>
<li><a href="https://segmentfault.com/a/1190000017542396" target="_blank" rel="noopener">使用Maven Helper解决Maven插件冲突</a></li>
<li><a href="https://mp.weixin.qq.com/s/swqB37wSv4kdYcHSQwbsHg" target="_blank" rel="noopener">好机会，我要帮女同事解决Maven冲突问题</a></li>
</ul>
<h5 id="依赖的原则"><a href="#依赖的原则" class="headerlink" title="依赖的原则"></a>依赖的原则</h5><p>[1] 路径最短者优先</p>
<p><img src="/images/blog/2019-10-23-3.png" alt></p>
<p>[2] 路径相同时，先声明者优先</p>
<p><img src="/images/blog/2019-10-23-4.png" alt></p>
<p>这里指的先声明者优先，是指 A、B 模块在A模块中 denpendency 标签配置的先后顺序。谁在上面，谁就是先声明者。</p>
<h5 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h5><p>继承可以帮我们做什么？统一管理依赖的版本。一个典型的例子就是：现在项目下A、B、C三个模块，三个模块都依赖着不同的 Junit 的版本，依赖范围为 test 时，依赖又不能传递过来，但是我们希望能够统一Junit版本，可能有的人自然会想到，你让A、B、C模块pom中的Junit依赖版本写一致不就行了，这也是个办法。但是 maven 在设计的时候估计也考虑到了这种情况，这称之为继承。</p>
<p>继承该如何使用？</p>
<p>首先需要创建一个 maven 父工程，<strong>注意指定打包方式为 pom</strong>  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;groupId&gt;com.cxkStudy.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">&lt;artifactId&gt;Parent&lt;&#x2F;artifactId&gt;</span><br><span class="line">&lt;version&gt;0.0.1-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">&lt;description&gt;学习Maven使用&lt;&#x2F;description&gt;</span><br><span class="line">&lt;packaging&gt;pom&lt;&#x2F;packaging&gt;</span><br></pre></td></tr></table></figure>

<p>再创建一个maven工程,在该模块中指定它的父工程是谁</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;parent&gt;</span><br><span class="line">    &lt;groupId&gt;com.cxkStudy.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;Parent&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;0.0.1-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;parent&gt;</span><br></pre></td></tr></table></figure>

<p>在父项目中确定子模块的位置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;modules&gt;</span><br><span class="line">    &lt;module&gt;..&#x2F;studyDenpen&lt;&#x2F;module&gt;</span><br><span class="line">    &lt;module&gt;..&#x2F;Hello&lt;&#x2F;module&gt;</span><br><span class="line">&lt;&#x2F;modules&gt;</span><br></pre></td></tr></table></figure>

<p>module 标签中填的是相对于父项目的位置。同时这也就是聚合</p>
<p>这样就可以在父工程里面指定Junit的版本, 在子模块使用Junit的时候不写版本就好了</p>
<p>项目顶层(即父工程)的POM文件中，会看到dependencyManagement元素。通过它元素来管理jar包的版本，让子项目中引用一个依赖而不用显示的列出版本号。</p>
<p>父工程中执行Junit的版本</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;dependencyManagement&gt;</span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;junit&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;junit&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;4.9&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;scope&gt;test&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;&#x2F;dependencies&gt;</span><br><span class="line">&lt;&#x2F;dependencyManagement&gt;</span><br></pre></td></tr></table></figure>

<p>子模块中引入Junit的时候不指定版本</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;junit&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;junit&lt;&#x2F;artifactId&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>

<p>请注意配置继承后, 执行安装命令要先安装父工程。</p>
<h5 id="聚合"><a href="#聚合" class="headerlink" title="聚合"></a>聚合</h5><p>为什么要使用聚合？</p>
<p>将多个工程拆分为模块后，需要手动逐个安装到仓库后依赖才能够生效。修改源码后也需要逐个手动进<br>行 clean 操作。而使用了聚合之后就可以批量进行 Maven 工程的安装、清理工作。</p>
<p>聚合的好处还有继承，可以在子模块中不需要写相同依赖的版本了，上面有说。</p>
<p>关于继承、聚合的理解可以结合一个实际的项目来参考，我这边参考的是百度的图数据库<a href="https://github.com/hugegraph/hugegraph" target="_blank" rel="noopener">hugegraph</a></p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/u010072711/article/details/80516899" target="_blank" rel="noopener">maven settings.xml配置多个镜像地址</a></li>
<li><a href="https://mp.weixin.qq.com/s/hQ8ycjQvavxQoS9d9xFllA" target="_blank" rel="noopener">一文带你彻底搞懂 Maven</a></li>
<li><a href="https://blog.csdn.net/haitianxueyuan521/article/details/81736155" target="_blank" rel="noopener">Apache beam 和Flink 结合使用 出现 java.lang.NoClassDefFoundError 问题解决！</a></li>
<li><a href="https://blog.csdn.net/guokezhongdeyuzhou/article/details/79670233" target="_blank" rel="noopener">Maven本地项目之间的相互依赖</a></li>
<li><a href="https://blog.csdn.net/qq_28524127/article/details/80475398" target="_blank" rel="noopener">maven工程项目与项目之间的依赖方式</a></li>
<li><a href="https://blog.csdn.net/zhaojianting/article/details/80324533" target="_blank" rel="noopener">理解maven命令package、install、deploy的联系与区别</a></li>
</ul>
]]></content>
      <categories>
        <category>Tools</category>
      </categories>
  </entry>
  <entry>
    <title>Filebeat 采集日志到 Kafka 配置及使用</title>
    <url>/2019/08/05/Filebeat-Kafka/</url>
    <content><![CDATA[<p>Filebeat 采集日志到 Kafka 配置及使用</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>想把 Hadoop 集群中的日志收集起来进行流式分析</p>
<h4 id="Filebeat"><a href="#Filebeat" class="headerlink" title="Filebeat"></a>Filebeat</h4><p>日志采集器选择了 Filebeat 而不是Logstash，是由于 Logstash 是跑在 JVM 上面，资源消耗比较大，<br>后来作者用 GO 写了一个功能较少但是资源消耗也小的轻量级的 Agent 叫 Logstash-forwarder，<br>后来改名为FileBeat。</p>
<p>官网下载对应环境的<a href="https://www.elastic.co/cn/downloads/beats/filebeat" target="_blank" rel="noopener">Filebeat</a></p>
<p>我下载的是 7.3.0 版本 LINUX 64-BIT</p>
<h5 id="filebeat-yml-配置"><a href="#filebeat-yml-配置" class="headerlink" title="filebeat.yml 配置"></a>filebeat.yml 配置</h5><p>最核心的部分在于FileBeat配置文件的配置，需要指定paths（日志文件路径）、hosts（kafka主机ip和端口）、<br>topic（kafka主题）、name（本机IP）、logging.level（filebeat日志级别）。更多参考官网说明<a href="https://www.elastic.co/guide/en/beats/filebeat/current/configuring-howto-filebeat.html" target="_blank" rel="noopener">配置Filebeat</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">filebeat.inputs:</span><br><span class="line">- type: log</span><br><span class="line">  enabled: true</span><br><span class="line">  paths:</span><br><span class="line">    - &#x2F;var&#x2F;log&#x2F;hadoop-hdfs&#x2F;lihm.log</span><br><span class="line">  multiline:</span><br><span class="line">    pattern: &#39;^[0-9]&#123;4&#125;-[0-9]&#123;2&#125;-[0-9]&#123;2&#125;&#39;</span><br><span class="line">    negate: true</span><br><span class="line">    match: after</span><br><span class="line"></span><br><span class="line">output.kafka:</span><br><span class="line">  enabled: true</span><br><span class="line">  hosts: [&quot;127.0.0.1:9092&quot;]</span><br><span class="line">  topic: test</span><br><span class="line"></span><br><span class="line">logging.level: error</span><br><span class="line">name: 100.73.12.124</span><br></pre></td></tr></table></figure>

<h5 id="常用运维指令"><a href="#常用运维指令" class="headerlink" title="常用运维指令"></a>常用运维指令</h5><p>终端启动（退出终端或ctrl+c会退出运行）<br><code>./filebeat -e -c filebeat.yml</code></p>
<p>以后台守护进程启动启动filebeats<br><code>nohup ./filebeat -e -c filebeat.yml &amp;</code>    </p>
<p>停止运行FileBeat进程<br><code>ps -ef | grep filebeat</code><br><code>Kill -9 线程号</code></p>
<h4 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h4><p>一个典型的 Kafka 集群包含若干 Producer，若干 broker、若干 Consumer Group，以及一个 Zookeeper 集群。</p>
<p>官网下载 <a href="http://kafka.apache.org/downloads" target="_blank" rel="noopener">kafka_2.12-2.3.0.tgz</a></p>
<p>参考自<a href="https://blog.csdn.net/shubingzhuoxue/article/details/82868956" target="_blank" rel="noopener">安装kafka_2.12-2.0.0</a>文章自行安装</p>
<p>因是个人测试，只部署了一台kafka机器。</p>
<h4 id="调试"><a href="#调试" class="headerlink" title="调试"></a>调试</h4><p>首先肯定得在 Kafka 创建 filebeat.yml 配置的 Kakfa topic</p>
<p><code>bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test</code></p>
<p>创建完成 topic 后，FileBeat 就可以往 Kafka 传输日志。</p>
<p>通过两个步骤验证 Filebeat 的采集输送是否正常</p>
<h5 id="采集验证"><a href="#采集验证" class="headerlink" title="采集验证"></a>采集验证</h5><p>终端执行命令，查看控制台输出，如果服务有异常会直接打印出来并自动停止服务。</p>
<p><code>./filebeat -e -c filebeat.yml</code></p>
<p>如果以后台守护进程启动则查看 nohup.out 文件有没有异常</p>
<h5 id="接收验证"><a href="#接收验证" class="headerlink" title="接收验证"></a>接收验证</h5><p>Kafka集群控制台直接消费消息，验证接收到的日志信息</p>
<p><code>bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning</code></p>
<p>–from-beginning 表示从 Kafka 保存的最开头的 message 消费。如果想以当前为准就去掉该参数即可</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>这过程算踩坑过程，尤其是 filebeat.yml 配置踩了很多坑。</p>
<p>原理和思想方面的知识就自行搜索学习。这里是记录一个实操的过程</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/RM5wLt8qtJjHX6lwWCIhnA" target="_blank" rel="noopener">基于Kafka+ELK搭建海量日志平台</a></li>
<li><a href="https://my.oschina.net/openplus/blog/1589846" target="_blank" rel="noopener">filebeat合并多行日志示例</a></li>
<li><a href="https://www.jianshu.com/p/229c01447e54" target="_blank" rel="noopener">filebeat采集日志到kafka配置及使用</a></li>
<li><a href="https://www.cnblogs.com/zlslch/p/6621834.html" target="_blank" rel="noopener">Filebeat的下载（图文讲解）</a></li>
<li><a href="https://www.cnblogs.com/qinwengang/p/10982424.html" target="_blank" rel="noopener">filebeat配置日志记录（等级）</a></li>
<li><a href="https://www.cnblogs.com/liangyou666/p/9185274.html" target="_blank" rel="noopener">Filebeat+ELK部署文档</a></li>
</ul>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title>Kafka 认识</title>
    <url>/2019/08/05/Kafka-Introduction/</url>
    <content><![CDATA[<p>Kafka 消息系统</p>
<hr>
<h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p><a href="https://mp.weixin.qq.com/s/opAYVXIJoy4tCWaPcX5u6g" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/opAYVXIJoy4tCWaPcX5u6g</a></p>
]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
  </entry>
  <entry>
    <title>Flink Time</title>
    <url>/2019/08/02/Flink-Time/</url>
    <content><![CDATA[<p>Flink 在流程序中支持不同的 Time 概念，就比如有 Processing Time、Event Time 和 Ingestion Time。</p>
<hr>
<h4 id="时间类型"><a href="#时间类型" class="headerlink" title="时间类型"></a>时间类型</h4><p>Flink 中的时间与现实世界中的时间是不一致的 </p>
<p>在Flink中被划分为事件时间，摄入时间，处理时间三种</p>
<p>如果以 EventTime 为基准来定义时间窗口将形成 EventTimeWindow,要求消息本身就应该携带 EventTime</p>
<p>如果以 IngesingtTime 为基准来定义时间窗口将形成 IngestingTimeWindow,以 source 的 systemTime 为准。</p>
<p>如果以 ProcessingTime 基准来定义时间窗口将形成 ProcessingTimeWindow，以 operator 的 systemTime 为准。</p>
<p><img src="/images/blog/2019-08-02-1.png" alt></p>
<h4 id="时间详解"><a href="#时间详解" class="headerlink" title="时间详解"></a>时间详解</h4><h5 id="Processing-Time"><a href="#Processing-Time" class="headerlink" title="Processing Time"></a>Processing Time</h5><p>Processing Time 是指事件被处理时机器的系统时间。</p>
<p>当流程序在 Processing Time 上运行时，所有基于时间的操作(如时间窗口)将使用当时机器的系统时间。每小时 Processing Time 窗口将包括在系统时钟指示整个小时之间到达特定操作的所有事件。</p>
<p>例如，如果应用程序在上午 9:15 开始运行，则第一个每小时 Processing Time 窗口将包括在上午 9:15 到上午 10:00 之间处理的事件，下一个窗口将包括在上午 10:00 到 11:00 之间处理的事件。</p>
<p>Processing Time 是最简单的 “Time” 概念，不需要流和机器之间的协调，它提供了最好的性能和最低的延迟。但是，在分布式和异步的环境下，Processing Time 不能提供确定性，因为它容易受到事件到达系统的速度（例如从消息队列）、事件在系统内操作流动的速度以及中断的影响。</p>
<h5 id="Event-Time"><a href="#Event-Time" class="headerlink" title="Event Time"></a>Event Time</h5><p>Event Time 是事件发生的时间，一般就是数据本身携带的时间。这个时间通常是在事件到达 Flink 之前就确定的，并且可以从每个事件中获取到事件时间戳。在 Event Time 中，时间取决于数据，而跟其他没什么关系。Event Time 程序必须指定如何生成 Event Time 水印，这是表示 Event Time 进度的机制。</p>
<p>完美的说，无论事件什么时候到达或者其怎么排序，最后处理 Event Time 将产生完全一致和确定的结果。但是，除非事件按照已知顺序（按照事件的时间）到达，否则处理 Event Time 时将会因为要等待一些无序事件而产生一些延迟。由于只能等待一段有限的时间，因此就难以保证处理 Event Time 将产生完全一致和确定的结果。</p>
<p>假设所有数据都已到达， Event Time 操作将按照预期运行，即使在处理无序事件、延迟事件、重新处理历史数据时也会产生正确且一致的结果。 例如，每小时事件时间窗口将包含带有落入该小时的事件时间戳的所有记录，无论它们到达的顺序如何。</p>
<p>请注意，有时当 Event Time 程序实时处理实时数据时，它们将使用一些 Processing Time 操作，以确保它们及时进行。</p>
<h5 id="Ingestion-Time"><a href="#Ingestion-Time" class="headerlink" title="Ingestion Time"></a>Ingestion Time</h5><p>Ingestion Time 是事件进入 Flink 的时间。 在源操作处，每个事件将源的当前时间作为时间戳，并且基于时间的操作（如时间窗口）会利用这个时间戳。</p>
<p>Ingestion Time 在概念上位于 Event Time 和 Processing Time 之间。 与 Processing Time 相比，它稍微贵一些，但结果更可预测。因为 Ingestion Time 使用稳定的时间戳（在源处分配一次），所以对事件的不同窗口操作将引用相同的时间戳，而在 Processing Time 中，每个窗口操作符可以将事件分配给不同的窗口（基于机器系统时间和到达延迟）。</p>
<p>与 Event Time 相比，Ingestion Time 程序无法处理任何无序事件或延迟数据，但程序不必指定如何生成水印。</p>
<p>在 Flink 中，Ingestion Time 与 Event Time 非常相似，但 Ingestion Time 具有自动分配时间戳和自动生成水印功能。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">作者： 王知无</span><br><span class="line">出处： &lt;https:&#x2F;&#x2F;github.com&#x2F;wangzhiwubigdata&#x2F;God-Of-BigData&#x2F;blob&#x2F;master&#x2F;Flink&#x2F;9-Flink%E4%B8%AD%E7%9A%84Time.md&gt;</span><br><span class="line">本文版权归作者所有，欢迎转载，但未经作者同意必须保留此段声明，且在页面明显位置给出原文链接。</span><br></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>Flink</category>
      </categories>
  </entry>
  <entry>
    <title>CDH6 安装</title>
    <url>/2019/08/02/CDH6-Install/</url>
    <content><![CDATA[<p>CDH6 安装</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>因有业务团队想要自己部署一套CDH6的集群。</p>
<p>前来寻求帮助，应该不难，但是肯定得踩坑。只能撸起袖子干</p>
<p>选择最新 6.1 版本</p>
<p>官方文档链接  <a href="https://www.cloudera.com/documentation/enterprise/latest.html" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/latest.html</a> </p>
<p>选择自己要安装的版本。找到版本的官方的安装文档</p>
<p><img src="/images/blog/2019-08-02-2.png" alt></p>
<p>安装之前建议先阅读要求</p>
<p><img src="/images/blog/2019-08-02-3.png" alt></p>
<p>本人在安装过程中只关注了操作系统要求。</p>
<p>硬件是用公司物理机安装的。</p>
<p>数据库的话是用CM自带的 Psql</p>
<p><img src="/images/blog/2019-08-02-4.png" alt></p>
<h4 id="系统环境"><a href="#系统环境" class="headerlink" title="系统环境"></a>系统环境</h4><p>操作系统  CentOS 7.2 x64 </p>
<h4 id="安装之前"><a href="#安装之前" class="headerlink" title="安装之前"></a>安装之前</h4><h5 id="Host"><a href="#Host" class="headerlink" title="Host"></a>Host</h5><p><img src="/images/blog/2019-08-02-5.png" alt></p>
<p>主机YZ-JDB-106-38-13的名称中有大写字符。在这种情况下，通过Kerberos进行的身份验证将无法正常工作。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 一定要注意 hostname 不能包含下划线</span><br><span class="line">  </span><br><span class="line"># 修改hostname</span><br><span class="line">hostname yz-100-73-18-59  # 直接修改命令</span><br><span class="line">hostname                  # 查看修改后的hostname</span><br><span class="line">  </span><br><span class="line"># 修改网络配置文件</span><br><span class="line">vim &#x2F;etc&#x2F;sysconfig&#x2F;network</span><br><span class="line">HOSTNAME&#x3D;yz-JDB-106-35-3</span><br><span class="line">  </span><br><span class="line"># 修改hosts文件</span><br><span class="line"># 把集群所有的ip与对应的hostname</span><br><span class="line">vim &#x2F;etc&#x2F;hosts     </span><br><span class="line">100.73.41.20 yz-100-73-41-20</span><br><span class="line">100.73.41.21 yz-100-73-41-21</span><br><span class="line">100.73.41.22 yz-100-73-41-22</span><br></pre></td></tr></table></figure>

<h5 id="ssh免密"><a href="#ssh免密" class="headerlink" title="ssh免密"></a>ssh免密</h5><p>手动操作如下的代码块所示。 参考自<a href="https://www.cnblogs.com/keitsi/p/5653520.html" target="_blank" rel="noopener">CentOS 配置集群机器之间SSH免密码登录</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 集群密码登录</span><br><span class="line"># 集群中的每台主机上执行下面命令，一路回车，可生成本机的 rsa 类型的密钥</span><br><span class="line">ssh-keygen -t rsa</span><br><span class="line"># 执行完之后在 ~&#x2F;.ssh&#x2F; 目录下会生成一个保存有公钥的文件：id_rsa.pub</span><br><span class="line">   </span><br><span class="line"># 把公钥写入 authorized_keys 文件</span><br><span class="line"># 把自己的公钥拷贝到集群中的Master机</span><br><span class="line">ssh-copy-id root@HadoopMaster</span><br><span class="line">   </span><br><span class="line"># 最终Master机上生成~&#x2F;.ssh&#x2F;authorized_keys文件   该文件保存所有机器的公钥</span><br><span class="line"># 把~&#x2F;.ssh&#x2F;authorized_keys 拷贝到集群的其他机器上</span><br><span class="line">scp ~&#x2F;.ssh&#x2F;authorized_keys root@HadoopSlave1:~&#x2F;.ssh&#x2F;</span><br><span class="line">scp ~&#x2F;.ssh&#x2F;authorized_keys root@HadoopSlave2:~&#x2F;.ssh&#x2F;</span><br></pre></td></tr></table></figure>

<h5 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看防火墙</span><br><span class="line">systemctl status firewalld</span><br><span class="line"> </span><br><span class="line"># 关闭防火墙</span><br><span class="line">systemctl stop firewalld</span><br><span class="line"> </span><br><span class="line"># 开机禁用</span><br><span class="line">systemctl disable firewalld</span><br></pre></td></tr></table></figure>

<h5 id="禁用-SELinux"><a href="#禁用-SELinux" class="headerlink" title="禁用 SELinux"></a>禁用 SELinux</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 关闭SELINUX</span><br><span class="line"># 临时生效</span><br><span class="line">setenforce 0</span><br><span class="line"> </span><br><span class="line"># 永久生效</span><br><span class="line">vim &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line">SELINUX&#x3D;disabled</span><br></pre></td></tr></table></figure>

<h5 id="启用-NTP"><a href="#启用-NTP" class="headerlink" title="启用 NTP"></a>启用 NTP</h5><p>这个是 SA 同事指导的。ntp 配置文件是他给的。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 安装网络时间组件</span><br><span class="line">yum -y install ntp</span><br><span class="line"> </span><br><span class="line"># 备份原先ntp配置文件</span><br><span class="line">cp -p &#x2F;etc&#x2F;ntp.conf &#x2F;etc&#x2F;ntp.conf_&#96;date +%F_%H-%M-%S&#96;</span><br><span class="line"> </span><br><span class="line"># 配置ntp配置文件</span><br><span class="line">vim &#x2F;etc&#x2F;ntp.conf</span><br><span class="line">....</span><br><span class="line"></span><br><span class="line"># 配置其他配置文件</span><br><span class="line">vim &#x2F;etc&#x2F;sysconfig&#x2F;ntpd</span><br><span class="line"># Command line options for ntpd</span><br><span class="line">OPTIONS&#x3D;&quot;-4 -L -g&quot;</span><br><span class="line"></span><br><span class="line"># 停止影响的服务</span><br><span class="line">systemctl stop chronyd.service</span><br><span class="line"> </span><br><span class="line"># 启动ntp服务</span><br><span class="line">systemctl start ntpd.service</span><br><span class="line"></span><br><span class="line"># 查看ntp服务</span><br><span class="line">systemctl status ntpd.service</span><br><span class="line"></span><br><span class="line"># 确认是否同步成功</span><br><span class="line">ntpq -p  (若网络没问题，命令输出显示4台server并其中一台前会标记*号)</span><br></pre></td></tr></table></figure>

<h5 id="虚拟内存设置"><a href="#虚拟内存设置" class="headerlink" title="虚拟内存设置"></a>虚拟内存设置</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sysctl -w vm.swappiness&#x3D;0</span><br><span class="line">echo vm.swappiness &#x3D; 0 &gt;&gt; &#x2F;etc&#x2F;sysctl.conf</span><br></pre></td></tr></table></figure>

<h5 id="透明大页面设置"><a href="#透明大页面设置" class="headerlink" title="透明大页面设置"></a>透明大页面设置</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在root权限下，使用以下命令</span><br><span class="line"># 编辑文件，增加以下两行</span><br><span class="line">vim &#x2F;etc&#x2F;rc.d&#x2F;rc.local</span><br><span class="line">echo never &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled</span><br><span class="line">echo never &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;defrag（重启服务器生效）</span><br><span class="line"> </span><br><span class="line"># shell 下</span><br><span class="line">echo never &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled</span><br><span class="line">echo never &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;defrag（即时生效，重启失效）</span><br></pre></td></tr></table></figure>

<h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>安装过程就是页面点点点了。这里略过</p>
<h4 id="遇到的问题记录"><a href="#遇到的问题记录" class="headerlink" title="遇到的问题记录"></a>遇到的问题记录</h4><h5 id="agent-依赖冲突"><a href="#agent-依赖冲突" class="headerlink" title="agent 依赖冲突"></a>agent 依赖冲突</h5><p><code>Transaction check error:
file /usr/lib/systemd/system/supervisord.service from install of cloudera-manager-agent-6.1.0-769885.el7.x86_64 conflicts with file from package supervisor-3.1.3-3.el7.noarch</code></p>
<p>根据报错提示知道依赖冲突。卸载冲突依赖, 然后安装agent 的是会安装它所依赖的 supervisor</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rpm -e --nodeps supervisor-3.1.3-3.el7.noarch</span><br></pre></td></tr></table></figure>

<h5 id="升级软件依赖版本"><a href="#升级软件依赖版本" class="headerlink" title="升级软件依赖版本"></a>升级软件依赖版本</h5><p><img src="/images/blog/2019-08-02-6.png" alt></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum -y install python-pip</span><br><span class="line">pip install --upgrade psycopg2</span><br></pre></td></tr></table></figure>

<h5 id="JDBC-驱动"><a href="#JDBC-驱动" class="headerlink" title="JDBC 驱动"></a>JDBC 驱动</h5><p><img src="/images/blog/2019-08-02-7.png" alt></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp mysql-connector-java-5.1.47-bin.jar &#x2F;usr&#x2F;share&#x2F;java</span><br><span class="line">mv mysql-connector-java-5.1.47-bin.jar mysql-connector-java.jar</span><br></pre></td></tr></table></figure>

<h5 id="数据库没有创建"><a href="#数据库没有创建" class="headerlink" title="数据库没有创建"></a>数据库没有创建</h5><p>Able to find the Database server, but not the specified database. Please check if the database name is correct and make sure that the user can access the database.</p>
<p><img src="/images/blog/2019-08-02-8.png" alt></p>
<p>解决方法：创建需要的数据库</p>
<p><img src="/images/blog/2019-08-02-9.png" alt></p>
<h5 id="开启-gtid-问题"><a href="#开启-gtid-问题" class="headerlink" title="开启 gtid 问题"></a>开启 gtid 问题</h5><p>User cannot run DDL statements on the specified database. Attempt to create and drop a table failed.</p>
<p>因为 mysql 开启了gtid ，所以导致这个问题。</p>
<p>关闭 mysql 的gtid 即可。</p>
<p>修改 mysql的 my.cnf 配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># GTID #</span><br><span class="line">gtid-mode &#x3D; OFF</span><br><span class="line">enforce-gtid-consistency &#x3D; 0</span><br></pre></td></tr></table></figure>

<h4 id="Kudu-路径配置"><a href="#Kudu-路径配置" class="headerlink" title="Kudu 路径配置"></a>Kudu 路径配置</h4><p>Cloudera 默认没有给 Kudu 配置路径</p>
<p><img src="/images/blog/2019-08-02-10.png" alt></p>
<h4 id="Hive-Metastore-Canary"><a href="#Hive-Metastore-Canary" class="headerlink" title="Hive Metastore Canary"></a>Hive Metastore Canary</h4><p>Hive 需要 jdbc 的包</p>
<p><img src="/images/blog/2019-08-02-11.png" alt></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp mysql-connector-java.jar &#x2F;usr&#x2F;share&#x2F;java</span><br><span class="line">cp mysql-connector-java.jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.1.0-1.cdh6.1.0.p0.770702&#x2F;lib&#x2F;hive&#x2F;lib&#x2F;</span><br></pre></td></tr></table></figure>

<hr>
<p>参考链接</p>
<ul>
<li><a href="https://segmentfault.com/q/1010000013538821/a-1020000013541726" target="_blank" rel="noopener">按照CDH时创建 Hive Metastore 数据库表失败</a></li>
<li><a href="https://www.cnblogs.com/zwgblog/p/6063993.html" target="_blank" rel="noopener">hive 未初始化元数据库报错</a></li>
</ul>
]]></content>
      <categories>
        <category>CDH</category>
      </categories>
  </entry>
  <entry>
    <title>二叉树前序中序后序遍历的迭代实现</title>
    <url>/2019/07/29/Binary-Tree/</url>
    <content><![CDATA[<p>二叉树前序中序后序遍历的迭代实现</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>二叉树的前序、中序、后序遍历用递归实现较为简单。</p>
<p>在阅读他人代码时，发现有人用迭代方式实现，因此想扩展下自己。</p>
<p>二叉树有前序、中序、后序、层次遍历四种。</p>
<p>下面的结点按照访问的顺序标号，从左到右顺序依次是后序、前序、中序、层次遍历</p>
<p><img src="/images/blog/2019-07-29-1.png" alt></p>
<p>如标题所述，这里只讲前序、中序、后序迭代遍历</p>
<h4 id="前序"><a href="#前序" class="headerlink" title="前序"></a>前序</h4><p>借用栈的结构，先后将右子树和左子放入栈中，利用栈后入先出的原理遍历。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span> 借用栈的结构</span><br><span class="line"><span class="number">2.</span> 先push(root)</span><br><span class="line"><span class="number">3.1</span> 出栈 node = st.top(); st.pop()</span><br><span class="line"><span class="number">3.2</span> 记录当前值 res.push_back(p-&gt;val);</span><br><span class="line"><span class="number">3.3</span> 将右子树入栈 push(p-&gt;right)</span><br><span class="line"><span class="number">3.4</span> 将左子树入栈 push(p-&gt;left)</span><br><span class="line"><span class="number">4.</span> 循环步骤<span class="number">3</span>直到栈空</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">preorderTraversal</span><span class="params">(TreeNode *root)</span> </span>&#123;</span><br><span class="line">     <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res;</span><br><span class="line">     <span class="keyword">if</span>(!root) <span class="keyword">return</span> res;</span><br><span class="line">     <span class="built_in">stack</span>&lt;TreeNode*&gt; st;</span><br><span class="line">     st.push(root);</span><br><span class="line">     </span><br><span class="line">     <span class="keyword">while</span>(!st.empty())&#123;</span><br><span class="line">         TreeNode* p = st.top();</span><br><span class="line">         res.push_back(p-&gt;val);</span><br><span class="line">         st.pop();</span><br><span class="line">         <span class="keyword">if</span>(p-&gt;right) st.push(p-&gt;right);</span><br><span class="line">         <span class="keyword">if</span>(p-&gt;left) st.push(p-&gt;left);</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="keyword">return</span> res;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<h4 id="中序"><a href="#中序" class="headerlink" title="中序"></a>中序</h4><p>当前节点有左子节点时，将当前节点压栈，并将左子节点作为当前处理；<br>当前节点无左子节点时，表示左子树都已遍历完成，此时访问当前节点，并将右子节点设为当前节点。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span> 借用栈的结构</span><br><span class="line"><span class="number">2.</span> 把root、以及root左孩子都压入栈中</span><br><span class="line"><span class="number">3.1</span> 出栈 root = st.top(); st.pop();</span><br><span class="line"><span class="number">3.2</span> 记录当前值 res.push_back(root-&gt;val);</span><br><span class="line"><span class="number">3.3</span> 将右子节点设为当前节点 root = root-&gt;right;</span><br><span class="line"><span class="number">4.</span> 循环步骤<span class="number">2</span>直到栈为空且root为null</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">inorderTraversal</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">stack</span>&lt;TreeNode*&gt; st;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res;</span><br><span class="line">    <span class="keyword">while</span> (!st.empty() || root != <span class="literal">NULL</span>) &#123;</span><br><span class="line">        <span class="keyword">while</span> (root != <span class="literal">NULL</span>) &#123;</span><br><span class="line">            st.push(root);</span><br><span class="line">            root = root-&gt;left;</span><br><span class="line">        &#125;</span><br><span class="line">        root = st.top(); st.pop();</span><br><span class="line">        res.push_back(root-&gt;val);</span><br><span class="line">        root = root-&gt;right;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="后序"><a href="#后序" class="headerlink" title="后序"></a>后序</h4><p>当前节点被读取的条件为: 无左右孩子，或者上一次读取的为其左右孩子。<br>否则按照先右后左的方式对子节点压栈。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vector&lt;int&gt; postorderTraversal(TreeNode *root)&#123;</span><br><span class="line">    vector&lt;int&gt; res;</span><br><span class="line">    if(!root) return res;</span><br><span class="line">    stack&lt;TreeNode *&gt; st;</span><br><span class="line">    TreeNode * pre &#x3D; nullptr;</span><br><span class="line">    st.push(root);</span><br><span class="line">    while(!st.empty())&#123;</span><br><span class="line">        TreeNode * p &#x3D; st.top();</span><br><span class="line">        if((!p-&gt;left &amp;&amp; !p-&gt;right) || </span><br><span class="line">            (pre &amp;&amp; (pre &#x3D;&#x3D; p-&gt;left || pre &#x3D;&#x3D; p-&gt;right))) &#123;</span><br><span class="line">            res.push_back(p-&gt;val);</span><br><span class="line">            st.pop();</span><br><span class="line">            pre &#x3D; p;</span><br><span class="line">        &#125;</span><br><span class="line">        else&#123;</span><br><span class="line">            if(p-&gt;right) st.push(p-&gt;right);</span><br><span class="line">            if(p-&gt;left) st.push(p-&gt;left);</span><br><span class="line">        &#125;</span><br><span class="line">     &#125;</span><br><span class="line">     return res;        </span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p>后序遍历有一种巧妙的方式：前序遍历根节点，先后将左右子节点压栈。<br>这样的遍历顺序为: 中，右，左。最后reverse结果，则遍历结果变为: 左，右，中。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vector&lt;int&gt; postorderTraversal(TreeNode *root)&#123;</span><br><span class="line">    vector&lt;int&gt; res;</span><br><span class="line">    if(!root)  </span><br><span class="line">        return res;</span><br><span class="line">    stack&lt;TreeNode *&gt; st;</span><br><span class="line">    st.push(root);</span><br><span class="line">    while(!st.empty())&#123;</span><br><span class="line">        TreeNode * p &#x3D; st.top();</span><br><span class="line">        res.push_back(p-&gt;val);</span><br><span class="line">        st.pop();</span><br><span class="line">        if(p-&gt;left) st.push(p-&gt;left);</span><br><span class="line">        if(p-&gt;right) st.push(p-&gt;right);</span><br><span class="line">    &#125;</span><br><span class="line">    reverse(res.begin(), res.end());</span><br><span class="line">    return res;        </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.jianshu.com/p/e0a8bbee76a9" target="_blank" rel="noopener">二叉树前序、中序、后序遍历的迭代实现</a></li>
<li><a href="https://www.cnblogs.com/qjmnong/p/9135386.html" target="_blank" rel="noopener">二叉树的前序、中序、后序遍历迭代实现</a></li>
</ul>
]]></content>
      <categories>
        <category>DataStructure</category>
      </categories>
  </entry>
  <entry>
    <title>Flink 作业提交方式</title>
    <url>/2019/07/25/Flink-Submit-jar/</url>
    <content><![CDATA[<p>Flink 作业提交方式</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>写了 Flink 代码之后，肯定得想着提交到集群上小跑一下，不然总觉得少了点什么。</p>
<h4 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h4><p>在 Yarn 上提交作业分为2种情况:</p>
<ul>
<li>yarn seesion</li>
<li>Flink run</li>
</ul>
<p>两者者对于现有大数据平台资源使用率有着很大的区别：</p>
<p>yarn seesion(Start a long-running Flink cluster on YARN) 这种方式需要先启动集群，然后在提交作业。</p>
<p>第二种 Flink run 直接在 YARN 上提交运行 Flink作业(Run a Flink job on YARN)，这种方式的好处是一个任务会对应一个job, 即没提交一个作业会根据自身的情况，向 yarn 申请资源，直到作业执行完成，并不会影响下一个作业的正常运行，除非是yarn上面没有任何资源的情况下。</p>
<h5 id="yarn-seesion"><a href="#yarn-seesion" class="headerlink" title="yarn seesion"></a>yarn seesion</h5><p>不管采用客户端模式还是分离模式，提交作业都是一样的。</p>
<p>下面以 Flink 目录下的 LICENSE 为例，计算 WordCount 将处理后的数据放到 HDFS。</p>
<p><strong>数据准备</strong></p>
<p>首先上传数据到 HDFS</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop fs -mkdir   &#x2F;user&#x2F;root&#x2F;test</span><br><span class="line">hadoop fs -put LICENSE &#x2F;user&#x2F;root&#x2F;test</span><br></pre></td></tr></table></figure>

<p><strong>提交作业并查看结果</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;flink run .&#x2F;examples&#x2F;batch&#x2F;WordCount.jar  --input hdfs:&#x2F;&#x2F;nameservice1&#x2F;user&#x2F;root&#x2F;test&#x2F;LICENSE  --output hdfs:&#x2F;&#x2F;nameservice1&#x2F;user&#x2F;root&#x2F;test&#x2F;result.txt</span><br><span class="line">hadoop fs -cat &#x2F;user&#x2F;root&#x2F;test&#x2F;result.txt</span><br></pre></td></tr></table></figure>

<p>对于Flink on yarn 作业提交后，若要在 Flink WEB UI上查看作业的，到完成 Job 里可以看到</p>
<p><img src="/images/blog/2019-07-25-8.png" alt></p>
<p>查看作业的详细</p>
<p><img src="/images/blog/2019-07-25-9.png" alt></p>
<p><strong>运行到指定的 yarn session</strong></p>
<p>指定 yarn applicationID 来运行到特定的 yarn session</p>
<p>可以指定yid -yid,–yarnapplicationId <arg> Attach to running YARN session来运行到特定的yarn session</arg></p>
<p>指定运行到 ID 为 application_1550579025929_62420 的yarn-session</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;flink run -yid application_1550579025929_62420 .&#x2F;examples&#x2F;batch&#x2F;WordCount.jar --input hdfs:&#x2F;&#x2F;nameservice1&#x2F;user&#x2F;root&#x2F;test&#x2F;LICENSE  --output hdfs:&#x2F;&#x2F;nameservice1&#x2F;user&#x2F;root&#x2F;test&#x2F;result4.txt</span><br></pre></td></tr></table></figure>

<h5 id="run-方式"><a href="#run-方式" class="headerlink" title="run 方式"></a>run 方式</h5><p>对于前面介绍的 yarn session 需要先启动一个集群，然后在提交作业。</p>
<p>对于Flink run 直接提交作业就相对比较简单，不需要额外的去启动一个集群，直接提交作业，即可完成 Flink 作业。</p>
<p>同样支持两种提交方式，默认不指定就是客户端方式</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 客户端会多出来一个CliFrontend进程，就是驱动进程, 会在终端打印执行信息</span><br><span class="line">bin&#x2F;flink run -m yarn-cluster -yn 2 -yjm 2048 -ytm 5120 .&#x2F;examples&#x2F;batch&#x2F;WordCount.jar  --input hdfs:&#x2F;&#x2F;nameservice1&#x2F;user&#x2F;root&#x2F;test&#x2F;LICENSE  --output hdfs:&#x2F;&#x2F;nameservice1&#x2F;user&#x2F;root&#x2F;test&#x2F;result4.txt</span><br></pre></td></tr></table></figure>

<p>需要使用分离方式提交的话。可以在提交作业的命令行中指定 -d 或者 –detached 进行分离提交。程序提交完毕退出客户端，</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 程序提交完毕退出客户端，不在打印作业进度等信息！</span><br><span class="line">bin&#x2F;flink run -m yarn-cluster -d -yn 2 -yjm 2048 -ytm 5120 .&#x2F;examples&#x2F;batch&#x2F;WordCount.jar  --input hdfs:&#x2F;&#x2F;nameservice1&#x2F;user&#x2F;root&#x2F;test&#x2F;LICENSE  --output hdfs:&#x2F;&#x2F;nameservice1&#x2F;user&#x2F;root&#x2F;test&#x2F;result4.txt</span><br></pre></td></tr></table></figure>

<p>通过<code>bin/flink run -help</code>命令获取帮助。这里将不在解释</p>
<p>有时候 Yarn 集群是 jdk1.7 的，但是 Flink 得 1.8以上，这时需要在启动命令中指定 jdk1.8</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;flink run -m yarn-cluster -yn 2 -yjm 2048 -ytm 5120 \</span><br><span class="line">-yD yarn.taskmanager.env.JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;jdk1.8.0_172-amd64 \</span><br><span class="line">-yD containerized.master.env.JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;jdk1.8.0_172-amd64 \</span><br><span class="line">-yD containerized.taskmanager.env.JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;jdk1.8.0_172-amd64 \</span><br><span class="line">-c cn.lihm.examples.streaming.KafkaLogHandle \</span><br><span class="line">&#x2F;root&#x2F;lihm&#x2F;flink-learning-examples-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>

<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>顺道提一下 slots 数的指定: </p>
<p>对于 standalone cluster 而言，由于一台机器上只有一个TaskManager，slots数应与机器核数相同。</p>
<p>对于 single job on yarn模式和 yarn cluster 模式而言，一台机器上可能有多个TaskManager（取决于yarn在该机器上分配的container数），理论上应该与该Container分配的核数一致为佳。</p>
<p>注意事项: 如果是平时的本地测试或者开发，可以采用yarn seesion, 如果是生产环境推荐使用 run 方式</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/xianpanjia4616/article/details/83958673" target="_blank" rel="noopener">Flink on yarn集群搭建</a></li>
<li><a href="https://mp.weixin.qq.com/s/VCmeuzz-euCmu9ojZJ3uaA" target="_blank" rel="noopener">Flink on yarn应用部署</a></li>
<li><a href="https://www.jianshu.com/p/1b05202c4fb6" target="_blank" rel="noopener">Flink on yarn部署模式</a></li>
<li><a href="https://blog.csdn.net/lb812913059/article/details/86601150" target="_blank" rel="noopener">Flink安装与运行——flink on yarn</a></li>
</ul>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
  </entry>
  <entry>
    <title>Impala 操作 Kudu 表报错影响使用</title>
    <url>/2019/07/24/Impala-Kudu-FATAL_UNAUTHORIZED/</url>
    <content><![CDATA[<p>Impala 操作 Kudu 表报 Not authorized: unencrypted connections from publicly routable IPs are prohibited.</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>又到晚饭点时间。一如以往的在去吃晚饭的路上，此时企业微信弹窗闪了一下。</p>
<p>数据平台部的同事说他们内部的 CDH 集群 Impala 操作 Kudu 表报错了，寻求解决帮助。</p>
<p>那还能说，吃饭的速度都快了很多，就为了早点回来解决。</p>
<h4 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h4><p>问题现场，执行 SQL 复现报错。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">WARNINGS: TransmitData() to 100.106.38.12:27000 failed: Not authorized: Client connection negotiation failed: client connection to 100.106.38.12:27000: FATAL_UNAUTHORIZED: Not authorized: unencrypted connections from publicly routable IPs are prohibited. See --trusted_subnets flag for more information.: 100.106.40.23:32811</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/2019-07-24-2.png" alt> </p>
<p>定眼一看，这种报错没见过，翻译过来就是<code>致命的未授权：未授权：禁止来自公共可路由IP的未加密连接。</code></p>
<p>没见过这种报错信息。看来只能寻求搜索引擎了。某度搜索没有找到想要的，还是谷歌给力，直接找到 cloudera 问题帖子。</p>
<p><a href="https://community.cloudera.com/t5/Interactive-Short-cycle-SQL/KUDU-trusted-subnets-does-not-work/td-p/69176" target="_blank" rel="noopener">KUDU trusted subnets does not work </a></p>
<p>问题相似度极高。而且还是已经解决的帖子。看来有成功的希望啊。</p>
<p><img src="/images/blog/2019-07-24-3.png" alt> </p>
<p>翻译过来就是 <code>Impala已经从5.15开始将krpc用于transmitdata（）rpc。因此，您也需要在IMPALA配置中以相同的方式配置“trusted_subnets”标志（它不会从kudu配置中获取）。</code></p>
<p>这套集群是 CHD 6.1.0 的，看来很可能符合这个问题，且也说了是不会从 Kudu 配置中获取 trusted_subnets 标志。</p>
<p>这里就有个问题了。trusted_subnets 是啥？ 碰巧的是原提问者也在问题中说到这个</p>
<p><img src="/images/blog/2019-07-24-8.png" alt> </p>
<p>我也不是很清楚，但是感觉这个很关键，去 Impala 配置和 Kudu 配置搜索了下这个关键字。</p>
<p>只在 Kudu 中搜索到了，Impala 未找到这个，也符合原文说过 IMPALA 需要以相同的方式配置 </p>
<p><img src="/images/blog/2019-07-24-4.png" alt> </p>
<p>这时候脑海中有个困惑，trusted_subnets 在 Impala 中怎么配置呢？</p>
<p>我对比了下 Kudu 的配置位置，Kudu 是在代码段位置配置，我猜测 Impala 应该也是在这配置。</p>
<p>在 Impala 配置中搜索<code>代码块</code>出现好几个地方可以添加。</p>
<p>这时候我只能凭经验猜测 <code>Impala 命令行参数高级配置代码段</code>和<code>Impala 服务环境高级配置代码段</code>可能性大，不太可能是其他服务。</p>
<p>抱着试一试心态在这两个地方添加下方配置。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">--trusted_subnets&#x3D;0.0.0.0&#x2F;0</span><br><span class="line">--rpc-authentication&#x3D;disabled</span><br><span class="line">--rpc-encryption&#x3D;disabled</span><br></pre></td></tr></table></figure>

<p>保存时CM报<code>Impala 服务环境高级配置代码段</code>配置错误</p>
<p><img src="/images/blog/2019-07-24-6.png" alt> </p>
<p>看来只能先去掉，去掉后没有报错。</p>
<p><img src="/images/blog/2019-07-24-5.png" alt> </p>
<p>重启 Impala 后再执行发现问题解决。</p>
<p><img src="/images/blog/2019-07-24-7.png" alt> </p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>谷歌大法好啊，感谢前人栽树，后人乘凉，没有他们的，我也不能把问题解决。</p>
<p>这里中的为什么，我就没过多研究了，因为本人对 Kudu 了解不多，暂时放着。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://community.cloudera.com/t5/Interactive-Short-cycle-SQL/KUDU-trusted-subnets-does-not-work/td-p/69176" target="_blank" rel="noopener"> KUDU trusted subnets does not work </a></li>
</ul>
]]></content>
      <categories>
        <category>Impala</category>
      </categories>
  </entry>
  <entry>
    <title>Flink 部署模式</title>
    <url>/2019/07/24/Flink-Deployment-Patterns/</url>
    <content><![CDATA[<p>大数据计算引擎 Flink 部署模式</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>大数据发展到今天，整个生态已经比较成熟比较丰富了。</p>
<p>在 Apache Flink China Meetup 上，把大数据的计算引擎分三代  </p>
<p>第一代无疑是 Hadoop 的 MapReduce，第二代是Spark，而对流处理和批处理统一性上做得很好的 Flink 被归作最新一代的计算引擎，也就是第三代大数据计算引擎。</p>
<p><img src="/images/blog/2019-07-24-1.png" alt> </p>
<h4 id="Local-本地部署"><a href="#Local-本地部署" class="headerlink" title="Local 本地部署"></a>Local 本地部署</h4><p>Flink 可以运行在 Linux、Mac OS X 和 Windows 上。</p>
<p>本地模式的安装唯一需要的只是 Java 1.8.x 及以上。<br>本地运行会启动Single JVM，主要用于测试调试代码。</p>
<h4 id="Standalone-Cluster-集群部署"><a href="#Standalone-Cluster-集群部署" class="headerlink" title="Standalone Cluster 集群部署"></a>Standalone Cluster 集群部署</h4><p>Flink 自带了集群模式 Standalone，对软件有些要求</p>
<p>1.安装Java1.8或者更高版本</p>
<p>2.集群各个节点需要ssh免密登录</p>
<h4 id="Flink-On-Yarn"><a href="#Flink-On-Yarn" class="headerlink" title="Flink On Yarn"></a>Flink On Yarn</h4><p>Apache Hadoop YARN是一个集群资源管理框架。它允许在群集之上运行各种分布式应用程序。</p>
<p>使用以下命令查看相关参数</p>
<p><code>./bin/yarn-session.sh -h</code></p>
<p>此命令将显示以下概述</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Usage:</span><br><span class="line">   Required</span><br><span class="line">     -n,--container &lt;arg&gt;   Number of YARN container to allocate (&#x3D;Number of Task Managers)</span><br><span class="line">   Optional</span><br><span class="line">     -D &lt;property&#x3D;value&gt;             use value for given property</span><br><span class="line">     -d,--detached                   If present, runs the job in detached mode</span><br><span class="line">     -h,--help                       Help for the Yarn session CLI.</span><br><span class="line">     -id,--applicationId &lt;arg&gt;       Attach to running YARN session</span><br><span class="line">     -j,--jar &lt;arg&gt;                  Path to Flink jar file</span><br><span class="line">     -jm,--jobManagerMemory &lt;arg&gt;    Memory for JobManager Container with optional unit (default: MB)</span><br><span class="line">     -m,--jobmanager &lt;arg&gt;           Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified in the configuration.</span><br><span class="line">     -n,--container &lt;arg&gt;            Number of YARN container to allocate (&#x3D;Number of Task Managers)</span><br><span class="line">     -nl,--nodeLabel &lt;arg&gt;           Specify YARN node label for the YARN application</span><br><span class="line">     -nm,--name &lt;arg&gt;                Set a custom name for the application on YARN</span><br><span class="line">     -q,--query                      Display available YARN resources (memory, cores)</span><br><span class="line">     -qu,--queue &lt;arg&gt;               Specify YARN queue.</span><br><span class="line">     -s,--slots &lt;arg&gt;                Number of slots per TaskManager</span><br><span class="line">     -sae,--shutdownOnAttachedExit   If the job is submitted in attached mode, perform a best-effort cluster shutdown when the CLI is terminated abruptly, e.g., in response to a user interrupt, such</span><br><span class="line">                                     as typing Ctrl + C.</span><br><span class="line">     -st,--streaming                 Start Flink in streaming mode</span><br><span class="line">     -t,--ship &lt;arg&gt;                 Ship files in the specified directory (t for transfer)</span><br><span class="line">     -tm,--taskManagerMemory &lt;arg&gt;   Memory per TaskManager Container with optional unit (default: MB)</span><br><span class="line">     -yd,--yarndetached              If present, runs the job in detached mode (deprecated; use non-YARN specific option instead)</span><br><span class="line">     -z,--zookeeperNamespace &lt;arg&gt;   Namespace to create the Zookeeper sub-paths for high availability mode</span><br></pre></td></tr></table></figure>

<p>通过命令 yarn-session.sh 来实现，本质上是在 Yarn 集群上启动一个 Flink 集群。</p>
<p>由 Yarn 预先给 Flink 集群分配若干个 container 给 flink 使用，在yarn的界面上只能看到一个 Flink session cluster 的任务。</p>
<p><img src="/images/blog/2019-07-25-5.png" alt></p>
<p>只有一个 Flink 界面，可以从 Yarn 的  ApplicationMaster链接进入。</p>
<p><img src="/images/blog/2019-07-25-6.png" alt></p>
<p>这里需要注意一下如果没有提交 job, 只启动 yarn-session 的话, 打开 Flink 的 ui 界面是看不到上面箭头所指的资源数的。</p>
<p><img src="/images/blog/2019-07-25-7.png" alt></p>
<p>只有提交一个job后, 才会显示。上图是提交了一个job, 当然还可以提交多个job到这个容器中运行。</p>
<p><strong>Flink 页面中能看到目前只启动了一个 TaskMananger（一个JVM进程），并且有 FreeSlot，新启动的 Flink Job 会在这些 slots 中启动，直到没有更多 FreeSlots 了才会分配新的 TaskMananger。</strong></p>
<h5 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h5><p><img src="/images/blog/2019-07-25-1.png" alt></p>
<p>Flink ON YARN工作流程如下所示</p>
<p>首先提交 job 给 YARN，就需要有一个 Flink YARN Client </p>
<ol>
<li><p>Client 首先检查要请求的资源(containers 和 memory)是否可用。然后将 Flink 相关 jar 包和配置文件上传到 HDFS。</p>
</li>
<li><p>Client 向 ResourceManager 申请一个 yarn container 用以启动 ApplicationMaster。由于客户端已经将配置文件和 jar 包注册为了 container 的资源，所以 NodeManager 会直接使用这些资源准备好 container（例如，下载文件等）。一旦该过程结束，AM就被启动了。</p>
</li>
<li><p>JobManager 和 ApplicationMaster 运行于同一个 Container。一旦创建成功，AM 就知道了 JobManager 的地址。它会生成一个新的 Flink 配置文件，这个配置文件是给将要启动的 TaskManager 用的，该配置文件也会上传到 HDFS。</p>
</li>
<li><p>ApplicationMaster 为 Flink 的 TaskManagers 分配 Container 并启动 TaskManager，TaskManager 内部会划分很多个Slot，它会自动从 HDFS 下载jar文件和修改后的配置，然后运行相应的 task。<br>TaskManager也会与APPMaster中的JobManager进行交互，维持心跳等。</p>
</li>
</ol>
<p>ApplicationMaster 的 Container 也提供了 Flink 的 web 接口。Yarn 代码申请的端口都是临时端口，目的是为了让用户并行启动多个 Flink YARN Session。</p>
<p>用 yarn session 在启动集群时，有2种方式可以进行集群启动分别是:</p>
<ul>
<li>客户端模式</li>
<li>分离式模式</li>
</ul>
<h5 id="客户端模式"><a href="#客户端模式" class="headerlink" title="客户端模式"></a>客户端模式</h5><p>默认可以直接执行<code>bin/yarn-session.sh</code></p>
<p>默认启动的配置是<code>{masterMemoryMB=1024, taskManagerMemoryMB=1024, numberTaskManagers=1, slotsPerTaskManager=1}</code></p>
<p>要自己自定义配置的话，自己可以根据参数来启动</p>
<p>示例: 发出以下命令以启动Yarn会话集群。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;yarn-session.sh -n 2 -jm 1024 -tm 4096 -s 6</span><br></pre></td></tr></table></figure>

<p>其中 JobManager 内存1GB，2个 TaskManager，每个 TaskManager 4GB内存和6个处理插槽启动。</p>
<p>-n: TaskManager的数量，相当于executor的数量</p>
<p>-s: 每个JobManager的core的数量，executor-cores。建议将slot的数量设置每台机器的处理器数量</p>
<p>-tm: 每个TaskManager的内存大小，executor-memory</p>
<p>-jm: JobManager的内存大小，driver-memory</p>
<p>对于客户端模式而言，可以启动多个 yarn session。<br>一个 yarn session 模式对应一个 JobManager,并按照需求提交作业。同一个 Session 中可以提交多个Flink作业。</p>
<p><img src="/images/blog/2019-07-25-4.png" alt></p>
<p>根据图中 Flink JobManager is now running 关键字可以知道 JobManager 位置</p>
<p>可以通过停止 unix 进程（使用CTRL + C）或在客户端输入“stop”来停止YARN会话。还可以通过yarn application -kill 命令来停止。</p>
<p>对于客户端模式进程如下:</p>
<p>在启动的 yarn-session 服务器上查看 Java 进程</p>
<p><img src="/images/blog/2019-07-25-2.png" alt></p>
<p>在 JobManager 所在服务器查看 Java 进程</p>
<p><img src="/images/blog/2019-07-25-3.png" alt></p>
<h5 id="分离模式"><a href="#分离模式" class="headerlink" title="分离模式"></a>分离模式</h5><p>如果不希望Flink YARN客户端始终保持运行，则还可以启动分离的 YARN 会话。</p>
<p>对于分离式模式，并不像客户端那样可以启动多个 yarn session，如果启动多个，会出现后新建的 session一直处在等待状态。</p>
<p>JobManager的个数只能是一个，同一个 Session 中可以提交多个 Flink 作业。</p>
<p>客户端在启动 Flink Yarn Session 后，就不再属于 Yarn 集群的一部分。</p>
<p>分离模式启动命令上比客户端多一个参数。该参数称为 -d 或 –detached。-nm 给 yarn-session 起一个 test3 名字。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;yarn-session.sh -n 2 -jm 1024 -tm 4096 -s 6 -d -nm test3</span><br></pre></td></tr></table></figure>

<p>在这种情况下，Flink YARN 客户端将仅向群集提交 Flink，然后自行关闭。</p>
<p>请注意，在这种情况下，无法使用 Flink 停止YARN会话。使用YARN实用程序（yarn application -kill <appId>）来停止YARN会话。</appId></p>
<p><img src="/images/blog/2019-07-25-10.png" alt></p>
<p>分离模式 Java 进程只会有 YarnSessionClusterEntrypoint，而没有 FlinkYarnSessionCli，可以看到客户端模式和分离式模式的区别，除了进程外，其他都一样。</p>
<h5 id="附加到现有会话"><a href="#附加到现有会话" class="headerlink" title="附加到现有会话"></a>附加到现有会话</h5><p>假设现在在其他机器上启动了一个会话，但是这边又有需要附加到那个会话，这时就有作用了</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Usage:</span><br><span class="line">   Required</span><br><span class="line">     -id,--applicationId &lt;yarnAppId&gt; YARN application Id</span><br></pre></td></tr></table></figure>

<p>示例: 发出以下命令以附加到正在运行的Flink YARN会话application_1463870264508_0029：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;yarn-session.sh -id application_1463870264508_0029</span><br></pre></td></tr></table></figure>

<p>附加到正在运行的会话使用 YARN ResourceManager 来确定作业管理器RPC端口。</p>
<p>通过停止 unix 进程（使用CTRL + C）或在客户端输入“stop”来停止YARN会话。</p>
<h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><p>系统默认使用 con/flink-conf.yaml 里的配置。</p>
<p>Flink on yarn将会覆盖掉几个参数:</p>
<ul>
<li>jobmanager.rpc.address 因为jobmanager的在集群的运行位置并不是实现确定的，前面也说到了就是am的地址</li>
<li>taskmanager.tmp.dirs 使用yarn给定的临时目录</li>
<li>parallelism.default也会被覆盖掉，如果在命令行里指定了slot数。</li>
</ul>
<p>如果不想更改 conf/flink-conf.yaml 配置文件以设置配置参数，则可以选择通过-D标志传递动态属性。所以可以这样传递参数：-Dfs.overwrite-files=true -Dtaskmanager.network.memory.min=536346624。</p>
<p>yarn-session 的 参数介绍<br>  -n: 指定TaskManager的数量;<br>  -d: 以分离模式运行;<br>  -id: 指定yarn的任务ID;<br>  -j: Flink jar文件的路径;<br>  -jm: JobManager容器的内存（默认值：MB）;<br>  -nl: 为YARN应用程序指定YARN节点标签;<br>  -nm: 在YARN上为应用程序设置自定义名称;<br>  -q: 显示可用的YARN资源（内存，内核）;<br>  -qu: 指定YARN队列;<br>  -s: 指定TaskManager中slot的数量;<br>  -st: 以流模式启动Flink;<br>  -tm: 每个TaskManager容器的内存（默认值：MB）;<br>  -z: 命名空间，用于为高可用性模式创建Zookeeper子路径;</p>
<p><strong>客户端模式与分离模式选用</strong></p>
<p>简单来说就是如果用分离式模式 ，那么在启动的时候会在yarn中常驻一个进程，并且已经确定了之后提交的job的内存等资源的大小，比如8G内存，如果某一个job把8G内存全部占完了，只能是第一个job执行完成把资源释放了，第二个job才能继续执行。<br>如果是客户端模式，那么提交后，资源的大小是由yarn的队列所决定的，多个job提交，资源的占用和竞争都是由yarn所控制。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://baijiahao.baidu.com/s?id=1626634215130992812&wfr=spider&for=pc" target="_blank" rel="noopener">大数据框架Flink的三大部署模式</a></li>
<li><a href="https://developer.aliyun.com/ask/133627?spm=a2c6h.13159736" target="_blank" rel="noopener">Flink on Yarn 有两种模式 分离模式 和 客户端模式</a></li>
<li><a href="https://mp.weixin.qq.com/s/VCmeuzz-euCmu9ojZJ3uaA" target="_blank" rel="noopener">Flink on yarn应用部署</a></li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/deployment/yarn_setup.html#flink-yarn-session" target="_blank" rel="noopener">Fliin YARN设置</a></li>
<li><a href="https://www.jianshu.com/p/1b05202c4fb6" target="_blank" rel="noopener">Flink on yarn部署模式</a></li>
</ul>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
  </entry>
  <entry>
    <title>Flink WordCount 程序</title>
    <url>/2019/07/23/Flink-quick-start/</url>
    <content><![CDATA[<p>Mac 上搭建 Flink 1.8.0 环境并构建运行简单 WordCount 简单程序入门</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>18年关注到的 Flink。那是还懵懵懂懂参加了 Flink Forward in China 2018。</p>
<p>因岗位为 SRE 和公司暂无需要。一直没有认真入门过。</p>
<p>这次打算作为 19 年下半年计划正式入坑学习。</p>
<p>开始前建议先阅读下云邪<a href="https://mp.weixin.qq.com/s/8jEMQuU1PsuZOYj0mTtztQ" target="_blank" rel="noopener">入门教程 5分钟从零构建第一个 Flink 应用</a></p>
<h4 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h4><p>Flink 可以运行在 Linux、Mac 和 Windows 上，唯一的要求就是必须安装 Java 8 或以上版本。</p>
<p>可以通过发出以下命令来检查Java的正确安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">java -version</span><br></pre></td></tr></table></figure>

<p>有 Java 8，输出将如下所示</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">java version &quot;1.8.0_201&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_201-b09)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.201-b09, mixed mode)</span><br></pre></td></tr></table></figure>

<p>Java 安装后去<a href="https://flink.apache.org/zh/downloads.html" target="_blank" rel="noopener">官网</a>下载 Flink，然后解压即可运行。这里以 flink-1.8.0 为例。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd ~&#x2F;Downloads&#x2F;</span><br><span class="line">tar xzf flink-1.8.0-bin-scala_2.12.tgz</span><br><span class="line">cd flink-1.8.0</span><br></pre></td></tr></table></figure>

<p>配置全局变量。FLINK_HOME 写你自己的 Flink 路径</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim ~&#x2F;.bash_profile</span><br><span class="line"></span><br><span class="line"># Flink HOME</span><br><span class="line">FLINK_HOME&#x3D;&#x2F;Users&#x2F;tu&#x2F;Public&#x2F;SoftWare&#x2F;flink-1.8.0 </span><br><span class="line">export PATH&#x3D;$FLINK_HOME&#x2F;bin:$PATH</span><br></pre></td></tr></table></figure>

<p>对于 MacOS 用户，可以选择通过 homebrew 安装 Flink。一键安装就可以用，不需要配置全局变量</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">brew install apache-flink</span><br></pre></td></tr></table></figure>

<p>检查安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ flink --version</span><br><span class="line">Version: 1.8.0, Commit ID: 4caec0d</span><br></pre></td></tr></table></figure>

<p>在 Flink 目录下运行以下命令即可启动</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[16:29:35] tu flink-1.8.0 $ .&#x2F;bin&#x2F;start-cluster.sh</span><br><span class="line">Starting cluster.</span><br><span class="line">Starting standalonesession daemon on host lihuimindeMacBook-Pro.local.</span><br><span class="line">Starting taskexecutor daemon on host lihuimindeMacBook-Pro.local.</span><br></pre></td></tr></table></figure>

<p>接着可以打开浏览器访问 <a href="http://localhost:8081/" target="_blank" rel="noopener">http://localhost:8081/</a> 查看</p>
<p><img src="/images/blog/2019-07-23-1.png" alt> </p>
<p>通过 jps 可以看到多出来两个JVM进程，运行的主类</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[16:29:45] tu bin $ jps -l</span><br><span class="line">9169 org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint</span><br><span class="line">9702 sun.tools.jps.Jps</span><br><span class="line">9658 org.apache.flink.runtime.taskexecutor.TaskManagerRunner</span><br></pre></td></tr></table></figure>

<h4 id="开发"><a href="#开发" class="headerlink" title="开发"></a>开发</h4><p>集群启动后就可以开发 Flink 程序</p>
<p>使用 IDEA 新建一个 maven 项目</p>
<p><img src="/images/blog/2019-07-23-2.png" alt> </p>
<p>如果没有看到 flink-quickstart-java 的话通过右上角的 Add Archetype 按钮来添加。</p>
<p><img src="/images/blog/2019-07-23-3.png" alt> </p>
<p>创建一个 SocketTextStreamWordCount Java 文件，加入以下代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.lihm.examples.streaming;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.utils.ParameterTool;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> lihm</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2019-07-23 16:18</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span> TODO</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SocketTextStreamWordCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// the port to connect to</span></span><br><span class="line">        <span class="keyword">final</span> String hostname;</span><br><span class="line">        <span class="keyword">final</span> <span class="keyword">int</span> port;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">final</span> ParameterTool params = ParameterTool.fromArgs(args);</span><br><span class="line">            port = params.getInt(<span class="string">"port"</span>);</span><br><span class="line">            hostname = params.get(<span class="string">"hostname"</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            System.err.println(<span class="string">"USAGE: Please run 'SocketWindowWordCount --hostname &lt;hostname&gt; --port &lt;port&gt;'"</span>);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// set up the streaming execution environment</span></span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取数据</span></span><br><span class="line">        DataStreamSource&lt;String&gt; stream = env.socketTextStream(hostname, port);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 计数</span></span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = stream.flatMap(<span class="keyword">new</span> LineSplitter())</span><br><span class="line">                .keyBy(<span class="number">0</span>)</span><br><span class="line">                .sum(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        sum.print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"Java WordCount from SocketTextStream Example"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">LineSplitter</span> <span class="keyword">implements</span> <span class="title">FlatMapFunction</span>&lt;<span class="title">String</span>, <span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector)</span> </span>&#123;</span><br><span class="line">            String[] tokens = s.toLowerCase().split(<span class="string">"\\W+"</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (String token: tokens) &#123;</span><br><span class="line">                <span class="keyword">if</span> (token.length() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                    collector.collect(<span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(token, <span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>官网还有 Scala 版本的。自己学习尝试即可。这里不介绍了</p>
<h4 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h4><p>接着进入工程目录，使用以下命令打包。</p>
<p><code>mvn clean package -Dmaven.test.skip=true</code></p>
<p><img src="/images/blog/2019-07-23-4.png" alt> </p>
<p>然后开启监听 9000 端口</p>
<p><code>nc -l 9000</code></p>
<p><img src="/images/blog/2019-07-23-5.png" alt> </p>
<p>最后进入 flink 安装目录 bin 下执行以下命令提交job任务。注意换成你自己项目的路径</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">flink run -c cn.lihm.examples.streaming.SocketTextStreamWordCount flink-learning-examples-1.0-SNAPSHOT.jar --port 9000 --hostname 127.0.0.1</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/2019-07-23-6.png" alt> </p>
<p>执行完上述命令后，可以在 webUI 中看到正在运行的程序</p>
<p><img src="/images/blog/2019-07-23-7.png" alt> </p>
<p>可以在 nc 监听端口中输入 text</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ nc -l 9000</span><br><span class="line">lorem ipsum</span><br><span class="line">ipsum ipsum ipsum</span><br><span class="line">bye</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/2019-07-23-8.png" alt> </p>
<p>该任务的输出在 flink 家目录下 log 下以 .out 结尾的文件下。<br>通过 tail 命令看一下输出的文件，来观察统计结果。</p>
<p><img src="/images/blog/2019-07-23-9.png" alt> </p>
<p>最后测试完可以关闭集群</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;stop-cluster.sh</span><br></pre></td></tr></table></figure>

<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>整个过程下来还是有些成就感的。提高了自己动手能力。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install/" target="_blank" rel="noopener">Flink 从 0 到 1 学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></li>
<li><a href="https://www.jianshu.com/p/c9b1081215e7" target="_blank" rel="noopener">flink WordCount初体验</a></li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/tutorials/local_setup.html" target="_blank" rel="noopener">本地安装教程</a></li>
<li><a href="https://mp.weixin.qq.com/s/8jEMQuU1PsuZOYj0mTtztQ" target="_blank" rel="noopener">入门教程 5分钟从零构建第一个 Flink 应用</a></li>
</ul>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
  </entry>
  <entry>
    <title>IDEA 常用快捷键</title>
    <url>/2019/07/19/IDEA-Shortcut-Keys/</url>
    <content><![CDATA[<p>记录自己工作中遇到的 IDEA 快捷键。 可以当作字典来查找</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>平常工作中用的最多的 IDE 就是大名鼎鼎的 IDEA。</p>
<p>然而作为一个吃饭的工具。好多地方不熟悉，都是每次需要各种搜索。</p>
<p>觉得有必要通过文字的方式积累。</p>
<p>本人使用的是 Mac 下的 IDEA。</p>
<h4 id="查看子类"><a href="#查看子类" class="headerlink" title="查看子类"></a>查看子类</h4><p>按键: command + option + b</p>
<p>光标在方法名上输入快捷键就会自动跳转到实现方法上。参考<a href="https://blog.csdn.net/u010003835/article/details/79036666" target="_blank" rel="noopener">IDEA_查找接口的实现 的快捷键</a></p>
<h4 id="外能快捷键"><a href="#外能快捷键" class="headerlink" title="外能快捷键"></a>外能快捷键</h4><p>IDEA里有一个万能快捷键(alt enter)，功能非常强大，同一个快捷键，可以根据不同的语境提示你不同的操作。</p>
<p>万能快捷键可以做很多事情，大概17点:</p>
<ul>
<li>转换lambda表达式</li>
<li>equals的翻转</li>
<li>自动导包</li>
<li>异常捕获</li>
<li>SimpleDateFormat转换为ThreadLocal封装的SimpleDateFormat</li>
<li>注释移动</li>
<li>便捷的json字符串处理</li>
<li>转化为高效运算的二进制</li>
<li>if……else变成简单三目运算 以及一些简化操作</li>
<li>引入局部变量</li>
<li>实现接口</li>
<li>实现抽象类</li>
<li>修复方法返回值、增加入参、减少入参、方法生成等</li>
<li>字符串相关操作</li>
<li>， + &lt; 等位置互换</li>
<li>java访问修饰符的更改操作</li>
<li>自动生成属性操作</li>
</ul>
<p>作者零度冰炫还制作了 GIF 图。可以在<a href="http://www.jiangxinlingdu.com/idea/2019/07/19/idea.html" target="_blank" rel="noopener">idea万能快捷键(alt enter)，你不知道的17个实用技巧！！！</a> 查看</p>
]]></content>
      <categories>
        <category>Tools</category>
      </categories>
  </entry>
  <entry>
    <title>平均负载</title>
    <url>/2019/07/14/load-average/</url>
    <content><![CDATA[<p>到底应该怎么理解“平均负载”？</p>
<hr>
<h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><p>什么是平均负载？</p>
<p>正确定义: 单位时间内，系统处于<strong>可运行状态</strong>和<strong>不可中断状态</strong>的平均进程数，也就是平均活跃进程数 </p>
<p>先解释下，可运行状态和不可中断状态</p>
<p><img src="/images/blog/2019-12-11-1.png" alt></p>
<p>可运行状态的进程: 正在使用 CPU 或者正在等待 CPU 的进程,即 ps aux 命令下STAT处于R状态（Running 或 Runnable）的进程<br>不可中断状态的进程: 正处于内核态关键流程中的进程，并且这些流程是不可打断的，比如常见是等待硬件设备的 I/O 响应。也就是我们在Ps 命令看到的D状态(Uninterruptible Sleep，也称为 Disk Sleep）的进</p>
<p>比如，当一个进程向磁盘读写数据时，为了保证数据的一致性，在得到磁盘回复前，它是不能被其他进程或者中断打断的，这个时间的进程就处于不可中断状态。如果此时的进程被打断，就容易出现磁盘数据与进程数据不一致的问题。   </p>
<p>所以，不可中断状态实际上是系统对进程和硬件设备的一种保护机制</p>
<p>因此，可以简单理解为，平均负载其实就是平均活跃进程数。平均活跃进程数，直观上的理解就是单位时间内的活跃进程数。</p>
<p>理想状态: 每个cpu上都有一个活跃进程，即平均负载数等于 cpu 数<br>过载经验值: 平均负载高于 cpu 数量70%的时候</p>
<p>为什么是 CPU 数量的70%呢？ 请阅读<a href="https://blog.csdn.net/zwldx/article/details/82812704" target="_blank" rel="noopener">Linux系统平均负载3个数字的含义</a></p>
<p>类比的形象理解可以阅读<a href="https://blog.csdn.net/zuozewei/article/details/86483503" target="_blank" rel="noopener">性能基础之理解Linux系统平均负载和CPU使用率</a></p>
<h4 id="相关命令"><a href="#相关命令" class="headerlink" title="相关命令"></a>相关命令</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CPU 核数: lscpu、 grep &#39;model name&#39; &#x2F;proc&#x2F;cpuinfo | wc -l</span><br></pre></td></tr></table></figure>

<p>显示平均负载: uptime、top，显示的顺序是最近1分钟、5分钟、15分钟，从此可以看出平均负载的趋势</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@massive-dataset-new-002 ~]# uptime</span><br><span class="line"> 20:36:29 up 202 days,  1:32,  1 user,  load average: 0.00, 0.00, 0.00</span><br></pre></td></tr></table></figure>

<p>分别是当前时间、系统运行时间以及正在登录用户数、依次则是过去 1 分钟、5 分钟、15 分钟的平均负载（Load Average）</p>
<p>查看 cpu 负载变化的命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">watch -d uptime  # -d 会高亮显示变化的区域</span><br></pre></td></tr></table></figure>

<p>mpstat: 多核cpu性能分析工具，-P ALL监视所有cpu</p>
<p>pidstat: 进程性能分析工具，-u 显示cpu利用率</p>
<h4 id="平均负载与cpu使用率的区别"><a href="#平均负载与cpu使用率的区别" class="headerlink" title="平均负载与cpu使用率的区别"></a>平均负载与cpu使用率的区别</h4><p>CPU使用率: 单位时间内cpu繁忙情况的统计</p>
<ul>
<li><p>CPU密集型进程，CPU使用率和平均负载基本一致</p>
</li>
<li><p>IO密集型进程，平均负载升高，CPU使用率不一定升高</p>
</li>
<li><p>大量等待CPU的进程调度，平均负载升高，CPU使用率也升高                      </p>
</li>
</ul>
<h4 id="惯例"><a href="#惯例" class="headerlink" title="惯例"></a>惯例</h4><p>系统过载并超过1.0的负载值有时不是问题，因为即使有一些延迟，CPU也会处理队列中的作业，负载将再次降低到1.0以下的值。但是如果系统的持续负载值大于1，则意味着它无法吸收执行中的所有负载，因此其响应时间将增加，系统将变得缓慢且无响应。高于1的高值，尤其是最后5分钟和15分钟的负载平均值是一个明显的症状，要么我们需要改进计算机的硬件，通过限制用户可以对系统的使用来节省更少的资源，或者除以多个相似节点之间的负载。</p>
<p>因此，我们提出以下建议：</p>
<ul>
<li>&gt;=0.70：没有任何反应，但有必要监控 CPU 负载。如果在一段时间内保持这种状态，就必须在事情变得更糟之前进行调查。</li>
<li>&gt;=1.00：存在问题，您必须找到并修复它，否则系统负载的主要高峰将导致您的应用程序变慢或无响应。</li>
<li>&gt;=3.00：你的系统变得 非常慢。甚至很难从命令行操作它来试图找出问题的原因，因此修复问题需要的时间比我们之前采取的行动要长。你冒的风险是系统会更饱和并且肯定会崩溃。</li>
<li>&gt;=5.00：你可能无法恢复系统。你可以等待奇迹自发降低负载，或者如果你知道发生了什么并且可以负担得起，你可以在控制台中启动 kill -9 <process_name> 之类的命令 ，并祈求它运行在某些时候，以减轻系统负荷并重新获得其控制权。否则，你肯定别无选择，只能重新启动计算机。</process_name></li>
</ul>
<h4 id="调优"><a href="#调优" class="headerlink" title="调优"></a>调优</h4><p>平均负载过高时，如何调优</p>
<p>调优的思路无非就是找到导致平均负载高进程，在对相应的进程进行处理</p>
<p>sysstat，yum 即可安装</p>
<ol>
<li><p>CPU密集型进程<br>mpstat -P ALL 5: -P ALL表示监控所有CPU，5表示每5秒刷新一次数据，观察是否有某个cpu的%usr会很高，但%iowait应很低<br>pidstat -u 5 1：每5秒输出一组数据，观察哪个进程%cpu很高，但是%wait很低，极有可能就是这个进程导致cpu飚高</p>
</li>
<li><p>IO密集型进程<br>mpstat -P ALL 5: 观察是否有某个cpu的%iowait很高，同时%usr也较高(比大多数的高，但是又不是特别高)，这说明，平均负载的升高是由于 iowait 的升高。<br>pidstat -u 5 1：观察哪个进程的各项数值比其他的高</p>
</li>
<li><p>大量进程<br>pidstat -u 5 1：观察那些%wait较高的进程是否有很多， %wait 列就是每个进程等待 CPU 的时间</p>
</li>
</ol>
<p>注意因为centos 自带的sysstat版本低，没有%wait这一列，所以需要升级为11.5.5。</p>
<p>升级步骤请参<a href="https://www.cnblogs.com/wuzm/p/11098008.html" target="_blank" rel="noopener">sysstat安装升级</a></p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/zuozewei/article/details/86483503" target="_blank" rel="noopener">性能基础之理解Linux系统平均负载和CPU使用率</a></li>
</ul>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>2019年六月份总结与七月份计划</title>
    <url>/2019/07/14/July/</url>
    <content><![CDATA[<p>2019年六月份总结与七月份计划</p>
<hr>
<h4 id="老骥伏枥-志在千里"><a href="#老骥伏枥-志在千里" class="headerlink" title="老骥伏枥,志在千里"></a>老骥伏枥,志在千里</h4><p>六月份未制定计划</p>
<h4 id="前路虽长，犹可期许"><a href="#前路虽长，犹可期许" class="headerlink" title="前路虽长，犹可期许"></a>前路虽长，犹可期许</h4><h4 id="对酒当歌，人生几何"><a href="#对酒当歌，人生几何" class="headerlink" title="对酒当歌，人生几何"></a>对酒当歌，人生几何</h4><h4 id="凡事预则立，不预则废"><a href="#凡事预则立，不预则废" class="headerlink" title="凡事预则立，不预则废"></a>凡事预则立，不预则废</h4><p>七月份，想做一个新的自己。</p>
<p><strong>技术</strong></p>
<ul>
<li>Q3 季度输出 20 篇个人博客以上，要求涵盖 HBase 知识体系至少5篇 、Flink 知识体系至少5 篇，Linux 运维体系至少5 篇</li>
<li>Leetcode 周赛，评分1800+以上</li>
<li>Leetcode 热题 Top 100 做完</li>
</ul>
<p><strong>生活</strong></p>
<ul>
<li>少说脏话</li>
<li>体重增加至 120 斤</li>
<li>煲一次鸡汤</li>
<li>做一次苦瓜酿肉</li>
<li>驾照</li>
<li>利用番茄时钟和四象限安排法来合理利用时间。12 周时间内每周目标40个</li>
<li>字帖一本</li>
</ul>
<p><strong>阅读</strong></p>
<ul>
<li>《代码之外的生存指南》</li>
<li>《理财不用懂太多》</li>
<li>《未来简史》</li>
</ul>
<p><strong>理财</strong></p>
<ul>
<li>通过月底分析每月支付宝、京东消费数据, 逐渐减掉一些不理性, 没有意义的消费手段达到3个月娱乐网购方面总消费不超过 2k</li>
<li>分析两三种理财产品, 要分析清楚其利弊, 收益等各项指标, 为以后理财做准。输出文章</li>
<li>提高财务方面认知，听8周薛兆丰经济学课音频</li>
<li>个人存款 1.5 w+ </li>
</ul>
]]></content>
      <categories>
        <category>Life</category>
      </categories>
  </entry>
  <entry>
    <title>Hadoop API</title>
    <url>/2019/07/12/Hadoop-API/</url>
    <content><![CDATA[<p>记录自己工作中遇到的 Hadoop API。 可以当作字典来查找</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>记录自己工作中遇到的 Hadoop API。 可以当作字典来查找</p>
<h4 id="HDSF"><a href="#HDSF" class="headerlink" title="HDSF"></a>HDSF</h4><h5 id="HDFS-获取目录大小"><a href="#HDFS-获取目录大小" class="headerlink" title="HDFS 获取目录大小"></a>HDFS 获取目录大小</h5><p>获取文件大小，在命令行上，使用 hadoop fs -du 命令可以，但是通过 Java API 怎么获取呢</p>
<p>最开始想到的是递归的方法，这个方法很慢，后来发现 FileSystem.getContentSummary 的方法。</p>
<p>参考自<a href="https://superlxw1234.iteye.com/blog/1514586" target="_blank" rel="noopener">java api获取hdfs目录大小</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">fs.getContentSummary(new Path(PathSrc)).getLength();</span><br></pre></td></tr></table></figure>

<h5 id="获取-HDFS-存储信息"><a href="#获取-HDFS-存储信息" class="headerlink" title="获取 HDFS 存储信息"></a>获取 HDFS 存储信息</h5><p>FsStatus 对象提供了获取 hadoop 总空间、已使用空间、剩余空间 的方法。使用方式如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FileSystem fileSystem &#x3D; FileSystem.get(new URI(hdfsURL), configuration);</span><br><span class="line">FsStatus fsStatus &#x3D; fileSystem.getStatus();</span><br><span class="line">long capacity &#x3D; fsStatus.getCapacity(); &#x2F;&#x2F;Configured Capacity</span><br><span class="line">long remaining &#x3D; fsStatus.getRemaining(); &#x2F;&#x2F;DFS Remaining</span><br><span class="line">long used &#x3D; fsStatus.getUsed(); &#x2F;&#x2F; DFS Used</span><br></pre></td></tr></table></figure>

<h5 id="获取-NN-的-active-节点"><a href="#获取-NN-的-active-节点" class="headerlink" title="获取 NN 的 active 节点"></a>获取 NN 的 active 节点</h5><p>是使用 Hadoop 源码提供的 HAUtil 工具类来做的</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.FileSystem;</span><br><span class="line">import org.apache.hadoop.hdfs.HAUtil;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line">import java.net.InetSocketAddress;</span><br><span class="line">import java.net.URI;</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * @author lihm</span><br><span class="line"> * @date 2019-07-26 09:48</span><br><span class="line"> * @description TODO</span><br><span class="line"> *&#x2F;</span><br><span class="line"></span><br><span class="line">public class DemoApplication &#123;</span><br><span class="line"></span><br><span class="line">    public void getNameNodeAdress() throws Exception &#123;</span><br><span class="line">        Configuration conf &#x3D; new Configuration();</span><br><span class="line">        String nameservices &#x3D; &quot;testcluster&quot;;</span><br><span class="line">        String[] namenodesAddr &#x3D; &#123;&quot;massive-dataset-new-001:8020&quot;, &quot;massive-dataset-new-008:8020&quot;&#125;;</span><br><span class="line">        String[] namenodes &#x3D; &#123;&quot;nn1&quot;, &quot;nn2&quot;&#125;;</span><br><span class="line">        conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs:&#x2F;&#x2F;&quot; + nameservices);</span><br><span class="line">        conf.set(&quot;dfs.nameservices&quot;, nameservices);</span><br><span class="line">        conf.set(&quot;dfs.ha.namenodes.&quot; + nameservices, namenodes[0] + &quot;,&quot; + namenodes[1]);</span><br><span class="line">        conf.set(&quot;dfs.namenode.rpc-address.&quot; + nameservices + &quot;.&quot; + namenodes[0], namenodesAddr[0]);</span><br><span class="line">        conf.set(&quot;dfs.namenode.rpc-address.&quot; + nameservices + &quot;.&quot; + namenodes[1], namenodesAddr[1]);</span><br><span class="line">        conf.set(&quot;dfs.client.failover.proxy.provider.&quot; + nameservices, &quot;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&quot;);</span><br><span class="line">        String hdfsRPCUrl &#x3D; &quot;hdfs:&#x2F;&#x2F;&quot; + nameservices + &quot;:&quot; + 8020;</span><br><span class="line"></span><br><span class="line">        FileSystem fs &#x3D; null;</span><br><span class="line">        try &#123;</span><br><span class="line">            fs &#x3D; FileSystem.get(new URI(hdfsRPCUrl), conf);</span><br><span class="line">            InetSocketAddress active &#x3D; HAUtil.getAddressOfActive(fs);</span><br><span class="line">            System.out.println(active.getHostString());</span><br><span class="line">            &#x2F;&#x2F; System.out.println(&quot;hdfs port:&quot; + active.getPort());</span><br><span class="line">            &#x2F;&#x2F; InetAddress address &#x3D; active.getAddress();</span><br><span class="line">            &#x2F;&#x2F; System.out.println(&quot;hdfs:&#x2F;&#x2F;&quot; + address.getHostAddress() + &quot;:&quot;+ active.getPort()); </span><br><span class="line">        &#125; catch (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125; finally &#123;</span><br><span class="line">            try &#123;</span><br><span class="line">                if (fs !&#x3D; null) &#123;</span><br><span class="line">                    fs.close();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; catch (IOException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        DemoApplication nn &#x3D; new DemoApplication() ;</span><br><span class="line">        try &#123;</span><br><span class="line">            nn.getNameNodeAdress();</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>参考自<a href="https://blog.csdn.net/xiaoyutongxue6/article/details/81120589" target="_blank" rel="noopener">通过api获取NN的active节点</a></p>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title>MAC 终端走代理服务器</title>
    <url>/2019/07/01/Mac-sock/</url>
    <content><![CDATA[<p>MAC 终端走代理服务器</p>
<hr>
<h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p>朋友让我到 Google 一个仓库下 git clone 一个代码仓库</p>
<p>使用终端 git clone 一直报超时。马上意识到终端没有走 ssr。</p>
<p>MAC 终端，默认不走代理服务器；即浏览器已经可以FQ，但是终端不行。</p>
<h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><p>直接设置终端的代理，本文 用的是 shadowSocksX-NG-R8</p>
<p><img src="/images/blog/2019-07-01-8.png" alt> </p>
<p>打开终端，直接执行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export all_proxy&#x3D;socks5:&#x2F;&#x2F;127.0.0.1:1080 # 配置http和https访问</span><br></pre></td></tr></table></figure>

<p>注意: shadowSocks 设置中，需要和上面的设置是对应的</p>
<p><img src="/images/blog/2019-07-01-9.png" alt> </p>
<p>如端口设置为1086。就把1080改成1086</p>
<p>附: 如果想每次打开终端，都想默认走代理的话，可以把 设置命令，放到 shell 配置文件中；mac 下的路径为：~/.bash_profile  </p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.cnblogs.com/agang-php/p/10618299.html" target="_blank" rel="noopener">MAC 终端走代理服务器</a></li>
</ul>
]]></content>
      <categories>
        <category>Tools</category>
      </categories>
  </entry>
  <entry>
    <title>HBase Meta 表</title>
    <url>/2019/06/30/HBase-Meta/</url>
    <content><![CDATA[<p>HBase Meta 表</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>目录表 hbase:meta 作为HBase 表存在，并从 HBase shell 的list命令中过滤掉（命名空间”hbase”下的表都会过滤掉），但实际上是一个表，就像任何其他表一样。</p>
<p><img src="/images/blog/2019-07-01-1.png" alt> </p>
<h4 id="meta-表"><a href="#meta-表" class="headerlink" title="meta 表"></a>meta 表</h4><p>hbase:meta 表属于系统表</p>
<p>META表是一个保存的了系统中所有 Region 列表的 HBase 表</p>
<p>它保存在一个 RS 上面，那如何去知道它在哪个 RS 上呢？ 这就要利用 zk 了</p>
<p>meta 表的地址信息保存在 zk 的 /hbase 路径下的 meta-region-server 节点上</p>
<p><img src="/images/blog/2019-07-01-2.png" alt> </p>
<h4 id="表结构"><a href="#表结构" class="headerlink" title="表结构"></a>表结构</h4><p><img src="/images/blog/2019-07-01-3.png" alt> </p>
<p>Key: Region key of the format ([table表名],[region start key起始键],[region id])</p>
<p><code>region id 由该 region 生成的时间戳（精确到毫秒）与 region encoded 组成</code><br><code>region encoded 由 region 所在的 表名, StartKey, 时间戳这三者的MD5值产生，
HBase 在 HDFS 上存储 region 的路径就是 region encoded。</code></p>
<p>key 被用来表示 region name</p>
<p><img src="/images/blog/2019-07-01-4.png" alt> </p>
<p>values:</p>
<ul>
<li><p>info:regioninfo, RegionInfo 的 encodeValue值</p>
</li>
<li><p>info:seqnumDuringOpen, 序列号</p>
</li>
<li><p>info:server, Region 所在的 RS</p>
</li>
<li><p>info:serverstartcode, RS 启动的 timestamp</p>
</li>
</ul>
<p><img src="/images/blog/2019-07-01-5.png" alt> </p>
<p>关于 HRegionInfo 的注释</p>
<p>空键用于表示表开始和表结束。具有空开始键的区域是表中的第一个区域。如果某个区域同时具有空开始和空结束键，则它是表中唯一的区域</p>
<h4 id="2-x-变化"><a href="#2-x-变化" class="headerlink" title="2.x 变化"></a>2.x 变化</h4><p>HBase 2.x meta 表有所变化</p>
<p><img src="/images/blog/2019-12-20-1.png" alt> </p>
<table>
<thead>
<tr>
<th>列名</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>info:state</td>
<td>Region状态</td>
</tr>
<tr>
<td>info:sn</td>
<td>Region Server Node，由 server 和 serverstartcode 组成</td>
</tr>
<tr>
<td>info:serverstartcode</td>
<td>Region Server启动Code，实质上就是Region Server启动的时间戳</td>
</tr>
<tr>
<td>info:server</td>
<td>Region Server 地址和端口</td>
</tr>
<tr>
<td>info:seqnumDuringOpen</td>
<td>表示Region在线时长的一个二进制串</td>
</tr>
<tr>
<td>info:regioninfo</td>
<td>Region 的详细信息，和 .regioninfo 内容相同</td>
</tr>
</tbody></table>
<p>其中，regioninfo 是重要信息</p>
<ul>
<li>ENCODED：基于${表名},${起始键},${region时间戳}生成的32位md5字符串，region数据存储在hdfs上时使用的唯一编号，可以从meta表中根据该值定位到hdfs中的具体路径。 rowkey中最后的${encode编码}就是 ENCODED 的值，其是rowkey组成的一部分。</li>
<li>NAME：与 ROWKEY 值相同</li>
<li>STARTKEY：该 region 的起始键</li>
<li>ENDKEY：该 region 的结束键</li>
</ul>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://yq.aliyun.com/articles/586755?utm_content=m_48695" target="_blank" rel="noopener">HBase运维基础——元数据逆向修复原理</a></li>
</ul>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/83237810" target="_blank" rel="noopener">HBase数据块NotServingRegionException排查</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>HBase Region 过多导致集群问题事件</title>
    <url>/2019/06/27/HBase-IO-Case/</url>
    <content><![CDATA[<p>HBase Region 过多导致的集群性能不佳</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>有 10 个节点的HBase集群。每天早上 10 点所有 RS warn 告警。</p>
<p><img src="/images/blog/2019-06-27-1.png" alt> </p>
<p>持续过程 30 分钟。</p>
<p><img src="/images/blog/2019-06-27-2.png" alt> </p>
<p>GC 时间过长，还触发了 RS 宕机。</p>
<h4 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h4><p>优先看 GC 告警的机器。查看 CM 监控发现该机器的 CPU，磁盘吞吐量在 10:00 - 10:30 这一段时间都是飙高的。</p>
<p><img src="/images/blog/2019-06-27-3.png" alt> </p>
<p>我接着查看发现所有的 RS 都是 10:00 - 10:30 这一段时间都是飙高的。</p>
<p><img src="/images/blog/2019-06-27-4.png" alt> </p>
<p>肯定是什么触发的。先看读写读写量。</p>
<p><img src="/images/blog/2019-06-27-5.png" alt> </p>
<p>读写量也不大，说明不是由大批量请求造成的。</p>
<p>监控看不出问题，那就转战看日志。</p>
<p>发现有很多类似如下的输出:</p>
<p><code>... because info has and old edit so flush to free WALs after random delay ...</code></p>
<p><img src="/images/blog/2019-06-27-6.png" alt> </p>
<p>这是很明显的写入量很小，因为周期性 flush 线程触发的行为，比如某 store 很久没更新了而最新的 edit 距今超过阈值（默认 1小时），<br>那么就会 delay 一个 random 时间去执行刷新。参阅<a href="https://lihuimintu.github.io/2019/06/25/HBase-Flush/" target="_blank" rel="noopener">HBase Flush 时机</a><br>第5点定期刷新</p>
<p>通过如下关键字去看历次触发的 flush 产生的文件大小</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">grep &#39;org.apache.hadoop.hbase.regionserver.HStore: Added hdfs&#39; &#x2F;var&#x2F;log&#x2F;hbase&#x2F;hbase-cmf-hbase-REGIONSERVER-$&#123;FQDN_HOSTNAME&#125;.log.out | awk &#39;&#123;print $1&quot; &quot;$2&quot; &quot;$(NF-1)$NF&#125;&#39;</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/2019-06-27-7.png" alt> </p>
<p>发现几乎每次刷出来的都是小文件，不到 100KB。</p>
<p>为什么这么多小文件呢？小文件过多会触发 compaction 机制。</p>
<p>刚好同事已经排查出小文件过多原因。</p>
<p><img src="/images/blog/2019-06-27-8.png" alt> </p>
<p>HBase 集群的 Region 个数达到了 2w 个。因为该 HBase 集群上有 Kylin 服务。Kylin 生成了大量的临时表。<br>而清理临时表的定时脚本因配置不当没有启动起来。导致该 HBase 集群 Region 个数越来越多。Region 越多，MemStore 刷新越小，<br>所以产生了小 HFile 文件。</p>
<p>compaction 机制检查到 Store 中 HFile 个数达到 3 个时就会执行 Compaction</p>
<p>同事手动运行脚本之后已经将 Region 个数降到 2k 左右了，平摊到 10 个RS就是 200+ 个。符合合理范围。<a href="https://mp.weixin.qq.com/s/0tGNpmBRHbI673TwxIC2NA" target="_blank" rel="noopener">HBase最佳实践之Region数量&amp;大小</a></p>
<p>原因找到了，那我就找找 compaction 发生的痕迹。验证是由 compaction 触发导致服务器压力大。</p>
<p>根据监控上的时间点。看到 CPU 在 10:06 分达到峰值。在日志里查询 10:06 左右发生了什么事。</p>
<p><img src="/images/blog/2019-06-27-9.png" alt> </p>
<p>功夫不负有心人，看到关键<code>CompactionChecker missed its start time</code>(图中绿色框)。</p>
<p>翻译过来就是 CompactionChecker 线程丢失了开始时间。为什么丢失开始时间是因为发生了 GC 造成了 STW(图中蓝色框)，线程被挂起了。这不是重点，接着往下看。</p>
<p>CompactionChecker 线程是干什么的？</p>
<p>CompactionChecker 是RS上的工作线程(Chore)。后台线程 CompactionChecker 定期触发检查是否需要执行 compaction，设置执行周期是通过 threadWakeFrequency 指定。<br>大小通过 hbase.server.thread.wakefrequency 配置(默认10000)，然后乘以默认倍数 hbase.server.compactchecker.interval.multiplier (1000), 毫秒时间转换为秒。因此，在不做参数修改的情况下，CompactionChecker大概是2hrs 46mins 40sec 执行一次。</p>
<p>接着日志我往前翻上一次 CompactionChecker 时间。得到以下信息。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2019-06-27 07:12:02,093 INFO org.apache.hadoop.hbase.ScheduledChore: Chore: CompactionChecker missed its start time</span><br><span class="line">...skipping...</span><br><span class="line">2019-06-27 10:06:20,649 INFO org.apache.hadoop.hbase.ScheduledChore: Chore: CompactionChecker missed its start time</span><br></pre></td></tr></table></figure>

<p>可以计算可以知道两个时间相差 2时 54分 18秒。符合 CompactionChecker 间隔时间。查看 RS 6点到 8点 CPU 监控。</p>
<p><img src="/images/blog/2019-06-27-10.png" alt> </p>
<p>可以看出 7:12 之后 CPU 峰值降下来了。</p>
<p>正如猜想一样。CompactionChecker 是关键。</p>
<h4 id="思路归纳"><a href="#思路归纳" class="headerlink" title="思路归纳"></a>思路归纳</h4><p>Region 过多会影响 HBase 状态。Region 之间共享 RS 的 MemStore 内存区域，因此 Region 过多，MemStore 刷新越小。</p>
<p>当 memstore 满后，就不得不刷新到文件系统，会创建一个数据存储在 HDFS 上的 HFile。</p>
<p>这也意味着 Region 越多，产生的 HFile 越小。</p>
<p>这个也迫使 HBase 执行大量的合并操作才能保持 HFile 的数量低至合理数目。</p>
<p>这些合并操作对集群产生了过多扰动，进而影响集群性能。</p>
<p>到了这里基本可以结案了。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.jianshu.com/p/a0056a35f982" target="_blank" rel="noopener">HBase 线上问题排查 - 为什么读写这么少还会触发巨量 IO ？</a></li>
<li><a href="https://blog.csdn.net/qq_23160237/article/details/89309402" target="_blank" rel="noopener">一次region过多导致HBase服务宕机事件</a></li>
<li><a href="https://mp.weixin.qq.com/s/0tGNpmBRHbI673TwxIC2NA" target="_blank" rel="noopener">HBase最佳实践之Region数量&amp;大小</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>Linux 命令记录</title>
    <url>/2019/06/26/Linux-Command/</url>
    <content><![CDATA[<p>运维过程中所接触到的 Linux 命令</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>话说搞运维的人没有两把“刷子”，都不好意思上服务器操作。</p>
<p>因此想总结下自己运维过程中所接触到的 Linux 命令</p>
<p>不会记录的很详细。想到记录什么就记录什么。</p>
<h4 id="lsb-release"><a href="#lsb-release" class="headerlink" title="lsb_release"></a>lsb_release</h4><p>查看Linux系统版本</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@massive-dataset-new-005 ~]# lsb_release -a</span><br><span class="line">LSB Version:    :base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch</span><br><span class="line">Distributor ID: CentOS</span><br><span class="line">Description:    CentOS release 6.5 (Final)</span><br><span class="line">Release:        6.5</span><br><span class="line">Codename:       Final</span><br></pre></td></tr></table></figure>

<p>当然，还可以直接查看文件看<code>/etc/os-release</code></p>
<p>CentOS下查看<code>/etc/redhat-release</code></p>
<h4 id="uname"><a href="#uname" class="headerlink" title="uname"></a>uname</h4><p>查看Linux内核版本命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@massive-dataset-new-005 ~]# uname -a</span><br><span class="line">Linux massive-dataset-new-005 2.6.32-696.16.1.el6.x86_64 #1 SMP Wed Nov 15 16:51:15 UTC 2017 x86_64 x86_64 x86_64 GNU&#x2F;Linux</span><br></pre></td></tr></table></figure>

<h4 id="crontab"><a href="#crontab" class="headerlink" title="crontab"></a>crontab</h4><p>查看某个用户的 cron 任务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">crontab -u username -l</span><br></pre></td></tr></table></figure>

<p>查看所有用户的 cron 任务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 以root用户执行</span><br><span class="line">cat &#x2F;etc&#x2F;passwd | cut -f 1 -d : |xargs -I &#123;&#125; crontab -l -u &#123;&#125;</span><br></pre></td></tr></table></figure>

<h4 id="bc"><a href="#bc" class="headerlink" title="bc"></a>bc</h4><p>bc命令是一种支持任意精度的交互执行的计算器语言。是Linux简单的计算器,能进行进制转换与计算。</p>
<p>执行浮点运算</p>
<p>参数 scale=3 是将bc输出结果的小数位设置为3位</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo &quot;scale&#x3D;3;10&#x2F;3&quot; | bc</span><br><span class="line">3.333</span><br></pre></td></tr></table></figure>

<p>进制转换</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo &quot;obase&#x3D;2;$255&quot; | bc</span><br><span class="line">11111111</span><br></pre></td></tr></table></figure>

<h4 id="ps"><a href="#ps" class="headerlink" title="ps"></a>ps</h4><p>查进程有两个命令</p>
<ul>
<li>ps -ef</li>
<li>ps aux</li>
</ul>
<p>上面两个命令都是列出所有的进程，还是通过 &#124; 管道和 grep 来过滤掉想要查的进程</p>
<p>比如说 <code>ps -ef | grep java</code></p>
<p>把进程查出来干嘛？知道它的进程ID了，可以把他给杀掉</p>
<p>kill -9 processId 杀掉某个进程</p>
<h4 id="netstat"><a href="#netstat" class="headerlink" title="netstat"></a>netstat</h4><p>查端口也是一个很常见的操作</p>
<p>常见命令  <code>netstat -lntup</code></p>
<p>查看当前所有 tcp/udp 端口的信息</p>
<h4 id="lsof"><a href="#lsof" class="headerlink" title="lsof"></a>lsof</h4><p>查看某个端口详细的信息 <code>lsof -i:4000</code></p>
<h4 id="tail"><a href="#tail" class="headerlink" title="tail"></a>tail</h4><p>最常用的 tail -f</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tail -300f shopbase.log # 倒数300行并进入实时监听文件写入模式</span><br></pre></td></tr></table></figure>

<h4 id="grep"><a href="#grep" class="headerlink" title="grep"></a>grep</h4><p>在文件中搜索字符串匹配的行并输出</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">grep forest f.txt     # 文件查找</span><br><span class="line">grep forest f.txt cpf.txt # 多文件查找</span><br><span class="line">grep &#39;log&#39; &#x2F;home&#x2F;admin -r -n # 目录下查找所有符合关键字的文件</span><br><span class="line">grep ERROR *&#x2F;* 搜索当前路径下所有的文件</span><br><span class="line">cat f.txt | grep -i shopbase  #  -i 不区分大小写	-v 排除指定字符串</span><br><span class="line">grep &#39;shopbase&#39; &#x2F;home&#x2F;admin -r -n --include *.&#123;vm,java&#125; # 指定文件后缀</span><br><span class="line">grep &#39;shopbase&#39; &#x2F;home&#x2F;admin -r -n --exclude *.&#123;vm,java&#125; # 反匹配</span><br><span class="line">seq 10 | grep 5 -A 3    #上匹配  显示匹配后和它后面的3行</span><br><span class="line">seq 10 | grep 5 -B 3    #下匹配  显示匹配行和它前面的3行</span><br><span class="line">seq 10 | grep 5 -C 3    #上下匹配，平时用这个就妥了  匹配行和它前后各3行</span><br><span class="line">cat f.txt | grep -c &#39;SHOPBASE&#39; # -c 统计文件中某字符串的个数</span><br></pre></td></tr></table></figure>

<p>比如在面对比较大的日志文件查看时，用 grep 来更好。</p>
<p><code>grep 13888888888 service.log -C 30 | less</code></p>
<p>这么一搞，就能将 service.log 中所有含有 13888888888 的记录上下 30 行给搜出来，搜索的速度还是贼快的。</p>
<h4 id="awk"><a href="#awk" class="headerlink" title="awk"></a>awk</h4><p>内建变量</p>
<p>NR: NR表示从awk开始执行后，按照记录分隔符读取的数据次数，默认的记录分隔符为换行符<br>因此<strong>默认的就是读取的数据行数</strong>，NR可以理解为Number of Record的缩写。</p>
<p>FNR: 在awk处理多个输入文件的时候，在处理完第一个文件后，NR并不会从1开始，而是继续累加<br>因此就出现了FNR，每当处理一个新文件的时候，FNR就从1开始计数，FNR可以理解为File Number of Record。</p>
<p>NF: NF表示目前的行记录用分割符分割的字段的数目，NF可以理解为Number of Field。</p>
<p>基本命令<br>默认分隔符为空白字符(tab、空格)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">awk &#39;&#123;print $4,$6&#125;&#39; f.txt  # 第4、5列</span><br><span class="line">awk &#39;&#123;print NR,$0&#125;&#39; f.txt cpf.txt   # 以行号输出，行号累加、$0 表示整行、多个文件</span><br><span class="line">awk &#39;&#123;print FNR,$0&#125;&#39; f.txt cpf.txt # 每个文件的行号从1开始计算</span><br><span class="line">awk &#39;&#123;print FNR,FILENAME,$0&#125;&#39; f.txt cpf.txt  # FILENAME 多文件匹配时输出该行所在的文件名</span><br><span class="line">awk &#39;&#123;print FILENAME,&quot;NR&#x3D;&quot;NR,&quot;FNR&#x3D;&quot;FNR,&quot;$&quot;NF&quot;&#x3D;&quot;$NF&#125;&#39; f.txt cpf.txt  # NF 分割字段数</span><br><span class="line">echo 1:2:3:4 | awk -F: &#39;&#123;print $1,$2,$3,$4&#125;&#39; # -F 指定分割符</span><br></pre></td></tr></table></figure>

<p>匹配</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">awk &#39;&#x2F;ldb&#x2F; &#123;print&#125;&#39; f.txt   #匹配ldb</span><br><span class="line">awk &#39;!&#x2F;ldb&#x2F; &#123;print&#125;&#39; f.txt  #不匹配ldb</span><br><span class="line">awk &#39;&#x2F;ldb&#x2F; &amp;&amp; &#x2F;LISTEN&#x2F; &#123;print&#125;&#39; f.txt   #匹配ldb和LISTEN</span><br><span class="line">awk &#39;$5 ~ &#x2F;ldb&#x2F; &#123;print&#125;&#39; f.txt #第五列匹配ldb</span><br></pre></td></tr></table></figure>


<h4 id="find"><a href="#find" class="headerlink" title="find"></a>find</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo -u admin find &#x2F;home&#x2F;admin &#x2F;tmp &#x2F;usr -name &quot;*.log&quot;  (多个目录去找)</span><br><span class="line">find . -iname &quot;*.txt&quot; (大小写都匹配)</span><br><span class="line">find . -type d  (当前目录下的所有子目录)</span><br><span class="line">find &#x2F;usr -type l (目录下所有的符号链接)</span><br><span class="line">find &#x2F;usr -type l -name &quot;z*&quot; (目录下符合z开头的符号链接)</span><br><span class="line">find &#x2F;usr -type l -name &quot;z*&quot; -ls (符号链接的详细信息 eg:inode,目录)</span><br><span class="line">find &#x2F;home&#x2F;admin -size +250000k(超过250000k的文件，当然+改成-就是小于了)</span><br><span class="line">find &#x2F;home&#x2F;admin -tyep f -perm 777 (按照权限查询文件)</span><br><span class="line">find &#x2F;home&#x2F;admin -tyep f -perm 777 -ls (按照权限查询文件)</span><br><span class="line">find &#x2F;home&#x2F;admin -type f -perm 777 -exec ls -l &#123;&#125; \; (按照权限查询文件)</span><br><span class="line">find &#x2F;home&#x2F;admin -atime -1  1天内访问过的文件</span><br><span class="line">find &#x2F;home&#x2F;admin -ctime -1  1天内状态改变过的文件    </span><br><span class="line">find &#x2F;home&#x2F;admin -mtime -1  1天内修改过的文件</span><br><span class="line">find &#x2F;home&#x2F;admin -amin -1  1分钟内访问过的文件</span><br><span class="line">find &#x2F;home&#x2F;admin -cmin -1  1分钟内状态改变过的文件    </span><br><span class="line">find &#x2F;home&#x2F;admin -mmin -1  1分钟内修改过的文件</span><br><span class="line">find &#x2F;home&#x2F;admin -newer while2 -type f -exec ls -l &#123;&#125; \; # 搜索比 while2 文件更新的且类型是普通文件的文件，搜索到时打印该文件的信息</span><br></pre></td></tr></table></figure>

<h4 id="top"><a href="#top" class="headerlink" title="top"></a>top</h4><p>top 命令查看进程的状态</p>
<p><img src="/images/blog/2019-07-26-1.png" alt></p>
<p><strong>第一行: 系统运行时间和平均负载</strong></p>
<p>当前时间、系统已运行时间、当前登录用户的数量、最近5、10、15分钟内的平均负载</p>
<p>其中有个 load average 可能不是那么好理解。</p>
<p>load average: 在特定时间间隔内运行队列中(在CPU上运行或者等待运行多少进程)的平均进程数。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">load average 有三个值，分别代表：1分钟、5分钟、15分钟内运行进程队列中的平均进程数量。</span><br><span class="line"></span><br><span class="line">- 正在运行的进程 + 准备好等待运行的进程   在特定时间内（1分钟，5分钟，10分钟）的平均进程数</span><br><span class="line"></span><br><span class="line">load average 数据是每隔5秒钟检查一次活跃的进程数</span><br><span class="line"></span><br><span class="line">Linux进程可以分为三个状态</span><br><span class="line"></span><br><span class="line">- 阻塞进程</span><br><span class="line">- 可运行的进程</span><br><span class="line">- 正在运行的进程</span><br><span class="line"></span><br><span class="line">比如现在系统有2个正在运行的进程，3个可运行进程，那么系统的 load 就是5，load average 就是一定时间内的 load 数量均值。</span><br></pre></td></tr></table></figure>

<p><strong>第二行: 任务</strong></p>
<p>任务的总数、运行中(running)的任务、休眠(sleeping)中的任务、停止(stopped)的任务、僵尸状态(zombie)的任务</p>
<p><strong>第三行: cpu状态</strong></p>
<table>
<thead>
<tr>
<th>字段</th>
<th>字段释义</th>
</tr>
</thead>
<tbody><tr>
<td>us</td>
<td>user: 运行(未调整优先级的) 用户进程的CPU时间</td>
</tr>
<tr>
<td>sy</td>
<td>system: 运行内核进程的CPU时间</td>
</tr>
<tr>
<td>ni</td>
<td>niced: 运行已调整优先级的用户进程的CPU时间</td>
</tr>
<tr>
<td>id</td>
<td>idle: 空闲时间</td>
</tr>
<tr>
<td>wa</td>
<td>IO wait: 用于等待IO完成的CPU时间</td>
</tr>
<tr>
<td>hi</td>
<td>处理硬件中断的CPU时间</td>
</tr>
<tr>
<td>si</td>
<td>处理软件中断的CPU时间</td>
</tr>
<tr>
<td>st</td>
<td>这个虚拟机被 hypervisor 偷去的CPU时间</td>
</tr>
</tbody></table>
<p>注意: 对于st中，如果当前处于一个 hypervisor 下的vm，实际上 hypervisor 也是要消耗一部分CPU处理时间的</p>
<p><strong>第四行: 内存</strong></p>
<p>全部可用内存、已使用内存、空闲内存、缓冲内存</p>
<p><strong>第五行: swap</strong></p>
<p>全部、已使用、空闲和缓冲交换空间</p>
<p><strong>第七行之后</strong></p>
<p>PID  进程ID，进程的唯一标识符</p>
<p>USER  进程所有者的实际用户名</p>
<p>PR  进程的调度优先级。这个字段的一些值是’rt’。这意味这这些进程运行在实时态。</p>
<p>NI  进程的nice值（优先级）。越小的值意味着越高的优先级。负值表示高优先级，正值表示低优先级</p>
<p>VIRT  virtual memory usage 虚拟内存,进程使用的虚拟内存<br>进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES<br>1  进程“需要的”虚拟内存大小，包括进程使用的库、代码、数据等<br>2  假如进程申请100m的内存，但实际只使用了10m，那么它会增长100m，而不是实际的使用量</p>
<p>RES  resident memory usage 常驻内存,驻留内存大小<br>驻留内存是任务使用的非交换物理内存大小。进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA<br>1  进程当前使用的内存大小，但不包括swap out<br>2  包含其他进程的共享<br>3  如果申请100m的内存，实际使用10m，它只增长10m，与VIRT相反<br>4  关于库占用内存的情况，它只统计加载的库文件所占内存大小  </p>
<p>SHR  shared memory 共享内存<br>1  除了自身进程的共享内存，也包括其他进程的共享内存<br>2  虽然进程只使用了几个共享库的函数，但它包含了整个共享库的大小<br>3  计算某个进程所占的物理内存大小公式：RES – SHR<br>4  swap out后，它将会降下来</p>
<p>S  这个是进程的状态</p>
<ul>
<li>D - 不可中断的睡眠态。</li>
<li>R – 运行态</li>
<li>S – 睡眠态</li>
<li>T – 被跟踪或已停止</li>
<li>Z – 僵尸态</li>
</ul>
<p>%CPU  自从上一次更新时到现在任务所使用的CPU时间百分比。<br>%CPU显示的是进程占用一个核的百分比，而不是整个cpu（N核）的百分比，有时候可能大于100，那是因为该进程启用了多线程占用了多个核心，所以有时候我们看该值得时候会超过100%，但不会超过总核数*100</p>
<p>%MEM  进程使用的可用物理内存百分比</p>
<p>TIME+  任务启动后到现在所使用的全部CPU时间，精确到百分之一秒</p>
<p>COMMAND  运行进程所使用的命令。进程名称（命令名/命令行）</p>
<p><strong>交互命令</strong></p>
<p>top 运行中可以通过 top 的内部命令对进程的显示方式进行控制。</p>
<p>按”h”或者”?”, 会显示帮助</p>
<p><img src="/images/blog/2019-07-26-2.png" alt></p>
<p>内部命令如下:   </p>
<ul>
<li>s – 改变画面更新频率 – 常用</li>
<li>M – 以内存占用率大小的顺序排列进程列表 – 常用 </li>
<li>P – 以 CPU 占用率大小的顺序排列进程列表 – 常用</li>
<li>N – 以 PID 的大小的顺序排列表示进程列表 </li>
<li>R - 正常排序/反向排序</li>
<li>l – 关闭或开启第一部分第一行 top 信息的表示</li>
<li>t – 关闭或开启第一部分第二行 Tasks 和第三行 Cpus 信息的表示</li>
<li>m – 关闭或开启第一部分第四行 Mem 和 第五行 Swap 信息的表示</li>
<li>n – 设置在进程列表所显示进程的数量</li>
<li>c - 切换显示命令名称和完整命令行</li>
<li>u - 输入用户，显示用户的任务</li>
<li>i - 忽略闲置和僵死进程, 这是一个开关式命令</li>
<li>1 - 监控每个逻辑CPU的状况 – 常用</li>
<li>q – 退出 top – 常用</li>
</ul>
<p><strong>常用参数与命令</strong></p>
<ul>
<li>d - 指定每两次屏幕信息刷新之间的时间间隔。当然用户可以使用s交互命令来改变之</li>
<li>p - 通过指定监控进程ID来仅仅监控某个进程的状态</li>
<li>H - 设置线程模式。在设置线程模式下，第二行的tasks指的是线程个数</li>
</ul>
<p>配合来查询进程的各种问题了</p>
<p>1 如下显示某个进程所有活跃的线程消耗情况</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ps -ef | grep java</span><br><span class="line">top -H -p pid</span><br></pre></td></tr></table></figure>

<p>按f、再按j把P调出来,按回车确认</p>
<p><img src="/images/blog/2019-07-26-3.png" alt></p>
<p>P代表”Last used CPU”， 表示上一次运行在哪个CPU内核上</p>
<p><img src="/images/blog/2019-07-26-4.png" alt></p>
<p>2 排查一些进程内线程状态，<code>top -H -p pid</code>获得线程10进制转16进制后 jstack 去抓看这个线程到底在干啥 <a href="https://lihuimintu.github.io/2019/06/03/Occupying-CPU-high/" target="_blank" rel="noopener">Linux 系统 CPU 过高异常排查</a></p>
<h4 id="free"><a href="#free" class="headerlink" title="free"></a>free</h4><p>查看内存使用状况</p>
<p>包括系统物理内存、虚拟内存（swap 交换分区）、共享内存和系统缓存的使用情况。<br>其输出和 top 命令的内存部分非常相似。</p>
<p>命令常用的选项及各自的含义</p>
<ul>
<li>-b    以 Byte（字节）为单位，显示内存使用情况。</li>
<li>-k    以 KB 为单位，显示内存使用情况，此选项是 free 命令的默认选项。</li>
<li>-m    以 MB 为单位，显示内存使用情况。</li>
<li>-g    以 GB 为单位，显示内存使用情况。</li>
<li>-t    在输出的最终结果中，输出内存和 swap 分区的总量。</li>
<li>-o    不显示系统缓冲区这一列。</li>
<li>-s 间隔秒数    根据指定的间隔时间，持续显示内存使用情况。</li>
</ul>
<p><img src="/images/blog/2019-07-26-5.png" alt></p>
<p>第一行显示的是各个列的列表头信息，各自的含义如下所示:</p>
<ul>
<li>total 是总内存数</li>
<li>used 是已经使用的内存数</li>
<li>free 是空闲的内存数</li>
<li>shared 是多个进程共享的内存总数</li>
<li>buffers 是缓冲内存数</li>
<li>cached 是缓存内存数</li>
</ul>
<p>Mem 一行指的是内存的使用情况<br>-/buffers/cache 的内存数，相当于第一行的 used-buffers-cached<br>+/buffers/cache 的内存数，相当于第一行的 free+buffers+cached<br>Swap 一行指的就是 swap 分区的使用情况</p>
<p>可以看到，系统的物理内存为 62 GB，已经使用了 48 GB，空闲 14 GB。<br>而 swap 分区总大小为 4 GB，目前尚未使用。</p>
<p>Linux 的内存管理机制的思想包括（不敢说就是）内存利用率最大化，内核会把剩余的内存申请为 cached，而 cached 不属于 free 范畴。</p>
<p>如果 free 的内存不够，内核会把部分 cached 的内存回收，回收的内存再分配给应用程序。</p>
<p>所以对于linux系统，可用于分配的内存不只是free的内存，还包括cached的内存（其实还包括buffers）</p>
<p>可用内存 = free的内存 + cached的内存 + buffers。也就是<code>+/buffers/cache 的内存数</code>的数值</p>
<p>Free 中的 buffer 和 cache，它们都是占用内存</p>
<p>buffers: 作为 buffer cache 的内存，是块设备的读写缓冲区。用于CPU和内存之间的缓冲</p>
<p>cached: 作为 page cache 的内存, 文件系统的 cache。用于内存和硬盘的缓冲</p>
<p>Buffer Cache 和 Page Cache。前者针对磁盘块的读写，后者针对文件 inode 的读写。这些Cache有效缩短了 I/O系统调用(比如read,write,getdents)的时间。磁盘的操作有逻辑级（文件系统）和物理级（磁盘块)</p>
<h4 id="iostat"><a href="#iostat" class="headerlink" title="iostat"></a>iostat</h4><p>iostat 是 I/O statistics（输入/输出统计）的缩写。</p>
<p>iostat也有一个弱点，就是它不能对某个进程进行深入分析，仅对系统的整体情况进行分析。</p>
<p>iostat 常用命令格式 <code>iostat [参数] [时间] [次数]</code></p>
<p>命令参数说明如下:</p>
<ul>
<li>-c 显示CPU使用情况</li>
<li>-d 显示磁盘使用情况</li>
<li>-k 以K为单位显示</li>
<li>-m 以M为单位显示</li>
<li>-N 显示磁盘阵列(LVM) 信息</li>
<li>-n 显示NFS使用情况</li>
<li>-p 可以报告出每块磁盘的每个分区的使用情况</li>
<li>-t 显示终端和CPU的信息</li>
<li>-x 显示详细信息</li>
</ul>
<p>使用示例<code>iostat -x 3</code></p>
<p><img src="/images/blog/2019-08-27-1.png" alt></p>
<p>在日常运维中到底需要关注哪些字段呢？到底该关注哪些输出内容就可以确定这台服务器是否存在IO性能瓶颈。</p>
<p>%iowait: 如果该值较高，表示磁盘存在I/O瓶颈<br>await: 一般地，系统I/O响应时间应该低于5ms，如果大于10ms就比较大了<br>avgqu-sz: 如果I/O请求压力持续超出磁盘处理能力，该值将增加。如果单块磁盘的队列长度持续超过2，一般认为该磁盘存在I/O性能问题。需要注意的是，如果该磁盘为磁盘阵列虚拟的逻辑驱动器，需要再将该值除以组成这个逻辑驱动器的实际物理磁盘数目，以获得平均单块硬盘的I/O等待队列长度<br>%util: 一般地，如果该参数是100%表示设备已经接近满负荷运行了 </p>
<p>具体使用参考自<a href="https://www.jellythink.com/archives/438" target="_blank" rel="noopener">Linux iostat命令详解</a></p>
<h4 id="fdisk"><a href="#fdisk" class="headerlink" title="fdisk"></a>fdisk</h4><p><code>fdisk -l</code> 显示当前分区情况</p>
<p>分区是将一个硬盘驱动器分成若干个逻辑驱动器，分区是把硬盘连续的区块当做一个独立的磁盘使用。分区表是一个硬盘分区的索引,分区的信息都会写进分区表。</p>
<p>fdisk 主要对硬盘做分区操作。<code>fdisk /dev/vda</code>，具体操作自行搜索。</p>
<p>在使用硬盘之前必须对其分区进行格式化,并挂载。</p>
<p><img src="/images/blog/2019-11-30-1.png" alt></p>
<h4 id="lsblk"><a href="#lsblk" class="headerlink" title="lsblk"></a>lsblk</h4><p>lsblk 命令默认情况下将以树状列出所有块设备</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">NAME   MAJ:MIN RM SIZE RO TYPE MOUNTPOINT</span><br><span class="line">vda    253:0    0  40G  0 disk</span><br><span class="line">└─vda1 253:1    0  40G  0 part &#x2F;</span><br></pre></td></tr></table></figure>

<p>7个栏目名称如下: </p>
<ul>
<li>NAME: 这是块设备名。  </li>
<li>MAJ:MIN: 本栏显示主要和次要设备号。</li>
<li>RM: 本栏显示设备是否可移动设备。注意，如果RM值等于1，这说明他们是可移动设备。</li>
<li>SIZE: 本栏列出设备的容量大小信息。例如298.1G表明该设备大小为298.1GB，而1K表明该设备大小为1KB。</li>
<li>RO: 该项表明设备是否为只读。在本案例中，所有设备的RO值为0，表明他们不是只读的。</li>
<li>TYPE: 本栏显示块设备是否是磁盘或磁盘上的一个分区。disk 是磁盘，part是分区</li>
<li>MOUNTPOINT: 本栏指出设备挂载的挂载点。</li>
</ul>
<p><strong>lsblk和df命令区别</strong></p>
<p>lsblk 查看的是block device,也就是逻辑磁盘大小。</p>
<p>df查看的是file system, 也就是文件系统层的磁盘大小。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@iZbp144crtihiqovt4h5m4Z ~]# df -lh</span><br><span class="line">文件系统        容量  已用  可用 已用% 挂载点</span><br><span class="line">&#x2F;dev&#x2F;vda1        40G  2.4G   35G    7% &#x2F;</span><br><span class="line">devtmpfs        911M     0  911M    0% &#x2F;dev</span><br><span class="line">tmpfs           920M     0  920M    0% &#x2F;dev&#x2F;shm</span><br><span class="line">tmpfs           920M  308K  920M    1% &#x2F;run</span><br><span class="line">tmpfs           920M     0  920M    0% &#x2F;sys&#x2F;fs&#x2F;cgroup</span><br><span class="line">tmpfs           184M     0  184M    0% &#x2F;run&#x2F;user&#x2F;0</span><br></pre></td></tr></table></figure>

<h4 id="pwdx"><a href="#pwdx" class="headerlink" title="pwdx"></a>pwdx</h4><p>查看Linux程序的工作目录</p>
<p>在日常的工作中，尤其是测试机上，经常会看到多个相似的进程同时运行。有时候，需要针对其中1-2个进程进行操作，比如杀掉，也会经常担心杀错。pwdx命令，可以有效的解决这方面的问题。</p>
<p>同时使用 pwdx 命令根据 pid 找到业务进程路径，进而定位到负责人和项目</p>
<p>利用进程号作为参数，可以打印出指定进程号的工作目录，帮助我们区分不同的进程</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@iZbp144crtihiqovt4h5m4Z ~]# pwdx 27594</span><br><span class="line">27594: &#x2F;usr&#x2F;local&#x2F;aegis&#x2F;aegis_client&#x2F;aegis_10_75</span><br></pre></td></tr></table></figure>

<h4 id="sed"><a href="#sed" class="headerlink" title="sed"></a>sed</h4><p>sed 是一种流编编器，它是文本处理中非常中的工具，能够完美的配合正则表达式便用，功物能不同凡响</p>
<p>处理时，把当前处理的行存储在临时缓冲区中，称为”模式空间”（ oattern space），接看用sed命令处理缓冲区中的内容，处理成后，把缓冲区的内容送往屏幕显示。接着理下一行，这样不断重复，直到文件末。文件内容没有改改变，除非使用了写入的命令，将内容更新。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 打印整个文件</span><br><span class="line">sed -n &#39;1,$p&#39; lihm.py</span><br><span class="line"></span><br><span class="line"># 全局替换字符串</span><br><span class="line">sed &quot;s&#x2F;abc&#x2F;gcd&#x2F;g&quot; lihm.py</span><br><span class="line"></span><br><span class="line"># 删除行头空格键和tab键</span><br><span class="line">sed -i &#39;s&#x2F;^[ \t]*&#x2F;&#x2F;g&#39; lihm.py</span><br><span class="line"></span><br><span class="line"># 删除行尾空格</span><br><span class="line">sed -i &#39;s&#x2F;[ \t]*$&#x2F;&#x2F;g&#39; lihm.py</span><br></pre></td></tr></table></figure>

<p>^[  \t] 是以空格或者tab键开头   –  与[^]不同，[^]是匹配除[^字符]之外的任意字符，例如5[^12]，匹配50、53、54，但是不匹配51和52<br>* 是代表匹配0到多次<br>$ 行尾标志</p>
<p>更多的使用自行搜索</p>
<h4 id="sort"><a href="#sort" class="headerlink" title="sort"></a>sort</h4><p>默认以空格分隔，下方示例都指定按分隔符分隔</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 以第三列排序</span><br><span class="line">sort -t $&#39;\t&#39; -k3 system.txt  </span><br><span class="line"></span><br><span class="line"># 以第三列数字排序，第二列默认字符排序且r表示降序排序</span><br><span class="line">sort -t $&#39;\t&#39; -k3n -k2r system.txt</span><br></pre></td></tr></table></figure>

<p>更多查看<a href="https://www.cnblogs.com/itxiongwei/p/8528838.html" target="_blank" rel="noopener">Linux文件排序工具 sort 命令详解</a></p>
<h4 id="stat"><a href="#stat" class="headerlink" title="stat"></a>stat</h4><p>stat 命令主要用于显示文件或文件系统的详细信息</p>
<p>显示文件信息</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@TENCENT64 ~]# stat Changelog</span><br><span class="line">  File: ‘Changelog’</span><br><span class="line">  Size: 1598      	Blocks: 8          IO Block: 4096   regular file</span><br><span class="line">Device: fd01h&#x2F;64769d	Inode: 1579435     Links: 1</span><br><span class="line">Access: (0644&#x2F;-rw-r--r--)  Uid: (    0&#x2F;    root)   Gid: (    0&#x2F;    root)</span><br><span class="line">Access: 2018-11-06 22:39:54.110931887 +0800</span><br><span class="line">Modify: 2018-11-06 22:39:54.110931887 +0800</span><br><span class="line">Change: 2018-11-06 23:07:14.428548887 +0800</span><br><span class="line"> Birth: -</span><br></pre></td></tr></table></figure>

<p>信息解释</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">File: ‘Changelog’: 文件名称为Changelog</span><br><span class="line">Size: 1598: 文件大小1598字节</span><br><span class="line">Blocks: 8: 物理最小块是512，即扇区，而 IO Block 为4096，所以这里就占用了8个物理块的意思</span><br><span class="line">IO Block: 4096: 逻辑块的大小为4096个字节</span><br><span class="line">regular file: 文件类型（普通文件）</span><br><span class="line">Device: fd01h&#x2F;64769d: 文件所在设备号，分别以十六进制和十进制显示</span><br><span class="line">Inode: 1579435: 文件节点号</span><br><span class="line">Links: 1: 硬链接数</span><br><span class="line">Access: (0644&#x2F;-rw-r--r--): 访问权限</span><br><span class="line">Uid: 所有者ID与名称</span><br><span class="line">Gid: 所有者用户组ID与名称</span><br><span class="line">Access: access time 简写</span><br><span class="line">Modify: modify time 简写</span><br><span class="line">Change: change time 简写</span><br><span class="line">Birth -: 无法获知文件创建时间。注意：Linux下的文件未存储文件创建时间</span><br></pre></td></tr></table></figure>
<p>Linux 三种时间介绍: </p>
<ul>
<li>atime:（access time）显示的是文件中的数据最后被访问的时间，比如系统的进程直接使用或通过一些命令和脚本间接使用。（执行一些可执行文件或脚本）    </li>
<li>mtime:（modify time）显示的是文件内容被修改的最后时间，比如用vi编辑时就会被改变。（也就是Block的内容）  </li>
<li>ctime:（change time）显示的是文件的权限、拥有者、所属的组、链接数发生改变时的时间。当然当内容改变时也会随之改变（即inode内容发生改变和Block内容发生改变时）</li>
</ul>
<blockquote>
<p>有一个要注意的就是，在kernel 2.6.30之前，文件系统中默认会及时的更新atime，而在此之后的版本里有变化<br>Linux atime修改策略与 mount 有关，可选的值有 noatime、relatime 和 strictatime</p>
<ul>
<li>noatime: atime不会被更新，即使修改了文件内容</li>
<li>relatime:<ul>
<li>如果一个文件的 atime 比 ctime 或 mtime 更早，此时你去读取了该文件，atime 才会被更新为当前时间。</li>
<li>atime 比现在早一天，那么 atime 在文件读取时会被更新</li>
</ul>
</li>
<li>strictatime: atime 在文件每次被读取时，都能够被更新</li>
</ul>
<p>所以只有发生以下三种情况之一才会更新atime</p>
<ul>
<li>将分区 mount 的挂载的时候指定采用非 relatime 方式，使用方法就是通过<code>mount -o relatime /dir</code>来挂装目录</li>
<li>atime 小于 ctime 或者小于 mtime 的时候</li>
<li>本次的 access time 和上次的 atime 超过24个小时</li>
</ul>
</blockquote>
<p>显示文件所在文件系统信息</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@TENCENT64 &#x2F;data&#x2F;vas_pgg_proj&#x2F;apps&#x2F;penguin_game]# stat -f Makefile</span><br><span class="line">  File: &quot;Makefile&quot;</span><br><span class="line">    ID: 6f75a4f02634e23e Namelen: 255     Type: ext2&#x2F;ext3</span><br><span class="line">Block size: 4096       Fundamental block size: 4096</span><br><span class="line">Blocks: Total: 43830967   Free: 30155578   Available: 27923259</span><br><span class="line">Inodes: Total: 11162880   Free: 11077199</span><br></pre></td></tr></table></figure>

<p>信息解释</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">File: &quot;Makefile&quot;: 文件名称为&quot;Makefile&quot;；</span><br><span class="line">ID: 6f75a4f02634e23e: 文件系统ID</span><br><span class="line">Namelen: 255: 最大文件名称长度</span><br><span class="line">Type: ext2&#x2F;ext3: 文件系统类型名称</span><br><span class="line">Block size: 4096: 块大小为4096字节</span><br><span class="line">Fundamental block size: 4096: 基本块大小为4096字节</span><br><span class="line">Blocks: Total: 43830967   Free: 30155578   Available: 27923259</span><br><span class="line">Inodes: Total: 11162880   Free: 11077199</span><br></pre></td></tr></table></figure>

<h4 id="file"><a href="#file" class="headerlink" title="file"></a>file</h4><p>file 命令用于辨识文件类型</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">file  &#x2F;var&#x2F;log&#x2F;secure*    --&gt;显示是二进制文件</span><br><span class="line">file &#x2F;var&#x2F;log&#x2F;lastlog      --&gt;显示是data文件   --&gt;不能cat –&gt; 只能last命令查看</span><br></pre></td></tr></table></figure>

<p><a href="https://www.cnblogs.com/ftl1012/p/file.html" target="_blank" rel="noopener">shell 脚本小工具之万能解压和压缩器</a></p>
<h4 id="type"><a href="#type" class="headerlink" title="type"></a>type</h4><p>type 命令用来显示指定命令的类型，判断给出的指令是内部指令还是外部指令。</p>
<blockquote>
<p>命令类型<br>alias: 别名<br>keyword: 关键字，Shell保留字<br>function: 函数，Shell函数<br>builtin: 内建命令，Shell内建命令<br>file: 文件，磁盘文件，外部命令<br>unfoundL 没有找到  </p>
</blockquote>
<p>选项<br>-t: 输出“file”、“alias”或者“builtin”，分别表示给定的指令为“外部指令”、“命令别名”或者“内部指令”；<br>-p: 如果给出的指令为外部指令，则显示其绝对路径；<br>-a: 在环境变量“PATH”指定的路径中，显示给定指令的信息，包括命令别名。</p>
<h4 id="last"><a href="#last" class="headerlink" title="last"></a>last</h4><p>last 作用是显示近期用户或终端的登录情况。通过 last 命令查看该程序的 log，管理员可以获知谁曾经或者企图连接系统。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@region-168-1-228 opt]# last</span><br><span class="line">root     pts&#x2F;0        region-168-1-240 Fri Jan 17 10:58   still logged in   </span><br><span class="line">root     pts&#x2F;1        region-168-1-240 Thu Jan 16 21:11 - 05:29  (08:17)    </span><br><span class="line">root     pts&#x2F;0        region-168-1-227 Thu Jan 16 20:48 - 05:31  (08:42)    </span><br><span class="line">root     pts&#x2F;1        192.168.1.254    Wed Jan 15 15:36 - 22:16  (06:39)</span><br></pre></td></tr></table></figure>

<h4 id="uniq"><a href="#uniq" class="headerlink" title="uniq"></a>uniq</h4><p>从输入文件或者标准输入中筛选相邻的匹配行并写入到输出文件或标准输出</p>
<ul>
<li>-c 统计行数</li>
<li>-d 只显示重复的内容</li>
<li>-u 只显示出现一次的行</li>
</ul>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/eDL4-XJZO4Y8Hs4U0bCcnQ" target="_blank" rel="noopener">阿里员工排查问题的工具清单，总有一款适合你！</a></li>
<li><a href="https://mp.weixin.qq.com/s/f_yEBXHjC_v-PlcrFi9ryQ" target="_blank" rel="noopener">运维技巧之逼格高又实用的Linux命令</a></li>
<li><a href="https://www.cnblogs.com/fuqu/p/10230385.html" target="_blank" rel="noopener">Linux top命令的用法详细详解1</a></li>
<li><a href="https://www.cnblogs.com/zhoug2020/p/6336453.html" target="_blank" rel="noopener">Linux top命令的用法详细详解2</a></li>
<li><a href="http://c.biancheng.net/view/1083.html" target="_blank" rel="noopener">Linux free命令：查看内存使用状态</a></li>
<li><a href="https://mp.weixin.qq.com/s/wPBILQSl8-OVNYcJN9cBRg" target="_blank" rel="noopener">工作中常用到的Linux命令</a></li>
<li><a href="https://blog.csdn.net/zhangchenglikecc/article/details/52103737" target="_blank" rel="noopener">top命令输出解释以及load average 详解及排查思路</a></li>
<li><a href="https://www.jellythink.com/archives/438" target="_blank" rel="noopener">Linux iostat命令详解</a></li>
<li><a href="https://blog.csdn.net/u013184884/article/details/82993286" target="_blank" rel="noopener">利用pwdx查看Linux程序的工作目录</a></li>
<li><a href="https://mp.weixin.qq.com/s/Mb4E7GvfsKJapEyaZQl8_w" target="_blank" rel="noopener">Linux 系统监控、诊断工具之 IO wait</a></li>
<li><a href="https://blog.csdn.net/ryu2003/article/details/80324467" target="_blank" rel="noopener">Linux 下磁盘和CPU、内存、网络监控工具</a></li>
<li><a href="https://blog.csdn.net/K346K346/article/details/83832834" target="_blank" rel="noopener">Linux 命令（48）—— stat 命令</a></li>
<li><a href="https://www.cnblogs.com/klb561/p/9241228.html" target="_blank" rel="noopener">Linux系统stat指令用法</a></li>
<li><a href="https://www.cnblogs.com/ftl1012/p/file.html" target="_blank" rel="noopener">shell 脚本小工具之万能解压和压缩器</a></li>
<li><a href="https://blog.csdn.net/wodeqingtian1234/article/details/53975744" target="_blank" rel="noopener">linux中三种time（atime,mtime,ctime）</a></li>
<li><a href="https://www.cnblogs.com/qiaopei/p/5515189.html" target="_blank" rel="noopener">Linux常用命令之 查找命令 find —— 细说 -atime，-mtime，-ctime</a></li>
<li><a href="http://bigdatadecode.club/HDFS%E4%B8%ADatime%E5%92%8Cmtime.html" target="_blank" rel="noopener">HDFS中atime与mtime解析</a></li>
</ul>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>HBase Flush 时机</title>
    <url>/2019/06/25/HBase-Flush/</url>
    <content><![CDATA[<p>HBase MemStore 的刷写时机</p>
<hr>
<h4 id="Flush-时机"><a href="#Flush-时机" class="headerlink" title="Flush 时机"></a>Flush 时机</h4><p>HBase 会在如下几种情况下触发 flush 操作，需要注意的是 MemStore 的最小 flush 单元是 HRegion<br>而不是单个 MemStore。可想而知，如果一个 HRegion 中 Memstore 过多，每次 flush 的开销必然会很大，<br>因此建议在进行表设计的时候尽量减少 ColumnFamily 的个数。</p>
<p>根据 HBase 官方文档总结的刷写时机有6种:</p>
<h5 id="1-MemStore-级别限"><a href="#1-MemStore-级别限" class="headerlink" title="1. MemStore 级别限"></a>1. MemStore 级别限</h5><p>当 Region 中任意一个 MemStore 的大小达到了上限(hbase.hregion.memstore.flush.size，默认128MB)，会触发 MemStore 刷新。</p>
<h5 id="2-Region-级别限制"><a href="#2-Region-级别限制" class="headerlink" title="2. Region 级别限制"></a>2. Region 级别限制</h5><p>当 Region 中所有 MemStore 的大小总和达到了上限(hbase.hregion.memstore.block.multiplier * hbase.hregion.memstore.flush.size，默认 4 * 128M = 512M)，会触发 MemStore 刷新。</p>
<h5 id="3-Region-Server-级别限制"><a href="#3-Region-Server-级别限制" class="headerlink" title="3. Region Server 级别限制"></a>3. Region Server 级别限制</h5><p>当 RegionServer 中 MemStore 的大小总和超过低水位阈值<code>hbase.regionserver.global.memstore.size.lower.limit</code> * <code>hbase.regionserver.global.memstore.size</code>，RegionServer开始强制执行 flush，先 flush MemStore 最大的Region，再 flush 次大的，依次执行。</p>
<p>如果此时写入吞吐量依然很高，导致总 MemStore 大小超过高水位阈值<code>hbase.regionserver.global.memstore.size</code>，RegionServer 会阻塞更新并强制执行 flush，直至总 MemStore 大小下降到低水位阈值。</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>默认值</th>
</tr>
</thead>
<tbody><tr>
<td>hbase.regionserver.global.memstore.size</td>
<td>0.4</td>
</tr>
<tr>
<td>hbase.regionserver.global.memstore.size.lower.limit</td>
<td>0.95</td>
</tr>
</tbody></table>
<p>如果有 16G 堆内存，默认情况下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 达到该值会触发刷写</span><br><span class="line">16 * 0.4 * 0.95 &#x3D; 6.08</span><br><span class="line"># 达到该值会触发刷写阻塞</span><br><span class="line">16 * 0.4 &#x3D; 6.4</span><br></pre></td></tr></table></figure>

<p>相信好多人会在网上看到 hbase.regionserver.global.memstore.upperLimit、hbase.regionserver.global.memstore.lowerLimit 这两个参数。</p>
<p>这两个参数已经发生了改变了。对应的新参数如下:</p>
<table>
<thead>
<tr>
<th>新参数</th>
<th>老参数</th>
</tr>
</thead>
<tbody><tr>
<td>hbase.regionserver.global.memstore.size</td>
<td>hbase.regionserver.global.memstore.upperLimit</td>
</tr>
<tr>
<td>hbase.regionserver.global.memstore.size.lower.limit</td>
<td>hbase.regionserver.global.memstore.lowerLimit</td>
</tr>
</tbody></table>
<blockquote>
<p>的确，当时我也看了老版本的 Region Server 级别限制:<br>当一个Region Server中所有Memstore的大小总和达到了上限（hbase.regionserver.global.memstore.upperLimit ＊ hbase_heapsize，默认 40%的JVM内存使用量），会触发部分Memstore刷新。Flush顺序是按照Memstore由大到小执行，先Flush Memstore最大的Region，再执行次大的，直至总体Memstore内存使用量低于阈值（hbase.regionserver.global.memstore.lowerLimit ＊ hbase_heapsize，默认 38%的JVM内存使用量）。</p>
</blockquote>
<p>按照老版本来看，我发现新搭建的集群配置怎么不对，lowerLimit 怎么比 upperLimit 还高。一度让我弄混。</p>
<p><img src="/images/blog/2019-06-25-1.png" alt></p>
<p>直到我弄清楚 hbase.regionserver.global.memstore.size.lower.limit 是分配给 MemStore 最大内存的刷新的低水位线才明白。</p>
<p>大家可以参阅官方文档对两个新参数的解释。</p>
<p><img src="/images/blog/2019-06-25-2.png" alt></p>
<h5 id="4-HLog-数量限制"><a href="#4-HLog-数量限制" class="headerlink" title="4. HLog 数量限制"></a>4. HLog 数量限制</h5><p>当一个 Region Server 中 HLog 数量达到上限(可通过参数 hbase.regionserver.maxlogs 配置)时，系统会选取最早的一个 HLog 对应的一个或多个 Region 进行 flush</p>
<p><img src="/images/blog/2019-06-25-3.png" alt></p>
<h5 id="5-定期刷新"><a href="#5-定期刷新" class="headerlink" title="5. 定期刷新"></a>5. 定期刷新</h5><p>HBase 定期刷新 MemStore (hbase.regionserver.optionalcacheflushinterval), 默认周期为1小时，确保 MemStore 不会长时间没有持久化。为避免所有的 MemStore 在同一时间都进行 flush 导致的问题，定期的 flush 操作有 20000 左右的随机延时。</p>
<p>hbase.regionserver.optionalcacheflushinterval 参数设置为 0 可关闭。CDH 管理页面上没有直接设置</p>
<p><img src="/images/blog/2019-06-25-4.png" alt></p>
<h5 id="6-手动执行"><a href="#6-手动执行" class="headerlink" title="6. 手动执行"></a>6. 手动执行</h5><p>可以通过 shell 命令 flush ‘tableName’ 或者 flush ‘regionName’ 分别对一个表或者一个 Region 进行 flush。</p>
<h5 id="7-其他"><a href="#7-其他" class="headerlink" title="7. 其他"></a>7. 其他</h5><p>执行 Compact 和 Split 之前，会进行一次 flush</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://cloud.tencent.com/developer/article/1005744" target="_blank" rel="noopener">Hbase memstore 的刷写时机</a></li>
<li><a href="https://hbase.apache.org/1.2/book.html" target="_blank" rel="noopener">HBase1.2官方文档</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>SSH 使用私钥登陆</title>
    <url>/2019/06/24/SSH-ssh_rsa/</url>
    <content><![CDATA[<p>SSH 使用私钥登陆</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>今天大数据开发找到我，让我帮他到业务运维机器上 scp 一个文件到大数据的接口机上。</p>
<p>业务运维发了一个 key 文件给我，让我通过 key 文件上去拿。</p>
<p>我登陆服务器都是用密码登陆的，这种用 key 的方式不会。</p>
<p>赶紧某歌学习下，并记录下来</p>
<h4 id="私钥、公钥"><a href="#私钥、公钥" class="headerlink" title="私钥、公钥"></a>私钥、公钥</h4><p>在当前用户家目录下的 .ssh 可以看到如下文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@zhazhahui .ssh]# ls</span><br><span class="line">authorized_keys  id_rsa  id_rsa.pub  known_hosts</span><br></pre></td></tr></table></figure>

<p>id_rsa 为私钥、id_rsa.pub 为公钥</p>
<p>对比同事发的内容，知道发给我的是私钥。</p>
<p>那就是他把私钥对应的公钥复制到了 authorized_keys 文件内了</p>
<p>我直接拿他给的私钥登陆即可</p>
<h4 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h4><p>1 导入私钥，将私钥文件放到当前登陆用户目录下的 .ssh 目录下。 </p>
<p>将发给我的私钥写入 ssh_rsa 文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim ssh_rsa</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>指定私钥登陆</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ssh -i .ssh&#x2F;ssh_rsa  root@target.com</span><br></pre></td></tr></table></figure>

<p>如果出现了下面这种情况</p>
<p><img src="/images/blog/2019-06-24-1.png" alt></p>
<p>这是因为私钥文件权限太高了，比较不安全，所以被阻止，需要将ssh_rsa的权限设置低一些比如 0400</p>
<p>chmod 0400 .ssh/ssh_rsa</p>
<p>这样就可以登陆成功了</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.cnblogs.com/demonxian3/p/8331545.html" target="_blank" rel="noopener">SSH私用私钥登陆</a></li>
<li><a href="https://www.cnblogs.com/hellotracy/articles/5179985.html" target="_blank" rel="noopener">ssh public key private key 免密码</a></li>
<li><a href="https://www.runoob.com/w3cnote/set-ssh-login-key.html" target="_blank" rel="noopener">设置 SSH 通过密钥登录</a></li>
</ul>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>GC 日志详解</title>
    <url>/2019/06/24/GC-Log-Detailed/</url>
    <content><![CDATA[<p>Java GC 日志详解</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>GC日志是一个很重要的工具，它准确记录了每一次的GC的执行时间和执行结果，通过分析GC日志可以调优堆设置和GC设置，或者改进应用程序的对象分配模式</p>
<p>开启的JVM启动参数参考<a href="https://lihuimintu.github.io/2019/02/19/gcLog/" target="_blank" rel="noopener">JVM 配置GC日志</a></p>
<h4 id="图解"><a href="#图解" class="headerlink" title="图解"></a>图解</h4><p>以 ParallelGC 为例</p>
<p>YoungGC 日志解释如下（图片源地址：<a href="http://ww4.sinaimg.cn/large/6c8effc1tw1dmbux7knpoj.jpg" target="_blank" rel="noopener">这里</a>）</p>
<p><img src="/images/blog/2019-06-24-2.png" alt="YoungGC"></p>
<p>FullGC（图片源地址：<a href="http://ww1.sinaimg.cn/large/6c8effc1tw1dmc55axrbsj.jpg" target="_blank" rel="noopener">这里</a>）:</p>
<p><img src="/images/blog/2019-06-24-3.png" alt="FullGC"></p>
<p>图来源于网络，侵删。</p>
<h4 id="图解2"><a href="#图解2" class="headerlink" title="图解2"></a>图解2</h4><p>常见的Young GC、Full GC日志含义如下</p>
<p><img src="/images/blog/2019-09-29-2.png" alt></p>
<p><img src="/images/blog/2019-09-29-3.png" alt></p>
<p>免费的GC日志图形分析工具推荐 <a href="https://gceasy.io" target="_blank" rel="noopener">gceasy</a> </p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.jianshu.com/p/ade514d7c56b" target="_blank" rel="noopener">Java GC 日志详解（一图读懂）</a></li>
</ul>
]]></content>
      <categories>
        <category>JVM</category>
      </categories>
  </entry>
  <entry>
    <title>记一次 RegionServer 宕机排查过程</title>
    <url>/2019/06/24/HBase-RS-shutdown/</url>
    <content><![CDATA[<p>记一次 RegionServer 宕机排查过程</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>头大，今天 HBase 集群写入压力过大，一直报压缩队列的警告。</p>
<p>接着突然间来了一个 RS 宕机的告警。凉凉。赶紧查看，同事已经先行一步将其启动起来了。</p>
<p>查看 CM 读写请求曲线，发现并不是很大。</p>
<p><img src="/images/blog/2019-06-24-4.png" alt></p>
<p>查看日志</p>
<p><img src="/images/blog/2019-06-24-5.png" alt></p>
<p>可以看到有一段时间日志没有更新，往后看到下方日志</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2019-06-24 16:47:28,787 WARN org.apache.hadoop.hbase.util.Sleeper: We slept 258006ms instead of 3000ms, this is likely due to a long garbage collecting pause and it&#39;s usually bad, see http:&#x2F;&#x2F;hbase.apache.org&#x2F;book.html#trouble.rs.runtime.zkexpired</span><br><span class="line">2019-06-24 16:47:28,788 WARN org.apache.hadoop.hbase.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 257656ms</span><br><span class="line">No GCs detected</span><br></pre></td></tr></table></figure>

<p>可以看出，HBase 发生了 Stop The World(停顿类型STW) ，导致整个进程所有线程被挂起了 257s。将近 4 分钟多。</p>
<p>查看对应的 GC 日志</p>
<p><img src="/images/blog/2019-06-24-6.png" alt></p>
<p>发现 CMS-concurrent-sweep 持续了 433.98 secs 。 orz </p>
<h4 id="排查"><a href="#排查" class="headerlink" title="排查"></a>排查</h4><p>接着来看看 RS 宕机的直接原因</p>
<p><img src="/images/blog/2019-06-24-7.png" alt></p>
<p><img src="/images/blog/2019-06-24-8.png" alt></p>
<p>log 回滚报错，报错原因是  Parent directory doesn’t exist: /hbaseploan/WALs/yz-jdb-106-37-25,60020,1561114085906。<br>因为文件不存在了， log writer 获取失败。</p>
<p>为什么不存在呢？ 接着看</p>
<p><img src="/images/blog/2019-06-24-9.png" alt></p>
<p>看样子估计是 WAL 日志所在的块丢失了。</p>
<p>接着看为什么会丢失</p>
<p><img src="/images/blog/2019-06-24-10.png" alt></p>
<p>从日志内容来看应该是 HBase 调用DFSClient向 DN 写入 block 数据”BP-326682921-100.106.41.1-1466507608648:blk_5358896052_4480910033″，但是 DN 返回失败， recover 过程也是 ERROR的。具体失败原因需要查看 DN 节点日志，如下所示</p>
<p><img src="/images/blog/2019-06-24-11.png" alt></p>
<p>很显然，从日志可以看出，DN 一直在等待来自客户端的 read 请求，但是直至 SocketTimeout，请求都没有过来，此时 DN 会将该连接断开。</p>
<p><img src="/images/blog/2019-06-24-12.png" alt></p>
<p>估计因为断开连接，所以这个 block 出现了写故障，恢复也失败了。</p>
<p>然后该块丢失导致 log 操作文件时失败。</p>
<p>因此 RS 才宕机的。</p>
<p>罪魁祸首就是 GC 导致的 Stop The World</p>
<p>但是什么导致的 GC。由于监控简陋，未清楚原因。打算研究部署一套 Ganglia 来实现监控。</p>
<p>为什么 block 恢复也失败。我也没找到原因。涉及到 HDFS pipeline 写。有机会补补这方面知识。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="http://hbasefly.com/2016/04/15/hbase-regionserver-crash/" target="_blank" rel="noopener">HBase问题诊断 – RegionServer宕机</a></li>
<li><a href="https://blog.yoodb.com/sugarliny/article/detail/1306" target="_blank" rel="noopener">Hbase Region Server异常分析</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>HBase LinkFile</title>
    <url>/2019/06/20/HBase-LinkFile/</url>
    <content><![CDATA[<p>clone_snapshot 的关键 linkfile</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>今天 HBase 维护过程中打开了 /hbase/archive 路径，发现该路径下面存在文件。</p>
<p>/hbase/archive 为归档路径，HBase 在做 Split , Compact，Drop 操作完成之后，<br>会将 HFile 移到 archive 目录中, 然后将之前的 hfile 删除掉, 该目录由 HMaster 上的一个定时任务定期5分钟去清理.  </p>
<p><img src="/images/blog/2019-06-20-1.png" alt></p>
<p>都18年6月份的数据了，怎么还没删掉。这不科学。这开启了我的探索之旅</p>
<h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><p>某度搜索到对表做了 snapshot 的话，archive 不会被清理。</p>
<p>然后 hbase shell 中输入 list_snapshots 的确看到该表存在快照。(忘记留图了)</p>
<p>确认该快照已经不需要的情况下，我对其进行了删除。</p>
<p>10 分钟后观察发现还未删除。。。。这就有点尴尬了。</p>
<p>之前也遇到过一次 archive 目录下表的 HFile 文件是用 bulkload 方式加载的，HFile 文件权限不够，HBase 无法删除，<br>所以这次我也猜测是不是这个原因。</p>
<p><img src="/images/blog/2019-06-20-2.png" alt></p>
<p>ls 查看后发现都是 hbase 的权限。这可咋办啊。这时我留意到有个 .links-ba0feda3ca8d499a9eb375503ad943c2 的目录</p>
<p>这个目录干嘛的。我对此产生疑惑</p>
<h4 id="LinkFile"><a href="#LinkFile" class="headerlink" title="LinkFile"></a>LinkFile</h4><p><img src="/images/blog/2019-06-20-3.png" alt></p>
<p>进去看了之后，发现是个 back-reference 文件，这时就明白了，估计是 Snapshot 的恢复表一个指引。</p>
<p><img src="/images/blog/2019-06-20-4.png" alt></p>
<p>正如猜想一样。就是恢复表的一个指引, 即 clone 生成的 LinkFile。</p>
<p>HBase 就是利用在删除 archive 中原始表文件的时候知道这些文件还被一些恢复表的 LinkFile 引用着</p>
<p>LinkFile 和 back-reference 文件格式如下所说。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">（1）原始文件: &#x2F;hbase&#x2F;data&#x2F;table-x&#x2F;region-x&#x2F;cf&#x2F;file-x</span><br><span class="line">（2）clone 生成的 LinkFile: &#x2F;hbase&#x2F;data&#x2F;table-cloned&#x2F;region-y&#x2F;cf&#x2F;&#123;table-x&#125;&#x3D;&#123;region-x&#125;-&#123;file-x&#125;，因此可以很容易根据LinkFile定位到原始文件</span><br><span class="line">（3）back-reference 文件: &#x2F;hbase&#x2F;.archive&#x2F;data&#x2F;table-x&#x2F;region-x&#x2F;cf&#x2F;.links-file-x&#x2F;&#123;region-y&#125;.&#123;table-cloned&#125;，可以看到，back-reference文件路径中包含所有原始文件和LinkFile的信息，因此可以有效的根据原始文件&#x2F;table-x&#x2F;region-x&#x2F;cf&#x2F;file-x定位到LinkFile：&#x2F;table-cloned&#x2F;region-y&#x2F;cf&#x2F;&#123;table-x&#125;-&#123;region-x&#125;-&#123;file-x&#125;</span><br></pre></td></tr></table></figure>

<p>举个例子。假设</p>
<p>LinkFile 文件名为 music=5e54d8620eae123761e5290e618d556b-f928e045bb1e41ecbef6fc28ec2d5712。</p>
<p>根据定义可以知道 music 为原始文件的表名，5e54d8620eae123761e5290e618d556b 为引用文件所在的 region，f928e045bb1e41ecbef6fc28ec2d5712 为引用的 HFile 文件</p>
<p><img src="/images/blog/2019-06-20-5.png" alt></p>
<p>可以依据规则可以直接根据LinkFile的文件名定位到引用文件所在位置</p>
<p>***/music/5e54d8620eae123761e5290e618d556b/cf/f928e045bb1e41ecbef6fc28ec2d5712</p>
<p>如果在 music 表的 HFile 被放置到 archive 目录下，则会生成一个 back-reference 文件</p>
<p>根据这个 back-reference 文件 HBase 才知道该 HFile 该不该清理。</p>
<p>HFileLinkCleaner 负责清理这类 HFile。</p>
<p><img src="/images/blog/2019-06-20-6.png" alt></p>
<p>通过注释可以了解到，如果对 archive 中的文件的引用不存在了，则可以删除。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line"> * HFileLink cleaner that determines if a hfile should be deleted.</span><br><span class="line"> * HFiles can be deleted only if there&#39;re no links to them.</span><br><span class="line"> *</span><br><span class="line"> * When a HFileLink is created a back reference file is created in:</span><br><span class="line"> *      &#x2F;hbase&#x2F;archive&#x2F;table&#x2F;region&#x2F;cf&#x2F;.links-hfile&#x2F;ref-region.ref-table</span><br><span class="line"> * To check if the hfile can be deleted the back references folder must be empty.</span><br><span class="line"> *&#x2F;</span><br></pre></td></tr></table></figure>

<h4 id="清理"><a href="#清理" class="headerlink" title="清理"></a>清理</h4><p>如果想尽快清理掉，有两种方法。</p>
<ul>
<li>对恢复表做个 major_compact 即可把指引的 HFile 写入到恢复表上。指引文件会消失。</li>
<li>删除恢复表</li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>其实之前就学过 snapshot 的知识，只是太久没回顾了，又有点忘记了。导致这次花了点时间。</p>
<p>如果有没讲清的地方，可以留言，也可以自行阅读参考链接。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="http://hbasefly.com/2017/09/17/hbase-snapshot/" target="_blank" rel="noopener">HBase原理 – 分布式系统中snapshot是怎么玩的？</a></li>
<li><a href="https://www.jianshu.com/p/f82aafd7b381" target="_blank" rel="noopener">HMaster 功能之定期清理archive</a></li>
<li><a href="https://blog.csdn.net/qq_31598113/article/details/79934919" target="_blank" rel="noopener">Hbase在hdfs上的archive目录占用空间过大</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>HBase 目录结构</title>
    <url>/2019/06/19/HBase-File-ZK-Structure/</url>
    <content><![CDATA[<p>HBase 在 HDFS 存储目录结构、在 Zookeeper 的目录结构</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>HBase 在 HDFS 存储目录结构、在 Zookeeper 的目录结构</p>
<h4 id="在-HDFS-路径"><a href="#在-HDFS-路径" class="headerlink" title="在 HDFS 路径"></a>在 HDFS 路径</h4><p>HBase 在 HDFS 默认根目录: <code>/hbase</code></p>
<p>配置项 <code>&lt;name&gt; hbase.rootdir &lt;/name&gt;</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@massive-dataset-new-005 ~]# hadoop fs -ls &#x2F;hbase</span><br><span class="line">Found 10 items</span><br><span class="line">drwxr-xr-x   - hbase hbase          0 2019-02-22 11:20 &#x2F;hbase&#x2F;.hbase-snapshot   # 存储的是 Snapshot 的相关信息</span><br><span class="line">drwxr-xr-x   - hbase hbase          0 2019-06-10 20:57 &#x2F;hbase&#x2F;.tmp              # 临时目录。当对表进行操作的时候，首先会将表移动到该目录下，然后再进行操作。</span><br><span class="line">drwxr-xr-x   - hbase hbase          0 2019-06-19 20:17 &#x2F;hbase&#x2F;MasterProcWALs    # 一个HMaster主节点状态日志文件。</span><br><span class="line">drwxr-xr-x   - hbase hbase          0 2019-06-19 19:12 &#x2F;hbase&#x2F;WALs              # 预写日志，和 Hadoop 中的 edits 文件类似，作用是容灾</span><br><span class="line">drwxr-xr-x   - hbase hbase          0 2019-06-19 19:25 &#x2F;hbase&#x2F;archive           # HBase 在做 Split , Compact，Drop 操作完成之后，会将 HFile 移到 archive 目录中,然后将之前的 hfile 删除掉,该目录由 HMaster 上的一个定时任务定期去清理. </span><br><span class="line">drwxr-xr-x   - hbase hbase          0 2019-01-30 16:55 &#x2F;hbase&#x2F;corrupt           # 存储 HBase 做损坏的日志文件，一般都是为空的</span><br><span class="line">drwxr-xr-x   - hbase hbase          0 2019-06-11 11:44 &#x2F;hbase&#x2F;data              # HBase 的表数据目录，0.98版本里支持 namespace 的概念模型，系统会预置两个 namespace 即: hbase 和 default</span><br><span class="line">-rw-r--r--   3 hbase hbase         42 2018-12-25 15:31 &#x2F;hbase&#x2F;hbase.id          # 它是一个文件，存储集群唯一的 cluster id 号，是一个 uuid</span><br><span class="line">-rw-r--r--   3 hbase hbase          7 2018-12-25 15:31 &#x2F;hbase&#x2F;hbase.version     # 同样也是一个文件，存储集群的版本号，貌似是加密的，看不到，只能通过web-ui 才能正确显示出来  </span><br><span class="line">drwxr-xr-x   - hbase hbase          0 2019-06-19 19:54 &#x2F;hbase&#x2F;oldWALs           # 预写日志归档目录，参数 hbase.master.logcleaner.ttl 控制此文件会定时被清除，默认是 10 分钟</span><br></pre></td></tr></table></figure>

<p>MasterProcWALs 解释下。由于目前几乎所有的集群操作都是通过 procedure 进行的。<br>procedure 执行的每一步都会以log的形式持久化在 HBase 的 MasterProcWals 目录下，这样 master 在重启时也能通过日志来恢复之前的状态并且继续执行。<br>完成的操作日志一段时间后会被删除</p>
<p>/hbase/archive 目录下的数据会过期清理，但是 Snapshot 对应的数据不会被清理，除非删除对应 Snapshot。<br>只要原来的文件有被删除的情况，如 Compaction，Split, Drop 操作，那么快照所引用的hfile文件都会归档到archive的对应表目录中。</p>
<p>HBase 的表数据目录首先是 namespace。hbase、default 是 HBase 自带的 namespace</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@massive-dataset-new-005 ~]# hadoop fs -ls &#x2F;hbase&#x2F;data</span><br><span class="line">Found 4 items</span><br><span class="line">drwxr-xr-x   - hbase hbase          0 2019-06-19 19:12 &#x2F;hbase&#x2F;data&#x2F;default</span><br><span class="line">drwxr-xr-x   - hbase hbase          0 2018-12-25 15:31 &#x2F;hbase&#x2F;data&#x2F;hbase</span><br><span class="line">drwxr-xr-x   - hbase hbase          0 2019-03-06 10:38 &#x2F;hbase&#x2F;data&#x2F;mytest</span><br><span class="line">drwxr-xr-x   - hbase hbase          0 2019-01-03 14:41 &#x2F;hbase&#x2F;data&#x2F;nametest</span><br></pre></td></tr></table></figure>

<p>hbase 这个 namespace 下面存储了 HBase 的 namespace、meta 和 acl 三个表。<br>如果没有开启 HBase 权限的话不会有 acl 这个表。 启用 HBase 授权。设置 hbase.security.authorization = true</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aaadddccc ~]# hadoop fs -ls &#x2F;hbase&#x2F;data&#x2F;hbase</span><br><span class="line">Found 3 items</span><br><span class="line">drwxr-xr-x   - hbase supergroup          0 2018-04-16 21:12 &#x2F;hbase&#x2F;data&#x2F;hbase&#x2F;acl</span><br><span class="line">drwxr-xr-x   - hbase supergroup          0 2016-06-30 19:03 &#x2F;hbase&#x2F;data&#x2F;hbase&#x2F;meta</span><br><span class="line">drwxr-xr-x   - hbase supergroup          0 2016-06-30 19:04 &#x2F;hbase&#x2F;data&#x2F;hbase&#x2F;namespace</span><br></pre></td></tr></table></figure>

<p>每张表都维护 tabledesc 和 regioninfo</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@massive-dataset-new-005 ~]# hadoop fs -ls &#x2F;hbase&#x2F;data&#x2F;default&#x2F;tu</span><br><span class="line">Found 3 items</span><br><span class="line">drwxr-xr-x   - hbase hbase          0 2019-06-01 15:59 &#x2F;hbase&#x2F;data&#x2F;default&#x2F;tu&#x2F;.tabledesc    # 表的元数据目录</span><br><span class="line">drwxr-xr-x   - hbase hbase          0 2019-06-01 15:59 &#x2F;hbase&#x2F;data&#x2F;default&#x2F;tu&#x2F;.tmp          # 中间临时数据，当 .tableinfo 被更新时该目录就会被用到</span><br><span class="line">drwxr-xr-x   - hbase hbase          0 2019-06-08 21:06 &#x2F;hbase&#x2F;data&#x2F;default&#x2F;tu&#x2F;44b1f7dc0ce95cfd867d69a17f26b1fd  # encoded_regionname 即 region 编码名</span><br><span class="line"></span><br><span class="line">[root@massive-dataset-new-005 ~]# hadoop fs -ls &#x2F;hbase&#x2F;data&#x2F;default&#x2F;tu&#x2F;.tabledesc</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   3 hbase hbase        285 2019-06-01 15:59 &#x2F;hbase&#x2F;data&#x2F;default&#x2F;tu&#x2F;.tabledesc&#x2F;.tableinfo.0000000001 # 表元数据信息具体文件</span><br><span class="line"></span><br><span class="line">[root@massive-dataset-new-005 ~]# hadoop fs -ls &#x2F;hbase&#x2F;data&#x2F;default&#x2F;tu&#x2F;44b1f7dc0ce95cfd867d69a17f26b1fd</span><br><span class="line">Found 4 items</span><br><span class="line">-rw-r--r--   3 hbase hbase         37 2019-06-01 15:59 &#x2F;hbase&#x2F;data&#x2F;default&#x2F;tu&#x2F;44b1f7dc0ce95cfd867d69a17f26b1fd&#x2F;.regioninfo      # region 描述目录</span><br><span class="line">drwxr-xr-x   - hbase hbase          0 2019-06-08 21:06 &#x2F;hbase&#x2F;data&#x2F;default&#x2F;tu&#x2F;44b1f7dc0ce95cfd867d69a17f26b1fd&#x2F;.tmp             # 存储临时文件，比如某个合并产生的重新写回的文件</span><br><span class="line">drwxr-xr-x   - hbase hbase          0 2019-06-08 21:06 &#x2F;hbase&#x2F;data&#x2F;default&#x2F;tu&#x2F;44b1f7dc0ce95cfd867d69a17f26b1fd&#x2F;picinfo          # 列族</span><br><span class="line">drwxr-xr-x   - hbase hbase          0 2019-06-01 16:52 &#x2F;hbase&#x2F;data&#x2F;default&#x2F;tu&#x2F;44b1f7dc0ce95cfd867d69a17f26b1fd&#x2F;recovered.edits  # WAL 日志回放目录</span><br><span class="line">[root@massive-dataset-new-005 ~]# hadoop fs -ls &#x2F;hbase&#x2F;data&#x2F;default&#x2F;tu&#x2F;44b1f7dc0ce95cfd867d69a17f26b1fd&#x2F;.regioninfo</span><br><span class="line">-rw-r--r--   3 hbase hbase         37 2019-06-01 15:59 &#x2F;hbase&#x2F;data&#x2F;default&#x2F;tu&#x2F;44b1f7dc0ce95cfd867d69a17f26b1fd&#x2F;.regioninfo      # region 描述文件</span><br></pre></td></tr></table></figure>

<h4 id="在-zk-目录结构"><a href="#在-zk-目录结构" class="headerlink" title="在 zk 目录结构"></a>在 zk 目录结构</h4><p>查看 Zookeeper 内部 HBase 相关数据，有三个主要的渠道:</p>
<ul>
<li>通过 hbase shell 命令 zk_dump 查看</li>
<li>通过 hbase zkcli 查看</li>
<li>通过 Zookeeper 查看</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] ls &#x2F;hbase</span><br><span class="line">[meta-region-server, acl, backup-masters, table, draining, region-in-transition, running, table-lock, master, balancer, namespace, hbaseid, online-snapshot, replication, splitWAL, recovering-regions, rs, flush-table-proc]</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase(main):001:0&gt; zk_dump</span><br><span class="line">HBase is rooted at &#x2F;hbase</span><br><span class="line">Active master address: datanode1,60000,1483706056881</span><br><span class="line">Backup master addresses:</span><br><span class="line">Region server holding hbase:meta: datanode3,60020,1483706055770</span><br><span class="line">Region servers:</span><br><span class="line"> datanode2,60020,1483706054731</span><br><span class="line"> datanode0,60020,1483706054027</span><br><span class="line"> datanode4,60020,1483706055881</span><br><span class="line"> datanode3,60020,1483706055770</span><br><span class="line"> datanode7,60020,1483706055693</span><br><span class="line"> datanode5,60020,1483706054452</span><br><span class="line">&#x2F;hbase&#x2F;replication: </span><br><span class="line">&#x2F;hbase&#x2F;replication&#x2F;peers: </span><br><span class="line">&#x2F;hbase&#x2F;replication&#x2F;rs: </span><br><span class="line">&#x2F;hbase&#x2F;replication&#x2F;rs&#x2F;datanode5,60020,1483706054452: </span><br><span class="line">&#x2F;hbase&#x2F;replication&#x2F;rs&#x2F;datanode7,60020,1483706055693: </span><br><span class="line">&#x2F;hbase&#x2F;replication&#x2F;rs&#x2F;datanode3,60020,1483706055770: </span><br><span class="line">&#x2F;hbase&#x2F;replication&#x2F;rs&#x2F;datanode4,60020,1483706055881: </span><br><span class="line">&#x2F;hbase&#x2F;replication&#x2F;rs&#x2F;datanode0,60020,1483706054027: </span><br><span class="line">&#x2F;hbase&#x2F;replication&#x2F;rs&#x2F;datanode2,60020,1483706054731: </span><br><span class="line">Quorum Server Statistics:</span><br><span class="line"> localhost:2181</span><br><span class="line">  Zookeeper version: 3.4.5-cdh5.7.2--1, built on 07&#x2F;22&#x2F;2016 19:18 GMT</span><br><span class="line">  Clients:</span><br><span class="line">   &#x2F;172.16.171.9:48487[1](queued&#x3D;0,recved&#x3D;5107,sent&#x3D;5107)</span><br><span class="line">   &#x2F;172.16.171.17:36252[1](queued&#x3D;0,recved&#x3D;50936,sent&#x3D;50936)</span><br><span class="line">   &#x2F;172.16.171.20:47011[1](queued&#x3D;0,recved&#x3D;19631,sent&#x3D;19631)</span><br><span class="line">   &#x2F;172.16.171.19:62253[1](queued&#x3D;0,recved&#x3D;7455,sent&#x3D;7455)</span><br><span class="line">   &#x2F;127.0.0.1:18625[1](queued&#x3D;0,recved&#x3D;22,sent&#x3D;22)</span><br><span class="line">   &#x2F;172.16.171.11:34643[1](queued&#x3D;0,recved&#x3D;7456,sent&#x3D;7456)</span><br><span class="line">   &#x2F;127.0.0.1:18621[1](queued&#x3D;0,recved&#x3D;3,sent&#x3D;3)</span><br><span class="line">   &#x2F;172.16.171.21:38192[1](queued&#x3D;0,recved&#x3D;7467,sent&#x3D;7467)</span><br><span class="line">   &#x2F;172.16.171.17:36254[1](queued&#x3D;0,recved&#x3D;50936,sent&#x3D;50936)</span><br><span class="line">   &#x2F;172.16.171.5:60302[1](queued&#x3D;0,recved&#x3D;7456,sent&#x3D;7456)</span><br><span class="line">   &#x2F;172.16.171.9:48540[1](queued&#x3D;0,recved&#x3D;25518,sent&#x3D;25518)</span><br><span class="line">   &#x2F;172.16.171.9:32467[1](queued&#x3D;0,recved&#x3D;7455,sent&#x3D;7455)</span><br><span class="line">   &#x2F;172.16.171.8:61522[1](queued&#x3D;0,recved&#x3D;10566,sent&#x3D;10586)</span><br><span class="line">   &#x2F;172.16.171.19:16777[1](queued&#x3D;0,recved&#x3D;25518,sent&#x3D;25518)</span><br><span class="line">   &#x2F;127.0.0.1:18626[0](queued&#x3D;0,recved&#x3D;1,sent&#x3D;0)</span><br><span class="line">   &#x2F;172.16.171.8:45515[1](queued&#x3D;0,recved&#x3D;7455,sent&#x3D;7455)</span><br><span class="line">   &#x2F;172.16.171.12:18371[1](queued&#x3D;0,recved&#x3D;381742,sent&#x3D;381742)</span><br><span class="line">  </span><br><span class="line">  Latency min&#x2F;avg&#x2F;max: 0&#x2F;3&#x2F;51663</span><br><span class="line">  Received: 1431771</span><br><span class="line">  Sent: 1461307</span><br><span class="line">  Connections: 17</span><br><span class="line">  Outstanding: 0</span><br><span class="line">  Zxid: 0x2600046a80</span><br><span class="line">  Mode: follower</span><br><span class="line">  Node count: 5349</span><br></pre></td></tr></table></figure>

<p>关于输出结果的解读，就不去细说了，感兴趣的兄弟，自己去问度娘吧。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.jianshu.com/p/4e4f5f558816" target="_blank" rel="noopener">hbase数据存储</a></li>
<li><a href="https://blog.csdn.net/L_15156024189/article/details/83444255" target="_blank" rel="noopener">hbase在hdfs上的详细目录结构</a></li>
<li><a href="https://www.cnblogs.com/hadoopdev/p/3984318.html" target="_blank" rel="noopener">HBase与Zookeeper数据结构查询</a></li>
<li><a href="https://blog.bcmeng.com/post/hbase-hdfs.html" target="_blank" rel="noopener">HBase 在HDFS上的物理目录结构</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>HBase Data Block Encoding Types</title>
    <url>/2019/06/19/HBase-Data-Block-Encoding-Types/</url>
    <content><![CDATA[<p>Data Block Encoding Types</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>AlstonWilliams 的<a href="https://www.jianshu.com/p/a62e49f749f3" target="_blank" rel="noopener">HBase Data Block Encoding Types介绍</a>写的挺好的</p>
<p>本文主要是翻译<a href="http://hbase.apache.org/book.html#compression" target="_blank" rel="noopener">官方Data Block Encoding Types 部分</a></p>
<h4 id="块压缩"><a href="#块压缩" class="headerlink" title="块压缩"></a>块压缩</h4><p>HBase 中提供了五种 Data Block Encoding Types，具体有: </p>
<ul>
<li>NONE</li>
<li>PREFIX</li>
<li>DIFF</li>
<li>FAST_DIFF</li>
<li>PREFIX_TREE</li>
</ul>
<p><code>NONE</code>这种就不介绍了，这个很容易理解。</p>
<h5 id="PREFIX"><a href="#PREFIX" class="headerlink" title="PREFIX"></a>PREFIX</h5><p>通常来说，key 总是相似的，拥有相同的前缀，只是末尾不相同。</p>
<p>例如，一个 Key 是 <code>RowKey:Family:Qualifier0</code>，<br>跟它相邻的下一个 Key 可能是 <code>RowKey:Family:Qualifier1</code></p>
<p>在 PREFIX 编码中，会额外添加一字段表示当前行的 key 和前一行的 key<br>前缀相同长度</p>
<p>很明显，如果相邻 Key 之间，完全没有共同点，那 PREFIX 显然毫无用处，还增加了额外的开销．</p>
<p>当使用 NONE 这种 Block Encoding 时，如下图所示:</p>
<p><img src="/images/blog/2019-06-19-1.png" alt></p>
<p>而如果采用 PREFIX 这种数据块编码，如下图所示:</p>
<p><img src="/images/blog/2019-06-19-2.png" alt></p>
<p>此处的第一个 key 与之前的 key 完全不同，则其前缀长度为 0，<br>第二个 key 前缀长度为 23，因为它们前 23 个字符相同。</p>
<h5 id="DIFF"><a href="#DIFF" class="headerlink" title="DIFF"></a>DIFF</h5><p>DIFF 是在 PREFIX 基础上扩展，不在把 Key 看成一个整体，而是将 Key 每个键字段拆分，以便可以更有效地压缩键的每个部分。</p>
<p>它添加了两个新的字段 <code>timestamp</code> 和 <code>type</code></p>
<p>如果 KeyB 的 ColumnFamily、key length、value length、Key type 和 KeyA 对应字段相同，那么它就会在 KeyB 中被省略．</p>
<p>此外，timestamp 存储的是相对于前一行 Row 的 timestamp 偏移量（即差值），而不是完整存储。</p>
<p>默认情况下，DIFF 是不建议启动的．因为它会导致写数据、Scan 数据更慢．但是相对于 PREFIX/NONE，它会在 BlockCache 中缓存更多数据．</p>
<p>用 DIFF 编码方式压缩之前的 block 如下图所示:</p>
<p><img src="/images/blog/2019-06-19-3.png" alt></p>
<p>示例中的第一行、第二行，并给出 timestamp 和 type 精确匹配，key 长度也是相等的，所以第二行的 key length 和 type 都不需要存储，timestamp 存储与上一行的差值。</p>
<h5 id="FAST-DIFF"><a href="#FAST-DIFF" class="headerlink" title="FAST_DIFF"></a>FAST_DIFF</h5><p>FAST_DIFF 跟 DIFF 非常相似，所不同的是，它额外增加了一个字段，表示 RowB 是否跟 RowA 完全一样，如果是的话，那数据就不需要重复保存了．</p>
<p>如果在你的场景下，Key很长，或者有很多Column，那么推荐使用FAST_DIFF。</p>
<p>数据格式几乎与 DIFF 编码相同，因此没有图像来说明它。</p>
<h5 id="PREFIX-TREE"><a href="#PREFIX-TREE" class="headerlink" title="PREFIX_TREE"></a>PREFIX_TREE</h5><p>PREFIX_TREE 是 HBase 0.96 中的一项实验功能。它已在 HBase-2.0.0 中删除。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.jianshu.com/p/a62e49f749f3" target="_blank" rel="noopener">HBase Data Block Encoding Types介绍</a></li>
<li><a href="http://hbasefly.com/2016/07/02/hbase-pracise-cfsetting/" target="_blank" rel="noopener">HBase最佳实践－列族设计优化</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>Hive 映射 HBase 表方式</title>
    <url>/2019/06/18/Hive-HBase-mapping/</url>
    <content><![CDATA[<p>Hive 映射 HBase 表方式</p>
<hr>
<h4 id="HBase-表映射到-Hive-中"><a href="#HBase-表映射到-Hive-中" class="headerlink" title="HBase 表映射到 Hive 中"></a>HBase 表映射到 Hive 中</h4><h5 id="Hive-内部表"><a href="#Hive-内部表" class="headerlink" title="Hive 内部表"></a>Hive 内部表</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE TABLE ods.s01_buyer_calllogs_info_ts (</span><br><span class="line">    key string comment &quot;hbase rowkey&quot;,</span><br><span class="line">    buyer_mobile string comment &quot;手机号&quot;,</span><br><span class="line">    contact_mobile string comment &quot;对方手机号&quot;,</span><br><span class="line">    call_date string comment &quot;发生时间&quot;,</span><br><span class="line">    call_type string comment &quot;通话类型&quot;,</span><br><span class="line">    init_type string comment &quot;0-被叫,1-主叫&quot;,</span><br><span class="line">    other_cell_phone string comment &quot;对方手机号&quot;,</span><br><span class="line">    place string comment &quot;呼叫发生地&quot;,</span><br><span class="line">    start_time string comment &quot;发生时间&quot;,</span><br><span class="line">    subtotal string comment &quot;通话费用&quot;,</span><br><span class="line">    use_time string comment &quot;通话时间（秒）&quot;</span><br><span class="line">)</span><br><span class="line">STORED BY &#39;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#39;    </span><br><span class="line">WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; &#x3D; &quot;:key,record:buyer_mobile,record:contact_mobile,record:call_date,record:call_type,record:init_type,record:other_cell_phone,record:place,record:start_time,record:subtotal,record:use_time&quot;)    </span><br><span class="line">TBLPROPERTIES(&quot;hbase.table.name&quot; &#x3D; &quot;s01_buyer_calllogs_info_ts&quot;);</span><br></pre></td></tr></table></figure>

<p>Hive 每一个字段得映射到 HBase 的列簇的列。如果只想映射列簇，看第三种方式</p>
<p>建好表之后，进入 hbase shell 执行 list 能看到表 s01_buyer_calllogs_info_ts  </p>
<p>Hive drop 掉此表时，HBase 也被 drop</p>
<p>创建内部表可以不指定 hbase.table.name</p>
<h5 id="Hive-外部表"><a href="#Hive-外部表" class="headerlink" title="Hive 外部表"></a>Hive 外部表</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># HBase 建表</span><br><span class="line">create &#39;buyer_calllogs_info_ts&#39;, &#39;record&#39;, &#123;SPLITS_FILE &#x3D;&gt; &#39;hbase_calllogs_splits.txt&#39;&#125;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"># Hive 建表</span><br><span class="line">CREATE EXTERNAL TABLE ods.s10_buyer_calllogs_info_ts (</span><br><span class="line">    key string comment &quot;hbase rowkey&quot;,</span><br><span class="line">    buyer_mobile string comment &quot;手机号&quot;,</span><br><span class="line">    contact_mobile string comment &quot;对方手机号&quot;,</span><br><span class="line">    call_date string comment &quot;发生时间&quot;,</span><br><span class="line">    call_type string comment &quot;通话类型&quot;,</span><br><span class="line">    init_type string comment &quot;0-被叫,1-主叫&quot;,</span><br><span class="line">    other_cell_phone string comment &quot;对方手机号&quot;,</span><br><span class="line">    place string comment &quot;呼叫发生地&quot;,</span><br><span class="line">    start_time string comment &quot;发生时间&quot;,</span><br><span class="line">    subtotal string comment &quot;通话费用&quot;,</span><br><span class="line">    use_time string comment &quot;通话时间（秒）&quot;</span><br><span class="line">)    </span><br><span class="line">STORED BY &#39;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#39;    </span><br><span class="line">WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; &#x3D; &quot;:key,record:buyer_mobile,record:contact_mobile,record:call_date,record:call_type,record:init_type,record:other_cell_phone,record:place,record:start_time,record:subtotal,record:use_time&quot;)    </span><br><span class="line">TBLPROPERTIES(&quot;hbase.table.name&quot; &#x3D; &quot;buyer_calllogs_info_ts&quot;);</span><br></pre></td></tr></table></figure>

<p>从方式需要先在 HBase 建好表，然后在 Hive 中建表</p>
<p>Hive drop 掉表，HBase 表不会变</p>
<h5 id="Hive-映射-HBase-的列簇"><a href="#Hive-映射-HBase-的列簇" class="headerlink" title="Hive 映射 HBase 的列簇"></a>Hive 映射 HBase 的列簇</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 内部表</span><br><span class="line">CREATE TABLE hbase_table_1 (</span><br><span class="line">    value map&lt;string,int&gt;, </span><br><span class="line">    row_key int</span><br><span class="line">)</span><br><span class="line">STORED BY &#39;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#39;</span><br><span class="line">WITH SERDEPROPERTIES ( &quot;hbase.columns.mapping&quot; &#x3D; &quot;cf:,:key&quot;);</span><br><span class="line"></span><br><span class="line"># Hive insert 插入方式</span><br><span class="line">INSERT OVERWRITE TABLE hbase_table_1 SELECT map(bar, foo), foo FROM pokes WHERE foo&#x3D;98 OR foo&#x3D;100;</span><br><span class="line"></span><br><span class="line"># 外部表</span><br><span class="line">CREATE EXTERNAL TABLE hbase_table_2 (</span><br><span class="line">    value map&lt;string,int&gt;, </span><br><span class="line">    row_key int</span><br><span class="line">)</span><br><span class="line">STORED BY &#39;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#39;</span><br><span class="line">WITH SERDEPROPERTIES ( &quot;hbase.columns.mapping&quot; &#x3D; &quot;cf:,:key&quot;) </span><br><span class="line">TBLPROPERTIES(&quot;hbase.table.name&quot; &#x3D; &quot;hbase_table_22&quot;);</span><br></pre></td></tr></table></figure>

<p>在 HBase 查看结果</p>
<p><img src="/images/blog/2019-06-18-2.png" alt></p>
<p>可以根据需求确定，详细参见<a href="https://cwiki.apache.org/confluence/display/Hive/HBaseIntegration" target="_blank" rel="noopener">官方文档</a></p>
<h4 id="一个简单的测试"><a href="#一个简单的测试" class="headerlink" title="一个简单的测试"></a>一个简单的测试</h4><p>在查阅资料时看到一个很有意思的测试。感兴趣的可以自己阅读 <a href="https://mp.weixin.qq.com/s/NelyoN5WZITwP-54KiqBfg" target="_blank" rel="noopener">一个简单的测试：HBase-Hive 映射表</a></p>
<p>测试结论: </p>
<ul>
<li>可以看到，同一张HBase表可以映射多张 Hive 外部表，并且查询列互不影响。</li>
<li>可见向不同 Hive 外部表中插入数据是不会影响HBase其他列的。</li>
<li>insert into 与 insert overwrite 操作HBase-Hive映射外部表结果是一样的，且均是基于Hive表所属列进行更新，不会影响其他列的值。</li>
</ul>
<h4 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h4><p>1、HBase 中的空 cell 在 Hive 中会补 null</p>
<p>2、Hive 和 HBase 中不匹配的字段会补 null</p>
<p>3、Bytes 类型的数据，建 Hive 表示加#b, 因为 HBase 中 double，int , bigint 类型以byte方式存储时，用字符串取出来必然是乱码。</p>
<p>4、其中:key代表的是 HBase 中的 rowkey, Hive 中也要有一个 key 与之对应, 不然会报错, cf 指的是 HBase 的列族,<br>创建完后,会自动把 HBase 表里的数据同步到 Hive 中</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.cnblogs.com/ChouYarn/p/7986830.html" target="_blank" rel="noopener">Hive映射HBase表的几种方式</a></li>
<li><a href="https://blog.csdn.net/jameshadoop/article/details/42162669" target="_blank" rel="noopener">hive与hbase数据类型对应关系</a></li>
</ul>
]]></content>
      <categories>
        <category>Hive</category>
      </categories>
  </entry>
  <entry>
    <title>HBase hbck 运维指南</title>
    <url>/2019/06/17/HBase-HBCK/</url>
    <content><![CDATA[<p>HBaseFsck（hbck）是一个用于检查区域一致性和表完整性问题并修复损坏的 HBase 的工具</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>HBaseFsck（hbck）是一个用于检查区域一致性和表完整性问题并修复损坏的HBase的工具。<br>它工作在两种基本模式 - 只读不一致识别模式和多阶段读写修复模式。</p>
<p>HBCK 现在有两个版本。HBCK2 是 HBase 2.x 系列的。</p>
<p>HBCK1 不适用 HBase2.x 的</p>
<p><a href="http://hbase.apache.org/1.2/book.html#hbck.in.depth" target="_blank" rel="noopener">HBCK1官方文档</a></p>
<p><a href="https://hbase.apache.org/2.0/book.html#hbck" target="_blank" rel="noopener">HBCK2官方文档</a></p>
<p>因公司 HBase 集群为 1.2.0 版本的。所以这里以 hbck1 为主讲。hbck2 可以看看这个<a href="https://mp.weixin.qq.com/s/GVMWwB1WsKcdvZGfvX1lcA" target="_blank" rel="noopener">HBase指南 HBase 2.0之修复工具HBCK2运维指南</a></p>
<h4 id="检查方面"><a href="#检查方面" class="headerlink" title="检查方面"></a>检查方面</h4><ol>
<li><p>HBase Region一致性</p>
<blockquote>
<ul>
<li>集群中所有 region 都 被 assign，而且 deploy 到唯一一台RegionServer上</li>
<li>该 region 的状态在内存中、hbase:meta 表中以及 zookeeper 这三个地方需要保持一致</li>
</ul>
</blockquote>
</li>
<li><p>HBase 表完整性</p>
<blockquote>
<ul>
<li>对于集群中任意一张表，每个rowkey都仅能存在于一个region区间</li>
</ul>
</blockquote>
</li>
</ol>
<h4 id="常用检查命令"><a href="#常用检查命令" class="headerlink" title="常用检查命令"></a>常用检查命令</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># hbck 帮助文档</span><br><span class="line">.&#x2F;bin&#x2F;hbase hbck -help </span><br><span class="line"></span><br><span class="line"># 检查 HBase 集群是否损坏</span><br><span class="line">.&#x2F;bin&#x2F;hbase hbck</span><br><span class="line"></span><br><span class="line"># 只检测元数据表的状态</span><br><span class="line">.&#x2F;bin&#x2F;hbase hbck -metaonly </span><br><span class="line"></span><br><span class="line"># 使用该 -details 选项将报告更多细节，包括所有表格中所有分割的代表性列表</span><br><span class="line">.&#x2F;bin&#x2F;hbase hbck –details</span><br><span class="line"></span><br><span class="line"># 以下命令只会尝试检查表 TableFoo 和 TableBar</span><br><span class="line">.&#x2F;bin&#x2F;hbase hbck TableFoo TableBar</span><br></pre></td></tr></table></figure>

<p>在命令输出结束时，它会打印 OK 或告诉存在的 INCONSISTENCIES 数量。</p>
<p>Status: OK，表示没有发现不一致问题。</p>
<p>Status: INCONSISTENT，表示有不一致问题。</p>
<p>可能还想运行 hbck 几次，因为一些不一致可能是暂时的。（例如，群集正在启动或区域正在分裂）</p>
<p><img src="/images/blog/2019-06-17-1.png" alt></p>
<h4 id="用法整理"><a href="#用法整理" class="headerlink" title="用法整理"></a>用法整理</h4><p>这里把 HBCK1 帮助文档的整理出来。整理来源作者白42的<a href="http://www.zhyea.com/2017/07/29/hbase-hbck-usage.html" target="_blank" rel="noopener">hbase hbck用法</a></p>
<pre><code>hbase hbck [opts] {only tables}

opts通用可选项
-help 展示help信息；

-detail 展示所有Region的详情；

-timelag &lt;秒级时间&gt;  处理在过去的指定时间内没有发生过元数据更新的region；

-sleepBeforeRerun &lt;秒级时间&gt;  在执行-fix指令后时睡眠指定的时间后再检查fix是否生效；

-summary 只打印表和状态的概要信息；

-metaonly 只检查hbase:meta表的状态；

-sidelineDir &lt;hdfs://&gt; 备份当前的元数据到HDFS上；

-boundaries  校验META表和StoreFiles的Region边界是否一致；

元数据修复选项
在不确定的情况下，慎用以下指令。

-fix 尝试修复Region的分配，通常用于向后兼容；

-fixAssignments 尝试修复Region的分配，用来替换-fix指令；

-fixMeta  尝试修复元数据问题；这里假设HDFS上的region信息是正确的；

-noHdfsChecking  不从HDFS加载/检查Region信息；这里假设hbase:meta表中的Region信息是正确的，不会在检查或修复任何HDFS相关的问题，如黑洞(hole)、孤岛(orphan)或是重叠(overlap)；

-fixHdfsHoles  尝试修复HDFS中的Region黑洞；

-fixHdfsOrphans  尝试修复hdfs中没有.regioninfo文件的region目录；

-fixTableOrphans  尝试修复hdfs中没有.tableinfo文件的table目录（只支持在线模式）；

-fixHdfsOverlaps  尝试修复hdfs中region重叠的现象；

-fixVersionFile  尝试修复hdfs中hbase.version文件缺失的问题；

-maxMerge &lt;n&gt;  在修复region重叠的现时，允许merge最多&lt;n&gt;个region（默认n等于5）；

-sidelineBigOverlaps  在修复region重叠问题时，允许暂时搁置重叠量较大的部分；

-maxOverlapsToSideline &lt;n&gt;  在修复region重叠问题时，允许一组里暂时搁置最多n个region不处理（默认n等于2）；

-fixSplitParents 尝试强制将下线的split parents上线；

-ignorePreCheckPermission  在执行检查时忽略文件系统权限；

-fixReferencesFiles 尝试下线引用断开（lingering reference）的StoreFile；

-fixEmptyMetaCells  尝试修复hbase:meta表中没有引用到任何region的entry（REGIONINFO_QUALIFIER为空的行）。

Datafile修复选项
专业命令，慎用。

-checkCorruptHFiles  检查所有HFile —— 通过逐一打开所有的HFile来确定其是否可用；

-sidelineCorruptHFiles  隔离损坏的HFile。该指令中包含-checkCorruptHFiles操作。

Meta修复快捷指令
-repair  是以下指令的简写：-fixAssignments -fixMeta -fixHdfsHoles -fixHdfsOrphans -fixHdfsOverlaps -fixVersionFile -sidelineBigOverlaps -fixReferenceFiles -fixTableLocks -fixOrphanedTableZnodes；

-repairHoles  是以下指令的简写：-fixAssignments -fixMeta -fixHdfsHoles。

Table lock选项
-fixTableLocks 删除已持有超过很长时间的table lock（(hbase.table.lock.expire.ms配置项，默认值为10分钟）。

Table Znode选项
-fixOrphanedTableZnodes  如果表不存在，则将其在zookeeper中ZNode状态设置为disabled。</code></pre><h4 id="修复"><a href="#修复" class="headerlink" title="修复"></a>修复</h4><h5 id="1-局部低危修复"><a href="#1-局部低危修复" class="headerlink" title="1. 局部低危修复"></a>1. 局部低危修复</h5><p>-fixAssignments ：修复没有assign、assign不正确或者同时assign到多台RegionServer的问题region。</p>
<p>-fixMeta ：主要修复.regioninfo文件和hbase:meta元数据表的不一致。修复的原则是以HDFS文件为准：如果region在HDFS上存在，但在hbase.meta表中不存在，就会在hbase:meta表中添加一条记录。反之如果在HDFS上不存在，而在hbase:meta表中存在，就会将hbase:meta表中对应的记录删除。</p>
<h5 id="2-高危修复"><a href="#2-高危修复" class="headerlink" title="2. 高危修复"></a>2. 高危修复</h5><p>region区间overlap相关问题的修复属于高危修复操作，因为这类修复通常需要修改HDFS上的文件，有时甚至需要人工介入。</p>
<p>对于这类高危修复操作，建议先执行hbck -details详细了解更多的问题细节，再执行相应的修复命令</p>
<p><strong>-repair｜-fix 命令强烈不建议生产线使用</strong></p>
<p><a href="https://mp.weixin.qq.com/s/Ke12M7rVfntJL2s1uG-xVg" target="_blank" rel="noopener">HBase排查 小心hbck的-repair参数</a></p>
<p>-fix 用来修复 region 级别的不一致性。</p>
<p>其内部的操作顺序如下: </p>
<ol>
<li>先查是否存在不一致的。 </li>
<li>如有表级别的不一致性，则先修复表的不一致问题。 </li>
<li>如有region级别的不一致性，则修复该级别的问题。在修复期间region是关闭状态 </li>
</ol>
<h4 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h4><p>hbck 可以修复各种错误 <a href="https://blog.csdn.net/liliwei0213/article/details/53639275" target="_blank" rel="noopener">https://blog.csdn.net/liliwei0213/article/details/53639275</a></p>
<p>网上很多案例。这里就不写了，以后遇到处理的。我在补回记录</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/xiao_jun_0820/article/details/28602213" target="_blank" rel="noopener">HBase hbck——检察HBase集群的一致性</a></li>
<li><a href="http://www.zhyea.com/2017/07/29/hbase-hbck-usage.html" target="_blank" rel="noopener">hbase hbck用法</a></li>
<li><a href="https://mp.weixin.qq.com/s/RBXctAm9YGMPCQHR1NN8TQ" target="_blank" rel="noopener">Apache HBase 问题排查思路</a></li>
<li><a href="https://mp.weixin.qq.com/s/yt4X2tDQrLx35NsviRHbPg" target="_blank" rel="noopener">HBase运维基础——元数据逆向修复原理</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>HBase 计数器 Increment</title>
    <url>/2019/06/17/HBase-Increment/</url>
    <content><![CDATA[<p>天上那么多星，你可否数得清？</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>在互联网企业中，经常会有这样的一些需求，比如说：统计下某网站某日pv/uv量或是统计下某签约作者某篇文章的所获点赞数等等类似的需求，<br>传统的做法可能需要我们先读出该列的原有值，然后+1后再覆盖原有值，同时还要加锁处理等等。<br>为了保证原子性的完成一个客户端请求，HBase 引入了计数器的概念。本文主要简要概述一下 HBase 计数器的使用及应注意的一些问题。</p>
<h4 id="概念介绍"><a href="#概念介绍" class="headerlink" title="概念介绍"></a>概念介绍</h4><ol>
<li><p>HBase 计数器是什么？<br>一个计数器就是一个与其他列类似的简单列，列值要求且必须以长整型转码插入，否则将破坏该计数器结构。用户可以一次更新多个计数器，但它们都必须属于同一行。更新多行的计数器需多次RPC请求调用，暂不支持 batch(Increment)。</p>
</li>
<li><p>HBase 计数器解决什么问题？<br>HBase 计数器的引入主要解决了 read-and-modify 场景下的锁竞争与原子性问题。</p>
</li>
</ol>
<h4 id="Shell-Api"><a href="#Shell-Api" class="headerlink" title="Shell Api"></a>Shell Api</h4><h5 id="1-创建计数器并插入值"><a href="#1-创建计数器并插入值" class="headerlink" title="1. 创建计数器并插入值"></a>1. 创建计数器并插入值</h5><p>注意：步长值可为正可为负可为0.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase(main):003:0&gt; incr &#39;test&#39;,&#39;r1&#39;,&#39;f:count&#39;,1</span><br><span class="line">COUNTER VALUE &#x3D; 1</span><br><span class="line">0 row(s) in 0.0790 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):004:0&gt; incr &#39;test&#39;,&#39;r1&#39;,&#39;f:count&#39;,2</span><br><span class="line">COUNTER VALUE &#x3D; 3</span><br><span class="line">0 row(s) in 0.0200 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):005:0&gt; incr &#39;test&#39;,&#39;r1&#39;,&#39;f:count&#39;,-1</span><br><span class="line">COUNTER VALUE &#x3D; 2</span><br><span class="line">0 row(s) in 0.0170 seconds</span><br></pre></td></tr></table></figure>

<h5 id="2-Get-Api-获取计数器值"><a href="#2-Get-Api-获取计数器值" class="headerlink" title="2. Get Api 获取计数器值"></a>2. Get Api 获取计数器值</h5><p>可见：一个计数器就是一个与其他列类似的简单列。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase(main):006:0&gt; get &#39;test&#39;,&#39;r1&#39;,&#39;f:count&#39;</span><br><span class="line">COLUMN                                         CELL                                                                                                                                 </span><br><span class="line"> f:count                                       timestamp&#x3D;1552830973949, value&#x3D;\x00\x00\x00\x00\x00\x00\x00\x02                                                                      </span><br><span class="line">1 row(s) in 0.0600 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):007:0&gt; get &#39;test&#39;,&#39;r1&#39;,&#39;f:count:toLong&#39;</span><br><span class="line">COLUMN                                         CELL                                                                                                                                 </span><br><span class="line"> f:count                                       timestamp&#x3D;1552830973949, value&#x3D;2                                                                                                     </span><br><span class="line">1 row(s) in 0.0070 seconds</span><br></pre></td></tr></table></figure>

<h5 id="3-标准-Api-获取计数器值"><a href="#3-标准-Api-获取计数器值" class="headerlink" title="3. 标准 Api 获取计数器值"></a>3. 标准 Api 获取计数器值</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase(main):008:0&gt; get_counter &#39;test&#39;,&#39;r1&#39;,&#39;f:count&#39;</span><br><span class="line">COUNTER VALUE &#x3D; 2</span><br></pre></td></tr></table></figure>

<h5 id="4-错误示范"><a href="#4-错误示范" class="headerlink" title="4. 错误示范"></a>4. 错误示范</h5><p>如下操作将破坏计数器结构。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 以字符串类型PUT值</span><br><span class="line">hbase(main):009:0&gt; put &#39;test&#39;,&#39;r1&#39;,&#39;f:count&#39;,&#39;123&#39;</span><br><span class="line">0 row(s) in 0.0950 seconds</span><br><span class="line"></span><br><span class="line"># GET 获取值正常（该计数器已转为普通列）</span><br><span class="line">hbase(main):010:0&gt; get &#39;test&#39;,&#39;r1&#39;,&#39;f:count&#39;</span><br><span class="line">COLUMN                                         CELL                                                                                                                                 </span><br><span class="line"> f:count                                       timestamp&#x3D;1552831549637, value&#x3D;123                                                                                                   </span><br><span class="line">1 row(s) in 0.0160 seconds</span><br><span class="line"></span><br><span class="line"># 执行计数器 get_counter 操作</span><br><span class="line">hbase(main):011:0&gt; get_counter &#39;test&#39;,&#39;r1&#39;,&#39;f:count&#39;</span><br><span class="line"></span><br><span class="line">ERROR: offset (0) + length (8) exceed the capacity of the array: 3</span><br><span class="line"></span><br><span class="line"># 执行计数器 incr 操作</span><br><span class="line">hbase(main):012:0&gt; incr &#39;test&#39;,&#39;r1&#39;,&#39;f:count&#39;,-1</span><br><span class="line"></span><br><span class="line">ERROR: org.apache.hadoop.hbase.DoNotRetryIOException: Field is not a long, it&#39;s 3 bytes wide</span><br><span class="line">    at org.apache.hadoop.hbase.regionserver.HRegion.getLongValue(HRegion.java:7690)</span><br><span class="line">    at org.apache.hadoop.hbase.regionserver.HRegion.applyIncrementsToColumnFamily(HRegion.java:7644)</span><br><span class="line">    at org.apache.hadoop.hbase.regionserver.HRegion.doIncrement(HRegion.java:7530)</span><br><span class="line">    at org.apache.hadoop.hbase.regionserver.HRegion.increment(HRegion.java:7487)</span><br><span class="line">    at org.apache.hadoop.hbase.regionserver.RSRpcServices.increment(RSRpcServices.java:592)</span><br><span class="line">    at org.apache.hadoop.hbase.regionserver.RSRpcServices.mutate(RSRpcServices.java:2246)</span><br><span class="line">    at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:32383)</span><br><span class="line">    at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2150)</span><br><span class="line">    at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:112)</span><br><span class="line">    at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:187)</span><br><span class="line">    at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:167)</span><br></pre></td></tr></table></figure>

<p><strong>可以看出，计数器对数据类型是极其敏感的，使用过程中一定要注意。</strong></p>
<h4 id="Java-Api"><a href="#Java-Api" class="headerlink" title="Java Api"></a>Java Api</h4><h5 id="1-编码与解码"><a href="#1-编码与解码" class="headerlink" title="1. 编码与解码"></a>1. 编码与解码</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1) 编码：Bytes.toBytes(long)</span><br><span class="line"></span><br><span class="line">2) 解码：Bytes.toLong(bytes)</span><br></pre></td></tr></table></figure>

<h5 id="2-单列计数器"><a href="#2-单列计数器" class="headerlink" title="2. 单列计数器"></a>2. 单列计数器</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    Table table &#x3D; ...; &#x2F;&#x2F; 表实例</span><br><span class="line">    String rowkey &#x3D; ...; &#x2F;&#x2F; 行键</span><br><span class="line">    String columnFamily &#x3D; ...; &#x2F;&#x2F; 列族</span><br><span class="line">    </span><br><span class="line">    table.incrementColumnValue(Bytes.toBytes(rowkey),Bytes.toBytes(columnFamily), Bytes.toBytes(counter), 1L);</span><br><span class="line">    </span><br><span class="line">    table.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="3-多列计数器"><a href="#3-多列计数器" class="headerlink" title="3. 多列计数器"></a>3. 多列计数器</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    Table table &#x3D; ...; &#x2F;&#x2F; 表实例</span><br><span class="line">    String rowkey &#x3D; ...; &#x2F;&#x2F; 行键</span><br><span class="line">    String columnFamily &#x3D; ...; &#x2F;&#x2F; 列族</span><br><span class="line">    </span><br><span class="line">    Increment increment &#x3D; new Increment(Bytes.toBytes(rowkey));</span><br><span class="line">    increment.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(&quot;pv&quot;), 6L);</span><br><span class="line">    increment.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(&quot;uv&quot;), 10L);</span><br><span class="line">    </span><br><span class="line">    Result result &#x3D; table.increment(increment);</span><br><span class="line">    </span><br><span class="line">    table.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="4-获取计数器的值"><a href="#4-获取计数器的值" class="headerlink" title="4. 获取计数器的值"></a>4. 获取计数器的值</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Get get &#x3D; new Get(Bytes.toBytes(rowkey));</span><br><span class="line"></span><br><span class="line">get.setMaxVersions(1);</span><br><span class="line">get.addColumn(Bytes.toBytes(columnfamily), Bytes.toBytes(counter));</span><br><span class="line">Result result &#x3D; table.get(get);</span><br><span class="line">byte[] value &#x3D; result.getValue(Bytes.toBytes(columnfamily), Bytes.toBytes(counter));</span><br><span class="line"></span><br><span class="line">long count &#x3D; Bytes.toLong(value);</span><br><span class="line">String countStr &#x3D; String.valueOf(Bytes.toLong(value));</span><br></pre></td></tr></table></figure>

<h4 id="HBase-Hive-映射表"><a href="#HBase-Hive-映射表" class="headerlink" title="HBase-Hive 映射表"></a>HBase-Hive 映射表</h4><p>这里需要注意一下，如果想通过 Hive 映射 HBase 表获取计数器的值，<br>建表语句中计数器列语法要有别于常规列，否则将返回乱码或NULL值。</p>
<p>下面提供一个简单的示例:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE counters(</span><br><span class="line">	key string,</span><br><span class="line">	daily_hits bigint,</span><br><span class="line">	weekly string,</span><br><span class="line">	monthly string</span><br><span class="line">)   </span><br><span class="line">STORED BY &#39;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#39;   </span><br><span class="line">WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; &#x3D;&quot;:key,daily:hits#b,weekly:a,monthly:b&quot;)   </span><br><span class="line">TBLPROPERTIES(&quot;hbase.table.name&quot; &#x3D; &quot;counters&quot;);</span><br></pre></td></tr></table></figure>

<p>当 HBase 中 double，int，long 类型以byte方式存储时，用字符串取出来必然是乱码。</p>
<p><img src="/images/blog/2019-06-17-6.png" alt></p>
<p><img src="/images/blog/2019-06-17-5.png" alt></p>
<p>Bytes类型的数据，建 hive 映射表示加 #b</p>
<p>创建 HBase 表，计数器加1</p>
<p><img src="/images/blog/2019-06-17-2.png" alt></p>
<p>创建 Hive 映射表没有 #b， select 查询为 NULL，错误</p>
<p><img src="/images/blog/2019-06-17-3.png" alt></p>
<p>创建 Hive 映射表使用 #b， select 查询为 1，正确</p>
<p><img src="/images/blog/2019-06-17-4.png" alt></p>
<p>当在 Hive 中创建 HBase 已经存在的外部表时，默认的 hbase.table.default.storage.type 类型为 string。<br>daily_hits 为 bigint 字段的话，映射过来的值为 NULL。也可以修改 hbase.table.default.storage.type 为 binary</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE HisDiagnose(</span><br><span class="line">	key string, </span><br><span class="line">	doctorId int, </span><br><span class="line">	patientId int, </span><br><span class="line">	description String, </span><br><span class="line">	rtime int</span><br><span class="line">) </span><br><span class="line">STORED BY &#39;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#39; </span><br><span class="line">WITH SERDEPROPERTIES (</span><br><span class="line">    &quot;hbase.columns.mapping&quot; &#x3D; &quot;:key,diagnoseFamily:doctorId,diagnoseFamily:patientId,diagnoseFamily:description,diagnoseFamily:rtime&quot;, </span><br><span class="line">    &quot;hbase.table.default.storage.type&quot;&#x3D;&quot;binary&quot;</span><br><span class="line">) </span><br><span class="line">TBLPROPERTIES(&quot;hbase.table.name&quot; &#x3D; &quot;HisDiagnose&quot;);</span><br></pre></td></tr></table></figure>

<p>修改 hbase.table.default.storage.type 为 binary之后如果想表示类型为string， 加 cf:val#s 即可。</p>
<p>对此感兴趣可以阅读参考链接<a href="https://blog.csdn.net/jameshadoop/article/details/42162669" target="_blank" rel="noopener">hive与hbase数据类型对应关系</a>, <a href="https://hai19850514.iteye.com/blog/1918099" target="_blank" rel="noopener">hive创建外部表映射hbase中已存在表问题</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">作者： 禅克</span><br><span class="line">出处： &lt;https:&#x2F;&#x2F;mp.weixin.qq.com&#x2F;s&#x2F;EmG57gIJtyLZlYPwdHWNfA&gt;</span><br><span class="line">本文版权归作者所有，欢迎转载，但未经作者同意必须保留此段声明，且在页面明显位置给出原文链接。</span><br></pre></td></tr></table></figure>

<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/xianpanjia4616/article/details/81046077" target="_blank" rel="noopener">hbase的表映射到hive中</a></li>
<li><a href="https://www.cnblogs.com/similarface/p/5834347.html" target="_blank" rel="noopener">HBase之计数器</a></li>
<li><a href="https://blog.csdn.net/jameshadoop/article/details/42162669" target="_blank" rel="noopener">hive与hbase数据类型对应关系</a></li>
<li><a href="https://hai19850514.iteye.com/blog/1918099" target="_blank" rel="noopener">hive创建外部表映射hbase中已存在表问题</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>Flume</title>
    <url>/2019/06/15/Flume/</url>
    <content><![CDATA[<p>一个高效的流式数据收集系统</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>Flume 是一个高效的流式数据收集系统。支持在系统中定制各类数据发送方，用于收集数据；<br>同时，Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。</p>
<h4 id><a href="#" class="headerlink" title></a></h4>]]></content>
      <categories>
        <category>Flume</category>
      </categories>
  </entry>
  <entry>
    <title>HBase 数据模型</title>
    <url>/2019/06/14/HBase-Data-Model/</url>
    <content><![CDATA[<p>HBase 数据模型</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>HBase表主要包含 namespace(命名空间)、tableName(表名)、rowKey(主键)、column Family(列簇)、column qualifier(列)、cell(值)、timeStamp(时间戳) </p>
<h4 id="Namespace"><a href="#Namespace" class="headerlink" title="Namespace"></a>Namespace</h4><p>表命名空间，将多个表分到一个组进行统一管理。相当于 MySQL 里的 database </p>
<p>表命名空间主要是用来对表分组，那么对表分组有什么用？<br>命名空间可以填补 HBase 无法在一个实例上分库的缺憾，通过命名空间我们可以像关系型数据库一样将表分组，对于不同的组进行不同的环境设定，比如配额管理、安全管理等。</p>
<p>HBase 中有两个保留表空间是预先定义好的</p>
<blockquote>
<ul>
<li>hbase: 系统表空间，用于组织 HBase 内部表；</li>
<li>default: 那些没有定义表空间的表都被自动分配到这个表空间下。</li>
</ul>
</blockquote>
<h4 id="Table"><a href="#Table" class="headerlink" title="Table"></a>Table</h4><p>下图是一张 HBase 的表，概括了列族、列名和行之间的关系</p>
<p><img src="/images/blog/2019-06-14-1.png" alt></p>
<p>术语统一:</p>
<p>一行由很多列组成，全部由相同的主键引用。一个特定的主键和特定的列蔟:列称为单元格。一个单元格可以有很多版本，由不同时间戳的版本来区分。单元格可以叫做键值对</p>
<p>因此，一行由一个键引用，每行由一组单元格组成，其中每一个单元格又由特定的主键和列名确定</p>
<p>表名，列蔟是需要建表时给出，列可以在插入时指定加入，动态生成。一个列蔟可以有多个列。</p>
<p><strong>因为表名和列簇将在文件系统中被用于创建文件和目录，表名称和列簇名称需要使用可以打印的字符，此限制不适用与列。</strong></p>
<h4 id="Row"><a href="#Row" class="headerlink" title="Row"></a>Row</h4><p>一个行包含了多个列，这些列通过列族来分类 </p>
<p>行中的数据所属列族只能从该表所定义的列族中选取 </p>
<p>由于 HBase 是一个列式数据库，所以一个行中的数据可以分布在不同的服务器上。</p>
<p>HBase支持单行事务，行的一次读写是原子操作 （不论一次读写多少列），这个设计决策能够使用户很容易的理解程序在对同一个行进行并发更新操作时的行为。</p>
<h4 id="Row-Key"><a href="#Row-Key" class="headerlink" title="Row Key"></a>Row Key</h4><p>行键，Table的主键，用来检索记录。访问HBase table中的行，只有三种方式:<br>通过单个row key访问<br>通过row key的range<br>全表扫描</p>
<p>row key可以是任意字符串（最大长度是 64KB，实际应用中长度一般为 10-100bytes左右），<br>在HBase内部，row key保存为字节数组。</p>
<p>数据按照row key的字典序（byte order）排序存储，因此设计key时要充分排序存储这个特性，<br>将经常一起读取的行存储放到一起。<br>为了加快查询，键和列会按照字典排序存储在表中。</p>
<p><strong>RowKey 数字如果想保持数字顺序可以存储它们的字节表示</strong></p>
<h4 id="Column-Family"><a href="#Column-Family" class="headerlink" title="Column Family"></a>Column Family</h4><p>列簇，Table在水平方向有一个或者多个Column Family组成。 </p>
<p>列族是多个列的集合。HBase 会尽量把同一个列族的列放到同一个服务器上，<br>这样可以提高存取性能，并且可以批量管理有关联的一堆列</p>
<p>所有的数据属性都是定义在列族上</p>
<p>在 HBase 中，建表定义的不是列，而是列族</p>
<h4 id="Column-Qualifier"><a href="#Column-Qualifier" class="headerlink" title="Column Qualifier"></a>Column Qualifier</h4><p>列族和列经常用 Column Family: Column Qualifier 来一起表示，列是可以随意定义的，一个行中的列不限名字、不限数量。</p>
<p>所有 Column Qualifier 均以二进制格式存储，用户需要自行进行类型转换。</p>
<h4 id="Timestamp"><a href="#Timestamp" class="headerlink" title="Timestamp"></a>Timestamp</h4><p>时间戳，每次数据操作对应的时间戳，可以看作是数据的version number。</p>
<p>时间戳的类型是 64位整型，用长整型long表示。</p>
<p>时间戳可以由HBase在数据写入时自动赋值，此时时间戳是<strong>精确到毫秒</strong>的当前系统时间；</p>
<p>时间戳也可以由客户显式赋值，如果应用程序要避免数据版本冲突，就必须自己生成具有唯一性的时间戳。</p>
<p>为了避免数据存在过多版本造成的的管理 （包括存储和索引）负担，</p>
<p>HBase提供了两种数据版本回收方式:<br>一是保存数据的最后n个版本<br>二是TTL，即为保存最近一段时间内的版本（比如最近七天）<br>用户可以针对每个列族进行设置。</p>
<h4 id="Cell"><a href="#Cell" class="headerlink" title="Cell"></a>Cell</h4><p>hbase面向列存储的。这意味着每一列都是单独存储的，而不是单独存储整个行。只有带有值的列才会存储，没有值的列将不会存储。</p>
<p>由 {row key, column family, column qualifier, version} 唯一确定的单元。cell中的数据是没有类型的，全部是字节码形式存贮。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>HBase 所有数据都是以字节的方式存储的。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/37xUpZi13rysjzoylVC03A" target="_blank" rel="noopener">HBase原理 | HBase内部探险</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>RPC 原理</title>
    <url>/2019/06/12/rpc/</url>
    <content><![CDATA[<p>RPC 是Remote Procedure Call的缩写，译为远程过程调用。是一个计算机通信协议。</p>
<hr>
<h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><p>现在互联网应用的量级越来越大，单台计算机的能力有限，需要借助可扩展的计算机集群来完成。 </p>
<p><img src="/images/blog/2019-06-12-1.png" alt></p>
<p>上面是一个简单的软件系统结构，拆分出来用户系统和订单系统做为服务存在，让不同的站点去调用。</p>
<p>由于各服务部署在不同机器上，要想在服务间进行远程调用免不了网络通信过程，<br>服务消费方每调用一个服务都要写一坨网络通信相关的代码，不仅复杂而且极易出错。</p>
<p>这时如果有一种方式能让我们像调用本地服务一样调用远程服务，而让调用者对网络通信这些细节透明，<br>那么将大大提高生产力，比如服务消费方在执行orderService.buy(“HHKB键盘”)时，<br>实质上调用的是远端的服务。这种方式其实就是 RPC。而提供了这种功能的工具我们称之为RPC框架。</p>
<p>RPC(Remote Procedure Call) 即远程过程调用，允许一台计算机调用另一台计算机上的程序得到结果，<br>而代码中不需要做额外的编程，就像在本地调用一样。<br>通过网络通信调用不同的服务，共同支撑一个软件系统，微服务实现的基石技术。<br>使用RPC可以解耦系统，方便维护，同时增加系统处理请求的能力。</p>
<h4 id="框架原理"><a href="#框架原理" class="headerlink" title="框架原理"></a>框架原理</h4><p>在 RPC 框架中主要有三个角色: Provider、Consumer和Registry。如下图所示</p>
<p><img src="/images/blog/2019-06-12-2.png" alt></p>
<ul>
<li>Server: 暴露服务的服务提供方。</li>
<li>Client: 调用远程服务的服务消费方。</li>
<li>Registry: 服务注册与发现的注册中心。</li>
</ul>
<h4 id="基本流程"><a href="#基本流程" class="headerlink" title="基本流程"></a>基本流程</h4><p><img src="/images/blog/2019-06-12-3.png" alt></p>
<p>一次完整的RPC调用流程（同步调用，异步另说）如下:</p>
<p>1）服务消费方（client）调用以本地调用方式调用服务；<br>2）client stub接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体；<br>3）client stub找到服务地址，并将消息发送到服务端；<br>4）server stub收到消息后进行解码；<br>5）server stub根据解码结果调用本地的服务；<br>6）本地服务执行并将结果返回给server stub；<br>7）server stub将返回结果打包成消息并发送至消费方；<br>8）client stub接收到消息，并进行解码；<br>9）服务消费方得到最终结果。</p>
<p>RPC框架的目标就是要2~8这些步骤都封装起来，让用户对这些细节透明。</p>
<h4 id="服务注册中心"><a href="#服务注册中心" class="headerlink" title="服务注册中心"></a>服务注册中心</h4><p><img src="/images/blog/2019-06-12-4.png" alt></p>
<p>服务提供者启动后主动向注册中心注册机器ip、port以及提供的服务列表</p>
<p>服务消费者启动时向注册中心获取服务提供方地址列表，可实现软负载均衡和 Failover</p>
<h4 id="开源RPC框架"><a href="#开源RPC框架" class="headerlink" title="开源RPC框架"></a>开源RPC框架</h4><p><strong>Dubbo</strong></p>
<p>Dubbo 是阿里巴巴公司开源的一个Java高性能优秀的服务框架，使得应用可通过高性能的 RPC 实现服务的输出和输入功能，可以和 Spring框架无缝集成。目前已经进入Apache孵化器。</p>
<p><strong>Motan</strong></p>
<p>Motan是新浪微博开源的一个Java RPC框架。2016年5月开源。Motan 在微博平台中已经广泛应用，每天为数百个服务完成近千亿次的调用。</p>
<p><strong>gRPC</strong></p>
<p>gRPC是Google开发的高性能、通用的开源RPC框架，其由Google主要面向移动应用开发并基于HTTP/2协议标准而设计，基于ProtoBuf(Protocol Buffers)序列化协议开发，且支持众多开发语言。本身它不是分布式的，所以要实现上面的框架的功能需要进一步的开发。</p>
<p><strong>thrift</strong></p>
<p>thrift是Apache的一个跨语言的高性能的服务框架，也得到了广泛的应用。</p>
<h4 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h4><p>一个RPC框架大致需要动态代理、序列化、网络请求、网络请求接受(netty实现)、动态加载、反射这些知识点。<br>现在开源及各公司自己造的RPC框架层出不穷，唯有掌握原理是一劳永逸的。<br>掌握原理最好的方法莫不是阅读源码，自己动手写是最快的。</p>
<p>可以尝试借鉴优秀的 Demo 入手</p>
<p><a href="http://github.com/yangzhenkun/krpc" target="_blank" rel="noopener">http://github.com/yangzhenkun/krpc</a><br><a href="https://github.com/TFdream/mango" target="_blank" rel="noopener">https://github.com/TFdream/mango</a></p>
<p>因水平问题。。。本人只了解到原理层面。动手实践还是有点难度。后续争取补上自己的 rpc demo</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.jianshu.com/p/dbfac2b876b1" target="_blank" rel="noopener">从零开始实现RPC框架 - RPC原理及实现</a></li>
<li><a href="https://mp.weixin.qq.com/s/JkXrPcuKtE2qYgmDcH2uww" target="_blank" rel="noopener">漫话：如何给女朋友解释什么是RPC</a></li>
<li><a href="https://mp.weixin.qq.com/s/W0NBWCKOd96VJfNQbGLmcw" target="_blank" rel="noopener">一篇文章了解RPC框架原理</a></li>
</ul>
]]></content>
      <categories>
        <category>Distributed</category>
      </categories>
  </entry>
  <entry>
    <title>HBase 自动大合并脚本</title>
    <url>/2019/06/09/HBase-timing-major-compaction/</url>
    <content><![CDATA[<p>HBase 自动大合并脚本</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>HBASE 有默认的 major compaction 机制。</p>
<p>一般情况下，Major Compaction 时间会持续比较长，整个过程会消耗大量系统资源，对上层业务有比较大的影响。</p>
<p>因此线上业务都会将关闭自动触发 Major Compaction 功能，改为手动在业务低峰期触发。可以使用 major_compact 命令手动合并</p>
<p>如果 HBase 中的表很多的时候，用脚本定时在凌晨时分触发。</p>
<h4 id="脚本"><a href="#脚本" class="headerlink" title="脚本"></a>脚本</h4><p>备注: 只使用了 major_compact 这个合并命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">time_start&#x3D;&#96;date &quot;+%Y-%m-%d %H:%M:%S&quot;&#96;</span><br><span class="line">echo &quot;开始进行HBase的大合并.时间:$&#123;time_start&#125;&quot;</span><br><span class="line">  </span><br><span class="line">str&#x3D;&#96;echo list | hbase shell | sed -n &#39;$p&#39;&#96;</span><br><span class="line"> </span><br><span class="line">str&#x3D;$&#123;str&#x2F;&#x2F;,&#x2F; &#125;</span><br><span class="line">arr&#x3D;($str)</span><br><span class="line">length&#x3D;$&#123;#arr[@]&#125;</span><br><span class="line">current&#x3D;1</span><br><span class="line"> </span><br><span class="line">echo &quot;HBase中总共有$&#123;length&#125;张表需要合并.&quot;</span><br><span class="line">echo &quot;balance_switch false&quot; | hbase shell | &gt; &#x2F;dev&#x2F;null</span><br><span class="line">echo &quot;HBase的负载均衡已经关闭&quot;</span><br><span class="line">  </span><br><span class="line">for each in $&#123;arr[*]&#125;</span><br><span class="line">do</span><br><span class="line">        table&#x3D;&#96;echo $each | sed &#39;s&#x2F;]&#x2F;&#x2F;g&#39; | sed &#39;s&#x2F;\[&#x2F;&#x2F;g&#39;&#96;</span><br><span class="line">        echo &quot;开始合并第$&#123;current&#125;&#x2F;$&#123;length&#125;张表,表的名称为:$&#123;table&#125;&quot;</span><br><span class="line">        echo &quot;major_compact $&#123;table&#125;&quot; | hbase shell | &gt; &#x2F;dev&#x2F;null</span><br><span class="line">        let current&#x3D;current+1</span><br><span class="line">done</span><br><span class="line">  </span><br><span class="line">echo &quot;balance_switch true&quot; | hbase shell | &gt; &#x2F;dev&#x2F;null</span><br><span class="line">echo &quot;HBase的负载均衡已经打开.&quot;</span><br><span class="line">  </span><br><span class="line">time_end&#x3D;&#96;date &quot;+%Y-%m-%d %H:%M:%S&quot;&#96;</span><br><span class="line">echo &quot;HBase的大合并完成.时间:$&#123;time_end&#125;&quot;</span><br><span class="line">duration&#x3D;$($(date +%s -d &quot;$finish_time&quot;)-$(date +%s -d &quot;$start_time&quot;))</span><br><span class="line">echo &quot;耗时:$&#123;duration&#125;s&quot;</span><br></pre></td></tr></table></figure>

<h4 id="Linux-定时"><a href="#Linux-定时" class="headerlink" title="Linux 定时"></a>Linux 定时</h4><p>使用 crontab 来业务低峰期启动。</p>
<p>每天凌晨3点30分执行 majorCompact.sh 脚本</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">30 3 * * * &#x2F;bin&#x2F;bash &#x2F;root&#x2F;bigdata&#x2F;majorCompact.sh &gt;&gt;  &#x2F;root&#x2F;bigdata&#x2F;majorCompact.log</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>HBase Compaction</title>
    <url>/2019/06/06/HBase-Compaction/</url>
    <content><![CDATA[<p>Compaction 是 buffer-&gt;flush-&gt;merge 的 LSM 树的关键操作</p>
<hr>
<h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p><img src="/images/blog/2019-06-06-1.png" alt></p>
<p>一个表中的数据存储到 RS 上，RS 会管理实际存储表的数据的 region，每个 region上 每一个 ColumnFamily<br>会有一个 MemStore。</p>
<p>写入的数据写入 MemStore 内存区，当内存缓冲区满了之后，会将数据刷新到磁盘，随时间推移，一个 Store 中的 StoreFile 数量将会增长</p>
<p>Compaction 是一个操作，通过merge，将会减少一个 Store 中的StoreFile的数量，从而提高读操作的性能。</p>
<p>Compaction 是资源密集型操作，将提高或者影响性能，取决于很多因素。</p>
<h4 id="类型"><a href="#类型" class="headerlink" title="类型"></a>类型</h4><p><a href="http://hbase.apache.org/book.html#compaction" target="_blank" rel="noopener">官方Compaction介绍</a></p>
<p>合并有两种类型: 小合并（minor Compression）和大合并（major Compression）</p>
<p>Minor &amp; Major Compaction的区别</p>
<p>1）Minor 操作只用来做部分文件的合并操作以及包括 minVersion = 0 并且设置 ttl 的过期版本清理，不做任何删除数据、多版本数据的清理工作。</p>
<p>2）Major 操作是对 Region 下的 HStore 下的所有 StoreFile 执行合并操作，最终的结果是整理合并出一个文件。</p>
<p><img src="/images/blog/2019-06-06-2.png" alt></p>
<h4 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h4><p>主要起到如下几个作用:</p>
<p>1）合并文件, 提高读写数据的效率</p>
<p>2）清除删除、过期、多余版本的数据</p>
<p>3）文件本地化</p>
<p>RegionServer 最终目的是要实现数据本地化，才能够快速查找数据，HDFS 客户端默认拷贝三份数据副本，<br>其中第一份副本写到本地节点上，第二和第三份则写在不同机器的节点上 (RegionServer)；</p>
<p>Region 的拆分会导致 RegionServer 需要读取非本地的 StoreFile，<br>此时，HDFS 将会自动通过网络拉取数据，但通过网络读写数据相对地比本地读写数据的效率要低，要提升效率，<br>必须尽可能采用数据本地性，这也是为什么 HBase 要不定时地进行大合并和刷新把数据聚合在本地磁盘上来实现数据本地化，提升查询效率。<br>其次，在用户查询数据的时候，将会减少查询文件的数量，提高HBase查询效率，减少HDFS上保持寻址所有小文件的压力。压缩可能需要大量资源，并且可能会因许多因素而有助于或阻碍性能。</p>
<h4 id="触发时机"><a href="#触发时机" class="headerlink" title="触发时机"></a>触发时机</h4><p>在什么情况下会发生 Compaction 呢？</p>
<p>HBase 中可以触发 compaction 的因素有很多，最常见的因素有这么三种: MemStore Flush、后台线程周期性检查、手动触发。</p>
<ol>
<li><p>MemStore Flush: 应该说 compaction 操作的源头就来自flush操作，MemStore flush会产生 HFile 文件，文件越来越多就需要 compact。因此在每次执行完 Flush 操作之后，都会对当前 Store 中的文件数进行判断，符合条件就会触发 compaction。<br>需要说明的是，compaction 都是以 Store 为单位进行的，而在 Flush 触发条件下，整个 Region 的所有 Store 都会执行compact，所以会在短时间内执行多次 compaction。</p>
</li>
<li><p>后台线程周期性检查: CompactionChecker 是RS上的工作线程(Chore)。后台线程 CompactionChecker 定期触发检查是否需要执行 compaction，设置执行周期是通过 threadWakeFrequency 指定。<br>大小通过 hbase.server.thread.wakefrequency 配置(默认10000毫秒)，然后乘以默认倍数 hbase.server.compactchecker.interval.multiplier (1000),<br>毫秒时间转换为秒。因此，在不做参数修改的情况下，CompactionChecker大概是2hrs 46mins 40sec 执行一次。</p>
</li>
<li><p>手动触发：一般来讲，手动触发 compaction 通常是为了执行 major compaction，原因有三，其一是因为很多业务担心自动 major compaction 影响读写性能，因此会选择低峰期手动触发；<br>其二也有可能是用户在执行完 alter 操作之后希望立刻生效，执行手动触发 major compaction API；<br>其三是 HBase 管理员发现硬盘容量不够的情况下手动触发 major compaction 删除大量过期数据；<br>无论哪种触发动机，一旦手动触发，HBase会不做很多自动化检查，直接执行合并。</p>
</li>
</ol>
<h4 id="Compaction-诱发因子"><a href="#Compaction-诱发因子" class="headerlink" title="Compaction 诱发因子"></a>Compaction 诱发因子</h4><table>
<thead>
<tr>
<th>参数名</th>
<th>配置项</th>
<th>默认值</th>
</tr>
</thead>
<tbody><tr>
<td>minFilesToCompact</td>
<td>hbase.hstore.compactionThreshold</td>
<td>3</td>
</tr>
<tr>
<td>maxFilesToCompact</td>
<td>hbase.hstore.compaction.max</td>
<td>10</td>
</tr>
<tr>
<td>minCompactSize</td>
<td>hbase.hstore.compaction.min.size</td>
<td>128 MB 即(memstoreFlushSize)</td>
</tr>
<tr>
<td>maxCompactSize</td>
<td>hbase.hstore.compaction.max.size</td>
<td>Long.MAX_VALUE</td>
</tr>
</tbody></table>
<p>说明: 在以前版本的HBase中，hbase.hstore.compaction.min 调用了该参数 hbase.hstore.compactionThreshold。</p>
<p>compactionChecker 实现类是 CompactionChecker, 其继承了ScheduledChore，在之前已经分析过，这个类通过实现 chore 方法而实现周期性的调用，其初始化是在initializeThreads中</p>
<p>直接看其Chore方法</p>
<p><img src="/images/blog/2019-09-02-1.png" alt></p>
<p>方法比较简单，取出所有在线的region，遍历region上的所有store（HStore）判断是否需要compact</p>
<p>判断是 needsCompaction() 方法去判断是否足够多的文件触发了 Compaction 的条件。</p>
<p><img src="/images/blog/2019-06-09-1.png" alt></p>
<p>条件为: HStore 中 StoreFiles 的个数 – 正在执行 Compacting 的文件个数 &gt; minFilesToCompact</p>
<p>chore 方法中 needsCompaction判断的是minor compact 是否需要执行</p>
<p>chore 方法中可以直观的看到 majorCompact 是通过 isMajorCompaction() 方法判断的这是很多判断条件的合成</p>
<p>isMajorCompaction 判断上次进行majorCompaction到当前的时间间隔是否超过 hbase.hregion.majorcompaction 设置的值</p>
<h4 id="选择合适-HFile-合并"><a href="#选择合适-HFile-合并" class="headerlink" title="选择合适 HFile 合并"></a>选择合适 HFile 合并</h4><p>选择合适的文件进行合并是整个 compaction 的核心，因为合并文件的大小以及其当前承载的IO数直接决定了 compaction 的效果。</p>
<p>最理想的情况是，这些文件承载了大量IO请求但是大小很小，这样compaction本身不会消耗太多IO，而且合并完成之后对读的性能会有显著提升。<br>然而现实情况可能大部分都不会是这样，在0.96版本和0.98版本，分别提出了两种选择策略，在充分考虑整体情况的基础上选择最佳方案。</p>
<p>无论哪种选择策略都会首先对该 Store 中所有 HFile 进行一一排查，排除不满足条件的部分文件:</p>
<p>执行 RatioBasedCompactionPolicy 的 selectCompaction 算法</p>
<p><img src="/images/blog/2019-09-02-2.png" alt></p>
<p>第一个红框: 排除当前正在执行compact的文件及其比这些文件更老的所有文件（SequenceId更大）</p>
<p><img src="/images/blog/2019-09-02-3.png" alt></p>
<p>第二个红框: 排除某些过大的单个文件，如果文件大小大于hbase.hzstore.compaction.max.size（默认Long最大值），则被排除，否则会产生大量IO消耗</p>
<p>经过排除的文件称为候选文件，HBase接下来会再判断是否满足major compaction条件，如果满足，就会选择全部文件进行合并。</p>
<p><img src="/images/blog/2019-09-02-4.png" alt></p>
<p>第三个红框: 判断是否需要进行 major Compaction，这是很多判断条件的合成。</p>
<blockquote>
<ul>
<li>用户强制执行 major compaction</li>
<li>长时间没有进行 compact。hbase.hregion.majorcompaction 设置的值，也就是判断上次进行 major Compaction 到当前的时间间隔，如果超过设置值，并且满足另外一个条件是 compactSelection.getFilesToCompact().size() &lt; this.maxFilesToCompact (即候选文件数小于 hbase.hstore.compaction.max）。</li>
<li>Store中含有Reference文件，Reference文件是split region产生的临时文件，只是简单的引用文件，一般必须在compact过程中删除</li>
</ul>
</blockquote>
<pre><code>因此，通过设置 hbase.hregion.majorcompaction = 0 可以关闭 CompactionChecke 触发的 major compaction，但是无法关闭用户调用级别的 major Compaction。</code></pre><p>如果不满足 major compaction 条件，就必然为 minor compaction，第四个红框绿线处是进行 minor 策略，HBase 主要有两种 minor 策略: RatioBasedCompactionPolicy 和 ExploringCompactionPolicy</p>
<p>Ratio 策略是 0.94版本的默认策略，而 0.96 版本之后默认策略就换为了 Exploring 策略</p>
<p>深入阅读<a href="https://my.oschina.net/u/220934/blog/363270" target="_blank" rel="noopener">HBase Compaction算法之ExploringCompactionPolicy</a></p>
<p>ExploringCompactionPolicy 跟 RatioBasedCompactionPolicy 的区别，简单的说就是 RatioBasedCompactionPolicy 是简单的从头到尾遍历StoreFile列表，遇到一个符合Ratio条件的序列就选定执行Compaction。而 ExploringCompactionPolicy 则是从头到尾遍历的同时记录下当前最优，然后从中选择一个全局最优列表。</p>
<p>ratio 默认值是1.2，但是打开了非高峰时间段的优化时，可以有不同的值，非高峰的ratio默认值是5.0，此优化目的是为了在业务低估时可以合并更多的数据，目前此优化只能是天的小说时间段，还不算灵活。其有如下四个参数控制</p>
<table>
<thead>
<tr>
<th>配置项</th>
<th>默认值</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>hbase.hstore.compaction.ratio</td>
<td>1.2F</td>
<td></td>
</tr>
<tr>
<td>hbase.hstore.compaction.ratio.offpeak</td>
<td>5.0F</td>
<td>与下面两个参数联用</td>
</tr>
<tr>
<td>hbase.offpeak.start.hour</td>
<td>-1</td>
<td>设置 HBase offpeak 开始时间[0,23]</td>
</tr>
<tr>
<td>hbase.offpeak.end.hour</td>
<td>-1</td>
<td>设置 HBase offpeak 结束时间 [0,23]</td>
</tr>
</tbody></table>
<p>如果默认没有设置offpeak时间的话，那么完全按照hbase.hstore.compaction.ration来进行控制。如下图所示，如果filesSize[i]过大，超过后面8个文件总和*1.2，那么该文件被认为过大，而不纳入minor Compaction的范围。</p>
<p><img src="/images/blog/2019-06-09-2.png" alt></p>
<p>这样做使得 Compaction 尽可能工作在最近刷入 HDFS 的小文件的合并，从而使得提高 Compaction 的执行效率。</p>
<p><strong>ExploringCompactionPolicy 继承 RatioBasedCompactionPolicy，重写了 applyCompactionPolicy 方法，applyCompactionPolicy 是对 minor compaction 的选择文件的策略算法。</strong></p>
<p>接着通过 selectCompaction 选出的文件，加入到 filesCompacting 队列中。创建compactionRequest，提交请求。</p>
<h4 id="挑选合适的线程池"><a href="#挑选合适的线程池" class="headerlink" title="挑选合适的线程池"></a>挑选合适的线程池</h4><p>HBase 实现中有一个专门的线程 CompactSplitThead 负责接收 compact 请求以及 split 请求，<br>而且为了能够独立处理这些请求，这个线程内部构造了多个线程池：largeCompactions、smallCompactions以及splits等，<br>其中splits线程池负责处理所有的split请求，<br>largeCompactions和smallCompaction负责接收处理CR(CompactionRequest)，<br>其中前者用来处理大规模compaction，后者处理小规模compaction。</p>
<p>哪些 compaction 应该分配给 largeCompactions 处理，哪些应该分配给 smallCompactions 处理？是不是 Major Compaction 就应该交给largeCompactions线程池处理？不对。<br>而是根据一个配置hbase.regionserver.thread.compaction.throttle的设置值(一般在hbase-site.xml没有该值的设置)<br>而是采用默认值2 * minFilesToCompact * memstoreFlushSize，如果 compactRequest 需要处理的 storefile 文件的大小总和，大于throttle的值，则会提交到largeCompactions线程池进行处理，反之亦然。</p>
<p>这里需要明白三点:</p>
<p>1 上述设计目的是为了能够将请求独立处理，提供系统的处理性能。</p>
<p>2 这两个线程池不是根据 CR 来自于 Major Compaction 和 Minor Compaction 来进行区分</p>
<p>3 largeCompactions 线程池和 smallCompactions 线程池默认都只有一个线程，用户可以通过参数 hbase.regionserver.thread.compaction.large 和<br>hbase.regionserver.thread.compaction.small 进行配置</p>
<h4 id="执行-HFile-文件合并"><a href="#执行-HFile-文件合并" class="headerlink" title="执行 HFile 文件合并"></a>执行 HFile 文件合并</h4><p>上文一方面选出了待合并的HFile集合，一方面也选出来了合适的处理线程，万事俱备，只欠最后真正的合并。</p>
<p>合并流程说起来也简单，主要分为如下几步:</p>
<ol>
<li><p>分别读出待合并hfile文件的KV，并顺序写到位于./tmp目录下的临时文件中</p>
</li>
<li><p>将临时文件移动到对应region的数据目录</p>
</li>
<li><p>将compaction的输入文件路径和输出文件路径封装为KV写入WAL日志，并打上compaction标记，最后强制执行sync</p>
</li>
<li><p>将对应region数据目录下的compaction输入文件全部删除</p>
</li>
</ol>
<p>上述四个步骤看起来简单，但实际是很严谨的，具有很强的容错性和完美的幂等性：</p>
<ol>
<li><p>如果RS在步骤2之前发生异常，本次compaction会被认为失败，如果继续进行同样的compaction，上次异常对接下来的compaction不会有任何影响，也不会对读写有任何影响。唯一的影响就是多了一份多余的数据。</p>
</li>
<li><p>如果RS在步骤2之后、步骤3之前发生异常，同样的，仅仅会多一份冗余数据。</p>
</li>
<li><p>如果在步骤3之后、步骤4之前发生异常，RS在重新打开region之后首先会从WAL中看到标有compaction的日志，因为此时输入文件和输出文件已经持久化到HDFS，因此只需要根据WAL移除掉compaction输入文件即可</p>
</li>
</ol>
<h4 id="Compaction-对读写请求的影响"><a href="#Compaction-对读写请求的影响" class="headerlink" title="Compaction 对读写请求的影响"></a>Compaction 对读写请求的影响</h4><h5 id="存储上的写入放大"><a href="#存储上的写入放大" class="headerlink" title="存储上的写入放大"></a>存储上的写入放大</h5><p>HBase Compaction会带来写入放大，特别是在写多读少的场景下，写入放大就会比较明显，下图简单示意了写入放大的效果。</p>
<p><img src="/images/blog/2019-09-18-3.png" alt></p>
<p>（图片来源：<a href="https://mmbiz.qpic.cn/mmbiz_png/licvxR9ib9M6D6sDjXPZxHR1ic4LDKyicf2qfx417fJ8QHmfn82uBhSS1fC4mDqSB67JHzGs6kyqHiccrQEu2ryPKJA/640）" target="_blank" rel="noopener">https://mmbiz.qpic.cn/mmbiz_png/licvxR9ib9M6D6sDjXPZxHR1ic4LDKyicf2qfx417fJ8QHmfn82uBhSS1fC4mDqSB67JHzGs6kyqHiccrQEu2ryPKJA/640）</a></p>
<p>随着minor compaction以及major Compaction的发生，可以看到，这条数据被反复读取/写入了多次，这是导致写放大的一个关键原因，这里的写放大，涉及到网络IO与磁盘IO，因为数据在HDFS中默认有三个副本。</p>
<h5 id="读路径上的延时毛刺"><a href="#读路径上的延时毛刺" class="headerlink" title="读路径上的延时毛刺"></a>读路径上的延时毛刺</h5><p>HBase执行compaction操作结果会使文件数基本稳定，进而IO Seek次数相对稳定，延迟就会稳定在一定范围。然而，compaction操作会带来很大的带宽压力以及短时间IO压力。因此compaction就是使用短时间的IO消耗以及带宽消耗换取后续查询的低延迟。这种短时间的压力就会造成读请求在延时上会有比较大的毛刺。下图是一张示意图，可见读请求延时有很大毛刺，但是总体趋势基本稳定。</p>
<p><img src="/images/blog/2019-09-18-4.png" alt></p>
<h5 id="写请求上的短暂阻塞"><a href="#写请求上的短暂阻塞" class="headerlink" title="写请求上的短暂阻塞"></a>写请求上的短暂阻塞</h5><p>Compaction 对写请求也会有比较大的影响。主要体现在HFile比较多的场景下，HBase会限制写请求的速度。</p>
<p>当写请求非常多，导致不断生成 HFile，compact的速度远远跟不上HFile生成的速度，这样就会使HFile的数量会越来越多，<br>导致读性能急剧下降。</p>
<p>为了避免这种情况，在HFile的数量过多的时候会限制写请求的速度，每次 MemStore 执行 flush 的操作前，会查看 HFile 数量，<br>如果数量超过hbase.hstore.blockingStoreFiles 配置值，默认10，flush操作将会受到阻塞，阻塞时间为hbase.hstore.blockingWaitTime，默认90000，即1.5分钟，在这段时间内，如果compaction操作使得HFile下降到blockingStoreFiles配置值，则停止阻塞。另外阻塞超过时间后，也会恢复执行flush操作。这样做可以有效地控制大量写请求的速度，但同时这也是影响写请求速度的主要原因之一。</p>
<p>Compaction 执行合并操作生成的文件生效过程，需要对 Store 的写操作加锁，阻塞Store内的更新操作，直到更新 Store 的storeFiles完成为止。<br>Memstore 无法刷新磁盘，此时如果 memstore 的内存耗尽，客户端就会导致阻塞或者是超时。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>在大多数情况下，Major 是发生在 storefiles 和 filesToCompact 文件个数相同，并且满足各种条件的前提下执行。</p>
<p>触发 major compaction 的可能条件有: major_compact 命令、majorCompact() API、region server自动运行。<br>（相关参数：hbase.hregion.majoucompaction 默认为24 小时、hbase.hregion.majorcompaction.jetter 默认值为 0.5 防止region server 在同一时间进行major compaction）</p>
<p>hbase.hregion.majorcompaction.jetter 参数的作用是：对参数hbase.hregion.majoucompaction 规定的值起到浮动的作用</p>
<p>minor compaction的运行机制要复杂一些，它由一下几个参数共同决定:</p>
<p>hbase.hstore.compaction.min: 默认值为 3，表示至少需要三个满足条件的store file时，minor compaction才会启动。</p>
<p>hbase.hstore.compaction.max: 默认值为10，表示一次minor compaction中最多选取10个store file</p>
<p>hbase.hstore.compaction.min.size: 表示文件大小小于该值的store file 一定会加入到minor compaction的store file中</p>
<p>hbase.hstore.compaction.max.size: 表示文件大小大于该值的store file 一定会被minor compaction排除</p>
<p>hbase.hstore.compaction.ratio: 将store file 按照文件年龄排序（older to younger），minor compaction总是从older store file开始选择，如果该文件的size 小于它后面hbase.hstore.compaction.max 个store file size 之和乘以 该ratio，则该store file 也将加入到minor compaction 中。</p>
<p>另外，一般情况下，Major Compaction 时间会持续比较长，整个过程会消耗大量系统资源，对上层业务有比较大的影响。因此线上业务都会将关闭自动触发 Major Compaction 功能，改为手动在业务低峰期触发。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="http://hbasefly.com/2016/07/13/hbase-compaction-1/?xiruzu=vpafs3" target="_blank" rel="noopener">HBase Compaction的前生今世－身世之旅</a></li>
<li><a href="https://my.oschina.net/u/220934/blog/363270" target="_blank" rel="noopener">HBase Compaction算法之ExploringCompactionPolicy</a></li>
<li><a href="https://blog.csdn.net/zhoudetiankong/article/details/68924319" target="_blank" rel="noopener">hbase compaction 简单介绍</a></li>
<li><a href="https://blog.csdn.net/hljlzc2007/article/details/10980949#commentsedit" target="_blank" rel="noopener">深入分析HBase Compaction机制</a></li>
<li><a href="https://www.cnblogs.com/cxzdy/p/5368715.html" target="_blank" rel="noopener">HBase什么时候作minor major compact</a></li>
<li><a href="https://blog.csdn.net/Ancony_/article/details/84789142" target="_blank" rel="noopener">HBase自动大合并脚本</a></li>
<li><a href="https://blog.csdn.net/wodatoucai/article/details/71171348" target="_blank" rel="noopener">HBase在线系统性能优化</a></li>
<li><a href="https://blog.csdn.net/zjumath/article/details/6955082" target="_blank" rel="noopener">HBase Region操作实战分析之Store Compaction</a></li>
<li><a href="https://www.iteye.com/blog/blackproof-2192143" target="_blank" rel="noopener">hbase 定时进行compact CompactionChecker类</a></li>
<li><a href="https://cloud.tencent.com/developer/article/1005586" target="_blank" rel="noopener">Hbase Region Split compaction 过程分析以及调优</a></li>
<li><a href="https://blog.csdn.net/u011598442/article/details/90632702" target="_blank" rel="noopener">深入理解 HBase Compaction 机制</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>Linux 用户态 CPU 使用率高</title>
    <url>/2019/06/03/Occupying-CPU-high/</url>
    <content><![CDATA[<p>使用堆栈进行问题排查，如机器占用 CPU 高问题排查、死锁</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>运维经常会遇到 CPU 过高的情况。针对这种情况需要一定的处理机制</p>
<h4 id="定位方法"><a href="#定位方法" class="headerlink" title="定位方法"></a>定位方法</h4><p>传统的方案一般是4步</p>
<p>1 top oder by with P // 首先按进程负载排序找到  maxLoad(pid)</p>
<p>2 top -Hp 进程PID    // 找到相关负载 线程PID</p>
<p>3 printf “0x%x ” 线程PID  // 将线程PID转换为 16进制，为后面查找 jstack 日志做准备</p>
<p>4 jstack 进程PID &#124; grep &quot;0x6bd9&quot; -C 50 &#124; less  // 将进程PID所在行的上下 50 行搜索出来</p>
<p>但是对于线上问题定位来说，分秒必争，上面的 4 步还是太繁琐耗时了</p>
<p>淘宝的 oldratlee 同学就将上面的流程封装为了一个工具：<a href="https://github.com/oldratlee/useful-scripts" target="_blank" rel="noopener">show-busy-java-threads.sh</a></p>
<p>该工具可以很方便的定位线上的这类问题</p>
<h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>这里选择下载和运行单个文件，其他方式可以参阅<a href="https://github.com/oldratlee/useful-scripts/blob/master/docs/install.md" target="_blank" rel="noopener">下载使用</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget --no-check-certificate https:&#x2F;&#x2F;raw.github.com&#x2F;oldratlee&#x2F;useful-scripts&#x2F;release&#x2F;show-busy-java-threads</span><br><span class="line">chmod +x show-busy-java-threads</span><br><span class="line"></span><br><span class="line">.&#x2F;show-busy-java-threads</span><br></pre></td></tr></table></figure>

<h4 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h4><p>使用案例可以参阅下方几个链接了解</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/Cgequvi7cxtnjGQH4CbXLQ" target="_blank" rel="noopener">Linux 系统 CPU 100% 异常排查实践与总结</a></li>
<li><a href="https://mp.weixin.qq.com/s/mDVGyeOnMeq8EQwhe7M_jA" target="_blank" rel="noopener">线上服务 CPU 又 100% 啦？一键定位 so easy！</a></li>
<li><a href="https://github.com/oldratlee/useful-scripts/blob/master/docs/java.md#-show-busy-java-threads" target="_blank" rel="noopener">github show-busy-java-threads 官方示例</a></li>
</ul>
<p>可以处理CPU过高、线程死锁导致程序 hang 住的情景</p>
<p>有时候基于经验判断后不太确定，可以多多执行几次show-busy-java-threads，如果情况高概率出现，则可以确定判定。<br>因为调用越少代码执行越快，则出现在线程栈的概率就越低。<br>脚本有自动多次执行的功能，指定 重复执行的间隔秒数/重复执行的次数 参数 </p>
<h4 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h4><p>除了正文提到的 show-busy-java-threads.sh，oldratlee 同学还整合和不少常见的开发、运维过程中涉及到的脚本工具</p>
<ul>
<li>show-duplicate-java-classes   找出jar文件和class目录中的重复类。用于排查Java类冲突问题</li>
<li>find-in-jars    在目录下所有jar文件里，查找类或资源文件</li>
</ul>
<p>参考链接2里也提到一些其他的工具，我暂未了解，因此不在叙述。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/Cgequvi7cxtnjGQH4CbXLQ" target="_blank" rel="noopener">Linux 系统 CPU 100% 异常排查实践与总结</a></li>
<li><a href="https://mp.weixin.qq.com/s/mDVGyeOnMeq8EQwhe7M_jA" target="_blank" rel="noopener">线上服务 CPU 又 100% 啦？一键定位 so easy！</a></li>
<li><a href="https://github.com/oldratlee/useful-scripts" target="_blank" rel="noopener">useful-scripts</a></li>
</ul>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>HBase 表信息解析</title>
    <url>/2019/06/03/Analysis-of-metadata-information-in-HBase-tables/</url>
    <content><![CDATA[<p>HBase 表元数据信息解析</p>
<hr>
<h4 id="表元数据"><a href="#表元数据" class="headerlink" title="表元数据"></a>表元数据</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">desc &#39;dimensoft:user&#39;, &#x2F;&#x2F;namespace:tableName</span><br><span class="line">    &#123;</span><br><span class="line">        NAME &#x3D;&gt; &#39;info&#39;, &#x2F;&#x2F;列族</span><br><span class="line"> </span><br><span class="line">        DATA_BLOCK_ENCODING &#x3D;&gt; &#39;NONE&#39;, &#x2F;&#x2F;数据块编码方式设置</span><br><span class="line">        &#x2F;&#x2F;参见: http:&#x2F;&#x2F;hbase.apache.org&#x2F;book.html#compression</span><br><span class="line">        &#x2F;&#x2F;http:&#x2F;&#x2F;hbase.apache.org&#x2F;book.html#data.block.encoding.enable</span><br><span class="line"> </span><br><span class="line">        BLOOMFILT &#x3D;&gt; &#39;ROW&#39;, &#x2F;&#x2F;参见：http:&#x2F;&#x2F;hbase.apache.org&#x2F;book.html#bloom.filters.when</span><br><span class="line"> </span><br><span class="line">        REPLICATION_SCOPE &#x3D;&gt; &#39;0&#39;, &#x2F;&#x2F;配置HBase集群replication时需要将该参数设置为1.</span><br><span class="line"> </span><br><span class="line">        &#x2F;&#x2F;参见：http:&#x2F;&#x2F;blog.cloudera.com&#x2F;blog&#x2F;2012&#x2F;08&#x2F;hbase-replication-operational-overview&#x2F;?utm_source&#x3D;tuicool</span><br><span class="line">        &#39;REPLICATION_SCOPE is a column-family level attribute</span><br><span class="line">user has to alter each column family with the alter command as</span><br><span class="line">shown above, for all the column families he wants to replicate.&#39;</span><br><span class="line"> </span><br><span class="line">        VERSIONS &#x3D;&gt; &#39;1&#39;, &#x2F;&#x2F;设置保存的最大版本数</span><br><span class="line"> </span><br><span class="line">        COMPRESSION &#x3D;&gt; &#39;NONE&#39;, &#x2F;&#x2F;设置压缩算法</span><br><span class="line"> </span><br><span class="line">        MIN_VERSIONS &#x3D;&gt; &#39;0&#39;, &#x2F;&#x2F;最小存储版本数</span><br><span class="line"> </span><br><span class="line">        TTL &#x3D;&gt; &#39;FOREVER&#39;, &#x2F;&#x2F;参见：http:&#x2F;&#x2F;hbase.apache.org&#x2F;book.html#ttl</span><br><span class="line">        &#39;ColumnFamilies can set a TTL length in seconds, and HBase</span><br><span class="line">        reached. This applies to all versions of a row - even the current one.</span><br><span class="line">        The TTL time encoded in the HBase for the row is specified in</span><br><span class="line">        UTC.&#39;</span><br><span class="line"> </span><br><span class="line">        KEEP_DELETED_CELLS &#x3D;&gt; &#39;false&#39;, &#x2F;&#x2F;参见：http:&#x2F;&#x2F;hbase.apache.org&#x2F;book.html#cf.keep.deleted</span><br><span class="line"> </span><br><span class="line">        BLOCKSIZE &#x3D;&gt; &#39;65536&#39;, &#x2F;&#x2F;设置HFile数据块大小（默认64kb）</span><br><span class="line"> </span><br><span class="line">        IN_MEMORY &#x3D;&gt; &#39;false&#39;,&#x2F;&#x2F;设置激进缓存，优先考虑将该列族放入块缓存中,即放入BlockCache，</span><br><span class="line">                             &#x2F;&#x2F;针对随机读操作相对较多的列族可以设置该属性为true，不能设置meta为false，因为meta表经常使用</span><br><span class="line"> </span><br><span class="line">        BLOCKCACHE &#x3D;&gt; &#39;true&#39; &#x2F;&#x2F;数据块缓存属性</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>


<h4 id="TTL"><a href="#TTL" class="headerlink" title="TTL"></a>TTL</h4><p>在minor compaction时删除仅包含过期行的存储文件。设置hbase.store.delete.expired.storefile为false禁用此功能。将最小版本数设置为0以外也会禁用此功能。</p>
<h4 id="数据块编码"><a href="#数据块编码" class="headerlink" title="数据块编码"></a>数据块编码</h4><p>数据块编码是 HBase 的一个特性，即 key 会根据前一个 key 进行编码和压缩。</p>
<p>其中一个编码选项（FAST_DIFF）让hbase 只存储当前 key 和前一个 key 不同的地方。</p>
<p>HBase 独立存储每个单元，包括 key 和 value。当一行有多个 cell 时，为每个 cell 写入相同的 key 将会消耗大量空间，启用数据块编码可以节省大量空间。</p>
<p>多数情况下启动数据块编码是有用的。</p>
<h4 id="MinVersion"><a href="#MinVersion" class="headerlink" title="MinVersion"></a>MinVersion</h4><p>used when timeToLive is set</p>
<p>如果HBase中的表设置了TTL的时候，MinVersion才会起作用。</p>
<p>a）MIN_VERSION &gt; 0时：</p>
<p>Cell至少有MIN_VERSION个最新版本会保留下来。这样确保在你的查询以及数据早于TTL时有结果返回。</p>
<p>b）MIN_VERSION = 0时：</p>
<p>Cell中的数据超过TTL时间时，全部清空，不保留最低版本。</p>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>HBase 源码分析之 WAL</title>
    <url>/2019/06/03/WAL-of-HBase-Source-Code-Analysis/</url>
    <content><![CDATA[<p>HBase 源码分析之 WAL</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>WAL(Write-Ahead Logging)是数据库系统中保障原子性和持久性的技术，通过使用 WAL 可以将数据的随机写入变为顺序写入，可以提高数据写入的性能。</p>
<p>在 HBase 中写入数据时，会将数据写入内存同时写 WAL 日志, 为防止日志丢失，日志是写在 HDFS 上的</p>
<p>默认是每个 RegionServer 有1个WAL，在 HBase1.0 开始支持多个WAL <a href="https://issues.apache.org/jira/browse/HBASE-5699" target="_blank" rel="noopener">HBASE-5699</a><br>,这样可以提高写入的吞吐量。配置参数为 hbase.wal.provider=multiwal, 支持的值还有defaultProvider和filesystem(这2个是同样的实现)。</p>
<p><img src="/images/blog/2019-06-03-4.png" alt> </p>
<h4 id="WAL-持久化级别"><a href="#WAL-持久化级别" class="headerlink" title="WAL 持久化级别"></a>WAL 持久化级别</h4><p>HBase 中可以通过设置 WAL 的持久化等级决定是否开启 WAL 机制、以及 HLog 的落盘方式。</p>
<ol>
<li>USE_DEFAULT: 如果没有指定持久化级别，则默认为 USE_DEFAULT, 这个为使用 HBase 全局默认级别(SYNC_WAL)</li>
<li>SKIP_WAL: 不写 WAL 日志, 这种可以较大提高写入的性能，但是会存在数据丢失的危险，只有在大批量写入的时候才使用(出错了可以重新运行)，其他情况不建议使用。</li>
<li>ASYNC_WAL: 异步写入</li>
<li>SYNC_WAL: 同步写入wal日志文件，保证数据写入了DataNode节点。</li>
<li>FSYNC_WAL: 目前不支持了，表现是与SYNC_WAL是一致的</li>
</ol>
<p><img src="/images/blog/2019-06-03-5.png" alt> </p>
<p>用户可以通过客户端设置WAL持久化等级，设置put的属性。<br>代码:put.setDurability(Durability. SYNC_WAL);</p>
<p>WAL 持久化保障 HBase 行级事务的持久性</p>
<h4 id="WAL-结构"><a href="#WAL-结构" class="headerlink" title="WAL 结构"></a>WAL 结构</h4><p>WAL 结构可以参阅<a href="https://lihuimintu.github.io/2019/04/29/hbase-mvcc/" target="_blank" rel="noopener">HBase 行锁与多版本并发控制 (MVCC)</a>的原子性保障</p>
<p>WALKey: WAL日志的key, 包括 log sequncece number 作为 HFile 中一个重要的元数据，和 HLog 的生命周期息息相关；regionName–日志所属的region, tablename–日志所属的表，writeTim–日志写入时间，clusterIds–cluster的id，在数据复制的时候会用到。 </p>
<p>WALEdit: 在 HBase 行级事务日志中记录一系列的修改的一条事务日志。另外WALEdit实现了Writable接口，可用于序列化处理。 </p>
<p><img src="/images/blog/2019-06-03-6.png" alt></p>
<h4 id="WAL-写入模型"><a href="#WAL-写入模型" class="headerlink" title="WAL 写入模型"></a>WAL 写入模型</h4><p>这里将阐述 RegionServer 是如何把多个 client 的“写”操作安全有序地落地日志文件，又如何让 client 端优雅地感知到已经真正的落地。</p>
<p>Write Ahead Log (WAL)提供了一种高并发、持久化的日志保存与回放机制。<br>每一个业务数据的写入操作（PUT / DELETE）执行前，都会记账在WAL中。</p>
<p>如果出现HBase服务器宕机，则可以从WAL中回放执行之前没有完成的操作。</p>
<ol>
<li><p>由于多个HBase客户端可以对某一台HBase Region Server发起并发的业务数据写入请求，因此WAL也要支持并发的多线程日志写入。——确保日志写入的线程安全、高并发。</p>
</li>
<li><p>对于单个HBase客户端，它在WAL中的日志顺序，应该与这个客户端发起的业务数据写入请求的顺序一致。</p>
</li>
</ol>
<p>（对于以上两点要求，大家很容易想到，用一个队列就搞定了。见下文的架构图。）</p>
<ol start="3">
<li><p>为了保证高可靠，日志不仅要写入文件系统的内存缓存，而且应该尽快、强制刷到磁盘上（即WAL的Sync操作）。但是Sync太频繁，性能会变差。所以：</p>
<p>(1) Sync应当在多个后台线程中异步执行</p>
<p>(2) 频繁的多个Sync，可以合并为一次Sync——适当放松对可靠性的要求，提高性能。</p>
</li>
</ol>
<p>其线程模型主要实现实在 FSHLog 中，FSHLog 是 WAL 接口的实现类，负责将数据写入文件系统，其实现了最关键的 apend() 和 sync() 方法</p>
<p><img src="/images/blog/2019-06-03-8.png" alt></p>
<p>这个图主要描述了 HRegion 中调用 append() 和 sync() 后, HBase 的 WAL 线程流转模型。</p>
<p>最左边是有多个 client 提交到 HRegion 的 append 和 sync 操作。<br>当调用 append 后 WALEdit 和 WALKey 会被封装成 FSWALEntry 类进而再封装成 RinbBufferTruck 类放入一个线程安全的 Buffer(LMAX Disruptor RingBuffer) 中。可以理解成服务进程的缓存中</p>
<p>这里的队列是一个<a href="https://www.cnblogs.com/ohuang/p/5799467.html" target="_blank" rel="noopener">LMAX Disrutpor RingBuffer</a>，可以简单理解为是一个无锁高并发队列。</p>
<p>当调用 sync 后会生成一个 SyncFuture 进而封装成 RinbBufferTruck 类同样放入这个 Buffer 中，然后工作线程此时会被阻塞等待被 notify() 唤醒。</p>
<p>在最右边会有一个且只有一个线程 (即RingBufferEventHandler) 专门去处理这些 RinbBufferTruck，如果是 FSWALEntry 则写入 hadoop sequence file。因为文件缓存的存在，这时候很可能 client 数据并没有落盘。<br>所以进一步如果是 SyncFuture 会被批量的放到一个线程池中，异步的批量去刷盘, 刷盘成功后唤醒工作线程完成 WAL。</p>
<p>这是我按照自己的理解画的图</p>
<p><img src="/images/blog/2019-06-03-7.png" alt></p>
<h4 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h4><h5 id="Region-Server-RPC服务线程"><a href="#Region-Server-RPC服务线程" class="headerlink" title="Region Server RPC服务线程"></a>Region Server RPC服务线程</h5><p>这些工作线程处理 HBase 客户端通过 RPC 服务调用（实际上是 Google Protobuf 服务调用）发出的业务数据写入请求。</p>
<p><strong>对于Append操作</strong></p>
<p>工作线程中当 HRegion 准备好一个行事务“写”操作的，WALEdit，WALKey 后就会调用FSHLog的append方法: </p>
<p><img src="/images/blog/2019-06-03-9.png" alt></p>
<p>FSHLog 的 append 方法首先会从LAMX Disruptor RingbBuffer 中拿到一个序号作为 txid(sequence)，然后把 WALEdit, WALKey 和 sequence 等构建一个 FSALEntry 实例 entry，<br>并把 entry 放到 ringbuffer 中。<br>而 entry 以 truck.loadPayload(RingBufferTruck，ringbuffer实际存储类型) 通过<br>sequence 和 ringbuffer 中的 RingBufferTruck 一一对应。</p>
<p><img src="/images/blog/2019-06-03-10.png" alt></p>
<p><strong>对于Sync操作</strong></p>
<p><img src="/images/blog/2019-06-03-11.png" alt></p>
<p>如果 client 设置的持久化等级是 USER_DEFAULT，SYNC_WAL或FSYNC_WAL<br>那么工作线程的 HRegion 还将调用 FSHLog 的 sync() 方法</p>
<p><img src="/images/blog/2019-06-03-12.png" alt></p>
<p><img src="/images/blog/2019-06-03-13.png" alt></p>
<p><img src="/images/blog/2019-06-03-14.png" alt></p>
<p><img src="/images/blog/2019-06-03-15.png" alt></p>
<p>追踪代码可以分析出Sync()方法会往ringbuffer中放入一个SyncFuture对象，并阻塞等待完成(唤醒)。</p>
<h5 id="WAL-日志消费线程"><a href="#WAL-日志消费线程" class="headerlink" title="WAL 日志消费线程"></a>WAL 日志消费线程</h5><p>WAL机制中，只有一个 WAL 日志消费线程，从队列中获取 Append 和 Sync 操作。这样一个<strong>多生产者单消费者</strong>的模式，决定了 WAL 日志并发写入时日志的全局唯一顺序。</p>
<p>像模型图中所展示的多个工作线程封装后拿到由 ringbuffer 生成的 sequence 后作为生产者放入 ringbuffer 中。<br>在  FSHLog中有一个私有内部类 RingBufferEventHandler 类实现了 LAMX Disruptor的EventHandler 接口，也即是实现了 OnEvent 方法的 ringbuffer 的消费者。</p>
<p>Disruptor 通过 java.util.concurrent.ExecutorService 提供的线程来触发 Consumer 的事件处理，可以看到 HBase 的 WAL 中只启了一个线程。<br>从源码注释中也可以看到 RingBufferEventHandler 在运行中只有单个线程。</p>
<p><img src="/images/blog/2019-06-03-16.png" alt></p>
<p>由于消费者是按照 sequence 的顺序刷数据，这样就能保证 WAL 日志并发写入时只有一个线程在真正的写入日志文件的可感知的全局唯一顺序。</p>
<p>RingBufferEventHandler类的onEvent()(一个回调方法)是具体处理append和sync的方法。</p>
<p>在前面说明过 WAL 使用 RingBufferTruck 来封装 WALEntry 和 SyncFuture (如下图源码)</p>
<p><img src="/images/blog/2019-06-03-18.png" alt></p>
<p>这部分源码可以看到 RingBufferTruck 类的结构，从注释可以看到选择 SyncFuture 和 FSWALEntry 一个放入 ringbuffer 中。</p>
<p>在消费线程的实际执行方法 onEvent( )中就是被 ringbuffer 通知一个个的从 ringbfer 取出 RingBufferTruck，如果是 WALEntry(即为FSWALEntry) 则使用当前 HadoopSequence 文件writer写入文件（此时很可能写的是文件缓存）</p>
<p><img src="/images/blog/2019-06-03-17.png" alt></p>
<p>对于获取到的 Append 操作，直接调用 Hadoop Sequence File Writer 将这个 Append 操作（包括元数据和row key, family, qualifier, timestamp, value等业务数据）写入文件</p>
<p>如果是 SyncFuture 则简单的轮询处理放入 SyncRunner 线程异步去把文件缓存中数据刷到磁盘。</p>
<p>通过下面代码可以看到，先将 SyncFuture 累积到一个 syncFutures 数组中，syncFuturesCount 等于 this.syncFutures.length 长度时<br>endOfBatch 才等于 true</p>
<p><img src="/images/blog/2019-06-03-19.png" alt></p>
<p>如果 endOfBatch 为 false 则不执行轮询代码。只有积累到一定长度 endOfBatch 为 true 时才执行轮询</p>
<p>这部分源码是说明 syn c操作的 SyncFutur e会被提交到 SyncRunner 中，这里可以注意 SyncFuture 实例其实并不是一个个提交到 SyncRunner 中执行的，而是以 syncFutures (数组,多个SyncFuture实例)方式提交的。</p>
<p><img src="/images/blog/2019-06-03-20.png" alt></p>
<p>this.syncRunners 就是 SyncRunner 线程池。可以看到，通过计算syncRunnerIndex，采用了简单的轮循提交算法。</p>
<p><img src="/images/blog/2019-06-03-21.png" alt></p>
<p>这里再加一个异步操作去真正刷文件缓存的原因 wal 源码中有解释: 刷磁盘是很费时的操作，<br>如果每次都同步的去刷 client 的回应比较快，但是写效率不高，如果异步刷文件缓存，写效率提高但是友好性降低<br>在考虑了写吞吐率和对 client 友好回应平衡后，wal选择了后者，积累了一定量(通过 ringbuffer 的 sequence)的缓存再刷磁盘以此提高写效率和吞吐率。</p>
<p>这个决策从 HBase 存储机制最初采用lsm树把随机写转换成顺序写以提高写吞吐率，可以看出是目标一致的。</p>
<h5 id="SyncRunner线程"><a href="#SyncRunner线程" class="headerlink" title="SyncRunner线程"></a>SyncRunner线程</h5><p>SyncRunner 是一个线程，WAL 实际有一个SyncRunner的线程组，专门负责之前append到文件缓存的刷盘工作。</p>
<p><img src="/images/blog/2019-06-03-22.png" alt></p>
<p>SyncRunner的线程方法（run()）负责具体的刷写文件缓存到磁盘的工作。</p>
<p>首先队列中获取一个由 WAL 日志消费线程提交的 SyncFuture (第一个红框)</p>
<p>即去之前提交的 SyncFutures 中拿到其中 sequence 最大的 SyncFuture 实例，并拿到它对应 ringbuffer 的sequence。</p>
<p>再去对比当前最大的 sequence (即currentHighestSyncedSequence)，如果发现比当前最大的 sequence 小则去调用 releaseSyncFuture() 方法释放 synceFuture，<br>实际就是notify通知正被阻塞的sync操作，让工作线程可以继续往下继续。(第二个红框)</p>
<p>SyncRunner线程只会落实执行其中最新的 SyncFuture 所代表的Sync操作。而忽略之前的 SyncFuture。</p>
<p>调用文件系统 API，执行 sync() 操作 (第三个红框)</p>
<p>如果sync()完成，或者因为上面提到的合并忽略了某一个SyncFuture，那么会调用releaseSyncFuture() ==&gt; Object.notify()来通知SyncFuture阻塞退出。<br>之前阻塞在SyncFuture.get()上的Region Server RPC服务线程就可以继续往下执行了。 (第四个红框)</p>
<p><img src="/images/blog/2019-06-03-23.png" alt></p>
<p>至此，整个WAL写入流程完成。</p>
<p>为什么 SyncRunner 是个线程池？ 而不是单个线程。<a href="http://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/regionserver/wal/FSHLog.SyncRunner.html" target="_blank" rel="noopener">官方有介绍</a></p>
<p><img src="/images/blog/2019-06-04-1.png" alt></p>
<p>同事解释说只有一个 SyncRunner 线程情况下， 第一个 sync 发出以后，来的写请求，只能等下一轮了，就会很慢。</p>
<p>我是这么理解的。sync 是个很耗时的操作，假设耗时 A。</p>
<p>第一个 sync 发生后，后面来的写请求如果 sync 得等待前一个 sync 耗时完成。</p>
<p>等前一个 sync 完成，和自己所在的 sync 完成，就是两个 A 的耗时。所以降低延迟，用多线程来 sync 可以提高效率</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/tengxy_cloud/article/details/53579795" target="_blank" rel="noopener">Hbase WAL线程模型源码分析</a></li>
<li><a href="https://www.cnblogs.com/ohuang/p/5807543.html" target="_blank" rel="noopener">HBase的Write Ahead Log (WAL) —— 整体架构、线程模型</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>JVM 板斧</title>
    <url>/2019/06/01/Java-Tools/</url>
    <content><![CDATA[<p>Java 排查问题的命令和工具</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>JDK 自带些排查问题的命令和工具</p>
<h4 id="jps"><a href="#jps" class="headerlink" title="jps"></a>jps</h4><p>jps 主要用来输出JVM中运行的进程状态信息</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">jps [options]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[hadoop@hadoop3 ~]$ jps -ml</span><br><span class="line">14020 org.apache.hadoop.yarn.server.nodemanager.NodeManager</span><br><span class="line">28790 com.baidu.hugegraph.dist.HugeGraphServer conf&#x2F;gremlin-server.yaml conf&#x2F;rest-server.properties</span><br><span class="line">21704 org.apache.spark.deploy.worker.Worker --webui-port 8081 spark:&#x2F;&#x2F;hadoop1:7077</span><br><span class="line">13497 org.apache.hadoop.hdfs.server.datanode.DataNode</span><br></pre></td></tr></table></figure>

<p>参数说明</p>
<blockquote>
<p>-q 不输出类名、Jar名和传入main方法的参数</p>
<p> -m 输出传入main方法的参数</p>
<p> -l 输出main类或Jar的全限名</p>
<p>-v 输出传入JVM的参数</p>
</blockquote>
<p><img src="/images/blog/2019-06-05-1.png" alt> </p>
<p>如果遇到 process information unavailable 情况。到 jdk 的安装路径下运行命令 bin/jps</p>
<h4 id="jstack"><a href="#jstack" class="headerlink" title="jstack"></a>jstack</h4><p>jstack是JVM自带的Java堆栈跟踪工具，它用于打印出给定的java进程ID、core file、远程调试服务的Java堆栈信息.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">jstack -l 2815</span><br></pre></td></tr></table></figure>

<p>参数说明</p>
<blockquote>
<p>-F 当正常输出的请求不被响应时，强制输出线程堆栈<br>-l long listings，会打印出额外的锁信息，在发生死锁时可以用jstack -l pid来观察锁持有情况<br>-m mixed mode，不仅会输出Java堆栈信息，还会输出C/C++堆栈信息（比如Native方法）</p>
</blockquote>
<p>相关使用案例见:</p>
<ul>
<li><a href="https://www.cnblogs.com/aflyun/p/9194104.html" target="_blank" rel="noopener">Java死锁排查和Java CPU 100% 排查的步骤整理</a></li>
<li><a href="https://juejin.cn/post/6844904152850497543" target="_blank" rel="noopener">Java程序员必备：jstack命令解析</a></li>
</ul>
<h5 id="jinfo"><a href="#jinfo" class="headerlink" title="jinfo"></a>jinfo</h5><p>可看系统启动的参数，如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">jinfo 30670</span><br><span class="line">jinfo -flags  30670</span><br></pre></td></tr></table></figure>

<h5 id="jmap"><a href="#jmap" class="headerlink" title="jmap"></a>jmap</h5><p><a href="https://lihuimintu.github.io/2019/10/24/jmap/" target="_blank" rel="noopener">Java内存分析工具 jmap</a><br><a href="https://www.jianshu.com/p/a4ad53179df3" target="_blank" rel="noopener">jvm 性能调优工具之 jmap</a></p>
<h5 id="jstat"><a href="#jstat" class="headerlink" title="jstat"></a>jstat</h5><p>jstat参数众多，但是使用一个就够了</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">jstat -gcutil 2815 1000</span><br></pre></td></tr></table></figure>

<p>2815 为 PID；1000 为间隔时间，单位为毫秒；后面还可以跟查询次数</p>
<p><img src="/images/blog/2019-09-03-2.png" alt></p>
<p>S0C: 第一个幸存区的大小<br>S1C: 第二个幸存区的大小<br>S0U: 第一个幸存区的使用大小<br>S1U: 第二个幸存区的使用大小<br>EC: 伊甸园区的大小<br>EU: 伊甸园区的使用大小<br>OC: 老年代大小<br>OU: 老年代使用大小<br>MC: 方法区大小<br>MU: 方法区使用大小<br>CCSC: 压缩类空间大小<br>CCSU: 压缩类空间使用大小<br>YGC: 年轻代垃圾回收次数<br>YGCT: 年轻代垃圾回收消耗时间<br>FGC: 老年代垃圾回收次数<br>FGCT: 老年代垃圾回收消耗时间<br>GCT: 垃圾回收消耗总时间   </p>
<h4 id="VM-options"><a href="#VM-options" class="headerlink" title="VM options"></a>VM options</h4><p>有时候可以配置下 java 启动参数选项</p>
<p>你的类到底是从哪个文件加载进来的</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-XX:+TraceClassLoading</span><br><span class="line">结果形如[Loaded java.lang.invoke.MethodHandleImpl$Lazy from D:programmejdkjdk8U74jrelib</span><br><span class="line">t.jar]</span><br></pre></td></tr></table></figure>

<p>应用挂了输出dump文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath&#x3D;&#x2F;home&#x2F;admin&#x2F;logs&#x2F;java.hprof</span><br></pre></td></tr></table></figure>


<hr>
<p>参考链接</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/eDL4-XJZO4Y8Hs4U0bCcnQ" target="_blank" rel="noopener">阿里员工排查问题的工具清单，总有一款适合你！</a></li>
<li><a href="https://mp.weixin.qq.com/s/f_yEBXHjC_v-PlcrFi9ryQ" target="_blank" rel="noopener">运维技巧之逼格高又实用的Linux命令</a></li>
<li><a href="https://www.cnblogs.com/kongzhongqijing/articles/3630264.html" target="_blank" rel="noopener">java命令–jstack 工具</a></li>
</ul>
]]></content>
      <categories>
        <category>JVM</category>
      </categories>
  </entry>
  <entry>
    <title>使用 HBase 存储图片</title>
    <url>/2019/06/01/HBase-save-picture/</url>
    <content><![CDATA[<p>本文主要描述如何将图片文件转成 sequence file，然后保存到 HBase</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>HBase 可用于来数据文件。假设我们碰到的是图片文件呢，该如何保存或存储呢。<br>本文主要描述如何将图片文件转成sequence file，然后保存到HBase。</p>
<h4 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h4><ol>
<li><p>文件处理流程</p>
</li>
<li><p>准备上传文件的Java代码</p>
</li>
<li><p>运行代码</p>
</li>
<li><p>读HBase并本地输出</p>
</li>
</ol>
<h4 id="处理流程"><a href="#处理流程" class="headerlink" title="处理流程"></a>处理流程</h4><p><img src="/images/blog/2019-06-01-4.png" alt> </p>
<p>先在本地准备了一堆图片文件</p>
<p><img src="/images/blog/2019-06-01-1.png" alt> </p>
<p>上传到 HDFS</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">for i in &#96;ls imgs&#96;;</span><br><span class="line">do</span><br><span class="line">        hadoop fs -put imgs&#x2F;$i &#x2F;tmp&#x2F;lihm&#x2F;picHbase</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/2019-06-01-2.png" alt> </p>
<p>然后通过Java程序遍历所有图片生成一个Sequence File，然后把Sequence File入库到HBase<br>在入库过程中，我们读取图片文件的文件名作为Rowkey，另外将整个图片内容转为bytes存储在HBase表的一个column里。</p>
<p>最后可以通过Hue来进行查看图片，当然你也可以考虑对接到你自己的查询系统。</p>
<h4 id="准备上传文件的-Java-代码"><a href="#准备上传文件的-Java-代码" class="headerlink" title="准备上传文件的 Java 代码"></a>准备上传文件的 Java 代码</h4><p>Maven 配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;packaging&gt;jar&lt;&#x2F;packaging&gt;</span><br><span class="line">&lt;name&gt;hbase-exmaple&lt;&#x2F;name&gt;</span><br><span class="line">&lt;url&gt;http:&#x2F;&#x2F;maven.apache.org&lt;&#x2F;url&gt;</span><br><span class="line">&lt;properties&gt;</span><br><span class="line">    &lt;project.build.sourceEncoding&gt;UTF-8&lt;&#x2F;project.build.sourceEncoding&gt;</span><br><span class="line">&lt;&#x2F;properties&gt;</span><br><span class="line">&lt;repositories&gt;</span><br><span class="line">    &lt;repository&gt;</span><br><span class="line">        &lt;id&gt;cloudera&lt;&#x2F;id&gt;</span><br><span class="line">        &lt;url&gt;https:&#x2F;&#x2F;repository.cloudera.com&#x2F;artifactory&#x2F;cloudera-repos&#x2F;&lt;&#x2F;url&gt;</span><br><span class="line">        &lt;name&gt;Cloudera Repositories&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;snapshots&gt;</span><br><span class="line">            &lt;enabled&gt;false&lt;&#x2F;enabled&gt;</span><br><span class="line">        &lt;&#x2F;snapshots&gt;</span><br><span class="line">    &lt;&#x2F;repository&gt;</span><br><span class="line">&lt;&#x2F;repositories&gt;</span><br><span class="line">&lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;hadoop-client&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;version&gt;2.6.0-cdh5.7.1&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;!-- https:&#x2F;&#x2F;mvnrepository.com&#x2F;artifact&#x2F;org.apache.hbase&#x2F;hbase-client --&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.hbase&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;hbase-client&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;version&gt;1.2.0-cdh5.7.1&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;junit&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;junit&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;version&gt;3.8.1&lt;&#x2F;version&gt;</span><br><span class="line">        &lt;scope&gt;test&lt;&#x2F;scope&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;&#x2F;dependencies&gt;</span><br></pre></td></tr></table></figure>

<p>上传文件到HBase的Java代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">package cn.lihm.hbaseproj;</span><br><span class="line"></span><br><span class="line">import java.io.File;</span><br><span class="line">import java.io.FileOutputStream;</span><br><span class="line">import java.io.IOException;</span><br><span class="line">import java.net.URI;</span><br><span class="line">import java.net.URISyntaxException;</span><br><span class="line"></span><br><span class="line">import org.apache.commons.io.IOUtils;</span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.FSDataInputStream;</span><br><span class="line">import org.apache.hadoop.fs.FileStatus;</span><br><span class="line">import org.apache.hadoop.fs.FileSystem;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.hbase.HBaseConfiguration;</span><br><span class="line">import org.apache.hadoop.hbase.TableName;</span><br><span class="line">import org.apache.hadoop.hbase.client.*;</span><br><span class="line">import org.apache.hadoop.hbase.util.Bytes;</span><br><span class="line">import org.apache.hadoop.io.BytesWritable;</span><br><span class="line">import org.apache.hadoop.io.SequenceFile;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * @author lihm</span><br><span class="line"> * @date 2019-06-01 15:26</span><br><span class="line"> * @description TODO</span><br><span class="line"> *&#x2F;</span><br><span class="line"></span><br><span class="line">public class SequenceFileTest &#123;</span><br><span class="line"></span><br><span class="line">    private static String inpath &#x3D; &quot;&#x2F;tmp&#x2F;lihm&#x2F;picHbase&quot;;</span><br><span class="line">    private static String outpath &#x3D; &quot;&#x2F;tmp&#x2F;lihm&#x2F;out&quot;;</span><br><span class="line">    private static SequenceFile.Writer writer &#x3D; null;</span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line">     * 递归文件;并将文件写成SequenceFile文件</span><br><span class="line">     *</span><br><span class="line">     * @param fileSystem</span><br><span class="line">     * @param path</span><br><span class="line">     * @throws Exception</span><br><span class="line">     *&#x2F;</span><br><span class="line">    public static void listFileAndWriteToSequenceFile(FileSystem fileSystem, String path) throws Exception&#123;</span><br><span class="line">        final FileStatus[] listStatuses &#x3D; fileSystem.listStatus(new Path(path));</span><br><span class="line">        for (FileStatus fileStatus : listStatuses) &#123;</span><br><span class="line">            if(fileStatus.isFile())&#123;</span><br><span class="line">                Text fileText &#x3D; new Text(fileStatus.getPath().toString());</span><br><span class="line">                System.out.println(fileText.toString());</span><br><span class="line">                &#x2F;&#x2F;返回一个SequenceFile.Writer实例 需要数据流和path对象 将数据写入了path对象</span><br><span class="line">                FSDataInputStream in &#x3D; fileSystem.open(new Path(fileText.toString()));</span><br><span class="line">                byte[] buffer &#x3D; IOUtils.toByteArray(in);</span><br><span class="line">                in.read(buffer);</span><br><span class="line">                BytesWritable value &#x3D; new BytesWritable(buffer);</span><br><span class="line">                &#x2F;&#x2F;写成SequenceFile文件</span><br><span class="line">                writer.append(fileText, value);</span><br><span class="line">            &#125;</span><br><span class="line">            if(fileStatus.isDirectory())&#123;</span><br><span class="line">                listFileAndWriteToSequenceFile(fileSystem,fileStatus.getPath().toString());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line">     * 将二进制流转化后的图片输出到本地</span><br><span class="line">     *</span><br><span class="line">     * @param bs 二进制图片</span><br><span class="line">     * @param picPath 本地路径</span><br><span class="line">     *&#x2F;</span><br><span class="line">    public static void picFileOutput(byte[] bs, String picPath) &#123;</span><br><span class="line">        &#x2F;&#x2F; 将输出的二进制流转化后的图片的路径</span><br><span class="line">        File file &#x3D; new File(picPath);</span><br><span class="line">        try &#123;</span><br><span class="line">            FileOutputStream fos &#x3D; new FileOutputStream(file);</span><br><span class="line">            fos.write(bs);</span><br><span class="line">            fos.close();</span><br><span class="line">        &#125; catch (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line">     * 读 hbase 图片</span><br><span class="line">     *&#x2F;</span><br><span class="line">    public static void readPicHBase() &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            Configuration hbaseConf &#x3D; HBaseConfiguration.create();</span><br><span class="line">            &#x2F;&#x2F; 公司 HBase 集群</span><br><span class="line">            hbaseConf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;100.73.12.11&quot;);</span><br><span class="line">            hbaseConf.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;);</span><br><span class="line">            hbaseConf.set(&quot;zookeeper.znode.parent&quot;, &quot;&#x2F;hbase&quot;);</span><br><span class="line">            Connection connection &#x3D; ConnectionFactory.createConnection(hbaseConf);</span><br><span class="line">            Table table &#x3D; connection.getTable(TableName.valueOf(&quot;tu&quot;));</span><br><span class="line">            Get get &#x3D; new Get(&quot;8cd2f12410037fe73cfeb7b6a65be935.jpg&quot;.getBytes());</span><br><span class="line">            Result rs &#x3D; table.get(get);</span><br><span class="line">            &#x2F;&#x2F; 保存get result的结果，字节数组形式</span><br><span class="line">            byte[] bs &#x3D; rs.value();</span><br><span class="line">            picFileOutput(bs, &quot;&#x2F;Users&#x2F;tu&#x2F;Public&#x2F;ZaWu&#x2F;picHBase&#x2F;8cd2f12410037fe73cfeb7b6a65be935.jpg&quot;);</span><br><span class="line">            table.close();</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125; catch (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line">     * 将图片文件转成sequence file，然后保存到HBase</span><br><span class="line">     *&#x2F;</span><br><span class="line">    public static void picToHBase() &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            Configuration hbaseConf &#x3D; HBaseConfiguration.create();</span><br><span class="line">            &#x2F;&#x2F; 公司 HBase 集群</span><br><span class="line">            hbaseConf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;100.73.12.11&quot;);</span><br><span class="line">            hbaseConf.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;);</span><br><span class="line">            hbaseConf.set(&quot;zookeeper.znode.parent&quot;, &quot;&#x2F;hbase&quot;);</span><br><span class="line">            Connection connection &#x3D; ConnectionFactory.createConnection(hbaseConf);</span><br><span class="line">            Table table &#x3D; connection.getTable(TableName.valueOf(&quot;tu&quot;));</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; 设置读取本地磁盘文件</span><br><span class="line">            Configuration conf &#x3D; new Configuration();</span><br><span class="line">            conf.addResource(new Path(&quot;&#x2F;Users&#x2F;tu&#x2F;Public&#x2F;ZaWu&#x2F;conf.cloudera.yarn&#x2F;core-site.xml&quot;));</span><br><span class="line">            conf.addResource(new Path(&quot;&#x2F;Users&#x2F;tu&#x2F;Public&#x2F;ZaWu&#x2F;conf.cloudera.yarn&#x2F;yarn-site.xml&quot;));</span><br><span class="line">            conf.addResource(new Path(&quot;&#x2F;Users&#x2F;tu&#x2F;Public&#x2F;ZaWu&#x2F;conf.cloudera.yarn&#x2F;hdfs-site.xml&quot;));</span><br><span class="line">            URI uri &#x3D; new URI(inpath);</span><br><span class="line">            FileSystem fileSystem &#x3D; FileSystem.get(uri, conf,&quot;hdfs&quot;);</span><br><span class="line">            &#x2F;&#x2F; 实例化writer对象</span><br><span class="line">            writer &#x3D; SequenceFile.createWriter(fileSystem, conf, new Path(outpath), Text.class, BytesWritable.class);</span><br><span class="line">            &#x2F;&#x2F; 递归遍历文件夹，并将文件下的文件写入 sequenceFile 文件</span><br><span class="line">            listFileAndWriteToSequenceFile(fileSystem, inpath);</span><br><span class="line">            &#x2F;&#x2F; 关闭流</span><br><span class="line">            org.apache.hadoop.io.IOUtils.closeStream(writer);</span><br><span class="line">            &#x2F;&#x2F; 读取所有文件</span><br><span class="line">            URI seqURI &#x3D; new URI(outpath);</span><br><span class="line">            FileSystem fileSystemSeq &#x3D; FileSystem.get(seqURI, conf);</span><br><span class="line">            SequenceFile.Reader reader &#x3D; new SequenceFile.Reader(fileSystemSeq, new Path(outpath), conf);</span><br><span class="line">            Text key &#x3D; new Text();</span><br><span class="line">            BytesWritable val &#x3D; new BytesWritable();</span><br><span class="line">            &#x2F;&#x2F; key &#x3D; (Text) ReflectionUtils.newInstance(reader.getKeyClass(), conf);</span><br><span class="line">            &#x2F;&#x2F; val &#x3D; (BytesWritable) ReflectionUtils.newInstance(reader.getValueClass(), conf);</span><br><span class="line">            int i &#x3D; 0;</span><br><span class="line">            while(reader.next(key, val))&#123;</span><br><span class="line">                &#x2F;&#x2F; 读取图片文件的文件名作为 Rowkey</span><br><span class="line">                String temp &#x3D; key.toString();</span><br><span class="line">                temp &#x3D; temp.substring(temp.lastIndexOf(&quot;&#x2F;&quot;) + 1);</span><br><span class="line">                &#x2F;&#x2F; rowKey 设计</span><br><span class="line">                String rowKey &#x3D; temp;</span><br><span class="line">                &#x2F;&#x2F; String rowKey &#x3D; Integer.valueOf(tmp[0])-1+&quot;_&quot;+Integer.valueOf(tmp[1])&#x2F;2+&quot;_&quot;+Integer.valueOf(tmp[2])&#x2F;2;</span><br><span class="line">                System.out.println(rowKey);</span><br><span class="line">                &#x2F;&#x2F; 构造 Put</span><br><span class="line">                Put put &#x3D; new Put(Bytes.toBytes(rowKey));</span><br><span class="line">                &#x2F;&#x2F; 指定列簇名称、列修饰符、列值 temp.getBytes()</span><br><span class="line">                put.addColumn(&quot;picinfo&quot;.getBytes(), &quot;content&quot;.getBytes() , val.getBytes());</span><br><span class="line">                table.put(put);</span><br><span class="line">            &#125;</span><br><span class="line">            table.close();</span><br><span class="line">            connection.close();</span><br><span class="line">            org.apache.hadoop.io.IOUtils.closeStream(reader);</span><br><span class="line"></span><br><span class="line">        &#125; catch (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125; catch (URISyntaxException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        picToHBase();</span><br><span class="line">        readPicHBase();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="运行代码"><a href="#运行代码" class="headerlink" title="运行代码"></a>运行代码</h4><p>首先在 HBase 中建一张表用来保存文本文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create &#39;tu&#39;,  &#39;picinfo&#39;</span><br></pre></td></tr></table></figure>

<p>注意修改代码中的配置项，如文本文件所在的 HDFS 目录，集群的 Zookeeper 地址，conf 配置路径等。</p>
<p>我是在本地电脑上运行的。也可以将代码打成jar包并上传到集群服务器节点。该过程略。</p>
<h4 id="HBase-查询验证"><a href="#HBase-查询验证" class="headerlink" title="HBase 查询验证"></a>HBase 查询验证</h4><p>我是将其输出到本地路径下</p>
<p><img src="/images/blog/2019-06-01-3.png" alt> </p>
<p>也可以用 Hue 中查询验证。</p>
<p><img src="/images/blog/2019-06-19-4.png" alt> </p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/KA1EupPqm8uzRk7_-8D3KA" target="_blank" rel="noopener">HBase实操 如何使用HBase存储图片</a></li>
<li><a href="https://blog.csdn.net/love666666shen/article/details/82931783" target="_blank" rel="noopener">HBase存储、插入、修改、读取图片操作</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>一致性哈希算法</title>
    <url>/2019/05/24/Consistent-Hashing/</url>
    <content><![CDATA[<p>一致性哈希最初在 P2P 网络中作为分布式哈希表（DHT）的常用数据分布算法，目前这个算法在分布式系统中成为使用较为广泛的数据分布方式。</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>一致性哈希算法（Consistent Hashing）最早在1997年由 David Karger 等人在论文《Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web》中被提出，<br>其设计目标是为了解决因特网中的热点(Hot spot)问题；<br>一致性哈希最初在 P2P 网络中作为分布式哈希表（ DHT）的常用数据分布算法，<br>目前这个算法在分布式系统中成为使用较为广泛的数据分布方式。</p>
<h4 id="Hash-取模"><a href="#Hash-取模" class="headerlink" title="Hash 取模"></a>Hash 取模</h4><p>为什么要有 Hash 一致性算法？</p>
<p>当我们在做数据库分库分表或者是分布式缓存时，不可避免的都会遇到一个问题:<br>如何将数据均匀的分散到各个节点中，并且尽量的在加减节点时能使受影响的数据最少。</p>
<p>具体可以详细查阅 <a href="https://zhuanlan.zhihu.com/p/34985026" target="_blank" rel="noopener">面试必备：什么是一致性Hash算法？</a></p>
<p>随机放置就不说了，会带来很多问题。通常最容易想到的方案就是 hash取模了。</p>
<p>可以将传入的 Key 按照 index=hash(key)%N 这样来计算出需要存放的节点。其中 hash 函数是一个将字符串转换为正整数的哈希映射方法，N 就是节点的数量。</p>
<p>这样可以满足数据的均匀分配，但是这个算法的容错性和扩展性都较差。</p>
<p>比如增加或删除了一个节点时，所有的 Key 都需要重新计算，显然这样成本较高，为此需要一个算法满足分布均匀同时也要有良好的容错性和拓展性。</p>
<h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><p>一致性 Hash 算法也是使用取模的方法，只是，刚才描述的取模法是对服务器的数量进行取模，而一致性 Hash 算法是对2^32取模</p>
<p>一致性 Hash 算法将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数H的值空间为0-2^32-1（即哈希值是一个32位无符号整形）</p>
<p><img src="/images/blog/2019-05-24-1.png" alt> </p>
<p>整个空间按顺时针方向组织，圆环的正上方的点代表0，0点右侧的第一个点代表1，<br>以此类推，2、3、4、5、6……直到2^32-1，也就是说0点左侧的第一个点代表2^32-1，<br> 0和2^32-1在零点中方向重合，我们把这个由2^32个点组成的圆环称为Hash环。</p>
<p>由于是环形空间，所以0和232-1实际上是重叠的，上图只是为了表示方便没有画到一起。</p>
<h5 id="1-将机器映射到环中"><a href="#1-将机器映射到环中" class="headerlink" title="1. 将机器映射到环中"></a>1. 将机器映射到环中</h5><p>将各个服务器使用 Hash 进行一个哈希，具体可以选择服务器的IP或主机名作为关键字进行哈希，这样每台机器就能确定其在哈希环上的位置，这里假设将四台服务器使用IP地址哈希后在环空间的位置如下</p>
<p><img src="/images/blog/2019-05-24-2.png" alt> </p>
<h5 id="2-将数据映射到环中"><a href="#2-将数据映射到环中" class="headerlink" title="2. 将数据映射到环中"></a>2. 将数据映射到环中</h5><p>将数据key使用相同的函数Hash计算出哈希值，并确定此数据在环上的位置，从此位置沿环顺时针“行走”，第一台遇到的服务器就是其应该定位到的服务器！</p>
<p>例如我们有Object A、Object B、Object C、Object D四个数据对象，经过哈希计算后，在环空间上的位置如下</p>
<p><img src="/images/blog/2019-05-24-3.png" alt> </p>
<p>根据一致性Hash算法，数据A会被定为到Node A上，B被定为到Node B上，C被定为到Node C上，D被定为到Node D上</p>
<h4 id="容错性和可扩展性"><a href="#容错性和可扩展性" class="headerlink" title="容错性和可扩展性"></a>容错性和可扩展性</h4><p>现假设Node C不幸宕机，可以看到此时对象A、B、D不会受到影响，只有C对象被重定位到Node D。<br>一般的，在一致性Hash算法中，如果一台服务器不可用，<br>则受影响的数据仅仅是此服务器到其环空间中前一台服务器<br>（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响，如下所示: </p>
<p><img src="/images/blog/2019-05-24-4.png" alt> </p>
<p>下面考虑另外一种情况，如果在系统中增加一台服务器Node X，如下图所示: </p>
<p><img src="/images/blog/2019-05-24-5.png" alt> </p>
<p>此时对象Object A、B、D不受影响，只有对象C需要重定位到新的Node X ！<br>一般的，在一致性Hash算法中，如果增加一台服务器，则受影响的数据仅仅是新服务器到其环空间中前一台服务器<br>（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它数据也不会受到影响。</p>
<p>一致性Hash算法对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性。</p>
<h4 id="数据倾斜问题"><a href="#数据倾斜问题" class="headerlink" title="数据倾斜问题"></a>数据倾斜问题</h4><p>一致性Hash算法在服务节点太少时，容易因为节点分部不均匀而造成数据倾斜<br>（被缓存的对象大部分集中缓存在某一台服务器上）问题，例如系统中只有两台服务器，其环分布如下:</p>
<p><img src="/images/blog/2019-05-24-6.png" alt> </p>
<p>此时必然造成大量数据集中到Node A上，而只有极少量会定位到Node B上。为了解决这种数据倾斜问题，一致性Hash算法引入了虚拟节点机制，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。具体做法可以在服务器IP或主机名的后面增加编号来实现。</p>
<p>例如上面的情况，可以为每台服务器计算三个虚拟节点，于是可以分别计算 “Node A#1”、“Node A#2”、“Node A#3”、“Node B#1”、“Node B#2”、“Node B#3”的哈希值，于是形成六个虚拟节点</p>
<p><img src="/images/blog/2019-05-24-7.png" alt> </p>
<p>同时数据定位算法不变，只是<strong>多了一步虚拟节点到实际节点的映射</strong>，<br>例如定位到“Node A#1”、“Node A#2”、“Node A#3”三个虚拟节点的数据均定位到Node A上。<br>这样就解决了服务节点少时数据倾斜的问题。<br>在实际应用中，通常将虚拟节点数设置为32甚至更大，因此即使很少的服务节点也能做到相对均匀的数据分布。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>分析了什么是一致性Hash算法，主要是考虑到分布式系统每个节点都有可能失效，<br>并且新的节点很可能动态的增加进来的情况，如何保证当系统的节点数目发生变化的时候，<br>我们的系统仍然能够对外提供良好的服务，这是值得考虑的！</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/34985026" target="_blank" rel="noopener">面试必备：什么是一致性Hash算法？</a></li>
<li><a href="https://mp.weixin.qq.com/s/H-jqrM3ehsImXgskH5Y5-g" target="_blank" rel="noopener">什么是一致性Hash算法？</a></li>
<li><a href="https://mp.weixin.qq.com/s/-d2fwE0wqkLDOBw4jKIP4g" target="_blank" rel="noopener">分布式原理：一致性哈希算法简介</a></li>
</ul>
]]></content>
      <categories>
        <category>Distributed</category>
      </categories>
  </entry>
  <entry>
    <title>二分查找</title>
    <url>/2019/05/22/binary-search/</url>
    <content><![CDATA[<p>一种在有序数组中查找某一特定元素的搜索算法</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>第一篇二分搜索论文是 1946 年发表，然而第一个没有 bug 的二分查找法却是在 1962 年才出现，中间用了 16 年的时间。</p>
<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>在计算机科学中，二分查找（英语：binary search），也称折半搜索（英语：half-interval search）、对数搜索（英语：logarithmic search），是一种在有序数组中查找某一特定元素的搜索算法。</p>
<p>搜索过程从数组的中间元素开始，如果中间元素正好是要查找的元素，则搜索过程结束；</p>
<p>如果某一特定元素大于或者小于中间元素，则在数组大于或小于中间元素的那一半中查找，而且跟开始一样从中间元素开始比较。</p>
<p>如果在某一步骤数组为空，则代表找不到。</p>
<p>这种搜索算法每一次比较都使搜索范围缩小一半。</p>
<h4 id="二分查找法代码"><a href="#二分查找法代码" class="headerlink" title="二分查找法代码"></a>二分查找法代码</h4><p>二分查找有很多种变体，使用时需要注意查找条件，判断条件和左右边界的更新方式，<br>三者配合不好就很容易出现死循环或者遗漏区域，本篇中我们将介绍常见的几种查找方式的模板代码，包括：</p>
<ol>
<li>标准的二分查找</li>
<li>二分查找左边界</li>
<li>二分查找右边界</li>
<li>二分查找左右边界</li>
<li>二分查找极值点</li>
</ol>
<h5 id="1-标准二分查找"><a href="#1-标准二分查找" class="headerlink" title="1. 标准二分查找"></a>1. 标准二分查找</h5><p>首先给出标准二分查找的模板</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">int binSearch(int nums[], int sz, int target) &#123;</span><br><span class="line">    int left &#x3D; 0;</span><br><span class="line">    int right &#x3D; sz - 1;</span><br><span class="line">    while (left &lt;&#x3D; right) &#123;</span><br><span class="line">        int mid &#x3D; left + ((right - left) &gt;&gt; 1);</span><br><span class="line">        if (nums[mid] &#x3D;&#x3D; target) return mid;</span><br><span class="line">        else if (nums[mid] &gt; target) &#123;</span><br><span class="line">            right &#x3D; mid - 1;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            left &#x3D; mid + 1;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return -1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>循环条件: left &lt;= right</p>
<p>中间位置计算: mid = left + ((right -left) &gt;&gt; 1)</p>
<p>左边界更新: left = mid + 1</p>
<p>右边界更新: right = mid - 1</p>
<p>返回值: mid / -1</p>
<p>这里有几点需要注意:</p>
<ol>
<li><p>我们的循环条件中包含了 left == right的情况，则我们必须在每次循环中改变 left 和 right的指向，以防止进入死循环</p>
</li>
<li><p>循环终止的条件包括：</p>
<ul>
<li>找到了目标值</li>
<li>left &gt; right （这种情况发生于当left, mid, right指向同一个数时，这个数还不是目标值，则整个查找结束。）</li>
</ul>
</li>
<li><p>left + ((right -left) &gt;&gt; 1) 其实和 (left + right) / 2是等价的，这样写的目的一个是为了防止 (left + right)出现溢出，一个是用右移操作替代除法提升性能。</p>
</li>
<li><p>left + ((right -left) &gt;&gt; 1) 对于目标区域长度为奇数而言，是处于正中间的，对于长度为偶数而言，是中间偏左的。因此左右边界相遇时，只会是以下两种情况: </p>
<ul>
<li>left/mid , right (left, mid 指向同一个数，right指向它的下一个数)</li>
<li>left/mid/right （left, mid, right 指向同一个数</li>
</ul>
</li>
</ol>
<p>即因为 mid 对于长度为偶数的区间总是偏左的，所以当区间长度小于等于 2 时，mid 总是和 left 在同一侧。</p>
<p>个人比较喜欢刘汝佳的二分算法。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">int bsearch(int *A, int x, int y, int v) &#123;</span><br><span class="line">    int m;</span><br><span class="line">    while (x &lt; y) &#123;</span><br><span class="line">        m &#x3D; x + ((y - x) &gt;&gt; 1);</span><br><span class="line">        if (A[m] &#x3D;&#x3D; v) return m;</span><br><span class="line">        else if (A[m] &gt; v) y &#x3D; m;</span><br><span class="line">        else x &#x3D; m + 1;</span><br><span class="line">    &#125;</span><br><span class="line">    return -1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>刘汝佳这里的 x, y 表示左闭右开区间的左右端点。即 y 不在取值范围内。这里小心 y 爆int。</p>
<h4 id="二分查找左边界"><a href="#二分查找左边界" class="headerlink" title="二分查找左边界"></a>二分查找左边界</h4><p>既然要寻找左边界，搜索范围就需要从右边开始，不断往左边收缩，<br>也就是说即使我们找到了nums[mid] == target, 这个mid的位置也不一定就是最左侧的那个边界，<br>我们还是要向左侧查找，所以我们在nums[mid]偏大或者nums[mid]就等于目标值的时候，继续收缩右边界，<br>算法模板如下: </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">int search(int nums[], int target) &#123;</span><br><span class="line">    int left &#x3D; 0;</span><br><span class="line">    int right &#x3D; nums.length - 1;</span><br><span class="line">    while (left &lt; right) &#123;</span><br><span class="line">        int mid &#x3D; left + (right - left) &#x2F; 2;</span><br><span class="line">        if (nums[mid] &lt; target) &#123;</span><br><span class="line">            left &#x3D; mid + 1;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            right &#x3D; mid;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return nums[left] &#x3D;&#x3D; target ? left : -1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>循环条件： left &lt; right</p>
<p>中间位置计算： mid = left + ((right -left) &gt;&gt; 1)</p>
<p>左边界更新：left = mid + 1</p>
<p>右边界更新： right = mid</p>
<p>返回值： nums[left] == target ? left : -1</p>
<p>与标准的二分查找不同：</p>
<p>首先，这里的右边界的更新是right = mid，因为我们需要在找到目标值后，继续向左寻找左边界。</p>
<p>其次，这里的循环条件是left &lt; right。<br>因为在最后left与right相邻的时候，mid和left处于相同的位置(前面说过，mid偏左)，则下一步，无论怎样，left, mid, right都将指向同一个位置，如果此时循环的条件是left &lt;= right，则我们需要再进入一遍循环，此时，如果nums[mid] &lt; target还好说，循环正常终止；否则，我们会令right = mid，这样并没有改变left,mid,right的位置，将进入死循环。</p>
<p>事实上，我们只需要遍历到left和right相邻的情况就行了，因为这一轮循环后，无论怎样，left,mid,right都会指向同一个位置，而如果这个位置的值等于目标值，则它就一定是最左侧的目标值；如果不等于目标值，则说明没有找到目标值，这也就是为什么返回值是nums[left] == target ? left : -1。</p>
<p>个人比较喜欢刘汝佳的代码</p>
<p>当 v 存在时返回它出现的第一个位置。如果不存在，返回这样一个下标 i :<br>在此处插入 v (原来的元素 A[i], A[i + 1], …. 全部往后移动一个位置) 后序列仍然有序。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">int lower_bound(int *A, int x, int y, int v) &#123;</span><br><span class="line">    int m;</span><br><span class="line">    while (x &lt; y) &#123;</span><br><span class="line">        m &#x3D; x + ((y - x) &gt;&gt; 1);</span><br><span class="line">        if (A[m] &gt;&#x3D; v) y &#x3D; m;</span><br><span class="line">        else x &#x3D; m + 1;</span><br><span class="line">    &#125;</span><br><span class="line">    return x;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="二分查找右边界"><a href="#二分查找右边界" class="headerlink" title="二分查找右边界"></a>二分查找右边界</h4><p>有了寻找左边界的分析之后，再来看寻找右边界就容易很多了，毕竟左右两种情况是对称的嘛，关于使用场景这里就不再赘述了，大家对称着理解就好。我们直接给出模板代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">int search(int nums[], int target) &#123;</span><br><span class="line">    int left &#x3D; 0;</span><br><span class="line">    int right &#x3D; nums.length - 1;</span><br><span class="line">    while (left &lt; right) &#123;</span><br><span class="line">        int mid &#x3D; left + ((right - left) &gt;&gt; 1) + 1;</span><br><span class="line">        if (nums[mid] &gt; target) &#123;</span><br><span class="line">            right &#x3D; mid - 1;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            left &#x3D; mid;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return nums[right] &#x3D;&#x3D; target ? right : -1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>循环条件： left &lt; right</p>
<p>中间位置计算： mid = left + ((right -left) &gt;&gt; 1) + 1</p>
<p>左边界更新：left = mid</p>
<p>右边界更新： right = mid - 1</p>
<p>返回值： nums[right] == target ? right : -1</p>
<p>这里大部分和寻找左边界是对称着来写的，唯独有一点需要尤其注意——中间位置的计算变了，我们在末尾多加了1。这样，无论对于奇数还是偶数，这个中间的位置都是偏右的。</p>
<p>对于这个操作的理解，从对称的角度看，寻找左边界的时候，中间位置是偏左的，那寻找右边界的时候，中间位置就应该偏右呗，但是这显然不是根本原因。根本原因是，在最后left和right相邻时，如果mid偏左，则left, mid指向同一个位置，right指向它们的下一个位置，在nums[left]已经等于目标值的情况下，这三个位置的值都不会更新，从而进入了死循环。所以我们应该让mid偏右，这样left就能向右移动。这也就是为什么我们之前一直强调查找条件，判断条件和左右边界的更新方式三者之间需要配合使用。</p>
<p>个人比较喜欢刘汝佳的代码</p>
<p>当 v 存在时返回它出现的最后一个位置的后面一个位置。<br>如果不存在，返回这样一个下标 i : 此处插入 v (原来的元素 A[i], A[i + 1], …. 全部往后移动一个位置) 后序列仍然有序。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">int upper_bound(int *A, int x, int y, int v) &#123;</span><br><span class="line">    int m;</span><br><span class="line">    while (x &lt; y) &#123;</span><br><span class="line">        m &#x3D; x + ((y - x) &gt;&gt; 1);</span><br><span class="line">        if (A[m] &gt; v) y &#x3D; m;</span><br><span class="line">        else x &#x3D; m + 1;</span><br><span class="line">    &#125;</span><br><span class="line">    return x;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>这样，对二分查找的讨论就相对比较完整了: 设lower_bound 和 upper_bound 的返回值分别为 L 和 R，<br>则 v 出现的子序列为 [L, R), 这个结论当 v 不存在时也成立: 此时 L = R，区间为空。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://segmentfault.com/a/1190000016825704#articleHeader10" target="_blank" rel="noopener">二分查找、二分边界查找算法的模板代码总结</a></li>
</ul>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title>从分布式一致性谈到 CAP 理论、BASE 理论</title>
    <url>/2019/05/17/Cap-Base/</url>
    <content><![CDATA[<p>从分布式一致性谈到 CAP 理论、BASE 理论</p>
<hr>
<h4 id="问题的提出"><a href="#问题的提出" class="headerlink" title="问题的提出"></a>问题的提出</h4><p>在计算机科学领域，分布式一致性是一个相当重要且被广泛探索与论证问题，首先来看三种业务场景。</p>
<h5 id="1-火车站售票"><a href="#1-火车站售票" class="headerlink" title="1. 火车站售票"></a>1. 火车站售票</h5><p>假如说我们的终端用户是一位经常坐火车的旅行家，通常他是去车站的售票处购买车 票，然后拿着车票去检票口，<br>再坐上火车，开始一段美好的旅行—-一切似乎都是那么和谐。想象一下，如果他选择的目的地是杭州，<br>而某一趟开往杭州的火车 只剩下最后一张车票，可能在同一时刻，不同售票窗口的另一位乘客也购买了同一张车票。<br>假如说售票系统没有进行一致性的保障，两人都购票成功了。而在检票口 检票的时候，<br>其中一位乘客会被告知他的车票无效—-当然，现代的中国铁路售票系统已经很少出现这样的问题了。<br>但在这个例子中我们可以看出，终端用户对 于系统的需求非常简单：</p>
<p>“请售票给我，如果没有余票了，请在售票的时候就告诉我票是无效的”</p>
<p>这就对购票系统提出了严格的一致性要求—-系统的数据（本例中指的就是那趟开往杭州的火车的余票数）<br>无论在哪个售票窗口，每时每刻都必须是准确无误的！</p>
<h5 id="2-银行转账"><a href="#2-银行转账" class="headerlink" title="2. 银行转账"></a>2. 银行转账</h5><p>假如我们的终端用户是一位刚毕业的大学生，通常在拿到第一个月工资的时候，都会选 择向家里汇款。<br>当他来到银行柜台，完成转账操作后，银行的柜台服务员会友善地提醒他：”您的转账将在N个工作日后到账！”。<br>此时这名毕业生有一定的沮丧， 会对那名柜台服务员叮嘱：”好吧，多久没关系，钱不要少就好了！”<br>—-这也成为了几乎所有用户对于现代银行系统最基本的需求</p>
<h5 id="3-网上购物"><a href="#3-网上购物" class="headerlink" title="3. 网上购物"></a>3. 网上购物</h5><p>假如说我们的终端用户是一位网购达人，当他看见一件库存量为5的心仪商品，会迅速地确认购买，写下收货地址，<br>然后下单—-然而，在下单的那个瞬间，系统可能会告知该用户：”库存量不足！”。<br>此时绝大部分消费者都会抱怨自己动作太慢，使得心爱的商品被其他人抢走了。</p>
<p>但其实有过网购系统开发经验的工程师一定明白，在商品详情页上显示的那个库存量，<br>通常不是该商品的真实库存量，只有在真正下单购买的时候，系统才会检查该商品的真实库存量。<br>但是，谁在意呢？</p>
<h4 id="问题的解读"><a href="#问题的解读" class="headerlink" title="问题的解读"></a>问题的解读</h4><p>对于上面三个例子，相信大家一定看出来了，我们的终端用户在使用不同的计算机产品时对于数据一致性的需求是不一样的：</p>
<p>1 有些系统，既要快速地响应用户，同时还要保证系统的数据对于任意客户端都是真实可靠的，就像火车站售票系统</p>
<p>2 有些系统，需要为用户保证绝对可靠的数据安全，虽然在数据一致性上存在延时，但最终务必保证严格的一致性，就像银行的转账系统</p>
<p>3 有些系统，虽然向用户展示了一些可以说是”错误”的数据，但是在整个系统使用过程中，一定会在某一个流程上对系统数据进行准确无误的检查，从而避免用户发生不必要的损失，就像网购系统</p>
<h4 id="分布一致性的提出"><a href="#分布一致性的提出" class="headerlink" title="分布一致性的提出"></a>分布一致性的提出</h4><p>在分布式系统中要解决的一个重要问题就是数据的复制。在我们的日常开发经验中，相 信很多开发人员都遇到过这样的问题：假设客户端C1将系统中的一个值K由V1更新为V2，但客户端C2无法立即读取到K的最新值，需要在一段时间之后才能 读取到。这很正常，因为数据库复制之间存在延时。</p>
<p>分布式系统对于数据的复制需求一般都来自于以下两个原因: </p>
<ol>
<li><p>为了增加系统的可用性，以防止单点故障引起的系统不可用</p>
</li>
<li><p>提高系统的整体性能，通过负载均衡技术，能够让分布在不同地方的数据副本都能够为用户提供服务</p>
</li>
</ol>
<p>数据复制在可用性和性能方面给分布式系统带来的巨大好处是不言而喻的，然而数据复制所带来的一致性挑战，也是每一个系统研发人员不得不面对的。</p>
<p>所谓分布一致性问题，是指在分布式环境中引入数据复制机制之后，不同数据节点之间 可能出现的，并无法依靠计算机应用程序自身解决的数据不一致的情况。简单讲，数据一致性就是指在对一个副本数据进行更新的时候，必须确保也能够更新其他的 副本，否则不同副本之间的数据将不一致。</p>
<p>那么如何解决这个问题？一种思路是<br><strong>“既然是由于延时动作引起的问题，那我可以将写入的动作阻塞，<br>直到数据复制完成后，才完成写入动作”</strong>。<br>没错，这似乎能解决问题，<br>而且有一些系统的架构也确实直接使用了这个思路。但这个思路在解决一致性问题的同时，<br>又带来了新的问题：写入的性能。如果你的应 用场景有非常多的写请求，那么使用这个思路之后，<br>后续的写请求都将会阻塞在前一个请求的写操作上，导致系统整体性能急剧下降。</p>
<p>总得来说，我们无法找到一种能够满足分布式系统所有系统属性的分布式一致性解决方案。因此，如何既保证数据的一致性，同时又不影响系统运行的性能，是每一个分布式系统都需要重点考虑和权衡的。</p>
<p>于是，一致性级别由此诞生: </p>
<p>1 强一致性</p>
<p>这种一致性级别是最符合用户直觉的，它要求系统写入什么，读出来的也会是什么，用户体验好，但实现起来往往对系统的性能影响大</p>
<p>2 弱一致性</p>
<p>这种一致性级别约束了系统在写入成功后，不承诺立即可以读到写入的值，也不久承诺多久之后数据能够达到一致，但会尽可能地保证到某个时间级别（比如秒级别）后，数据能够达到一致状态</p>
<p>3 最终一致性</p>
<p>最终一致性是弱一致性的一个特例，系统会保证在一定时间内，能够达到一个数据一致的状态。这里之所以将最终一致性单独提出来，是因为它是弱一致性中非常推崇的一种一致性模型，也是业界在大型分布式系统的数据一致性上比较推崇的模型</p>
<h4 id="分布式环境的各种问题"><a href="#分布式环境的各种问题" class="headerlink" title="分布式环境的各种问题"></a>分布式环境的各种问题</h4><p>分布式系统体系结构从其出现之初就伴随着诸多的难题和挑战</p>
<h5 id="1-通信异常"><a href="#1-通信异常" class="headerlink" title="1. 通信异常"></a>1. 通信异常</h5><p>从集中式向分布式演变的过程中，必然引入网络因素，由于网络本身的不可靠性，因此 也引入了额外的问题。<br>分布式系统需要在各个节点之间进行网络通信，因此每次网络通信都会伴随着网络不可用的风险，网络光纤、路由器或是DNS等硬件设备或 是系统不可用都会导致最终分布式系统无法顺利完成一次网络通信。另外，即使分布式系统各个节点之间的网络通信能够正常进行，其延时也会大于单机操作。通常 我们认为现代计算机体系结构中，单机内存访问的延时在纳秒数量级（通常是10ns），而正常的一次网络通信的延迟在0.1~1ms左右（相当于内存访问延 时的105倍），如此巨大的延时差别，也会影响到消息的收发过程，因此消息丢失和消息延迟变得非常普遍</p>
<h5 id="2-网络分区"><a href="#2-网络分区" class="headerlink" title="2. 网络分区"></a>2. 网络分区</h5><p>当网络由于发生异常情况，导致分布式系统中部分节点之间的网络延时不断增大，最终导致组成分布式系统的所有节点中，只有部分节点之间能够正常通信，而另一些节点则不能—-我们将这个现象称为网络分区。当网络分区出现时，分布式系统会出现局部小集群，在极端情况下，这些局部小集群会独立完成原本需要整个分布式系统才能完成的功能，包括对数据的事物处理，这就对分布式一致性提出了非常大的挑战</p>
<h5 id="3-三态"><a href="#3-三态" class="headerlink" title="3. 三态"></a>3. 三态</h5><p>上面两点，我们已经了解到在分布式环境下，网络可能会出现各式各样的问题，因此分布式系统的每一次请求与响应，存在特有的三态概念，即成功、失败、超时。 在传统的单机系统中，应用程序在调用一个函数之后，能够得到一个非常明确的响应：成功或失败。而在分布式系统中，由于网络是不可靠的，虽然在绝大部分情况 下，网络通信也能够接受到成功或失败的响应，当时当网络出现异常的情况下，就可能会出现超时现象，通常有以下两种情况: </p>
<p> (1) 由于网络原因，该请求并没有被成功地发送到接收方，而是在发送过程中就发生了消息丢失现象</p>
<p> (2) 该请求成功地被接收方接收后，进行了处理，但是在将响应反馈给发送方的过程中，发生了消息丢失现象</p>
<p>当出现这样的超时现象时，网络通信的发起方是无法确定当前请求是否被成功处理的</p>
<h5 id="4-节点故障"><a href="#4-节点故障" class="headerlink" title="4. 节点故障"></a>4. 节点故障</h5><p>节点故障则是分布式环境下另一个比较常见的问题，指的是组成分布式系统的服务器节点出现的宕机或”僵死”现象，通常根据经验来说，每个节点都有可能出现故障，并且每天都在发生</p>
<h4 id="分布式事物"><a href="#分布式事物" class="headerlink" title="分布式事物"></a>分布式事物</h4><p>随着分布式计算的发展，事物在分布式计算领域也得到了广泛的应用。在单机数据库中，我们很容易能够实现一套满足ACID特性的事物处理系统，但在分布式数据库中，数据分散在各台不同的机器上，如何对这些数据进行分布式的事物处理具有非常大的挑战。</p>
<p>分布式事物是指事物的参与者、支持事物的服务器、资源服务器以及事物管理器分别位于分布式系统的不同节点上，通常一个分布式事物中会涉及对多个数据源或业务系统的操作。</p>
<p>可以设想一个最典型的分布式事物场景：一个跨银行的转账操作涉及调用两个异地的银 行服务，其中一个是本地银行提供的取款服务，另一个则是目标银行提供的存款服务，这两个服务本身是无状态并且相互独立的，共同构成了一个完整的分布式事 物。如果从本地银行取款成功，但是因为某种原因存款服务失败了，那么就必须回滚到取款之前的状态，否则用户可能会发现自己的钱不翼而飞了。</p>
<p>从这个例子可以看到，一个分布式事务可以看做是多个分布式的操作序列组成的，例如 上面例子的取款服务和存款服务，通常可以把这一系列分布式的操作序列称为子事物。因此，分布式事务也可以被定义为一种嵌套型的事物，同时也就具有了 ACID事物特性。但由于在分布式事务中，各个子事物的执行是分布式的，因此要实现一种能够保证ACID特性的分布式事物处理系统就显得格外复杂。</p>
<h4 id="CAP理论"><a href="#CAP理论" class="headerlink" title="CAP理论"></a>CAP理论</h4><p>一个经典的分布式系统理论。CAP理论告诉我们：一个分布式系统不可能同时满足一致性（C：Consistency）、可用性（A：Availability）和分区容错性（P：Partition tolerance）这三个基本需求，最多只能同时满足其中两项。</p>
<p><strong>一致性</strong></p>
<p>在分布式环境下，一致性是指数据在多个副本之间能否保持一致的特性。在一致性的需求下，当一个系统在数据一致的状态下执行更新操作后，应该保证系统的数据仍然处于一致的状态。</p>
<p>对于一个将数据副本分布在不同分布式节点上的系统来说，如果对第一个节点的数据进 行了更新操作并且更新成功后，却没有使得第二个节点上的数据得到相应的更新，于是在对第二个节点的数据进行读取操作时，获取的依然是老数据（或称为脏数 据），这就是典型的分布式数据不一致的情况。在分布式系统中，如果能够做到针对一个数据项的更新操作执行成功后，所有的用户都可以读取到其最新的值，那么 这样的系统就被认为具有强一致性</p>
<p><strong>可用性</strong></p>
<p>可用性是指系统提供的服务必须一直处于可用的状态，对于用户的每一个操作请求总是能够在有限的时间内返回结果。这里的重点是”有限时间内”和”返回结果”。</p>
<p>“有限时间内”是指，对于用户的一个操作请求，系统必须能够在指定的时间内返回对 应的处理结果，如果超过了这个时间范围，那么系统就被认为是不可用的。另外，”有限的时间内”是指系统设计之初就设计好的运行指标，通常不同系统之间有很 大的不同，无论如何，对于用户请求，系统必须存在一个合理的响应时间，否则用户便会对系统感到失望。</p>
<p>“返回结果”是可用性的另一个非常重要的指标，它要求系统在完成对用户请求的处理后，返回一个正常的响应结果。正常的响应结果通常能够明确地反映出队请求的处理结果，即成功或失败，而不是一个让用户感到困惑的返回结果</p>
<p><strong>分区容错性</strong></p>
<p>分区容错性约束了一个分布式系统具有如下特性：分布式系统在遇到任何网络分区故障的时候，仍然需要能够保证对外提供满足一致性和可用性的服务，除非是整个网络环境都发生了故障。</p>
<p>网络分区是指在分布式系统中，不同的节点分布在不同的子网络（机房或异地网络） 中，由于一些特殊的原因导致这些子网络出现网络不连通的状况，但各个子网络的内部网络是正常的，从而导致整个系统的网络环境被切分成了若干个孤立的区域。 需要注意的是，组成一个分布式系统的每个节点的加入与退出都可以看作是一个特殊的网络分区。</p>
<p>既然一个分布式系统无法同时满足一致性、可用性、分区容错性三个特点，所以我们就需要抛弃一样</p>
<p><img src="/images/blog/2019-05-17-1.png" alt> </p>
<p>用一张表格说明一下</p>
<p>选择 | 说明 |<br>-|-|-<br>CA | 放弃分区容错性，加强一致性和可用性，其实就是传统的单机数据库的选择 |<br>AP | 放弃一致性（这里说的一致性是强一致性），追求分区容错性和可用性，这是很多分布式系统设计时的选择，例如很多NoSQL系统就是如此 |<br>CP | 放弃可用性，追求一致性和分区容错性，基本不会选择，网络问题会直接让整个系统不可用 | </p>
<p>需要明确的一点是，对于一个分布式系统而言，分区容错性是一个最基本的要求。因为 既然是一个分布式系统，那么分布式系统中的组件必然需要被部署到不同的节点，否则也就无所谓分布式系统了，因此必然出现子网络。而对于分布式系统而言，网 络问题又是一个必定会出现的异常情况，因此分区容错性也就成为了一个分布式系统必然需要面对和解决的问题。因此系统架构师往往需要把精力花在如何根据业务 特点在C（一致性）和A（可用性）之间寻求平衡。</p>
<h4 id="BASE理论"><a href="#BASE理论" class="headerlink" title="BASE理论"></a>BASE理论</h4><p>BASE是Basically Available（基本可用）、Soft state（软状态）和Eventually consistent（最终一致性）三个短语的缩写。BASE理论是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的总结， 是基于CAP定理逐步演化而来的。BASE理论的核心思想是：即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。</p>
<p><strong>基本可用</strong></p>
<p>基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性—-注意，这绝不等价于系统不可用。比如：</p>
<p> (1) 响应时间上的损失。正常情况下，一个在线搜索引擎需要在0.5秒之内返回给用户相应的查询结果，但由于出现故障，查询结果的响应时间增加了1~2秒</p>
<p> (2) 系统功能上的损失。正常情况下，在一个电子商务网站上进行购物的时候，消费者几乎能够顺利完成每一笔订单，但是在一些节日大促购物高峰的时候，由于消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面</p>
<p><strong>软状态</strong></p>
<p>软状态指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时</p>
<p><strong>最终一致性</strong></p>
<p>最终一致性强调的是所有的数据副本，在经过一段时间的同步之后，最终都能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。</p>
<p>总的来说，BASE理论面向的是大型高可用可扩展的分布式系统，和传统的事物ACID特性是相反的，它完全不同于ACID的强一致性模型，而是通过牺牲强一致性来获得可用性，并允许数据在一段时间内是不一致的，但最终达到一致状态。但同时，在实际的分布式场景中，不同业务单元和组件对数据一致性的要求是不同的，因此在具体的分布式系统架构设计过程中，ACID特性和BASE理论往往又会结合在一起。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">作者： IT·达人</span><br><span class="line">出处： &lt;https:&#x2F;&#x2F;www.cnblogs.com&#x2F;szlbm&#x2F;p&#x2F;5588543.html&gt;</span><br><span class="line">本文版权归作者所有，欢迎转载，但未经作者同意必须保留此段声明，且在页面明显位置给出原文链接。</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Distributed</category>
      </categories>
  </entry>
  <entry>
    <title>堆排序</title>
    <url>/2019/05/16/heap-sort/</url>
    <content><![CDATA[<p>堆排序是利用堆这种数据结构而设计的一种排序算法</p>
<hr>
<h4 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h4><p>堆排序是利用堆这种数据结构而设计的一种排序算法，<br>堆排序是一种选择排序，它的最坏，最好，平均时间复杂度均为O(nlogn)，它也是不稳定排序。</p>
<h4 id="堆"><a href="#堆" class="headerlink" title="堆"></a>堆</h4><p>堆是一种特殊的完全二叉树。堆又分为最大堆和最小堆。</p>
<blockquote>
<ul>
<li>大顶堆: 每个结点的值都大于或等于其左右孩子结点的值</li>
<li>小顶堆: 每个结点的值都小于或等于其左右孩子结点的值</li>
</ul>
</blockquote>
<p><img src="/images/blog/2019-05-16-1.png" alt> </p>
<p>同时，对堆中的结点按层进行编号，将这种逻辑结构映射到数组中就是下面这个样子</p>
<p><img src="/images/blog/2019-05-16-2.png" alt> </p>
<p>该数组从逻辑上讲就是一个堆结构，用简单的公式来描述一下堆的定义就是</p>
<p><strong>大顶堆: arr[i] &gt;= arr[2i+1] &amp;&amp; arr[i] &gt;= arr[2i+2]</strong></p>
<p><strong>小顶堆: arr[i] &lt;= arr[2i+1] &amp;&amp; arr[i] &lt;= arr[2i+2]</strong></p>
<h4 id="堆排序基本思想"><a href="#堆排序基本思想" class="headerlink" title="堆排序基本思想"></a>堆排序基本思想</h4><p>将待排序序列构造成一个大顶堆，此时，整个序列的最大值就是堆顶的根节点。</p>
<p>将其与末尾元素进行交换，此时末尾就为最大值。</p>
<p>接着将剩余n-1个元素重新构造成一个堆，这样会得到n个元素的次小值。</p>
<p>如此反复执行，便能得到一个有序序列了。</p>
<h4 id="1-堆化数组"><a href="#1-堆化数组" class="headerlink" title="1. 堆化数组"></a>1. 堆化数组</h4><p>构造初始堆。将给定无序序列构造成一个大顶堆（一般升序采用大顶堆，降序采用小顶堆)。</p>
<p>假设给定无序序列结构如下</p>
<p><img src="/images/blog/2019-05-16-3.png" alt> </p>
<p>此时我们从最后一个非叶子结点开始（叶结点自然不用调整，第一个非叶子结点 arr.length/2-1=5/2-1=1，也就是下面的6结点）<br>，从左至右，从下至上进行调整。</p>
<p><img src="/images/blog/2019-05-16-4.png" alt> </p>
<p>找到倒数第二个非叶节点4，由于[4,9,8]中9元素最大，4和9交换。</p>
<p><img src="/images/blog/2019-05-16-5.png" alt> </p>
<p>这时，交换导致了子根[4,5,6]结构混乱，继续调整，[4,5,6]中6最大，交换4和6。</p>
<p><img src="/images/blog/2019-05-16-6.png" alt> </p>
<p>此时，将一个无需序列构造成了一个大顶堆。</p>
<h4 id="2-堆排序"><a href="#2-堆排序" class="headerlink" title="2. 堆排序"></a>2. 堆排序</h4><p>将堆顶元素与末尾元素进行交换，使末尾元素最大。</p>
<p>然后继续调整堆，再将堆顶元素与末尾元素交换，得到第二大元素。</p>
<p>如此反复进行交换、重建、交换。</p>
<p>将堆顶元素9和末尾元素4进行交换</p>
<p><img src="/images/blog/2019-05-16-7.png" alt> </p>
<p>重新调整结构，使其继续满足堆定义</p>
<p><img src="/images/blog/2019-05-16-8.png" alt> </p>
<p>再将堆顶元素8与末尾元素5进行交换，得到第二大元素8</p>
<p><img src="/images/blog/2019-05-16-9.png" alt> </p>
<p>后续过程，继续进行调整，交换，如此反复进行，最终使得整个序列有序</p>
<p><img src="/images/blog/2019-05-16-10.png" alt> </p>
<p>再简单总结下堆排序的基本思路：</p>
<p>　　a. 将无需序列构建成一个堆，根据升序降序需求选择大顶堆或小顶堆;</p>
<p>　　b. 将堆顶元素与末尾元素交换，将最大元素”沉”到数组末端;</p>
<p>　　c. 重新调整结构，使其满足堆定义，然后继续交换堆顶元素与当前末尾元素，反复执行调整+交换步骤，直到整个序列有序。</p>
<h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;  从i节点开始调整,n为节点总数 从0开始计算 i节点的子节点为 2*i+1, 2*i+2</span><br><span class="line">void maxHeapFixDown(int arr[], int i, int n) &#123;</span><br><span class="line">    int j, tmp;</span><br><span class="line">    tmp &#x3D; arr[i];</span><br><span class="line">    j &#x3D; 2 * i + 1;</span><br><span class="line">    while (j &lt; n) &#123;</span><br><span class="line">        if ((j + 1) &lt; n &amp;&amp; arr[j + 1] &gt; arr[j]) j++;</span><br><span class="line">        if (arr[j] &lt;&#x3D; tmp) break;</span><br><span class="line">        arr[i] &#x3D; arr[j];</span><br><span class="line">        i &#x3D; j;</span><br><span class="line">        j &#x3D; 2 * i + 1;</span><br><span class="line">    &#125;</span><br><span class="line">    arr[i] &#x3D; tmp;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 堆化数组</span><br><span class="line">void makeMaxHeap(int arr[], int n) &#123;</span><br><span class="line">    for (int i &#x3D; (n &#x2F; 2 - 1) ; i &gt;&#x3D; 0 ; i--) &#123;</span><br><span class="line">        maxHeapFixDown(arr, i, n);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 堆排序</span><br><span class="line">void maxHeapSort(int arr[], int n)</span><br><span class="line">&#123;</span><br><span class="line">    makeMaxHeap(arr, n);</span><br><span class="line">    for (int i &#x3D; n - 1; i &gt;&#x3D; 1; i--)</span><br><span class="line">    &#123;</span><br><span class="line">        swap(arr[i], arr[0]);</span><br><span class="line">        maxHeapFixDown(arr, 0, i);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>堆排序是一种选择排序，整体主要由构建初始堆+交换堆顶元素和末尾元素并重建堆两部分组成。<br>其中构建初始堆经推导复杂度为O(n)，在交换并重建堆的过程中，需交换n-1次，而重建堆的过程中，<br>根据完全二叉树的性质，[log2(n-1),log2(n-2)…1]逐步递减，近似为nlogn。<br>所以堆排序时间复杂度一般认为就是O(nlogn)级。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">作者： dreamcatcher-cx</span><br><span class="line">出处： &lt;http:&#x2F;&#x2F;www.cnblogs.com&#x2F;chengxiao&#x2F;&gt;</span><br><span class="line">本文版权归作者和博客园共有，欢迎转载，但未经作者同意必须保留此段声明，且在页面明显位置给出原文链接。</span><br></pre></td></tr></table></figure>

<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.cnblogs.com/chengxiao/p/6129630.html" target="_blank" rel="noopener">图解排序算法(三)之堆排序</a></li>
<li><a href="https://blog.csdn.net/morewindows/article/details/6709644/" target="_blank" rel="noopener">白话经典算法系列之七 堆与堆排序</a></li>
</ul>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title>LSM Tree</title>
    <url>/2019/05/15/LSM/</url>
    <content><![CDATA[<p>日志结构合并树(Log-Structured Merge-Tree)</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>大多 NoSQL 数据库核心思想都是基于LSM Tree 的存储模型来做的，只是具体的实现不同。包括 LevelDB，HBase，<br>Google BigTable，Cassandra，InfluxDB ，携程 SessionDB，Facebook rocksdb 等。</p>
<p>LSM树，即日志结构合并树(Log-Structured Merge-Tree)。其实它并不属于一个具体的数据结构，<br>它更多是一种数据结构的设计思想。</p>
<h4 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h4><p>讲 LSM 树之前，需要提下三种基本的存储引擎，这样才能清楚 LSM 树的由来。</p>
<ol>
<li><p>哈希存储引擎: 是哈希表的持久化实现，支持增、删、改以及随机读取操作，但不支持顺序扫描，对应的存储系统为key-value存储系统。对于key-value的插入以及查询，哈希表的复杂度都是O(1)，明显比树的操作O(n)快,如果不需要有序的遍历数据，哈希表就非常适合。代表性的数据库有：Redis，Memcache，以及存储系统Bitcask。</p>
</li>
<li><p>B树存储引擎是B树的持久化实现，不仅支持单条记录的增、删、读、改操作，还支持顺序扫描（B+树的叶子节点之间的指针），对应的存储系统就是关系数据库(MySQL)。</p>
</li>
<li><p>LSM树（Log-Structured Merge Tree）存储引擎和B树存储引擎一样，同样支持增、删、读、改、顺序扫描操作。而且通过批量存储技术规避磁盘随机写入问题。</p>
</li>
</ol>
<p>当然凡事有利有弊，LSM树和B+树相比，LSM树牺牲了部分读性能，用来大幅提高写性能。</p>
<p>从技术角度: 由于磁盘的(磁柱、磁盘、磁道、磁头)结构与B树结构的特点,导致传统B树索引存在着随机写效率的上限挑战，所以当在那些索引插入频率远大于查询频率的应用场景下，如历史记录表和日志文件来说，B树索引显得捉襟见肘了。</p>
<h5 id="1-B树随机写特点"><a href="#1-B树随机写特点" class="headerlink" title="1. B树随机写特点"></a>1. B树随机写特点</h5><p>一个BTree，对于在没有缓存的Case情况下, 一个随机写分为两步进行：</p>
<ol>
<li>从磁盘Load目标块节点到内存，</li>
<li>修改它并写回磁盘。</li>
</ol>
<p>所以BTree在对于随机key值下的平均“blind-write”操作需要两次IO操作，其限定了BTree的随机写吞吐量。</p>
<p>为什么B+树会慢</p>
<p>从原理来说，B+树在查询过程中应该是不会慢的，但如果数据插入比较无序的时候，比如先插入5 然后10000然后3然后800 这样跨度很大的数据的时候，就需要先“找到这个数据应该被插入的位置”，然后插入数据。<br>这个查找到位置的过程，如果非常离散，那么就意味着每次查找的时候，他的叶子节点都不在内存中，这时候就必须使用磁盘寻道时间来进行查找了。更新基本与插入是相同的。</p>
<h5 id="2-LSM-“blind-write”吞吐量"><a href="#2-LSM-“blind-write”吞吐量" class="headerlink" title="2. LSM “blind-write”吞吐量"></a>2. LSM “blind-write”吞吐量</h5><p>既然随机写相对昂贵的，LSM采用“有序map的数据分层与延迟写”(all sorted-map write-deferral)的策略而代替立即写的操作；同时为了避免数据层数的过多造成对读的性能的伤害，数据层级之间会周期性地触发自顶向下地进行合并的操作。</p>
<p>LSM Tree采取了什么样的方式来优化这个问题呢?</p>
<p>简单来说，就是放弃磁盘读性能来换取写的顺序性。</p>
<h4 id="存储模型"><a href="#存储模型" class="headerlink" title="存储模型"></a>存储模型</h4><h5 id="1-WAL"><a href="#1-WAL" class="headerlink" title="1. WAL"></a>1. WAL</h5><p>在设计数据库的时候经常被使用，当插入一条数据时，数据先顺序写入 WAL 文件中，<br>之后插入到内存中的 MemTable 中。<br>这样就保证了数据的持久化，不会丢失数据，并且都是顺序写，速度很快。<br>当程序挂掉重启时，可以从 WAL 文件中重新恢复内存中的 MemTable。</p>
<h5 id="2-MemTable"><a href="#2-MemTable" class="headerlink" title="2. MemTable"></a>2. MemTable</h5><p>MemTable 对应的就是 WAL 文件，是该文件内容在内存中的存储结构，<br>通常用 SkipList 来实现。MemTable 提供了 k-v 数据的写入、删除以及读取的操作接口。<br>其内部将 k-v 对按照 key 值有序存储，这样方便之后快速序列化到 SSTable 文件中，仍然保持数据的有序性。</p>
<h5 id="3-Immutable-Memtable"><a href="#3-Immutable-Memtable" class="headerlink" title="3. Immutable Memtable"></a>3. Immutable Memtable</h5><p>顾名思义，Immutable Memtable 就是在内存中只读的 MemTable，由于内存是有限的，通常我们会设置一个阀值，当 MemTable 占用的内存达到阀值后就自动转换为 Immutable Memtable，Immutable Memtable 和 MemTable 的区别就是它是只读的，系统此时会生成新的 MemTable 供写操作继续写入。</p>
<p>之所以要使用 Immutable Memtable，就是为了避免将 MemTable 中的内容序列化到磁盘中时会阻塞写操作。</p>
<h5 id="4-SSTable"><a href="#4-SSTable" class="headerlink" title="4. SSTable"></a>4. SSTable</h5><p><img src="/images/blog/2019-05-15-1.png" alt> </p>
<p>SSTable 就是 MemTable 中的数据在磁盘上的有序存储，其内部数据是根据 key 从小到大排列的。通常为了加快查找的速度，需要在 SSTable 中加入数据索引，可以快读定位到指定的 k-v 数据。</p>
<h4 id="常用操作的实现"><a href="#常用操作的实现" class="headerlink" title="常用操作的实现"></a>常用操作的实现</h4><h5 id="1-写入"><a href="#1-写入" class="headerlink" title="1. 写入"></a>1. 写入</h5><p><img src="/images/blog/2019-05-15-2.png" alt> </p>
<p>在 LSM Tree 中，写入操作是相当快速的，只需要在 WAL 文件中顺序写入当次操作的内容，成功之后将该 k-v 数据写入 MemTable 中即可。尽管做了一次磁盘 IO，但是由于是顺序追加写入操作，效率相对来说很高，并不会导致写入速度的降低。数据写入 MemTable 中其实就是往 SkipList 中插入一条数据，过程也相当简单快速。</p>
<h5 id="2-更新"><a href="#2-更新" class="headerlink" title="2. 更新"></a>2. 更新</h5><p>更新操作其实并不真正存在，和写入一个 k-v 数据没有什么不同，只是在读取的时候，会从 Level0 层的 SSTable 文件开始查找数据，数据在低层的 SSTable 文件中必然比高层的文件中要新，所以总能读取到最新的那条数据。也就是说此时在整个 LSM Tree 中可能会同时存在多个 key 值相同的数据，只有在之后合并 SSTable 文件的时候，才会将旧的值删除。</p>
<h5 id="3-删除"><a href="#3-删除" class="headerlink" title="3. 删除"></a>3. 删除</h5><p>删除一条记录的操作比较特殊，并不立即将数据从文件中删除，而是记录下对这个 key 的删除操作标记，同插入操作相同，插入操作插入的是 k-v 值，而删除操作插入的是 k-del 标记，只有当合并 SSTable 文件时才会真正的删除。</p>
<h5 id="4-合并"><a href="#4-合并" class="headerlink" title="4. 合并"></a>4. 合并</h5><p>Compaction。当数据不断从 Immutable Memtable 序列化到磁盘上的 SSTable 文件中时，SSTable 文件的数量就不断增加，而且其中可能有很多更新和删除操作并不立即对文件进行操作，而只是存储一个操作记录，这就造成了整个 LSM Tree 中可能有大量相同 key 值的数据，占据了磁盘空间。</p>
<p>为了节省磁盘空间占用，控制 SSTable 文件数量，需要将多个 SSTable 文件进行合并，生成一个新的 SSTable 文件。比如说有 5 个 10 行的 SSTable 文件要合并成 1 个 50 行的 SSTable 文件，但是其中可能有 key 值重复的数据，我们只需要保留其中最新的一条即可，这个时候新生成的 SSTable 可能只有 40 行记录。</p>
<h5 id="5-读取"><a href="#5-读取" class="headerlink" title="5. 读取"></a>5. 读取</h5><p><img src="/images/blog/2019-05-15-3.png" alt> </p>
<p>LSM Tree 的读取效率并不高，当需要读取指定 key 的数据时，先在内存中的 MemTable 和 Immutable MemTable 中查找，如果没有找到，则继续从 Level 0 层开始，找不到就从更高层的 SSTable 文件中查找，如果查找失败，说明整个 LSM Tree 中都不存在这个 key 的数据。如果中间在任何一个地方找到这个 key 的数据，那么按照这个路径找到的数据都是最新的。</p>
<h5 id="6-优化读取"><a href="#6-优化读取" class="headerlink" title="6. 优化读取"></a>6. 优化读取</h5><p>因为这样的读取效率非常差，通常会进行一些优化，例如 LevelDB 中的 Mainfest 文件，这个文件记录了 SSTable 文件的一些关键信息，例如 Level 层数，文件名，最小 key 值，最大 key 值等，这个文件通常不会太大，可以放入内存中，可以帮助快速定位到要查询的 SSTable 文件，避免频繁读取。</p>
<p>另外一个经常使用的方法是布隆解析器(Bloom filter)，布隆解析器是一个使用内存判断文件是否包含一个关键字的有效方法。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>总的来说就是通过将大量的随机写转换为顺序写，从而极大地提升了数据写入的性能，虽然与此同时牺牲了部分读的性能。</p>
<p>只适合存储 key 值有序且写入大于读取的数据，或者读取操作通常是 key 值连续的数据。</p>
<p>B树和LSM树最主要的区别在于他们的结构和如何利用硬件，特别是磁盘。</p>
<p>LSM Tree 的思想非常实用，将随机写转换为顺序写来大幅提高写入操作的性能，但是牺牲了部分读的性能。</p>
<p>由于时间序列数据库的特性，运用 LSM Tree 的算法非常合适。持续写入数据量大，数据和时间相关，编码到 key 值中很容易使 key 值有序。读取操作相对来说较少，而且通常不是读取单个 key 的值，而是一段时间范围内的数据，这样就把 LSM Tree 读取性能差的劣势缩小了，反而由于数据在 SSTable 中是按照 key 值顺序排列，读取大块连续的数据时效率也很高。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">作者：fatedier </span><br><span class="line">本文出处：http:&#x2F;&#x2F;blog.fatedier.com&#x2F;2016&#x2F;06&#x2F;15&#x2F;learn-lsm-tree&#x2F; </span><br><span class="line">文章版权归本人所有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文链接，否则保留追究法律责任的权利。</span><br></pre></td></tr></table></figure>

<hr>
<p>参考链接</p>
<ul>
<li><a href="https://baijiahao.baidu.com/s?id=1613810327967900833&wfr=spider&for=pc" target="_blank" rel="noopener">看图轻松理解数据结构与算法系列(NoSQL存储-LSM树)</a></li>
<li><a href="http://blog.fatedier.com/2016/06/15/learn-lsm-tree/" target="_blank" rel="noopener">LSM Tree 学习笔记</a></li>
<li><a href="https://blog.csdn.net/u013928917/article/details/75912045" target="_blank" rel="noopener">B+树和LSM比较</a></li>
<li><a href="https://www.cnblogs.com/yanghuahui/p/3483754.html" target="_blank" rel="noopener">LSM树由来、设计思想以及应用到HBase的索引</a></li>
<li><a href="https://www.jianshu.com/p/d52edc9f33df" target="_blank" rel="noopener">LSM树原理、应用与优化</a></li>
<li><a href="https://www.cnblogs.com/gisorange/p/5272826.html" target="_blank" rel="noopener">日志结构合并树LSM代码解读</a></li>
<li><a href="https://www.jianshu.com/p/06f9f7f41fdb" target="_blank" rel="noopener">HBASE-LSM树</a></li>
<li><a href="https://www.2cto.com/database/201805/749826.html" target="_blank" rel="noopener">举例讲解B+树与LSM树的区别与联系</a></li>
<li><a href="https://cloud.tencent.com/developer/news/340271" target="_blank" rel="noopener">LSM-tree 基本原理及应用</a></li>
</ul>
]]></content>
      <categories>
        <category>DataStructure</category>
      </categories>
  </entry>
  <entry>
    <title>HDFS ACL 权限管理</title>
    <url>/2019/05/14/hdfs-acl/</url>
    <content><![CDATA[<p>HDFS 支持 POSIX 访问控制列表（ACLs），以及已支持的传统POSIX权限模型。</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>ACL 通过给特定命名的 user 和 group 设置不同的权限的方法来控制 HDFS 文件的访问。</p>
<p>ACL的方式增强了传统权限模型，因为它可以让你给任意组合的 user 和 group 来定义访问控制，<br>而不是为单个 owner/user 或单个 group。</p>
<h4 id="传统的POSIX权限模型"><a href="#传统的POSIX权限模型" class="headerlink" title="传统的POSIX权限模型"></a>传统的POSIX权限模型</h4><p>HDFS实现的是类似于POSIX系统的权限模型，如前所述，使用过Linux/Unix系统的用户对该模型应该都非常熟悉，就不再赘述。这里先对下文中最常被问到的问题做一些说明：</p>
<ol>
<li><p>访问某个路径时，用户必须具备该路径上每个目录的执行（x）权限，路径中最后一个目录/文件除外。例如 ls /user/foo/data操作要求用户必须具有根目录(/)，user目录，foo目录的执行权限。</p>
</li>
<li><p>创建一个文件或者目录时，owner是客户进程的用户，group则继承父目录</p>
</li>
<li><p>新建文件或目录的模式（mode）由client在rpc调用时传递给NameNode，它受配置参数umask的约束。新文件的模式是666 &amp; ^umask，新目录的模式是777 &amp; ^umask，即文件默认是没有执行(x)权限的。如果在 create(path, permission, …) 方法中指定了权限参数P，新文件的模式是P &amp; ^umask &amp; 666，如果在mkdirs(path, permission ) 方法中指定了权限参数P，新目录的模式是P &amp; ^umask &amp; 777。</p>
</li>
</ol>
<p>例1: 如果umask是022（默认值），那么新文件的模式就是644，新目录的模式就是755，即umask擦除掉了group和other的写权限。</p>
<p>例2: 如果umask是027，那么新文件的模式就是650，新目录的模式就是750，即umask擦除掉了group的写权限，以及other的读写执行权限。</p>
<ol start="4">
<li><p>umask通过client端hdfs-site.xml中的fs.permissions.umask-mode配置项来指定，默认是022。</p>
</li>
<li><p>只有超级用户才可以调用chown来修改目录和文件的owner。</p>
</li>
</ol>
<h4 id="开启-ACL"><a href="#开启-ACL" class="headerlink" title="开启 ACL"></a>开启 ACL</h4><p>使用 CM 开启 HDFS 的 ACL。在 HDFS 的配置页面搜索下方参数修改</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dfs.namenode.acls.enabled</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/2019-05-14-1.png" alt> </p>
<p>重启相关服务</p>
<h4 id="ACL-条目"><a href="#ACL-条目" class="headerlink" title="ACL 条目"></a>ACL 条目</h4><p><img src="/images/blog/2019-05-14-2.png" alt> </p>
<p>一个ACLs由一系列条目（entry）组成，上表表列出了条目的类型及格式，共有六种类型的ACL条目。</p>
<h4 id="最小-ACLs-和扩展-ACLs"><a href="#最小-ACLs-和扩展-ACLs" class="headerlink" title="最小 ACLs 和扩展 ACLs"></a>最小 ACLs 和扩展 ACLs</h4><p>最小ACLs就是与文件/目录模式权限位完全对应。</p>
<p><img src="/images/blog/2019-05-14-3.png" alt> </p>
<p>如上图所示，没有附加的权限</p>
<p>hadoop fs -ls 看到什么权限，最小 ACLs 就是这个权限。</p>
<p>拥有超过3个条目的ACLs称为<strong>扩展ACLs（extended ACLs）</strong></p>
<p>扩展 ACLs 会包含一个 mask 条目以及给其他指定用户和组授权的条目，<br>即有名ACL条目（named entry），与最小ACLs中无名条目相对应。</p>
<p>有名 ACL 条目即上表中 ACL 条目中 named user 和 named group 两种类型。</p>
<p><img src="/images/blog/2019-05-14-4.png" alt> </p>
<p>对于最小 ACLs 来说，文件模式权限中的 owner、group 和 other 分别映射到 ACLs owner、owning group 和 others 条目，<br>对于扩展 ACLs 则是分别映射到 ACLs owner、mask 和 others 条目。</p>
<h4 id="掩码-mask-影响"><a href="#掩码-mask-影响" class="headerlink" title="掩码(mask) 影响"></a>掩码(mask) 影响</h4><p>在最小 ACLs 中，group class 就是 owning group 条目权限</p>
<p>在扩展 ACLs 中，group class 为 掩码(mask) 条目</p>
<p>除了owner、other 条目以外，其他条目虚与掩码(mask)条目进行与运算，得到最后真实的权限。如下图所示</p>
<p><img src="/images/blog/2019-05-14-5.png" alt> </p>
<p>每一个ACL都有一个掩码(mask)，如果用户不提供掩码，那么该掩码会自动根据所有ACL条目的并集来获得(属主除外）</p>
<p>在该文件上运行 chmod 会改变掩码的权限。由于掩码用于过滤，这有效地限制了权限的扩展 ACL 条目，而不是仅仅改变组条目，并可能丢失的其他扩展ACL条目。</p>
<p><img src="/images/blog/2019-05-14-6.png" alt> </p>
<p>可以看到 mask 为 r-x。user:bigdatams:rwx 条目最后的权限为  #effective:r-x</p>
<h4 id="默认-ACL"><a href="#默认-ACL" class="headerlink" title="默认 ACL"></a>默认 ACL</h4><p>前面讨论的ACL条目定义了文件/目录当前的访问权限，称为访问ACLs（access ACLs），另一种类型叫做默认ACLs（default ACL），它定义了当一个文件系统对象被创建时如何从父目录继承权限。</p>
<p>只有目录可以被设置默认ACLs，默认ACLs不会用于权限检查，仅用于权限继承。</p>
<p>创建一个新的目录时，如果父目录设置了默认ACLs，则新目录会继承父目录的默认ACLs作为自己的访问ACLs，同时也作为自己的默认ACLs。</p>
<p>新的文件则只会继承父目录的默认ACLs作为自己的访问ACLs，但文件本身不再有默认ACLs。</p>
<p>从父目录默认 ACLs 继承来的权限并非最终的权限，由于在创建新的目录/文件时 client 一定会传给 NameNode 一个文件模式权限<br>（见“传统的POSIX权限模型”一节说明3），两者的计算结果才是最终的权限。计算方式采用与运算，即取继承的ACLs中某类权限与模式权限中对应类别权限的交集。</p>
<p><img src="/images/blog/2019-05-14-8.png" alt> </p>
<p>虽然只添加一条默认ACL条目，对于一个完整的ACLs来说所需的其他条目都已经被自动从访问ACLs中复制了过来。</p>
<p><img src="/images/blog/2019-05-14-7.png" alt> </p>
<p>default:mask 也是自动添加的，它的取值是根据所有ACL条目的并集来获得(属主除外）。</p>
<h4 id="权限检查"><a href="#权限检查" class="headerlink" title="权限检查"></a>权限检查</h4><p>当一个文件使用ACL时，权限检查的算法则变为: </p>
<blockquote>
<ul>
<li>当用户名为文件的属主时，会检查属主的权限。</li>
<li>否则如果用户名匹配命名用户条目中的一个时，权限会被检查并通过mask权限来进行过滤。</li>
<li>否则如果文件的组匹配到当前用户的组列表中的一个时，而这些权限经过mask过滤后仍然会授权，会被允许使用。</li>
<li>否则如果其中一个命名组条目匹配到组列表中的一个成员，而这些权限经过mask过滤后仍然会授权，会被允许使用。</li>
<li>否则如果文件组和任何命名组条目匹配到组列表中的一个成员时，但是访问不会被任何一个权限所授权时，访问会被拒绝。</li>
<li>除此之外，other权限位会被检查。</li>
</ul>
</blockquote>
<h4 id="使用-ACL"><a href="#使用-ACL" class="headerlink" title="使用 ACL"></a>使用 ACL</h4><p>要设置和获取文件的访问控制列表（ACLs），有两种方式API 和 shell。</p>
<p>文件系统的shell命令，setfacl 和 getfacl</p>
<h5 id="1-getfacl"><a href="#1-getfacl" class="headerlink" title="1. getfacl"></a>1. getfacl</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">haddop fs -getfacl [-R] &lt;path&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- COMMAND OPTIONS</span><br><span class="line">&lt;path&gt;: 需要列出ACLs的文件或者目录的路径。</span><br><span class="line">-R: 使用递归的方式列出所有文件和目录的ACLs。</span><br><span class="line">--&gt;</span><br></pre></td></tr></table></figure>

<h4 id="2-setfacl"><a href="#2-setfacl" class="headerlink" title="2. setfacl"></a>2. setfacl</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop fs -setfacl [-R] [-b|-k -m|-x &lt;acl_spec&gt; &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]</span><br><span class="line"></span><br><span class="line">&lt;!-- COMMAND OPTIONS</span><br><span class="line">&lt;path&gt;: 需要设置ACLs的文件或者目录的路径。</span><br><span class="line">-R:以递归方式将操作应用于所有文件和目录。</span><br><span class="line">-b: 撤回基本ACL条目以外的所有条目。保留用户，组和其他条目以与权限位兼容。</span><br><span class="line">-k: 移除default ACL。</span><br><span class="line">-m: 修改ACL。新条目将添加到ACL，并保留现有条目。不会影响已有的权限。</span><br><span class="line">-x: 仅移除指定的ACL。</span><br><span class="line">&lt;acl_spec&gt;: 逗号分隔的ACL权限。</span><br><span class="line">--set: 完全替换ACL，丢弃所有现有条目。 acl_spec必须包含用户，组和其他条目，以便与权限位兼容。</span><br><span class="line">--&gt;</span><br></pre></td></tr></table></figure>

<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>当 ls 的权限位输出以+结束时，那么该文件或目录正在启用一个ACL。</p>
<p>常用命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查询目录的ACL规则</span><br><span class="line">hadoop fs -getfacl &#x2F;data</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"># 递归查询目录的ACL规则</span><br><span class="line">hadoop fs -getfacl -R &#x2F;data  </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"># 为指定user添加权限(user名为hadoop)</span><br><span class="line">hadoop fs -setfacl -m user:hadoop:rwx &#x2F;data</span><br><span class="line"> </span><br><span class="line"># 为指定group添加权限(group名为group)</span><br><span class="line">hadoop fs -setfacl -m group:hadoop:rwx &#x2F;data</span><br><span class="line"> </span><br><span class="line"># 删除指定user的ACL规则</span><br><span class="line">hadoop fs -setfacl -x user:hadoop2 &#x2F;data</span><br><span class="line"> </span><br><span class="line"># 删除所有的ACL规则,但保留基础ACL条目</span><br><span class="line">hadoop fs -setfacl -b &#x2F;data</span><br><span class="line"> </span><br><span class="line"># 删除默认的ACL规则</span><br><span class="line">hadoop fs -setfacl -k &#x2F;data</span><br><span class="line"> </span><br><span class="line">#以上加个-R就是对该目录下进行递归操作</span><br></pre></td></tr></table></figure>


<hr>
<p>参考链接</p>
<ul>
<li><a href="http://ju.outofmemory.cn/entry/106652" target="_blank" rel="noopener">HDFS ACL操作实战</a></li>
<li><a href="http://blog.sina.com.cn/s/blog_1622ceba80102x516.html" target="_blank" rel="noopener">一篇文章搞懂HDFS权限管理</a></li>
<li><a href="https://mp.weixin.qq.com/s/rEMGHveB94x8QU_O135G3A" target="_blank" rel="noopener">0630-6.2-什么是HDFS ACL</a></li>
</ul>
]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title>浅学 Synchronized</title>
    <url>/2019/05/10/Java-synchronized/</url>
    <content><![CDATA[<p>Synchronized 是并发编程中重要的使用工具之一</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>JVM 自带的关键字，可在需要线程安全的业务场景中使用，来保证线程安全。</p>
<h4 id="用法"><a href="#用法" class="headerlink" title="用法"></a>用法</h4><blockquote>
<ul>
<li>按照锁的对象区分可以分为对象锁和类锁</li>
<li>按照在代码中的位置区分可以分为方法形式和代码块形式</li>
</ul>
</blockquote>
<h5 id="对象锁"><a href="#对象锁" class="headerlink" title="对象锁"></a>对象锁</h5><p>锁对象为当前 this 或者说是当前类的实例对象</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 对象锁方法形式</span><br><span class="line">public synchronized void method() &#123;</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 对象锁代码块形式</span><br><span class="line">public void method() &#123;</span><br><span class="line">	synchronized (this) &#123;</span><br><span class="line">		...</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="类锁"><a href="#类锁" class="headerlink" title="类锁"></a>类锁</h5><p>锁的是当前类或者指定类的Class对象。</p>
<p>一个类可能有多个实例对象，但它只可能有一个Class对象。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public static void synchronized method() &#123;</span><br><span class="line">    System.out.println(&quot;我是静态方法形式的类锁&quot;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public void method() &#123;</span><br><span class="line">    synchronized(*.class) &#123;</span><br><span class="line">        System.out.println(&quot;我是代码块形式的类锁&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h4 id="简单举例"><a href="#简单举例" class="headerlink" title="简单举例"></a>简单举例</h4><p>可以参看<a href="https://mp.weixin.qq.com/s/Zwl3fUyO4igP6wE5W5IwYw" target="_blank" rel="noopener">深入理解synchronized关键字</a>、<a href="https://www.cnblogs.com/paddix/p/5367116.html" target="_blank" rel="noopener">Java并发编程：Synchronized及其实现原理</a>。</p>
<p>原理也可以参看上面两个文章。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>一把锁只能同时被一个线程获取，没有拿到锁的线程必须等待；</p>
<p>每个实例都对应有自己的一把锁，不同实例之间互不影响；</p>
<p>锁对象是*.class以及synchronized修饰的static方法时，所有对象共用一把类锁；</p>
<p>无论是方法正常执行完毕或者方法抛出异常，都会释放锁；</p>
<p>使用 synchronized 修饰的方法都是可重入的</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class SynchronizedRecursion &#123;</span><br><span class="line">    int a &#x3D; 0;</span><br><span class="line">    int b &#x3D; 0;</span><br><span class="line">    private void method1() &#123;</span><br><span class="line">        System.out.println(&quot;method1正在执行，a &#x3D; &quot; + a);</span><br><span class="line">        if (a &#x3D;&#x3D; 0) &#123;</span><br><span class="line">            a ++;</span><br><span class="line">            method1();</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(&quot;method1执行结束，a &#x3D; &quot; + a);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private synchronized void method2() &#123;</span><br><span class="line">        System.out.println(&quot;method2正在执行，b &#x3D; &quot; + b);</span><br><span class="line">        if (b &#x3D;&#x3D; 0) &#123;</span><br><span class="line">            b ++;</span><br><span class="line">            method2();</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(&quot;method2执行结束，b &#x3D; &quot; + b);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        SynchronizedRecursion synchronizedRecursion &#x3D; new SynchronizedRecursion();</span><br><span class="line">        synchronizedRecursion.method1();</span><br><span class="line">        synchronizedRecursion.method2();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">结果为：</span><br><span class="line">    method1正在执行，a &#x3D; 0</span><br><span class="line">    method1正在执行，a &#x3D; 1</span><br><span class="line">    method1执行结束，a &#x3D; 1</span><br><span class="line">    method1执行结束，a &#x3D; 1</span><br><span class="line"></span><br><span class="line">    method2正在执行，b &#x3D; 0</span><br><span class="line">    method2正在执行，b &#x3D; 1</span><br><span class="line">    method2执行结束，b &#x3D; 1</span><br><span class="line">    method2执行结束，b &#x3D; 1</span><br></pre></td></tr></table></figure>

<p>可以看到method1()与method2()的执行结果一样的，method2()在获取到对象锁以后，在递归调用时不需要等上一次调用先释放后再获取，而是直接进入，这说明了synchronized的可重入性。</p>
<p>当然，除了递归调用，调用同类的其它同步方法，调用父类同步方法，都是可重入的，前提是同一对象去调用，这里就不一一列举了.</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/Zwl3fUyO4igP6wE5W5IwYw" target="_blank" rel="noopener">深入理解synchronized关键字</a></li>
<li><a href="https://www.cnblogs.com/paddix/p/5367116.html" target="_blank" rel="noopener">Java并发编程：Synchronized及其实现原理</a></li>
</ul>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title>浅学 CountDownLatch</title>
    <url>/2019/05/10/Java-CountDownLatch/</url>
    <content><![CDATA[<p>CountDownLatch 是一个同步的辅助类，允许一个或多个线程一直等待，直到其它线程完成它们的操作。</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>在阅读 HBase 快照源码时看到了 waitForLatch 的方法，由此认识了 CountDownLatch。</p>
<p><img src="/images/blog/2019-05-10-1.png" alt> </p>
<p><img src="/images/blog/2019-05-10-2.png" alt> </p>
<p>还是挺重要的知识点。于是花了点时间去了解一下。</p>
<h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><blockquote>
<ul>
<li>A synchronization aid that allows one or more threads to wait until a set of operations being performed in other threads completes.</li>
</ul>
</blockquote>
<p>翻译过来 CountDownLatch 是一个同步的辅助类，允许一个或多个线程一直等待，直到其它线程完成它们的操作。</p>
<p>Java中，CountDownLatch是一个同步辅助类，在完成一组其他线程执行的操作之前，它允许一个或多个线程阻塞等待。</p>
<p>CountDownLatch 使用给定的计数初始化，核心的两个方法: await() 和 countDown()<br>前者可以实现给定计数倒数一次，后者是等待计数倒数到 0，如果没有到达 0，就一直阻塞等待。</p>
<p><img src="/images/blog/2019-05-10-3.png" alt> </p>
<p>使用说明: </p>
<p>count 初始化 CountDownLatch，然后需要等待的线程调用 await 方法。await 方法会一直受阻塞直到 count=0。<br>而其它线程完成自己的操作后，调用 countDown() 使计数器 count 减 1。当 count 减到 0 时，<br>所有在等待的线程均会被释放</p>
<p>说白了就是通过 count 变量来控制等待，如果 count 值为 0 (其他线程的任务都完成了)，那就可以继续执行</p>
<h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><h5 id="1-我等待其他线程"><a href="#1-我等待其他线程" class="headerlink" title="1. 我等待其他线程"></a>1. 我等待其他线程</h5><p>tu 现在去做实习生了，其他的员工还没下班，tu 不好意思先走，等其他的员工都走光了，tu 再走。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.concurrent.CountDownLatch;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Application</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 等5个其他员工线程，所以计数器设置为 5</span></span><br><span class="line">        <span class="keyword">final</span> CountDownLatch countDownLatch = <span class="keyword">new</span> CountDownLatch(<span class="number">5</span>);</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">"现在6点下班了....."</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// tu 线程启动</span></span><br><span class="line">        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    <span class="comment">// 这里调用的是await()不是wait()</span></span><br><span class="line">                    countDownLatch.await();</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">                System.out.println(<span class="string">"...其他的5个员工走光了，tu 终于可以走了"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 其他员工线程启动</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                    System.out.println(<span class="string">"员工 XXX 下班了"</span>);</span><br><span class="line">                    countDownLatch.countDown();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;).start();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/2019-05-10-4.png" alt></p>
<h5 id="2-其他线程等我"><a href="#2-其他线程等我" class="headerlink" title="2. 其他线程等我"></a>2. 其他线程等我</h5><p>tu 现在负责仓库模块功能，但是能力太差了，写得很慢，别的员工都需要等 tu 写好了才能继续往下写。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.concurrent.CountDownLatch;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Application</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 只等 tu 线程，所以计数器设置为 1</span></span><br><span class="line">        <span class="keyword">final</span> CountDownLatch countDownLatch = <span class="keyword">new</span> CountDownLatch(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">"现在6点下班了....."</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// tu 线程启动</span></span><br><span class="line">        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    Thread.sleep(<span class="number">5</span>);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">                System.out.println(<span class="string">"tu 终于写完了"</span>);</span><br><span class="line">                countDownLatch.countDown();</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 其他员工线程启动</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                    System.out.println(<span class="string">"其他员工需要等待tu"</span>);</span><br><span class="line">                    <span class="keyword">try</span> &#123;</span><br><span class="line">                        countDownLatch.await();</span><br><span class="line">                    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                        e.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">                    System.out.println(<span class="string">"tu 终于写完了，其他员工可以开始了！"</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;).start();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/2019-05-10-5.png" alt></p>
<h4 id="深入理解"><a href="#深入理解" class="headerlink" title="深入理解"></a>深入理解</h4><p>对源码的理解可以参看 crossoverJie 的 <a href="https://mp.weixin.qq.com/s/JryG3UTj90rDORDa_KHhhw" target="_blank" rel="noopener">什么是CountDownLatch？</a>。 写得非常通俗易懂。</p>
<h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><p>结合线程安全的 map 容器，基于 test-and-set 机制，CountDownLatch 可以实现基本的互斥锁，原理如下：</p>
<ol>
<li><p>初始化：CountDownLatch 初始化计数为1</p>
</li>
<li><p>test过程：线程首先将临界资源作为key，latch作为value尝试插入线程安全的map中。如果返回失败，表示其他线程已经持有了该锁，调用await方法阻塞到该latch上，等待其他线程释放锁；</p>
</li>
<li><p>set过程：如果返回成功，就表示已经持有该锁，其他线程必然插入失败。持有该锁之后执行各种操作，执行完成之后释放锁，释放锁首先将map中对应的KeyValue移除，再调用latch的countDown方法，该方法会将计数减1，变为0之后就会唤醒其他阻塞线程。</p>
</li>
</ol>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/JryG3UTj90rDORDa_KHhhw" target="_blank" rel="noopener">什么是CountDownLatch？</a></li>
<li><a href="https://mp.weixin.qq.com/s/77yJj3amPr6Q2VQJxVAS2Q" target="_blank" rel="noopener">Java多线程打辅助的三个小伙子</a></li>
<li><a href="http://www.codeceo.com/article/hbase-transaction.html" target="_blank" rel="noopener">HBase 事务和并发控制机制原理</a></li>
</ul>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title>两阶段加锁协议</title>
    <url>/2019/05/06/Two-Phase-Locking/</url>
    <content><![CDATA[<p>两阶段加锁协议</p>
<hr>
<h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><p>并发控制: 所谓并发控制，是指多用户共享的系统中，许多用户可能同时对同一数据进行操作。</p>
<p>调度: 指的是事务的执行次序。</p>
<p>串行调度: 多个事务依次串行执行，且只有当一个事务的所有操作都执行完后才执行另一个事务的所有操作。<br>只要是串行调度，执行的结果都是正确的。</p>
<p>并行调度: 利用分时的方法同时处理多个事务。但是并行调度的调度结果可能是错误的，<br>可能产生不一致的状态，包括有: 丢失修改，不可重复读和读脏数据。</p>
<p>可串行化: 调度是一个或多个事务的重要操作按时间排序的一个序列。<br>如果一个调度的动作首先是一个事务的所有动作，然后是另一个事务的所有动作，以此类推，而没有动作的混合，那么我们说这一调度是串行的。</p>
<p>其实比较明显的是虽然串行调度能够保证调度结果的正确性，<br>但是却限制了系统并行性的发挥，不能有效利用资源，<br>但是并行调度的调度结果又可能出现错误，<br>而且可能不具有串行，正是因为这样，有一个具有串行调度效果的并行调度方法，<br>而两段锁协议就是保证并行事务可串化的方法。</p>
<p>如果一并行调度的结果等价于某一串行调度的结果，那么这个并行调度称为可串行化的</p>
<h4 id="规则"><a href="#规则" class="headerlink" title="规则"></a>规则</h4><p>两段锁协议规定所有的事务应遵守的规则：</p>
<blockquote>
<ul>
<li>① 在对任何数据进行读、写操作之前，首先要申请并获得对该数据的封锁。</li>
<li>② 在释放一个封锁之后，事务不再申请和获得其它任何封锁。</li>
</ul>
</blockquote>
<h4 id="阶段"><a href="#阶段" class="headerlink" title="阶段"></a>阶段</h4><p>两段锁协议：是指所有的事务必须分两个阶段对数据项加锁和解锁。</p>
<p>事务的执行分为两个阶段: </p>
<blockquote>
<ul>
<li>第一阶段是获得封锁的阶段，称为扩展阶段。</li>
<li>第二阶段是释放封锁的阶段，称为收缩阶段。</li>
</ul>
</blockquote>
<p>第一阶段是事务可以获得任何数据项上的任何类型的锁，但是不能释放。</p>
<p>其实也就是该阶段可以进入加锁操作，在对任何数据进行读操作之前要申请获得S锁，<br>在进行写操作之前要申请并获得X锁，加锁不成功，则事务进入等待状态，直到加锁成功才继续执行。<br>就是加锁后就不能解锁了。</p>
<p>第二阶段是释放封锁，事务可以释放任何数据项上的任何类型的锁，但不能申请。</p>
<p>当事务释放一个封锁后，事务进入封锁阶段，在该阶段只能进行解锁而不能再进行加锁操作。</p>
<p>定理: 若所有事务均遵守两段锁协议，则这些事务的所有交叉调度都是可串行化的。</p>
<p><img src="/images/blog/2019-05-06-5.png" alt></p>
<h4 id="死锁"><a href="#死锁" class="headerlink" title="死锁"></a>死锁</h4><p>遵循两段锁协议的事务有可能发生死锁。</p>
<p>两个事务都坚持请求加锁对方已经占有的数据，导致死锁。</p>
<p>为此，又有了一次封锁法。一次封锁法要求事务必须一次性将所有要使用的数据全部加锁，否则就不能继续执行。</p>
<p>因此，一次封锁法遵守两段锁协议，但两段锁并不要求事务必须一次性将所有要使用的数据全部加锁，<br>这一点与一次性封锁不同，这就是遵守两段锁协议仍可能发生死锁的原因所在。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>串行调度效果的并行调度方法。</p>
<p>简单的理解就是两段锁，就是分为两个阶段<br>第一阶段 只能去申请锁<br>第二阶段 只能去释放锁</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.jianshu.com/p/5c78f5c4d57b" target="_blank" rel="noopener">zookeeper学习之一：两段锁协议</a></li>
<li><a href="https://blog.csdn.net/sinat_36841379/article/details/75107985" target="_blank" rel="noopener">两段锁协议</a></li>
<li><a href="https://www.cnblogs.com/3013218061shang/p/5573476.html" target="_blank" rel="noopener">数据库管理（事务、ACID、并发、封锁、可串行化、隔离）（转）</a></li>
</ul>
]]></content>
      <categories>
        <category>Distributed</category>
      </categories>
  </entry>
  <entry>
    <title>桶排序</title>
    <url>/2019/05/01/bucket-sort/</url>
    <content><![CDATA[<p>桶排序(Bucket Sort)是时间复杂度为O(n)的排序</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>排序算法是每个程序员应该掌握的。</p>
<h4 id="适用范围"><a href="#适用范围" class="headerlink" title="适用范围"></a>适用范围</h4><p>待排序的元素能够均匀分布在某一个范围[MIN, MAX]之间。</p>
<h4 id="空间复杂度"><a href="#空间复杂度" class="headerlink" title="空间复杂度"></a>空间复杂度</h4><p>桶排序需要两个辅助空间：</p>
<ol>
<li><p>第一个辅助空间，是桶空间B</p>
</li>
<li><p>第二个辅助空间，是桶内的元素链表空间</p>
</li>
</ol>
<p>总的来说，空间复杂度是O(n)。</p>
<h4 id="关键步骤"><a href="#关键步骤" class="headerlink" title="关键步骤"></a>关键步骤</h4><p>桶排序有两个关键步骤: </p>
<blockquote>
<ol>
<li>扫描待排序数据A[N]，对于元素A[i]，放入对应的桶X</li>
<li>A[i]放入桶X，如果桶X已经有了若干元素，使用插入排序，将arr[i]放到桶内合适的位置</li>
</ol>
</blockquote>
<p>桶X内的所有元素，是一直有序的</p>
<p>插入排序是稳定的，因此桶内元素顺序也是稳定的。</p>
<p>PS: 桶排序是一种思想，桶里面的排序其实并不一定非得是插入排序。<br>将数组元素分配到多个桶后，可以使用不同的排序算法或递归应用桶排序算法，将每个桶单独排序。 </p>
<p>当arr[N]中的所有元素，都按照上述步骤放入对应的桶后，就完成了全量的排序。</p>
<p>举个例子</p>
<p>假设待排序的数组均匀分布在[0, 99]之间：</p>
<p>{5,18,27,33,42,66,90,8,81,47,13,67,9,36,62,22}</p>
<p>可以设定10个桶，申请额外的空间bucket[10]来作为辅助空间。其中，每个桶bucket[i]来存放[10 * i, 10 * i + 9]的元素链表。</p>
<p><img src="/images/blog/2019-05-01-3.png" alt> </p>
<p>上图所示:</p>
<blockquote>
<ul>
<li>待排序的数组为unsorted[16]</li>
<li>桶空间是buket[10]</li>
<li>扫描所有元素之后，元素被放到了自己对应的桶里</li>
<li>每个桶内，使用插入排序，保证一直是有序的</li>
</ul>
</blockquote>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>桶排序(Bucket Sort)，总结:</p>
<p>是一种复杂度为O(n)的排序</p>
<p>是一种稳定的排序</p>
<p>适用于数据均匀分布在一个区间内的场景</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/r8mArZzAJ-uC002z48HWJw" target="_blank" rel="noopener">拜托，面试别再问我桶排序了！！！</a></li>
</ul>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title>计数排序</title>
    <url>/2019/05/01/counting-sort/</url>
    <content><![CDATA[<p>计数排序(Counting Sort)是时间复杂度为O(n)的排序</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>排序算法是每个程序员应该掌握的。</p>
<h4 id="适用范围"><a href="#适用范围" class="headerlink" title="适用范围"></a>适用范围</h4><p>待排序的元素在某一个范围[MIN, MAX]之间。</p>
<p><strong>计数排序只适合元素是整数，小规模的排序。</strong></p>
<h4 id="空间复杂度"><a href="#空间复杂度" class="headerlink" title="空间复杂度"></a>空间复杂度</h4><p>计数排序需要一个辅助空间，空间大小为O(MAX-MIN)，用来存储所有元素出现次数（计数）。</p>
<p>计数排序的核心是，空间换时间。</p>
<h4 id="关键步骤"><a href="#关键步骤" class="headerlink" title="关键步骤"></a>关键步骤</h4><blockquote>
<ol>
<li>扫描待排序数据arr[N]，使用计数数组counting[MAX-MIN]，对每一个arr[N]中出现的元素进行计数</li>
<li>扫描计数数组counting[]，还原arr[N]，排序结束</li>
</ol>
</blockquote>
<p>举个例子</p>
<p>假设待排序的数组，</p>
<p>arr={5, 3, 7, 1, 8, 2, 9, 4, 7, 2, 6, 6, 2, 6, 6}</p>
<p>很容易发现，待排序的元素在[0, 10]之间，可以用counting[0, 10]来存储计数。</p>
<p>第一步: 统计计数</p>
<p><img src="/images/blog/2019-05-01-1.png" alt></p>
<p>扫描未排序的数组arr[N]，对每一个出现的元素进行计数。</p>
<p>扫描完毕后，计数数组counting[0, 10]会变成上图中的样子。</p>
<p>如粉色示意，6这个元素在arr[N]中出现了4次，在counting[0, 10]中，下标为6的位置计数是4。</p>
<p>第二步: 还原数组</p>
<p><img src="/images/blog/2019-05-01-2.png" alt></p>
<p>扫描计数数组counting[0, 10]，通过每个元素的计数，还原arr[N]。</p>
<p>如上图粉色示意，count[0, 10]下标为6的位置计数是4，排完序是4个连续的6。</p>
<p>从counting下标MIN到MAX，逐个还原，填满arr[N]时，排序结束。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>计数排序，时间复杂度为O(n)。</p>
<p>当待排序元素个数很多，但值域范围很窄时，计数排序是很节省空间的。</p>
<p>计数排序只适合元素是整数，小规模的排序。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/KU-AUGOnLeRtE_hivl2uSA" target="_blank" rel="noopener">拜托，面试别再问我计数排序了！！！</a></li>
</ul>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title>基数排序</title>
    <url>/2019/05/01/radix-sort/</url>
    <content><![CDATA[<p>基数排序(Radix Sort)是时间复杂度为O(n)的排序</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>时间复杂度为O(n)的排序，常见的有三种：</p>
<p>基数排序(Radix Sort)</p>
<p>计数排序(Counting Sort)</p>
<p>桶排序(Bucket Sort)</p>
<p>计数排序－＞桶排序－＞基数排序，三者的排序思想是相通的，是逐渐复杂，使用性更广的。</p>
<p>基数排序分为两类:</p>
<blockquote>
<ul>
<li>最低位优先法，简称LSD法</li>
<li>最高位优先法，简称MSD法</li>
</ul>
</blockquote>
<h4 id="适用范围"><a href="#适用范围" class="headerlink" title="适用范围"></a>适用范围</h4><p>整数、字符串。</p>
<h4 id="空间复杂度"><a href="#空间复杂度" class="headerlink" title="空间复杂度"></a>空间复杂度</h4><p>O(n)</p>
<h4 id="关键步骤"><a href="#关键步骤" class="headerlink" title="关键步骤"></a>关键步骤</h4><p>LSD 先从最低位开始排序，再对次低位排序，直到对最高位排序后得到一个有序序列。</p>
<p>两个关键要点：</p>
<p>(1) 基: 被排序的元素的“个位”“十位”“百位”，暂且叫“基”；</p>
<p>(2) 桶: “基”的每一位，都有一个取值范围，例子中“基”的取值范围是0-9共10种，所以需要10个桶（bucket），来存放被排序的元素；</p>
<p>LSD 步骤为：</p>
<p>FOR (每一个基) {</p>
<p>//循环内，以某一个“基”为依据<br>第一步：遍历数据集arr，将元素放入对应的桶bucket<br>第二步：遍历桶bucket，将元素放回数据集arr</p>
<p>}</p>
<p>举个例子</p>
<p>假设待排序的数组arr={72, 11, 82, 32, 44, 13, 17, 95, 54, 28, 79, 56}</p>
<p><img src="/images/blog/2019-05-01-4.png" alt></p>
<p>例子中“基”的个数是2（个位和十位）, 所以  FOR循环会执行两次。</p>
<p>【第一次：以“个位”为依据】</p>
<p><img src="/images/blog/2019-05-01-5.png" alt></p>
<p>上图中标红的部分，个位为“基”。</p>
<p>第一步：遍历数据集arr，将元素放入对应的桶bucket；</p>
<p><img src="/images/blog/2019-05-01-6.png" alt></p>
<p>操作完成之后，各个桶会变成上面这个样子，即：个位数相同的元素，会在同一个桶里。</p>
<p>第二步：遍历桶bucket，将元素放回数据集arr；需要注意，先入桶的元素要先出桶。</p>
<p><img src="/images/blog/2019-05-01-10.png" alt></p>
<p>操作完成之后，数据集会变成上面这个样子，即：整体按照个位数排序了, 个位数小的在前面，个位数大的在后面。</p>
<p>【第二次：以“十位”为依据】</p>
<p><img src="/images/blog/2019-05-01-7.png" alt></p>
<p>上图中标红的部分，十位为“基”。</p>
<p>第一步：依然遍历数据集arr，将元素放入对应的桶bucket；</p>
<p><img src="/images/blog/2019-05-01-8.png" alt></p>
<p>操作完成之后，各个桶会变成上面这个样子，即：十位数相同的元素，会在同一个桶里。</p>
<p>第二步：依然遍历桶bucket，将元素放回数据集arr；</p>
<p>操作完成之后，数据集会变成上面这个样子，即：整体按照十位数也排序了。</p>
<p>十位数小的在前面，十位数大的在后面。</p>
<p><img src="/images/blog/2019-05-01-9.png" alt></p>
<p>首次按照个位从小到大，第二次按照十位从小到大，即：排序结束。</p>
<p>MSD 与LSD相反，从高位向地位开始比较，是由高位数为基底开始进行分配，<br>但在分配之后并不马上合并回一个数组中，<br>而是在每个“桶子”中建立“子桶”，<br>将每个桶子中的数值按照下一数位的值分配到“子桶”中,在进行完最低位数的分配后再合并回单一的数组中。</p>
<p>举个例子</p>
<p>假设待排序的数组arr={15, 25, 105, 78, 34, 21, 32, 41}</p>
<p>从最高位百位依次入桶，只有105有百位，其他百位按0算；</p>
<p>检测每个桶中的数据。当桶中的元素个数多于1个的时候，要对这个桶递归进行下一位的分组。</p>
<p><img src="/images/blog/2019-05-01-11.png" alt></p>
<p>如果使用词典排序来对可变长度整数表示进行排序，则从1到10的数字的表示将被输出为1,10,2,3,4,5,6,7,8,9，<br>如同较短的按键左对齐，并在右侧用空白字符填充以使短按键与最长按键一样长，以确定排序顺序。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>基的选取，可以先从个位开始，也可以先从十位开始，结果是一样的；</p>
<p>基数排序，是一种稳定的排序；</p>
<p>时间复杂度，可以认为是线性的O(n)；</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/Z8gU9QLpMnA-zoMc9ZeR2w" target="_blank" rel="noopener">拜托，面试别再问我基数排序了！！！</a></li>
<li><a href="https://blog.csdn.net/sb985/article/details/79921571" target="_blank" rel="noopener">计数排序，桶排序，基数排序</a></li>
<li><a href="https://blog.csdn.net/u011948899/article/details/78027838" target="_blank" rel="noopener">基数排序（LSD+MSD）详解</a></li>
</ul>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title>HBase 行锁与多版本并发控制 (MVCC)</title>
    <url>/2019/04/29/hbase-mvcc/</url>
    <content><![CDATA[<p><a href="https://en.wikipedia.org/wiki/Multiversion_concurrency_control" target="_blank" rel="noopener">MVCC (Multiversion Concurrency Control)</a>，即多版本并发控制技术，<br>它使得大部分支持行锁的事务引擎不再单纯的使用行锁来进行数据库的并发控制。</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>HBase 是 BigTable 的开源实现，事务模型也与 BigTable 一脉相承 – 仅支持行级别的事务。</p>
<p>Jeff Dean大神在接受采访时公开承认目前在技术领域最后悔的事情就是没有在BigTable中加入跨行事务模型，<br>以至于之后很多团队都在BigTable之上重复造各种各样的分布式事务轮子。</p>
<h4 id="为什么-HBase-需要并发控制"><a href="#为什么-HBase-需要并发控制" class="headerlink" title="为什么 HBase 需要并发控制"></a>为什么 HBase 需要并发控制</h4><p>HBase 对需要并发控制的数据有什么保证？</p>
<p>答案是 HBase 保证每行的 ACID 特性。</p>
<p>ACID的含义: </p>
<blockquote>
<ul>
<li>原子性(Atomicity): 事务的所有操作要么全部成功要么全部失败。</li>
<li>一致性(Consistency): 事务前后数据的完整性必须保持一致。</li>
<li>隔离性(Isolation): 多个事务执行的时候相互之间不会产生影响彼此的执行。</li>
<li>持久性(Durability): 一旦事务被提交，数据便被持久化。</li>
</ul>
</blockquote>
<p>传统的关系型数据库一般都提供了跨越所有数据的 ACID 特性；</p>
<p>为了性能考虑，HBase只提供了基于单行的ACID。</p>
<p>在讲解HBase的MVCC之前，先了解一下现有的隔离级别，SQL 标准定义了4种隔离级别: </p>
<ol>
<li>read uncommitted    读未提交</li>
<li>read committed      读已提交</li>
<li>repeatable read     可重复读</li>
<li>serializable        可串行化</li>
</ol>
<p><strong>HBase 不支持跨行事务，目前只支持单行级别的 read uncommitted 和 read committed 隔离级别。</strong></p>
<h4 id="HBase-事务原子性保证"><a href="#HBase-事务原子性保证" class="headerlink" title="HBase 事务原子性保证"></a>HBase 事务原子性保证</h4><p>HBase 的写入主要分三步</p>
<ol>
<li>先写 WAL 日志   </li>
<li>先写入 memstore</li>
<li>再 Sync wal</li>
</ol>
<p>PS: 第一步没有刷盘，只是准备好 WAL 日志。这里你可以查看源码验证。</p>
<p>写入 memstore 异常很容易可以回滚，因此保证写入/更新原子性只需要保证写入WAL的原子性即可。</p>
<p>HBase 0.98 之前版本需要保证WAL写入的原子性并不容易，这由WAL的结构决定。</p>
<p>假设一个行级事务更新R行中的3列（c1, c2, c3），来看看之前版本和当前版本的WAL结构: </p>
<p>之前版本 WAL 结构:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;logseq1-for-edit1&gt;:&lt;KeyValue-for-edit-c1&gt;</span><br><span class="line"></span><br><span class="line">&lt;logseq2-for-edit2&gt;:&lt;KeyValue-for-edit-c2&gt;</span><br><span class="line"></span><br><span class="line">&lt;logseq3-for-edit3&gt;:&lt;KeyValue-for-edit-c3&gt;</span><br></pre></td></tr></table></figure>

<p>每个KV都会形成一个WAL单元，这样一行事务更新多少列就会产生多少个 WAL 单元。</p>
<p>在将这些WAL单元append到日志文件的时候，一旦出现宕机或其他异常，就会出现部分写入成功的情况，<br>原子性更新就无法保证。</p>
<p>当前版本 WAL 结构:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;logseq#-for-entire-txn&gt;:&lt;WALEdit-for-entire-txn&gt;</span><br><span class="line"></span><br><span class="line">&lt;logseq#-for-entire-txn&gt;:&lt;-1, 3, &lt;Keyvalue-for-edit-c1&gt;, &lt;KeyValue-for-edit-c2&gt;, &lt;KeyValue-for-edit-c3&gt;&gt;</span><br></pre></td></tr></table></figure>

<p>通过这种结构，每个事务只会产生一个WAL单元。这样就可以保证WAL写入时候的原子性。</p>
<h4 id="HBase-事务一致性保证"><a href="#HBase-事务一致性保证" class="headerlink" title="HBase 事务一致性保证"></a>HBase 事务一致性保证</h4><p>HBase 是一个强一致性数据库，不是“最终一致性”数据库，官网给出的介绍</p>
<p><img src="/images/blog/2019-05-06-1.png" alt> </p>
<blockquote>
<ul>
<li>每个值只出现在一个 Region</li>
<li>同一时间一个 Region 只分配给一个 RS</li>
<li>行内的 mutation 操作都是原子的</li>
</ul>
</blockquote>
<p>HBase 降低可用性提高了一致性。</p>
<p>当某台 RS fail 的时候，它管理的 Region failover 到其他 RS 时，<br>需要根据 WAL（Write-Ahead Logging）来 redo (redolog，有一种日志文件叫做重做日志文件)，<br>这时候进行 redo 的 Region 应该是不可用的，所以 HBase 降低了可用性，提高了一致性。</p>
<p>设想一下，如果 redo 的 Region 能够响应请求，那么可用性提高了，<br>则必然返回不一致的数据(因为 redo 可能还没完成)，那么 HBase 就降低一致性来提高可用性了。</p>
<h5 id="1-HBase-的强一致性"><a href="#1-HBase-的强一致性" class="headerlink" title="1. HBase 的强一致性"></a>1. HBase 的强一致性</h5><p>先假设 HDFS 的副本存储策略，也就是dfs.replication的值为3（默认值就是3）</p>
<p>那么，HBase 的存储实例，也就是 HFile 也有3个副本。</p>
<p>那么当某一个 RS 崩溃时，并不用担心数据的丢失，因为数据是存储在 HDFS 上，<br>哪怕崩溃的 RS 所在的 DN 上有一个副本，在其他 DN 上也还有2个副本。</p>
<p>HFile 是已经持久化在磁盘上了，而 HFile 是不能改变的。</p>
<p>更新的数据放在 memstore 中，当 memstore 达到阈值就 flush 到磁盘上，生成 HFile 文件，<br>而一旦生成HFile就是不可改变的</p>
<h5 id="2-WAL的一致性"><a href="#2-WAL的一致性" class="headerlink" title="2. WAL的一致性"></a>2. WAL的一致性</h5><p>WAL 是 memstore 里的数据在 RS 崩溃时得以恢复的保证。WAL 的实现是 HLog，<br>HLog 也是存储在 HDFS 上的，所以 RS 崩溃了也不会导致 HLog 的丢失，它也有备份。</p>
<p>每一次更新都会调用写日志的 sync() 方法，这个调用强迫写入日志的更新都会被文件系统确认。</p>
<h4 id="HBase-事务隔离性保证"><a href="#HBase-事务隔离性保证" class="headerlink" title="HBase 事务隔离性保证"></a>HBase 事务隔离性保证</h4><h5 id="写写并发控制"><a href="#写写并发控制" class="headerlink" title="写写并发控制"></a>写写并发控制</h5><h6 id="1-为什么需要写写并发控制？"><a href="#1-为什么需要写写并发控制？" class="headerlink" title="1. 为什么需要写写并发控制？"></a>1. 为什么需要写写并发控制？</h6><p>现在假设有两个并发写入请求同时进来，分别对同一行数据进行写入。</p>
<p>下图所示 RowKey 为 Greg，现在分别更新列族 info 下的 Company 列和 Role 列:</p>
<p><img src="/images/blog/2019-04-30-1.png" alt> </p>
<p>如果没有任何并发控制策略的话，写入数据（先写WAL，再写memstore）可能会出现不同KV写入”交叉”现象，如下图所示</p>
<p><img src="/images/blog/2019-04-30-2.png" alt> </p>
<p>这样的话，用户最终读取到的数据就会产生不一致，如下:</p>
<p><img src="/images/blog/2019-04-30-3.png" alt> </p>
<h6 id="2-如何实现写写并发控制？"><a href="#2-如何实现写写并发控制？" class="headerlink" title="2. 如何实现写写并发控制？"></a>2. 如何实现写写并发控制？</h6><p>实现写写并发其实很简单，只需要在写入（或更新）之前先获取行锁，<br>如果获取不到，说明已经有其他线程拿了该锁，就需要不断重试等待或者自旋等待，<br>直至其他线程释放该锁。</p>
<p>拿到锁之后开始写入数据，写入完成之后释放行锁即可。</p>
<p>这种行锁机制是实现写写并发控制最常用的手段。</p>
<p>最简单的方式是提供一个基于行的独占锁来保证对同一行写的独立性。</p>
<blockquote>
<ul>
<li>(0) 获取行锁</li>
<li>(1) 写WAL文件</li>
<li>(2) 更新MemStore：将每个cell写入到memstore</li>
<li>(3) 释放行锁</li>
</ul>
</blockquote>
<h6 id="3-如何实现批量写入多行的写写并发？"><a href="#3-如何实现批量写入多行的写写并发？" class="headerlink" title="3. 如何实现批量写入多行的写写并发？"></a>3. 如何实现批量写入多行的写写并发？</h6><p>HBase 支持批量写入（或批量更新），即一个线程同时更新同一个Region中的多行记录。</p>
<p>那如何保证当前事务中的批量写入与其他事务中的批量写入的并发控制呢？</p>
<p>思路还是一样的，使用行锁。</p>
<p>但这里需要注意的是必须使用两阶段锁协议: </p>
<p>(1) 获取所有待写入（更新）行记录的行锁</p>
<p>(2) 开始执行写入（更新）操作</p>
<p>(3) 写入完成之后再统一释放所有行记录的行锁</p>
<p>不能更新一行锁定（释放）一行，多个事务之间容易形成死锁。</p>
<p>题外话: 关于两阶段锁协议可以看下 <a href="https://www.cnblogs.com/zszmhd/p/3365220.html" target="_blank" rel="noopener">两阶段锁协议</a> 和 范神的 <a href="http://hbasefly.com/2017/07/26/transaction-2/" target="_blank" rel="noopener">数据库事务系列－HBase行级事务模型</a> FARMERJOHN 的评论</p>
<p>支持一个 Region 的多行。跨 Region 多行事务不支持</p>
<h6 id="4-范神疑惑"><a href="#4-范神疑惑" class="headerlink" title="4. 范神疑惑"></a>4. 范神疑惑</h6><p><img src="/images/blog/2019-04-30-4.png" alt> </p>
<p>针对这里。当初也没想明白，事后不知道在哪看到一个回答提到因为版本信息客户端可以修改为自己的时间。<br>因此 HBase 需要自己的机制来保障。 个人觉得这个回答是合理的。记录下来。</p>
<h5 id="读写并发控制"><a href="#读写并发控制" class="headerlink" title="读写并发控制"></a>读写并发控制</h5><h6 id="1-为什么需要读写并发控制？"><a href="#1-为什么需要读写并发控制？" class="headerlink" title="1. 为什么需要读写并发控制？"></a>1. 为什么需要读写并发控制？</h6><p>现在我们通过在写入更新之前加锁、写入更新之后释放锁实现写写并发控制，<br>那读写之间是不是也需要一定的并发控制呢？如果不加并发控制，会出现什么现象呢？接着看下图:</p>
<p><img src="/images/blog/2019-04-30-5.png" alt> </p>
<p>上图分别是两个事务更新同一行数据，现在假设第一个事务已经更新完成，<br>在第二个事务更新到一半的时候进来一个读请求，如果没有任何并发控制的话，<br>读请求就会读到不一致的数据，Company 列为 Restaurant，Role 列为 Engineer，如下图所示:</p>
<p><img src="/images/blog/2019-04-30-6.png" alt> </p>
<p>可见，读写之间也需要一种并发控制来保证读取的数据总能够保持一致性，不会出现各种诡异的不一致现象。</p>
<h6 id="2-如何实现读写并发控制？"><a href="#2-如何实现读写并发控制？" class="headerlink" title="2. 如何实现读写并发控制？"></a>2. 如何实现读写并发控制？</h6><p>实现读写并发最简单的方法就是仿照写写并发控制 – 加锁。<br>但几乎所有数据库都不会这么做，性能太差，对于读多写少的应用来说必然不可接受。</p>
<p>MVCC机制 – Mutil Version Concurrent Control 就闪亮登场了。</p>
<p>MVCC 又称为乐观锁，它在读取数据项时，不加锁；与 MVCC 相对，基于锁的并发控制机制称为悲观锁；</p>
<p>HBase 中 MVCC 机制实现主要分为两步：</p>
<p>(1) 为每一个写（更新）事务分配一个Region级别自增的序列号</p>
<p>(2) 为每一个读请求分配一个已完成的最大写事务序列号</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">对于写</span><br><span class="line"></span><br><span class="line">w1 获取到行锁后，每个写操作立刻被分配一个 write number</span><br><span class="line"></span><br><span class="line">w2 写中的每个 cell 存储其 write number</span><br><span class="line"></span><br><span class="line">w3 写操作完成以后要申明其已经完成了</span><br><span class="line"></span><br><span class="line">对于读</span><br><span class="line"></span><br><span class="line">r1 每个写操作首先被分配一个读时间戳，叫做 read point</span><br><span class="line"></span><br><span class="line">r2 每个 read point 被分配一个最大整数。这个最大整数是大于等于已完成 write number 的整数</span><br><span class="line"></span><br><span class="line">r3 某个（行，列）组合的读取 read 返回数据单元格，其匹配（行，列）的写入编号是小于或等于读取点的最大值 read</span><br></pre></td></tr></table></figure>

<p>示意图如下所示:</p>
<p><img src="/images/blog/2019-04-30-7.png" alt> </p>
<p>上图中两个写事务分别分配了序列号1和序列号2，读请求进来的时候事务1已经完成，事务2还未完成，<br>因此分配事务1对应的序列号1给读请求。此时序列号1对本次读可见，序列号2对本次读不可见，读到的数据是:</p>
<p><img src="/images/blog/2019-04-30-8.png" alt> </p>
<p>具体实现中，所有的事务都会生成一个Region级别的自增序列，并添加到队列中，<br>如下图最左侧队列，其中最底端为已经提交的事务，<br>队列中的事务为未提交事务。<br>现假设当前事务编号为15，并且写入完成（中间队列红色框框），<br>但之前的写入事务还未完成（序列号为12、13、14的事务还未完成），<br>此时当前事务必须等待，而且对读并不可见，<br>直至之前所有事务完成之后才会对读可见（即读请求才能读取到该事务写入的数据）。<br>如最右侧图，15号事务之前的所有事务都成功完成，<br>此时Read Point就会移动到15号事务处，表示15号事务之前的所有改动都可见。</p>
<p><img src="/images/blog/2019-04-30-9.png" alt></p>
<pre><code>为什么提交WriteNumber时，会出现调整后的ReadNumber小于本次写操作所分配的WriteNumber呢？

这是因为并发写入时，多个线程的写入速度是随机的，可能存在WriteNumber比较大（假设值为x）
的写入操作比WriteNumber较小的（假设值为y）写入操作先结束了，但此时并不能将ReadNumber的值调整为x，
因为此时还存在WriteNumber比x小的写入操作正在进行中，
ReadNumber为x即表示MemStore中所有WriteNumber小于或等于x的数据都可以被读取了，
但实际上还有值没有被写入完成，可能会出现数据不一致的情况，
所以如果写队列中WriteNumber比较大的写入操作如果较快的结束了，
则需要进行相应的等待，直到写队列中它前面的那些写入操作完成为止。</code></pre><p>自增序列就是 SequenceId</p>
<p>MVCC 的精髓是写入的时候分配递增版本信息（SequenceId），<br>读取的时候分配唯一的版本用于读取可见，比之大的版本不可见。</p>
<p>这里需要注意版本必须递增，而且版本递增的范围一定程度上决定了事务是什么事务，<br>比如 HBase 是 Region 级别的递增版本，那么事务就是 Region 级别事务。<br>MySQL 中版本是单机递增版本，那么 MySQL 事务就支持单机跨行事务。<br>Percolator 中版本是集群递增版本，那么 Percolator 事务就是分布式事务。</p>
<h4 id="HBase-事务持久性保证"><a href="#HBase-事务持久性保证" class="headerlink" title="HBase 事务持久性保证"></a>HBase 事务持久性保证</h4><p>HBase 事务持久化可以理解为 WAL 持久化。</p>
<p>HBase 中可以通过设置 WAL 的持久化等级决定是否开启 WAL 机制、以及 HLog 的落盘方式。</p>
<ol>
<li><p>SKIP_WAL：只写缓存，不写HLog日志。这种方式因为只写内存，因此可以极大的提升写入性能，但是数据有丢失的风险。在实际应用过程中并不建议设置此等级，除非确认不要求数据的可靠性。</p>
</li>
<li><p>ASYNC_WAL：异步将数据写入HLog日志中。</p>
</li>
<li><p>SYNC_WAL：同步将数据写入日志文件中，需要注意的是数据只是被写入文件系统中，并没有真正落盘。</p>
</li>
<li><p>FSYNC_WAL：同步将数据写入日志文件并强制落盘。最严格的日志写入等级，可以保证数据不会丢失，但是性能相对比较差。目前实现中 FSYNC_WAL 并没有实现！</p>
</li>
</ol>
<p>用户可以通过客户端设置WAL持久化等级。默认如果用户没有指定持久化等级，HBase 使用 SYNC_WAL 等级持久化数据。</p>
<h4 id="外话"><a href="#外话" class="headerlink" title="外话"></a>外话</h4><p>HBase 也同样支持 read uncommitted 级别，也就是我们在查询的时候将 scan 的 MVCC 值设置为一个超大的值，大于目前所有申请的 MVCC 值，<br>那么查询时同样会返回正在写入的数据。</p>
<p><img src="/images/blog/2019-05-06-2.png" alt></p>
<p><img src="/images/blog/2019-05-06-3.png" alt></p>
<p><img src="/images/blog/2019-05-06-4.png" alt></p>
<p>获取了 readPoint，注意只有在 READ_COMMITTED 下，<br>readpoint 才是生效的，然后会对 memstore 和 storeFile 进行查询，<br>过滤掉大于这次 readPoint 的数据，防止读取到还没提交成功的数据，也就是 READ_COMMITTED 读已提交。</p>
<p>如果是 READ_UNCOMMITTED 则 readPoint 会返回 Long.MAX_VALUE，就有会读到最新的的数据包括没提交的</p>
<p>总结下MVCC算法下写操作的执行流程: </p>
<blockquote>
<ul>
<li>(0) 获取行锁</li>
<li>(0a) 获取写序号</li>
<li>(1) 写WAL文件</li>
<li>(2) 更新MemStore：将每个cell写入到 memstore，同时附带写序号</li>
<li>(2a) 申明以写序号完成操作</li>
<li>(3) 释放行锁</li>
</ul>
</blockquote>
<p>WAL写入的时候能保证一行数据的原子性了，为什么还需要加写锁？是因为memstore会有并发写的问题吗？</p>
<p>整个写的过程由很多子步骤 对同一行数据更新 在memstore层面也要保证原子性</p>
<h4 id="关于行级别的-ACID"><a href="#关于行级别的-ACID" class="headerlink" title="关于行级别的 ACID"></a>关于行级别的 ACID</h4><p>在之前的版本中，行级别的任何并发写入/更新都是互斥的，由一个行锁控制。但在2.0版本中，这一点行为发生了变化，多个线程可以同时更新一行数据，这里的考虑点为：</p>
<p>如果多个线程写入同一行的不同列族，是不需要互斥的 </p>
<p>多个线程写同一行的相同列族，也不需要互斥，即使是写相同的列，也完全可以通过HBase的MVCC机制来控制数据的一致性 </p>
<p>当然，CAS操作(如checkAndPut)或increment操作，依然需要独占的行锁</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.cnblogs.com/zszmhd/p/3365220.html" target="_blank" rel="noopener">两阶段锁协议</a></li>
<li><a href="http://hbasefly.com/2017/07/26/transaction-2/" target="_blank" rel="noopener">数据库事务系列－HBase行级事务模型</a></li>
<li><a href="http://hbasefly.com/2016/03/23/hbase_writer/" target="_blank" rel="noopener">HBase － 数据写入流程解析</a></li>
<li><a href="https://blog.csdn.net/ruichaoo/article/details/78961770" target="_blank" rel="noopener">HBase 强一致性详解</a></li>
<li><a href="https://hellokangning.github.io/en/post/multiversion-concurrency-control-in-hbase/#where-mvcc-is-used" target="_blank" rel="noopener">HBase中的多版本并发控制</a></li>
<li><a href="https://blog.csdn.net/huanggang028/article/details/46047927" target="_blank" rel="noopener">大数据技术-HBase：HBase并发版本控制MVCC</a></li>
<li><a href="https://blog.csdn.net/javaman_chen/article/details/7220216" target="_blank" rel="noopener">HBase之Java API</a></li>
<li><a href="http://hbase.apache.org/acid-semantics.html" target="_blank" rel="noopener">官方文档acid-semantics</a></li>
<li><a href="http://www.codeceo.com/article/hbase-transaction.html" target="_blank" rel="noopener">HBase 事务和并发控制机制原理</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>GitFlow 工作流</title>
    <url>/2019/04/27/gitflow/</url>
    <content><![CDATA[<p>工作中会用到的 Git 分支模型。</p>
<hr>
<h4 id="闲言"><a href="#闲言" class="headerlink" title="闲言"></a>闲言</h4><p>学生生涯比较 low 。没用过分之管理。都是直接在 master 分之上简单粗爆。</p>
<p>毕业后进入一个公司，可能工作是运维。发现身边同事也没有分之管理的意识。也是在 master 上简单粗爆。</p>
<p>偶然机会好朋友跟我分享说他们是使用 GitFlow 分之管理的。带着改变自己开发习惯的心态了解了一波。</p>
<p>GitFlow 工作流定义了一个围绕项目发布的严格分支模型。虽然比功能分支工作流复杂几分，<br>但提供了用于一个健壮的用于管理大型项目的框架。</p>
<p><img src="/images/blog/2019-04-27-2.png" alt></p>
<p>偌大的图觉得有些晕。别怕，下面来看下具体描述</p>
<h4 id="GitFlow-工作流"><a href="#GitFlow-工作流" class="headerlink" title="GitFlow 工作流"></a>GitFlow 工作流</h4><p>在你开始阅读之前，请记住：流程应被视作为指导方针，而非“铁律”。我们只是想告诉你可能的做法。<br>因此，如果有必要的话，你可以组合使用不同的流程</p>
<p><img src="/images/blog/2019-04-27-3.png" alt></p>
<h4 id="工作方式"><a href="#工作方式" class="headerlink" title="工作方式"></a>工作方式</h4><p>GitFlow 工作流仍然用中央仓库作为所有开发者的交互中心。和其它的工作流一样，开发者在本地工作并push分支到要中央仓库中。</p>
<h4 id="分支介绍"><a href="#分支介绍" class="headerlink" title="分支介绍"></a>分支介绍</h4><pre><code>master: 这个分支的代码是发布到生产的代码

develop: 这个分支的代码是预发布到生产的代码

release: 这个分支的代码是新版本发布到生产的代码

feature: 这个分支的代码是新需求开发的代码

hotfix: 这个分支的代码是紧急修复生产 bug 的代码</code></pre><p>常用的分支约定</p>
<blockquote>
<ul>
<li>用于新建发布分支的分支: develop</li>
<li>用于合并的分支: master</li>
<li>分支命名: release-* 或 release/*</li>
</ul>
</blockquote>
<h4 id="历史分支"><a href="#历史分支" class="headerlink" title="历史分支"></a>历史分支</h4><p>GitFlow 工作流使用2个分支来记录项目的历史。master 分支存储了正式发布的历史，<br>而 develop 分支作为功能的集成分支。这样也方便master分支上的所有提交分配一个版本号。</p>
<p><img src="/images/blog/2019-04-27-4.png" alt></p>
<h4 id="功能分支"><a href="#功能分支" class="headerlink" title="功能分支"></a>功能分支</h4><p>每个新功能位于一个自己的分支，这样可以push到中央仓库以备份和协作。<br>但功能分支不是从master分支上拉出新分支，而是使用develop分支作为父分支。<br>当新功能完成时，合并回develop分支。新功能提交应该从不直接与master分支交互。</p>
<p><img src="/images/blog/2019-04-27-5.png" alt></p>
<p><strong>注意，从各种含义和目的上来看，功能分支加上develop分支就是功能分支工作流的用法。但Gitflow工作流没有在这里止步。</strong></p>
<h4 id="发布分支"><a href="#发布分支" class="headerlink" title="发布分支"></a>发布分支</h4><p>一旦 develop 分支上有了做一次发布（或者说快到了既定的发布日）的足够功能，就从develop分支上fork一个发布分支。<br>新建的分支用于开始发布循环，所以从这个时间点开始之后新的功能不能再加到这个分支上 —— 这个分支只应该做Bug修复、文档生成和其它面向发布任务。<br>一旦对外发布的工作都完成了，发布分支合并到master分支并分配一个版本号打好Tag(所有在 master 分支上的 commit应该 Tag)。<br>另外，这些从新建发布分支以来的做的修改要合并回develop分支。</p>
<p>使用一个用于发布准备的专门分支，使得一个团队可以在完善当前的发布版本的同时，另一个团队可以继续开发下个版本的功能。<br>这也打造定义良好的开发阶段（比如，可以很轻松地说，『这周我们要做准备发布版本4.0』，并且在仓库的目录结构中可以实际看到）。</p>
<p><img src="/images/blog/2019-04-27-6.png" alt></p>
<h4 id="维护分支"><a href="#维护分支" class="headerlink" title="维护分支"></a>维护分支</h4><p>维护分支或说是热修复（hotfix）分支用于生成快速给产品发布版本（production releases）打补丁，这是唯一可以直接从master分支fork出来的分支。修复完成，修改应该马上合并回master分支和develop分支（当前的发布分支），master分支应该用新的版本号打好Tag。</p>
<p><img src="/images/blog/2019-04-27-7.png" alt></p>
<p>为Bug修复使用专门分支，让团队可以处理掉问题而不用打断其它工作或是等待下一个发布循环。你可以把维护分支想成是一个直接在master分支上处理的临时发布。</p>
<h4 id="场景设想"><a href="#场景设想" class="headerlink" title="场景设想"></a>场景设想</h4><p>下面列举一些可能你在工作中会经常面对的场景</p>
<ol>
<li><p>组长分配新需求下来，安排下周上线（假设是 1227 号），你看看当前有没有下周版本的分支？有的话很简单，checkout 下周分支（feature_app1.1.0_1227）来开发就行，没有的话这时需要新建分支，从 develop 分支创建新的 feature 分支（feature_app1.1.0_1227）,然后将对应的 pom.xml 版本号修改成 1.1.0-SNAPSHOT，注意命名，比如这里我用 feature 做前缀，你也可以自己设定一个规则。</p>
</li>
<li><p>开发完 feature_app1.1.0_1227 需求，移交了测试，很遗憾，测试出现了 n 个 bug，这时依旧在 feature_app1.1.0_1227 上修复 bug。</p>
</li>
<li><p>终于到了发版前一天，测试 MM 说 n 轮测试完了，没问题，拉上线版本，再做一次回归测试。这时，你就需要把 feature_app1.1.0_1227 分支合并到 develop 分支，然后从 develop 分支中创建新的分支 release_app1.1.0_1227，然后修改对应的版本号为 1.1.0-RELEASE。</p>
</li>
<li><p>到了发版日早上了，测试 MM 用了 release_app1.1.0_1227 版本测试了一番，又发现了一个 bug。别慌，只要不是生产的 bug，都好解决。这时你要在 release_app1.1.0_1227 修复 bug，切记不能在 feature_app1.1.0_1227 上修改，feature_app1.1.0_1227 分支已经没有多大作用了，只用来看代码提交记录。</p>
</li>
<li><p>安安全全的到了晚上，开始发版了，发完版突然发现了有异常，定位问题后发现是有一行代码写错了，跟组长确认后，在 release_app1.1.0_1227 分支上做了修改，重新打包后发版，验证了一段时间，没问题了。。。</p>
</li>
<li><p>发版总算完成了，这时，别忘记把 release_app1.1.0_1227 版本合并到 develop 和 master 分支。还有一点很重要的，把 develop 分支代码合并到 1227 以后的版本（如果已经有1227 以后的版本的话）。注意：这个步骤合并代码要谨慎，如果有别人的代码合并冲突比较大，需要找那个开发的同事一起合并代码。总算可以睡个好觉了。。。</p>
</li>
<li><p>告别了旧需求，迎来了新需求，接下来的需求开发就按上面的步骤走。。。</p>
</li>
<li><p>第二天，突然生产上一直报 NullPointerException，定位发现是一行代码没有判空导致的，三番确认，原来这个数据以前是不为空的，现在确实需要支持有些数据为空的，需要紧急修复这个 bug，和组长确认之后，从 master 分支上拉了一个 hotfix_app1.1.1_1228 分支代码，修复了 NullPointerException，打包后上线，验证没问题后，把 hotfix_app1.1.1_1228 分支合并到 develop 和 master 分支，并把 develop 分支合并到 1227 以后的版本。</p>
</li>
</ol>
<h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><p>下面的示例演示本工作流如何用于管理单个发布循环。假设你已经创建了一个中央仓库。</p>
<h5 id="1-创建开发分支"><a href="#1-创建开发分支" class="headerlink" title="1. 创建开发分支"></a>1. 创建开发分支</h5><p><img src="/images/blog/2019-04-27-8.png" alt></p>
<p>第一步为master分支配套一个develop分支。简单来做可以本地创建一个空的develop分支，push到服务器上</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git branch develop</span><br><span class="line">git push -u origin develop</span><br></pre></td></tr></table></figure>
<p>以后这个分支将会包含了项目的全部历史，而master分支将只包含了部分历史。其它开发者这时应该克隆中央仓库，建好develop分支的跟踪分支</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone url</span><br><span class="line">git checkout -b develop origin&#x2F;develop</span><br></pre></td></tr></table></figure>
<p>现在每个开发人员都有了这些历史分支的本地拷贝。</p>
<h5 id="2-工程师A和工程师B开始开发新功能"><a href="#2-工程师A和工程师B开始开发新功能" class="headerlink" title="2. 工程师A和工程师B开始开发新功能"></a>2. 工程师A和工程师B开始开发新功能</h5><p><img src="/images/blog/2019-04-27-9.png" alt></p>
<p>这个示例中，工程师A和工程师B开始各自的功能开发。他们需要为各自的功能创建相应的分支。新分支不是基于master分支，而是应该基于develop分支：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git checkout -b some-feature develop</span><br></pre></td></tr></table></figure>

<p>他们用老套路添加提交到各自功能分支上：编辑、暂存、提交</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git status</span><br><span class="line">git add</span><br><span class="line">git commit</span><br></pre></td></tr></table></figure>

<h5 id="3-工程师A完成功能开发"><a href="#3-工程师A完成功能开发" class="headerlink" title="3. 工程师A完成功能开发"></a>3. 工程师A完成功能开发</h5><p><img src="/images/blog/2019-04-27-10.png" alt></p>
<p>添加了提交后，工程师A觉得她的功能OK了。</p>
<p>如果团队使用Pull Requests，这时候可以发起一个用于合并到develop分支。</p>
<p>否则她可以直接合并到她本地的develop分支后push到中央仓库。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git pull origin develop</span><br><span class="line">git checkout develop</span><br><span class="line">git merge some-feature</span><br><span class="line">git push</span><br><span class="line">git branch -d some-feature</span><br></pre></td></tr></table></figure>

<p>git pull 命令在合并功能前确保develop分支是最新的。</p>
<p>注意，功能决不应该直接合并到master分支。</p>
<h5 id="4-工程师A开始准备发布"><a href="#4-工程师A开始准备发布" class="headerlink" title="4. 工程师A开始准备发布"></a>4. 工程师A开始准备发布</h5><p><img src="/images/blog/2019-04-27-11.png" alt></p>
<p>这个时候工程师B正在实现他的功能，工程师A开始准备她的第一个项目正式发布。</p>
<p>像功能开发一样，她用一个新的分支来做发布准备。这一步也确定了发布的版本号：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git checkout -b release-0.1 develop</span><br></pre></td></tr></table></figure>

<p>这个分支是清理发布、执行所有测试、更新文档和其它为下个发布做准备操作的地方，像是一个专门用于改善发布的功能分支。</p>
<p>只要工程师A创建这个分支并push到中央仓库，这个发布就是功能冻结的。任何不在develop分支中的新功能都推到下个发布循环中。</p>
<h5 id="5-工程师A完成发布"><a href="#5-工程师A完成发布" class="headerlink" title="5. 工程师A完成发布"></a>5. 工程师A完成发布</h5><p><img src="/images/blog/2019-04-27-12.png" alt></p>
<p>一旦准备好了对外发布，工程师A合并修改到master分支和develop分支上，删除发布分支。合并回develop分支很重要，因为在发布分支中已经提交的更新需要在后面的新功能中也要是可用的。</p>
<p>另外，如果工程师A的团队要求Code Review，这是一个发起Pull Request的理想时机。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git checkout master</span><br><span class="line">git merge release-0.1</span><br><span class="line">git push</span><br><span class="line">git checkout develop</span><br><span class="line">git merge release-0.1</span><br><span class="line">git push</span><br><span class="line">git branch -d release-0.1</span><br></pre></td></tr></table></figure>

<p>发布分支是作为功能开发（develop分支）和对外发布（master分支）间的缓冲。只要有合并到master分支，就应该打好Tag以方便跟踪。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git tag -a 0.1 -m &quot;Initial public release&quot; master</span><br><span class="line">git push --tags</span><br></pre></td></tr></table></figure>

<p>Git有提供各种勾子（hook），即仓库有事件发生时触发执行的脚本。可以配置一个勾子，在你push中央仓库的master分支时，自动构建好对外发布。</p>
<h5 id="6-最终用户发现Bug"><a href="#6-最终用户发现Bug" class="headerlink" title="6. 最终用户发现Bug"></a>6. 最终用户发现Bug</h5><p><img src="/images/blog/2019-04-27-13.png" alt></p>
<p>对外发布后，工程师A回去和工程师B一起做下个发布的新功能开发，直到有最终用户开了一个Ticket抱怨当前版本的一个Bug。</p>
<p>为了处理Bug，工程师A（或工程师B）从master分支上拉出了一个维护分支，提交修改以解决问题，然后直接合并回master分支：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git checkout -b issue-#001 master</span><br></pre></td></tr></table></figure>

<p>Fix the bug</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git checkout master</span><br><span class="line">git merge issue-#001</span><br><span class="line">git push</span><br></pre></td></tr></table></figure>

<p>就像发布分支，维护分支中新加这些重要修改需要包含到develop分支中，所以工程师A要执行一个合并操作。然后就可以安全地删除这个分支了：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git checkout develop</span><br><span class="line">git merge issue-#001</span><br><span class="line">git push</span><br><span class="line">git branch -d issue-#001</span><br></pre></td></tr></table></figure>

<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>Git是一个不错的版本管理工具，但也仅仅是工具。记住，这里演示的工作流只是可能用法的例子，而不是在实际工作中使用Git不可违逆的条例。</p>
<p>所以不要畏惧按自己需要对工作流的用法做取舍。不变的目标就是让Git为你所用。</p>
<p>一个标准的 Git 分支模型，在不管工作上还是学习上，在需要分支管理的时候，回忆起有这么一个图，根据你的场景再应用进去，肯定会少走很多弯路。</p>
<p><strong>最后我想了想，应该保持分之不删除。这样以后可以在 log 中和仓库中可以追溯历史。因原文有，所以我保留了，待我实践过这套流程后再回来修复</strong></p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.cnblogs.com/myqianlan/p/4195994.html" target="_blank" rel="noopener">Git基本命令和GitFlow工作流</a></li>
<li><a href="https://mp.weixin.qq.com/s/Ypf4umT0mWb0lC5GHgc0Lg" target="_blank" rel="noopener">成熟的 Git 分支模型</a></li>
<li><a href="http://www.cnblogs.com/cnblogsfans/p/5075073.html" target="_blank" rel="noopener">Git 在团队中的最佳实践–如何正确使用Git Flow</a></li>
</ul>
]]></content>
      <categories>
        <category>Tools</category>
      </categories>
  </entry>
  <entry>
    <title>信息类 APP 组合</title>
    <url>/2019/04/27/Information-class-APP-combination/</url>
    <content><![CDATA[<p>信息类 APP 获取的信息，是为了知识的产生提供支持。</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>手机的优势在于它的便携性，加上入口窄使得人更加专注，已经成为获取信息的中流砥柱</p>
<p>首先，弄清什么是信息。<br>其次，明确需要哪些信息。<br>最后，才是选择适合自己的的APP。</p>
<p>什么是信息呢</p>
<p>基于这个解释，我需要获取的信息，是为了知识的产生提供支持。而哪些信息可以成为我的知识呢，我终结了以下几点：</p>
<p>时事动向，包括新闻，IT、财经。<br>技术提升<br>个人成长，主要通过读书。<br>财经知识，所有的问题，归根结底还是钱的问题。<br>另外，我还单独拎出一个分类，它虽然不能直接为行动提供依据，但也不可忽视。<br>开心生活</p>
<h5 id="个人成长"><a href="#个人成长" class="headerlink" title="个人成长"></a>个人成长</h5><h5 id="1-得到"><a href="#1-得到" class="headerlink" title="1. 得到"></a>1. 得到</h5><p>罗辑思维的产品。有些观点很新颖，但是有些观点不太认同，学会保持自己的判断能力。</p>
<p>现在更多的是批判性地看罗胖胖一本正经的胡说八道</p>
<h5 id="2-微信"><a href="#2-微信" class="headerlink" title="2. 微信"></a>2. 微信</h5><p>持续关注优秀的微信公众号</p>
<h5 id="3-微信读书"><a href="#3-微信读书" class="headerlink" title="3. 微信读书"></a>3. 微信读书</h5><p>买了自动续费会员，当作对自己的投资。</p>
<h4 id="财经资讯"><a href="#财经资讯" class="headerlink" title="财经资讯"></a>财经资讯</h4><h5 id="1-同花顺"><a href="#1-同花顺" class="headerlink" title="1. 同花顺"></a>1. 同花顺</h5><p>关注股票的动态</p>
<h5 id="2-支付宝"><a href="#2-支付宝" class="headerlink" title="2. 支付宝"></a>2. 支付宝</h5><p>关注个人的基金状态。</p>
<h4 id="傻乐"><a href="#傻乐" class="headerlink" title="傻乐"></a>傻乐</h4><h5 id="1-虎扑"><a href="#1-虎扑" class="headerlink" title="1. 虎扑"></a>1. 虎扑</h5><p>直男社区。</p>
<h5 id="2-微博"><a href="#2-微博" class="headerlink" title="2. 微博"></a>2. 微博</h5><p>微博获取写官方热点。有时候会分享些生活上的小事。</p>
]]></content>
      <categories>
        <category>Life</category>
      </categories>
  </entry>
  <entry>
    <title>高效的一天可以这样过</title>
    <url>/2019/04/27/my-life/</url>
    <content><![CDATA[<p>高效的一天可以这样过</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>很丧，想要个不一样的生活方式。</p>
<h4 id="工具列表"><a href="#工具列表" class="headerlink" title="工具列表"></a>工具列表</h4><p>全部免费: </p>
<blockquote>
<ul>
<li><a href="https://itunes.apple.com/cn/app/habify-%E6%89%93%E5%8D%A1%E7%AD%BE%E5%88%B0%E5%8A%A9%E6%89%8B-%E4%B9%A0%E6%83%AF%E5%85%BB%E6%88%90%E6%8F%90%E9%86%92/id1438388363?mt=8" target="_blank" rel="noopener">Habify</a></li>
<li><a href="https://itunes.apple.com/cn/app/7fen-zhong-duan-lian-seven/id650276551?mt=8" target="_blank" rel="noopener">Seven</a></li>
<li><a href="https://www.yinxiang.com/" target="_blank" rel="noopener">Evernote</a></li>
<li><a href="https://todoist.com/app?lang=zh_CN" target="_blank" rel="noopener">Todoist</a></li>
<li><a href="https://www.pomotodo.com/" target="_blank" rel="noopener">番茄土豆</a></li>
<li><a href="https://www.igetget.com/download" target="_blank" rel="noopener">得到</a></li>
<li><a href="https://www.gotokeep.com/" target="_blank" rel="noopener">Keep</a></li>
<li><a href="https://itunes.apple.com/cn/app/id687625208" target="_blank" rel="noopener">iHour</a></li>
<li><a href="https://itunes.apple.com/cn/app/focus-matrix-%E5%9F%BA%E4%BA%8E%E5%9B%9B%E8%B1%A1%E9%99%90%E6%B3%95%E5%88%99%E7%9A%84%E4%BB%BB%E5%8A%A1%E7%AE%A1%E7%90%86%E5%99%A8/id1107872631?mt=8" target="_blank" rel="noopener">Focus Matrix</a></li>
</ul>
</blockquote>
<h4 id="7-00-－-7-30"><a href="#7-00-－-7-30" class="headerlink" title="7:00 － 7:30"></a>7:00 － 7:30</h4><p>每天起床睁开眼，大声说出梦想（挣钱）三遍，给新的一天注入强力。</p>
<p>如果起不来，打开网易云音乐放两首音乐即可。</p>
<p>起来后喝杯白开水。谷歌搜索<a href="https://www.google.com/search?q=%E5%96%9D%E7%99%BD%E5%BC%80%E6%B0%B4%E5%A5%BD%E5%A4%84&oq=%E5%96%9D%E7%99%BD%E5%BC%80%E6%B0%B4%E5%A5%BD%E5%A4%84&aqs=chrome..69i57j69i60l3j69i61l2.5201j0j9&sourceid=chrome&ie=UTF-8" target="_blank" rel="noopener">喝白开水好处</a>。<br>烧一壶水，等水开的过程中，做一组7分钟锻炼。</p>
<p>Seven 在这方面做的好 。锻炼的时长和强度都可以自己设置，但是不建议太过剧烈，晨间锻炼的目的在于叫醒身体，而不是增肌或减脂。</p>
<p>洗漱。左手刷牙。我是右撇子，平时刻意增加了左手的锻炼。</p>
<p>穿衣前会查下天气预报根据气温来选择着装。</p>
<h4 id="7-30-－-8-30"><a href="#7-30-－-8-30" class="headerlink" title="7:30 － 8:30"></a>7:30 － 8:30</h4><p>稍作休息后开始晨间规律。在旁边准备一杯金味麦片填下肚子。</p>
<p>首先朗读备忘录的每日一读文章。激励下自己。</p>
<p>接着写日记，晨间日记。无非是为了得到反馈，从而及时调整，指导未来。<br>所以，“日记”可分成，过去发生事件的“过去日记”，和今日应做之事及未来想执行事项的“未来日记”。</p>
<p>曾经我也是晚上做总结日记，但再听从《晨间日记的奇迹》的劝导并加以实践之后，我深刻体会到了<a href="https://www.jianshu.com/p/7ca0e80104c1" target="_blank" rel="noopener">晨间日记的优点</a>：</p>
<blockquote>
<ul>
<li>可以做好一天的准备（计划性）</li>
<li>可以正确地写出昨天所发生的事情（效率性＆忠实性）</li>
<li>冷静思考前一天的事情，可以中立地看待事情（中立性）</li>
<li>对于一个午餐和晚餐都要忙于迎合客户的上班族来说，早上是自己最自由自在的个人时间，不会让写日记的习惯中断（持续性）</li>
<li>可以将过去宝贵的经验或回忆，运用在当天（灵活运用性）</li>
</ul>
</blockquote>
<p>以下是我使用的<a href="https://pan.baidu.com/s/19IwcUPZEYUS68JgZXknC_g" target="_blank" rel="noopener">晨间日记模板</a>。</p>
<p>这9个维度，几乎涵盖了生活工作的方方面面。每天复制粘贴一份在 <a href="https://www.yinxiang.com/" target="_blank" rel="noopener">Evernote</a>，开启新一天的纪录工作。</p>
<p><img src="/images/blog/2019-04-26-2.png" alt></p>
<p>在总结过程中，针对昨天的所做作为，我努力回答以下5个问题:</p>
<blockquote>
<ul>
<li>做出了怎样的努力？效果如何？</li>
<li>学到了什么？怎样付诸实践？</li>
<li>有哪些需要改进的地方？</li>
<li>做了什么来投资未来？</li>
<li>对哪些人和事心存感激？</li>
</ul>
</blockquote>
<p>整理 todo list。我使用免费的 <a href="https://todoist.com/app?lang=zh_CN" target="_blank" rel="noopener">Todoist</a> ，<br>按照<a href="http://book.douban.com/subject/4849382/" target="_blank" rel="noopener">《搞定》</a>的指导，<br>将脑海中所有纠缠不清的事情归入。</p>
<blockquote>
<ul>
<li>一张工作任务清单</li>
<li>一个保存与工作有关资料的文件夹</li>
</ul>
</blockquote>
<p><img src="/images/blog/2019-04-28-1.png" alt></p>
<p>这一时间段的关注点是工作计划和生活中的待办事项，浏览并调整清单。PS: Qing’s Landing 好像好几个清单。我个人觉得我不需要所以只留一个。</p>
<h4 id="8-30-9-30"><a href="#8-30-9-30" class="headerlink" title="8:30 - 9:30"></a>8:30 - 9:30</h4><p>出门前默念”出门”，默念“1234”口诀，提醒自己带上”手机、耳机、电脑、工卡”。PS: 因为租房是自如的。密码锁不需要钥匙。</p>
<p>锁房间门，转身前顺带拉一下把手。接着在门口的镜子前整理下仪容，确定无误后转身出门。保门已锁好，免除后顾之忧。</p>
<p><img src="/images/blog/2019-04-26-3.png" alt></p>
<p>锁上大门，听到砰的一声就可以走人了。</p>
<p>因为离公司较远，有8公里，且附近没有地铁。只能坐公交。且因为直达公交人太多，基本放弃坐直达的公交。选择转一趟公交到公司。</p>
<p>公交时间会首听罗辑思维的60秒语音。接着听每日得到的罗辑思维。剩下时间就是网易云音乐每日推荐歌曲。</p>
<p>公交上不会看书。毕竟在晃动的环境里阅读会加快眼睛疲劳，得不偿失。</p>
<p>如果没有座位就站着，锻炼下站姿。据说还能提高气质。如果有座位，改掉坐公交老翘腿的坏毛病。</p>
<p>到公司后先花 10 分钟解决早餐问题。PS: 公司早餐实惠且丰富。</p>
<h4 id="9-30-－-10-00"><a href="#9-30-－-10-00" class="headerlink" title="9:30 － 10:00"></a>9:30 － 10:00</h4><p>查看邮件、告警、jira，继续完善工作方面的 todo list。PS: 邮件基本是废的，邮件大概率跟自己没有关系。</p>
<p>这一过程中，采用紧急性和重要性的”四象限法”，<a href="https://itunes.apple.com/cn/app/focus-matrix-%E5%9F%BA%E4%BA%8E%E5%9B%9B%E8%B1%A1%E9%99%90%E6%B3%95%E5%88%99%E7%9A%84%E4%BB%BB%E5%8A%A1%E7%AE%A1%E7%90%86%E5%99%A8/id1107872631?mt=8" target="_blank" rel="noopener">Focus Matrix</a>对其任务的优先级进行调整。<a href="https://www.jianshu.com/p/4e4a23fd336e" target="_blank" rel="noopener">使用四象限法的正确姿势！</a></p>
<p>按照“要事第一”的法则，所有任务分为四类: </p>
<blockquote>
<ul>
<li>重要且紧急：需要尽快处理，最优先。</li>
<li>重要不紧急：可暂缓，但要加以足够的重视，最应该偏重做的事。</li>
<li>紧急不重要：不太重要，但需要尽快处理，可考虑是否安排他人。</li>
<li>不重要且不紧急：不重要，且也不需要尽快处理，可考虑是否不做、委派他人、或推迟。</li>
</ul>
</blockquote>
<h4 id="10-00-11-30"><a href="#10-00-11-30" class="headerlink" title="10:00 - 11:30"></a>10:00 - 11:30</h4><p>工作状态，使用番茄土豆来跟踪。番茄工作法并不是简单的工作25分钟、其后休息5分钟。</p>
<p>而是在每个番茄时间里完成  </p>
<p><strong>计划 -&gt; 执行 -&gt; 反馈 －&gt; 改进</strong></p>
<p>的循环往复。</p>
<p>仔细思考下，你会发现 每天24小时就是一个番茄时间。</p>
<p>在五分钟的休息时间里，我一般会</p>
<ol>
<li><p>起来打水，走动一下</p>
</li>
<li><p>伸展运动。转转颈椎。转动下上半身。</p>
</li>
<li><p>如果是下午时间，会绕着公司走上两圈放松放松。眺望远处的绿色放空自己。</p>
</li>
<li><p>闭眼深呼吸。</p>
</li>
<li><p>跟同事聊会。增加团队交流。</p>
</li>
</ol>
<h4 id="11-30-14-00"><a href="#11-30-14-00" class="headerlink" title="11:30 - 14:00"></a>11:30 - 14:00</h4><p>午饭和午休时间。午饭没得选啊 – 公司和外卖。头大。</p>
<p>吃完饭会做些娱乐的事。周一、三、五会把龙族五追更下。周二看唐唐说奇案。周四会看雄兵连动漫。</p>
<p>娱乐完之后。会到 <a href="https://leetcode-cn.com" target="_blank" rel="noopener">力扣网</a> 刷题。至少一道。PS: 有时候也会偷懒。</p>
<p>如果还有时间，又比较困就睡会。没时间就接着娱乐下，刷刷朋友圈、漫画、知识星球。</p>
<p>困的不行，又没时间休息可以冲杯咖啡提神，有些效果的。</p>
<h4 id="14-00-17-30"><a href="#14-00-17-30" class="headerlink" title="14:00 - 17:30"></a>14:00 - 17:30</h4><p>工作状态。继续使用番茄土豆来跟踪。</p>
<h4 id="17-30-18-30"><a href="#17-30-18-30" class="headerlink" title="17:30 - 18:30"></a>17:30 - 18:30</h4><p>晚饭时间。也是只能从公司和外卖选。头疼。</p>
<p>饭毕至睡前这段时间不吃其他东西，除非特别饿的情况或者有特别有食欲的加餐。</p>
<p>但话说回来，晚饭如果好吃的话。又怎么会饿得要加餐呢。</p>
<h4 id="18-30-21-00"><a href="#18-30-21-00" class="headerlink" title="18:30 - 21:00"></a>18:30 - 21:00</h4><p>晚间加班，仍然需要番茄土豆的督促。只要是需要效率和精力的地方，使用番茄法与否有着天壤之别。</p>
<p>把学习的重点聚焦在</p>
<blockquote>
<ul>
<li>技术能力 (书籍、文章)</li>
<li>个人管理（情绪、精力等）</li>
<li>独立思考</li>
<li>为人处事（沟通、谈吐等）</li>
<li>拓展视野（历史、经济、心理学等）</li>
</ul>
</blockquote>
<p>技术能力主要以动手实践和写技术博文来提高。</p>
<h4 id="21-00-22-00"><a href="#21-00-22-00" class="headerlink" title="21:00 - 22:00"></a>21:00 - 22:00</h4><p>下班回家。默念“1234”口诀，提醒自己带上”手机、耳机、电脑、工卡”。</p>
<p>两个选择: 听下读书音频、刷下知识星球</p>
<h4 id="22-00-－-23-00"><a href="#22-00-－-23-00" class="headerlink" title="22:00 － 23:00"></a>22:00 － 23:00</h4><p>两个方案：力量训练或者看书</p>
<p>无氧运动使用 Keep，参照<a href="https://book.douban.com/subject/11608712/" target="_blank" rel="noopener">《无器械健身》</a>和<a href="https://book.douban.com/subject/25717097/" target="_blank" rel="noopener">《囚徒健身》</a>，对难度和强度做了适当的调整。<br>需要注意的是，每个动作一定要标准规范，否则锻炼的效果就大打折扣了。</p>
<p>看书过程一定要合上电脑。翻盖手机，避免被无关信息打扰。</p>
<p>PS: 跑步不适合我。且受场地影响。旁边倒是有个公园，但是大半夜不想去跑步</p>
<h4 id="23-00-－-23-30"><a href="#23-00-－-23-30" class="headerlink" title="23:00 － 23:30"></a>23:00 － 23:30</h4><p>洗漱，泡脚。泡脚水温度不要太烫。不然前期只能凉着。</p>
<p>泡脚开始前先喝杯养乐多提提神。处理些待处理的小事。</p>
<p>其次朗读备忘录的每日一读文章。</p>
<p><strong>写作练习</strong>。没有固定选题，有感而发。每天500字，渐渐提高到1000字，然后<a href="https://coolshell.cn/articles/1719.html" target="_blank" rel="noopener">读给一只橡皮鸭听</a>。</p>
<h4 id="23-00-23-30"><a href="#23-00-23-30" class="headerlink" title="23:00 - 23:30"></a>23:00 - 23:30</h4><p>睡觉，晚安。</p>
<p>睡姿尽量放空自己，让自己舒适。</p>
<p>遇到失眠情况会选择换一个方向睡。保持卧室无光状态。挺有效果的。</p>
<h4 id="23-30"><a href="#23-30" class="headerlink" title="23:30"></a>23:30</h4><p>最后，有人可能会问，这样的循规蹈矩会不会枯燥无聊，失去了生活的趣味？在我看来，恰恰相反。每个早晨的回顾，都有种「西瓜最中间那一口」的甘甜。而要养成这些习惯并不难：</p>
<ol>
<li><p>改变认知，这是一切改变的开始，问问自己是否真正想要做出改变，为什么要做出改变，解决 why 的问题。</p>
</li>
<li><p>设定<a href="https://www.jianshu.com/p/0360eef33f42" target="_blank" rel="noopener">SMART目标</a>，一切低效都是源于目标的缺失。解决 what 的问题</p>
</li>
<li><p>制定计划，决定具体的行动。同时借助<strong>特定的时间和地点</strong>，作为行动的触发条件，解决 how 的问题</p>
</li>
</ol>
<p>本人处于未来投资的打算，是想要作出改变的。</p>
<p>一万个小时定律，成为你想成为的人。21天养成一个习惯，播种一个习惯，收获一种性格。</p>
<ol>
<li><p>监控进展，及时反馈。感谢 habify 的帮助。我用它为一天的活动打卡，每次回顾都是满满的成就感。</p>
</li>
<li><p>立即行动。想法停留在脑子里的时间越长就会变得越弱，不要说什么等待时机成熟，现在就是最好的时机。</p>
</li>
<li><p>有些工具是重复的。番茄土豆的任务我更多是使用来添加工作上突发要处理的任务。<br>Todoist 的清单更多针对于理性的规划的任务清单，但是不保证能完成。</p>
</li>
<li><p>日常修养行为我会在 Todoist 上记录，这样当我查看时我就知道我哪些修养行为没做。<br>因 Todoist 回顾任务是高级版才有的。我选择使用 habify 进行打卡计数的。</p>
</li>
</ol>
<p>补充</p>
<ol>
<li><p>学习的方法包括: (来自<a href="https://book.douban.com/subject/3609132/" target="_blank" rel="noopener">《把时间当作朋友》</a>)</p>
<blockquote>
<ul>
<li>试错</li>
<li>观察</li>
<li>阅读</li>
<li>思考（补充：教是最好的思考，更有助于整理思路）</li>
</ul>
</blockquote>
</li>
<li><p>碎片化的时间，使用我的<a href="https://lihuimintu.github.io/2019/04/27/Information-class-APP-combination/" target="_blank" rel="noopener">信息类APP组合</a>来填充。</p>
</li>
<li><p>以上所有的纪录，不管是晨间日记、todo list、番茄日记，抑或健身日志（Seven／Keep／咕咚运动），其目的都在于反馈。要及时总结，并将经验教训付诸于下一步的行动。</p>
</li>
</ol>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://hellokangning.github.io/zh/post/an-efficient-day/#7-30-9-00" target="_blank" rel="noopener">高效的一天可以这样过</a></li>
</ul>
]]></content>
      <categories>
        <category>Life</category>
      </categories>
  </entry>
  <entry>
    <title>yarn logs -applicationId 命令 Java 版本简单实现</title>
    <url>/2019/04/25/yarn-log-applicationId/</url>
    <content><![CDATA[<p>yarn logs -applicationId 命令 Java 版本简单实现</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>有个需求是需要把将 yarn logs 日志获得然后在前端页面上显示出来。</p>
<p>我一开始是直接读取 /tmp/logs 下面 log 文件。读取出来排版有点丑。而且开头和结尾处有乱码。</p>
<p>花了大量时间再纠结乱码的去除。被折腾的不要不要的。</p>
<p>最后在 任我行的<a href="https://www.cnblogs.com/lyy-blog/p/9635601.html" target="_blank" rel="noopener">yarn logs -applicationId命令java版本简单实现</a> 看到实现的可能性。</p>
<p>任我行代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import java.io.DataInputStream;</span><br><span class="line">import java.io.EOFException;</span><br><span class="line">import java.io.FileNotFoundException;</span><br><span class="line">import java.io.PrintStream;</span><br><span class="line"></span><br><span class="line">import org.apache.commons.lang.StringUtils;</span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.FileContext;</span><br><span class="line">import org.apache.hadoop.fs.FileStatus;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.fs.RemoteIterator;</span><br><span class="line">import org.apache.hadoop.security.UserGroupInformation;</span><br><span class="line">import org.apache.hadoop.yarn.api.records.ApplicationId;</span><br><span class="line">import org.apache.hadoop.yarn.conf.YarnConfiguration;</span><br><span class="line">import org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat;</span><br><span class="line">import org.apache.hadoop.yarn.logaggregation.LogAggregationUtils;</span><br><span class="line">import org.apache.hadoop.yarn.util.ConverterUtils;</span><br><span class="line"></span><br><span class="line">public class GetYarnLog &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        run(&quot;application_1535700682133_0496&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public static int run(String appIdStr) throws Throwable&#123;</span><br><span class="line"> </span><br><span class="line">    </span><br><span class="line">         Configuration conf &#x3D; new YarnConfiguration();</span><br><span class="line">         conf.addResource(new Path(&quot;&#x2F;etc&#x2F;hadoop&#x2F;conf.cloudera.yarn&#x2F;core-site.xml&quot;));</span><br><span class="line">         conf.addResource(new Path(&quot;&#x2F;etc&#x2F;hadoop&#x2F;conf.cloudera.yarn&#x2F;yarn-site.xml&quot;));</span><br><span class="line">         conf.addResource(new Path(&quot;&#x2F;etc&#x2F;hadoop&#x2F;conf.cloudera.yarn&#x2F;hdfs-site.xml&quot;));</span><br><span class="line">         if(appIdStr &#x3D;&#x3D; null || appIdStr.equals(&quot;&quot;))</span><br><span class="line">          &#123;</span><br><span class="line">             System.out.println(&quot;appId is null!&quot;);</span><br><span class="line">             return -1;</span><br><span class="line">          &#125;</span><br><span class="line">         PrintStream out&#x3D;new PrintStream(appIdStr); </span><br><span class="line">         ApplicationId appId &#x3D; null;</span><br><span class="line">         appId &#x3D; ConverterUtils.toApplicationId(appIdStr);</span><br><span class="line">         </span><br><span class="line">         Path remoteRootLogDir &#x3D; new Path(conf.get(&quot;yarn.nodemanager.remote-app-log-dir&quot;, &quot;&#x2F;tmp&#x2F;logs&quot;));</span><br><span class="line"></span><br><span class="line">         String user &#x3D;  UserGroupInformation.getCurrentUser().getShortUserName();;</span><br><span class="line">         String logDirSuffix &#x3D; LogAggregationUtils.getRemoteNodeLogDirSuffix(conf);</span><br><span class="line">         </span><br><span class="line">         Path remoteAppLogDir &#x3D; LogAggregationUtils.getRemoteAppLogDir(remoteRootLogDir, appId, user, logDirSuffix);</span><br><span class="line">         RemoteIterator&lt;FileStatus&gt; nodeFiles;</span><br><span class="line">         try</span><br><span class="line">         &#123;</span><br><span class="line">           Path qualifiedLogDir &#x3D; FileContext.getFileContext(conf).makeQualified(remoteAppLogDir);</span><br><span class="line">           nodeFiles &#x3D; FileContext.getFileContext(qualifiedLogDir.toUri(), conf).listStatus(remoteAppLogDir);</span><br><span class="line">         &#125;</span><br><span class="line">         catch (FileNotFoundException fnf)</span><br><span class="line">         &#123;</span><br><span class="line">           logDirNotExist(remoteAppLogDir.toString());</span><br><span class="line">           return -1;</span><br><span class="line">         &#125;</span><br><span class="line">         </span><br><span class="line">         boolean foundAnyLogs &#x3D; false;</span><br><span class="line">         while (nodeFiles.hasNext())</span><br><span class="line">         &#123;</span><br><span class="line">           FileStatus thisNodeFile &#x3D; (FileStatus)nodeFiles.next();</span><br><span class="line">           if (!thisNodeFile.getPath().getName().endsWith(&quot;.tmp&quot;))</span><br><span class="line">           &#123;</span><br><span class="line">               System.out.println(&quot;NodeFileName &#x3D; &quot;+thisNodeFile.getPath().getName());</span><br><span class="line">             AggregatedLogFormat.LogReader reader &#x3D; new AggregatedLogFormat.LogReader(conf, thisNodeFile.getPath());</span><br><span class="line">             try</span><br><span class="line">             &#123;</span><br><span class="line">               AggregatedLogFormat.LogKey key &#x3D; new AggregatedLogFormat.LogKey();</span><br><span class="line">               DataInputStream valueStream &#x3D; reader.next(key);</span><br><span class="line">               for (;;)</span><br><span class="line">               &#123;</span><br><span class="line">                 if (valueStream !&#x3D; null)</span><br><span class="line">                 &#123;</span><br><span class="line">                   String containerString &#x3D; &quot;\n\nContainer: &quot; + key + &quot; on &quot; + thisNodeFile.getPath().getName();</span><br><span class="line">                   </span><br><span class="line">                   out.println(containerString);</span><br><span class="line">                   out.println(StringUtils.repeat(&quot;&#x3D;&quot;, containerString.length()));</span><br><span class="line">                   try</span><br><span class="line">                   &#123;</span><br><span class="line">                     for (;;)</span><br><span class="line">                     &#123;</span><br><span class="line">                       AggregatedLogFormat.LogReader.readAContainerLogsForALogType(valueStream, out, thisNodeFile.getModificationTime());</span><br><span class="line">                       </span><br><span class="line">                       foundAnyLogs &#x3D; true;</span><br><span class="line">                     &#125;</span><br><span class="line">                         </span><br><span class="line">                   &#125;</span><br><span class="line">                   catch (EOFException eof)</span><br><span class="line">                   &#123;</span><br><span class="line">                     key &#x3D; new AggregatedLogFormat.LogKey();</span><br><span class="line">                     valueStream &#x3D; reader.next(key);</span><br><span class="line">                      </span><br><span class="line">                   &#125;</span><br><span class="line">                   </span><br><span class="line">                 &#125;else&#123;</span><br><span class="line">                     break;</span><br><span class="line">                 &#125;</span><br><span class="line">               &#125;</span><br><span class="line">             &#125;</span><br><span class="line">             finally</span><br><span class="line">             &#123;</span><br><span class="line">               reader.close();</span><br><span class="line">             &#125;</span><br><span class="line">           &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         if (!foundAnyLogs)</span><br><span class="line">         &#123;</span><br><span class="line">           emptyLogDir(remoteAppLogDir.toString());</span><br><span class="line">           return -1;</span><br><span class="line">         &#125;</span><br><span class="line">         return 0;</span><br><span class="line">       &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.jdb.bigdatams.util;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.*;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.nio.charset.StandardCharsets;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang.StringUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.yarn.conf.YarnConfiguration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat;</span><br><span class="line"><span class="keyword">import</span> org.springframework.beans.factory.annotation.Value;</span><br><span class="line"><span class="keyword">import</span> org.springframework.stereotype.Component;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> lihm</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2019-04-19 14:51</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span> TODO</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">YarnLogUtil</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Value</span>(<span class="string">"$&#123;hadooprequest.nn1Address&#125;"</span>)</span><br><span class="line">    <span class="keyword">private</span> String nn1;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Value</span>(<span class="string">"$&#123;hadooprequest.nn2Address&#125;"</span>)</span><br><span class="line">    <span class="keyword">private</span> String nn2;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Value</span>(<span class="string">"$&#123;hadooprequest.nameservices&#125;"</span>)</span><br><span class="line">    <span class="keyword">private</span> String HdfsNameservices;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">readLog</span><span class="params">(String applicationId, String userName)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf=<span class="keyword">new</span> Configuration(<span class="keyword">false</span>);</span><br><span class="line">        String nameservices = HdfsNameservices;</span><br><span class="line">        String[] namenodesAddr = &#123;nn1, nn2&#125;;</span><br><span class="line">        String[] namenodes = &#123;<span class="string">"nn1"</span>,<span class="string">"nn2"</span>&#125;;</span><br><span class="line">        conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://"</span> + nameservices);</span><br><span class="line">        conf.set(<span class="string">"dfs.nameservices"</span>,nameservices);</span><br><span class="line">        conf.set(<span class="string">"dfs.ha.namenodes."</span> + nameservices, namenodes[<span class="number">0</span>]+<span class="string">","</span>+namenodes[<span class="number">1</span>]);</span><br><span class="line">        conf.set(<span class="string">"dfs.namenode.rpc-address."</span> + nameservices + <span class="string">"."</span> + namenodes[<span class="number">0</span>], namenodesAddr[<span class="number">0</span>]);</span><br><span class="line">        conf.set(<span class="string">"dfs.namenode.rpc-address."</span> + nameservices + <span class="string">"."</span> + namenodes[<span class="number">1</span>], namenodesAddr[<span class="number">1</span>]);</span><br><span class="line">        conf.set(<span class="string">"dfs.client.failover.proxy.provider."</span> + nameservices,<span class="string">"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider"</span>);</span><br><span class="line">        String hdfsRPCUrl = <span class="string">"hdfs://"</span> + nameservices + <span class="string">":"</span> + <span class="number">8020</span>;</span><br><span class="line"></span><br><span class="line">        ByteArrayOutputStream os = <span class="keyword">new</span> ByteArrayOutputStream();</span><br><span class="line">        PrintStream out = <span class="keyword">new</span> PrintStream(os);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(hdfsRPCUrl), conf, userName);</span><br><span class="line">            FileStatus[]  paths = fs.listStatus(<span class="keyword">new</span> Path(<span class="string">"/tmp/logs/"</span> + userName + <span class="string">"/logs/"</span> + applicationId));</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (paths == <span class="keyword">null</span> || paths.length==<span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> FileNotFoundException(<span class="string">"Cannot access "</span> + <span class="string">"/tmp/logs/"</span> + userName + <span class="string">"/logs/"</span> + applicationId +</span><br><span class="line">                        <span class="string">": No such file or directory."</span>);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">long</span> sizeLength = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (FileStatus fileStatus : paths) &#123;</span><br><span class="line">                sizeLength += fs.getContentSummary(fileStatus.getPath()).getLength();</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (sizeLength &gt; <span class="number">1024</span> * <span class="number">1024</span> * <span class="number">1024</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"文件大于 1 G，请自行到集群上查看"</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; paths.length ; ++i)</span><br><span class="line">            &#123;</span><br><span class="line">                Configuration yarnConfiguration = <span class="keyword">new</span> YarnConfiguration();</span><br><span class="line">                <span class="comment">// yarnConfiguration.addResource(new Path("/Users/tu/Public/ZaWu/conf.cloudera.yarn/core-site.xml"));</span></span><br><span class="line">                <span class="comment">// yarnConfiguration.addResource(new Path("/Users/tu/Public/ZaWu/conf.cloudera.yarn/yarn-site.xml"));</span></span><br><span class="line">                <span class="comment">// yarnConfiguration.addResource(new Path("/Users/tu/Public/ZaWu/conf.cloudera.yarn/hdfs-site.xml"));</span></span><br><span class="line"></span><br><span class="line">                yarnConfiguration.addResource(<span class="keyword">new</span> Path(<span class="string">"/etc/hadoop/conf.cloudera.yarn/core-site.xml"</span>));</span><br><span class="line">                yarnConfiguration.addResource(<span class="keyword">new</span> Path(<span class="string">"/etc/hadoop/conf.cloudera.yarn/yarn-site.xml"</span>));</span><br><span class="line">                yarnConfiguration.addResource(<span class="keyword">new</span> Path(<span class="string">"/etc/hadoop/conf.cloudera.yarn/hdfs-site.xml"</span>));</span><br><span class="line">                AggregatedLogFormat.LogReader reader = <span class="keyword">new</span> AggregatedLogFormat.LogReader(yarnConfiguration, paths[i].getPath());</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    AggregatedLogFormat.LogKey key = <span class="keyword">new</span> AggregatedLogFormat.LogKey();</span><br><span class="line">                    DataInputStream valueStream = reader.next(key);</span><br><span class="line">                    <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">                        <span class="keyword">if</span> (valueStream != <span class="keyword">null</span>) &#123;</span><br><span class="line">                            String containerString = <span class="string">"\n\nContainer: "</span> + key + <span class="string">" on "</span> + paths[i].getPath().getName();</span><br><span class="line">                            out.println(containerString);</span><br><span class="line">                            out.println(StringUtils.repeat(<span class="string">"="</span>, containerString.length()));</span><br><span class="line"></span><br><span class="line">                            <span class="keyword">try</span> &#123;</span><br><span class="line">                                <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">                                    AggregatedLogFormat.LogReader.readAContainerLogsForALogType(valueStream, out, paths[i].getModificationTime());</span><br><span class="line"></span><br><span class="line">                                &#125;</span><br><span class="line">                            &#125; <span class="keyword">catch</span> (EOFException eof) &#123;</span><br><span class="line">                                key = <span class="keyword">new</span> AggregatedLogFormat.LogKey();</span><br><span class="line">                                valueStream = reader.next(key);</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            <span class="keyword">break</span>;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">                    reader.close();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">catch</span> (FileNotFoundException e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> e;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        out.close();</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> String(os.toByteArray(), StandardCharsets.UTF_8);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h4><h5 id="1-权限问题"><a href="#1-权限问题" class="headerlink" title="1. 权限问题"></a>1. 权限问题</h5><p>一开始遇到权限问题，尝试过在代码中解决，想以 application 的 appOwner 去读取。</p>
<p>没找到突破口。最后授权给运行的账号拥有 /tmp/logs 路径下的读权限解决。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">HDFS 开启了 ACL 权限</span><br><span class="line">hadoop fs -setfacl -R -m default:user:hadoop:rwx &#x2F;tmp&#x2F;logs</span><br></pre></td></tr></table></figure>

<p>开启 default ACL 的话后续生成的文件。hadoop 都有权限去读。<a href="https://lihuimintu.github.io/2019/05/14/hdfs-acl/" target="_blank" rel="noopener">HDFS ACL 权限管理</a></p>
<h5 id="2-输出-String"><a href="#2-输出-String" class="headerlink" title="2. 输出 String"></a>2. 输出 String</h5><p>按 任我行的代码。logs 日志一直是输出到本地代码的源路径下。</p>
<p>最后仔细研读代码后发现是跟 PrintStream 打印流有关。</p>
<p>问题就变成如何把 PrintStream 转为 String 了</p>
<p>具体参考 <a href="https://lihuimintu.github.io/2019/04/24/printstream-to-string/" target="_blank" rel="noopener">PrintStream 里的内容转为 String</a></p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.cnblogs.com/lyy-blog/p/9635601.html" target="_blank" rel="noopener">yarn logs -applicationId命令java版本简单实现</a></li>
</ul>
]]></content>
      <categories>
        <category>Yarn</category>
      </categories>
  </entry>
  <entry>
    <title>PrintStream 里的内容转为 String</title>
    <url>/2019/04/24/printstream-to-string/</url>
    <content><![CDATA[<p>PrintStream 里的内容转为 String</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>有个业务需求是将其 yarn log 日志返回到前端。</p>
<p>参考 <a href="https://www.cnblogs.com/lyy-blog/p/9635601.html" target="_blank" rel="noopener">yarn logs -applicationId命令java版本简单实现</a> 实现该业务需求</p>
<p>发现它是将其输出流作为文件输出的。</p>
<p>PrintStream out = new PrintStream(appIdStr); </p>
<p>这将会把输出流数据输出到源代码目录下。</p>
<p>因此需要将输出流处理后转为 String 返回到前端</p>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ByteArrayOutputStream os = <span class="keyword">new</span> ByteArrayOutputStream();</span><br><span class="line">PrintStream out = <span class="keyword">new</span> PrintStream(os);</span><br><span class="line">String s = <span class="keyword">new</span> String(os.toByteArray(), StandardCharsets.UTF_8);</span><br></pre></td></tr></table></figure>

<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.jianshu.com/p/83ad6eb5a905" target="_blank" rel="noopener">PrintStream里的内容转为String</a></li>
<li><a href="https://www.cnblogs.com/fnz0/p/5423201.html" target="_blank" rel="noopener">(JAVA）从零开始之–打印流PrintStream记录日志文件</a></li>
<li><a href="https://www.cnblogs.com/skywang12345/p/io_16.html" target="_blank" rel="noopener">java io系列16之 PrintStream(打印输出流)详解</a></li>
</ul>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title>一致性模型</title>
    <url>/2019/04/24/uniformity/</url>
    <content><![CDATA[<p>一致性（Consistency）是指多副本（Replications）问题中的数据一致性。</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>学习 HBase 的 MVCC 时有提到一致性的概念。抱着知识全面性的想法来学习。</p>
<h4 id="一致性模型"><a href="#一致性模型" class="headerlink" title="一致性模型"></a>一致性模型</h4><h5 id="强一致性"><a href="#强一致性" class="headerlink" title="强一致性"></a>强一致性</h5><p>当更新操作完成之后，任何多个后续进程或者线程的访问都会返回最新的更新过的值。<br>这种是对用户最友好的，就是用户上一次写什么，下一次就保证能读到什么。但是这种实现对性能影响较大。</p>
<h5 id="弱一致性"><a href="#弱一致性" class="headerlink" title="弱一致性"></a>弱一致性</h5><p>系统并不保证续进程或者线程的访问都会返回最新的更新过的值。系统在数据写入成功之后，不承诺立即可以读到最新写入的值，也不会具体的承诺多久之后可以读到。但会尽可能保证在某个时间级别（比如秒级别）之后，可以让数据达到一致性状态。</p>
<h5 id="最终一致性"><a href="#最终一致性" class="headerlink" title="最终一致性"></a>最终一致性</h5><p>弱一致性的特定形式。不保证在任意时刻任意节点上的同一份数据都是相同的，但是随着时间的迁移，不同节点上的同一份数据总是在向趋同的方向变化。<br>系统保证在没有后续更新的前提下，系统最终返回上一次更新操作的值。</p>
<p>简单说，就是在一段时间后，节点间的数据会最终达到一致状态。</p>
<p>在没有故障发生的前提下，不一致窗口的时间主要受通信延迟，系统负载和复制副本的个数影响。</p>
<p>最终一致性根据更新数据后各进程访问到数据的时间和方式的不同，又可以区分为:</p>
<blockquote>
<ul>
<li>因果一致性：如果进程A通知进程B它已更新了一个数据项，那么进程B的后续访问将返回更新后的值，且一次写入将保证取代前一次写入。与进程A无因果关系的进程C的访问，遵守一般的最终一致性规则。</li>
<li>读己所写一致性：因果一致性的特定形式。当进程A自己更新一个数据项之后，它总是访问到更新过的值，绝不会看到旧值。</li>
<li>会话一致性：读己所写一致性的特定形式。它把访问存储系统的进程放到会话的上下文中。只要会话还存在，系统就保证“读己之所写”一致性。如果由于某些失败情形令会话终止，就要建立新的会话，而且系统的保证不会延续到新的会话。</li>
<li>单调读一致性：如果进程已经看到过数据对象的某个值，那么任何后续访问都不会返回在那个值之前的值。</li>
<li>单调写一致性：系统保证来自同一个进程的写操作顺序执行。即同一个进程的写操作串行化。</li>
</ul>
</blockquote>
<p>上述最终一致性的不同方式可以进行组合，例如单调读一致性和读己之所写一致性就可以组合实现。<br>并且从实践的角度来看，这两者的组合，读取自己更新的数据，和一旦读取到最新的版本不会再读取旧版本，<br>对于此架构上的程序开发来说，会少很多额外的烦恼。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/chao2016/article/details/81149674" target="_blank" rel="noopener">强一致性、顺序一致性、弱一致性和共识</a></li>
<li><a href="http://wangxin123.com/2016/10/21/CAP%E5%8E%9F%E7%90%86%E3%80%81%E4%B8%80%E8%87%B4%E6%80%A7%E6%A8%A1%E5%9E%8B%E3%80%81BASE%E7%90%86%E8%AE%BA%E5%92%8CACID%E7%89%B9%E6%80%A7/" target="_blank" rel="noopener">CAP原理、一致性模型、BASE理论和ACID特性</a></li>
<li><a href="https://mp.weixin.qq.com/s/Smni4c4MxY2IZVD_h-428Q" target="_blank" rel="noopener">关于互联网“一致性”架构设计的一切</a></li>
<li><a href="https://www.cnblogs.com/gotodsp/p/6651310.html" target="_blank" rel="noopener">强一致性、弱一致性、最终一致性</a></li>
<li><a href="http://www.blogjava.net/hello-yun/archive/2012/04/27/376744.html" target="_blank" rel="noopener">CAP原理与最终一致性 强一致性 弱一致性</a></li>
</ul>
]]></content>
      <categories>
        <category>Distributed</category>
      </categories>
  </entry>
  <entry>
    <title>概念解释</title>
    <url>/2019/04/22/concept-learning/</url>
    <content><![CDATA[<p>记录一些名词的理解和解释</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>记录专有名词的解释。方便自己回看</p>
<h4 id="并发与并行"><a href="#并发与并行" class="headerlink" title="并发与并行"></a>并发与并行</h4><ul>
<li>并发: 多任务下，在同一时间段内同时发生。</li>
<li>并行: 多任务下，在同一时间点上同时发生。</li>
</ul>
<p>并发会多个任务之间是互相抢占资源的，而并行不会。</p>
<p>知乎上高赞例子: </p>
<ul>
<li>你吃饭吃到一半，电话来了，你一直到吃完了以后才去接，这就说明你不支持并发也不支持并行。</li>
<li>你吃饭吃到一半，电话来了，你停了下来接了电话，接完后继续吃饭，这说明你支持并发。</li>
<li>你吃饭吃到一半，电话来了，你一边打电话一边吃饭，这说明你支持并行。</li>
</ul>
<p>只有在多CPU的情况中，才会发生并行。否则，看似同时发生的事情，其实都是并发执行的。</p>
<p><a href="https://mp.weixin.qq.com/s/clr4yLkKeirhi203ZOmJWg" target="_blank" rel="noopener">漫话：如何给女朋友解释什么是并发和并行</a></p>
<h4 id="分布式、高并发、多线程"><a href="#分布式、高并发、多线程" class="headerlink" title="分布式、高并发、多线程"></a>分布式、高并发、多线程</h4><p>分布式是为了解决单个物理服务器容量和性能瓶颈问题而采用的优化手段。</p>
<blockquote>
<ul>
<li>水平扩展: 当一台机器扛不住流量时，就通过添加机器的方式，将流量平分到所有服务器上，<br>所有机器都可以提供相当的服务；</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>垂直拆分: 前端有多种查询需求时，一台机器扛不住，<br>可以将不同的需求分发到不同的机器上，比如A机器处理余票查询的请求，B机器处理支付的请求。</li>
</ul>
</blockquote>
<p>高并发在解决的问题上会集中一些，其反应的是同时有多少量</p>
<p>高并发可以通过分布式技术去解决，将并发流量分到不同的物理服务器上。<br>但除此之外，还可以有很多其他优化手段：比如使用缓存系统，将所有的，静态内容放到CDN等；<br>还可以使用多线程技术将一台服务器的服务能力最大化。</p>
<p>多线程是指从软件或者硬件上实现多个线程并发执行的技术，<br>它更多的是解决CPU调度多个进程的问题，从而让这些进程看上去是同时执行（实际是交替运行的）。</p>
<p>这几个概念中，多线程解决的问题是最明确的，手段也是比较单一的，<br>基本上遇到的最大问题就是线程安全。</p>
<p>总结一下: </p>
<p>● 分布式是从物理资源的角度去将不同的机器组成一个整体对外服务，技术范围非常广且难度非常大，有了这个基础，高并发、高吞吐等系统很容易构建；</p>
<p>● 高并发是从业务角度去描述系统的能力，实现高并发的手段可以采用分布式，也可以采用诸如缓存、CDN等，当然也包括多线程；</p>
<p>● 多线程则聚焦于如何使用编程语言将CPU调度能力最大化。</p>
<p><a href="https://mp.weixin.qq.com/s/k1Fz0NCN8KyV3VHF0nSGuA" target="_blank" rel="noopener">怎么理解分布式、高并发、多线程？</a></p>
<h4 id="分布式、集群"><a href="#分布式、集群" class="headerlink" title="分布式、集群"></a>分布式、集群</h4><p>分布式（distributed）是指在多台不同的服务器中部署不同的服务模块，通过远程调用协同工作，对外提供服务。</p>
<p>集群（cluster）是指在多台不同的服务器中部署相同应用或服务模块，构成一个集群，通过负载均衡设备对外提供服务。</p>
<p>在这里感觉分布式就是上章节的垂直拆分</p>
<p>集群就是上章节的水平扩展</p>
<p><a href="https://mp.weixin.qq.com/s/x1L8CDlshcJHhG70oP3yZg" target="_blank" rel="noopener">漫话：如何给女朋友解释什么是分布式和集群？</a></p>
<h4 id="两阶段加锁-2PL-协议、两阶段提交-2PC-协议"><a href="#两阶段加锁-2PL-协议、两阶段提交-2PC-协议" class="headerlink" title="两阶段加锁(2PL)协议、两阶段提交(2PC)协议"></a>两阶段加锁(2PL)协议、两阶段提交(2PC)协议</h4><p>2PL: 两阶段加锁协议: 主要用于单机事务中的一致性与隔离性。</p>
<p>2PC: 两阶段提交协议: 主要用于分布式事务。</p>
<h4 id="QPS、TPS"><a href="#QPS、TPS" class="headerlink" title="QPS、TPS"></a>QPS、TPS</h4><p>QPS: Queries Per Second意思是“每秒查询率”，是一台服务器每秒能够相应的查询次数，是对一个特定的查询服务器在规定时间内所处理流量多少的衡量标准。</p>
<p>TPS: 是 TransactionsPerSecond 的缩写，也就是事务数/秒。它是软件测试结果的测量单位。一个事务是指一个客户机向服务器发送请求然后服务器做出反应的过程。客户机在发送请时开始计时，收到服务器响应后结束计时，以此来计算使用的时间和完成的事务个数。</p>
<h4 id="数据仓库"><a href="#数据仓库" class="headerlink" title="数据仓库"></a>数据仓库</h4><p>Data Warehouse: 简称DW，中文名数据仓库，是商业智能（BI）中的核心部分。主要是将不同数据源的数据整合到一起，通过多维分析等方式为企业提供决策支持和报表生成。</p>
<p>那么它与我们熟悉的传统关系型数据库有什么不同呢？</p>
<p>简而言之，用途不同。数据库面向事务，而数据仓库面向分析。数据库一般存储在线的业务数据，<br>需要对上层业务的改变做出实时反应，涉及到增删查改等操作，所以需要遵循三大范式，需要ACID。<br>而数据仓库中存储的则主要是历史数据，主要目的是为企业决策提供支持，所以可能存在大量数据冗余，<br>但利于多个维度查询，为决策者提供更多观察视角。</p>
<p>在传统BI领域中，数据仓库的数据同样存储在Oracle、MySQL等数据库中，<br>而在大数据领域中最常用的数据仓库就是Apache Hive，Hive也是Apache Kylin默认的数据源。</p>
<h4 id="OLAP、OLTP"><a href="#OLAP、OLTP" class="headerlink" title="OLAP、OLTP"></a>OLAP、OLTP</h4><p>OLAP:（Online Analytical Process），联机分析处理，以多维度的方式分析数据，一般带有主观的查询需求，多应用在数据仓库。<br>OLTP:（Online Transaction Process），联机事务处理，侧重于数据库的增删查改等常用业务操作。</p>
<h4 id="at-most-once、at-least-once、exactly-once"><a href="#at-most-once、at-least-once、exactly-once" class="headerlink" title="at most once、at least once、exactly once"></a>at most once、at least once、exactly once</h4><p>At most once 的消息传输机制是每条消息传输零次或者一次，即消息可能会丢失<br>A t least once 意味着每条消息会进行多次传输尝试，至少一次成功，即消息传输可能重复但不会丢失<br>Exactly once 的消息传输机制是每条消息有且只有一次，即消息传输既不会丢失也不会重复</p>
<h4 id="CPU-用户态、内核态"><a href="#CPU-用户态、内核态" class="headerlink" title="CPU 用户态、内核态"></a>CPU 用户态、内核态</h4><p><img src="/images/blog/2020-01-30-1.png" alt></p>
<p>系统中既有操作系统的程序，也有普通用户程序。OS 的核心是内核，可以访问底层硬件设备，为了保证用户进程不能直接操作内核从而保证内核的安全，为了安全性和稳定性，需要切换到内核状态来访问底层硬件设备，这就是内核态。</p>
<p>内核态: 控制计算机的硬件资源，并提供上层应用程序运行的环境，内核态可以使用计算机所有的硬件资源<br>用户态: 上层应用程序的活动空间，应用程序的执行必须依托于内核提供的资源<br>系统调用: 为了使上层应用能够访问到这些资源，内核为上层应用提供访问的接口</p>
<p>从用户态到内核态的切换，一般存在以下三种情况</p>
<ul>
<li>系统调用: 用户态进程通过系统调用申请使用操作系统提供的服务程序完成工作，比如fork()实际上就是执行了一个创建新进程的系统调用。而系统调用的机制其核心还是使用了操作系统为用户特别开放的一个中断来实现，例如Linux的int 80h中断</li>
<li>异常事件: 当CPU正在执行运行在用户态的程序时，突然发生某些预先不可知的异常事件，这个时候就会触发从当前用户态执行的进程转向内核态执行相关的异常事件，典型的如缺页异常</li>
<li>外围设备的中断: 当外围设备完成用户的请求操作后，会像CPU发出中断信号，此时，CPU就会暂停执行下一条即将要执行的指令，转而去执行中断信号对应的处理程序，如果先前执行的指令是在用户态下，则自然就发生从用户态到内核态的转换</li>
</ul>
<p>其中系统调用可以认为是用户进程主动发起的，异常和外围设备中断则是被动的。所以系统调用的本质也是中断，相对于外围设备的硬中断，这种中断称为软中断</p>
<blockquote>
<ol>
<li>计算机系统中有“操作系统程序”和“普通用户程序”。</li>
<li>操作系统程序执行就是在“内核态”下执行的。</li>
<li>普通用户程序就是在“用户态”下执行的。</li>
<li>内核态可以使用所有的硬件资源，用户态不能直接使用系统资源</li>
<li>为了安全和稳定性，操作系统程序是不能随便访问的</li>
<li>引起“用户态切换到内核态”的本质就是“CPU实行了一次中断相应”</li>
</ol>
</blockquote>
<h4 id="用户空间、内核空间"><a href="#用户空间、内核空间" class="headerlink" title="用户空间、内核空间"></a>用户空间、内核空间</h4><p>用户空间就是用户进程所在的内存区域，相对的，系统空间就是操作系统占据的内存区域。用户进程和系统进程的所有数据都在内存中    </p>
<p>是谁来划分内存空间的呢？在电脑开机之前，内存就是一块原始的物理内存。什么也没有。开机加电，系统启动后，就对物理内存进行了划分。当然，这是系统的规定，物理内存条上并没有划分好的地址和空间范围。这些划分都是操作系统在逻辑上的划分。</p>
<h4 id="软中断、硬中断"><a href="#软中断、硬中断" class="headerlink" title="软中断、硬中断"></a>软中断、硬中断</h4><p>中断(硬)是一种电信号，当设备有某种事情发生的时候，他就会产生中断，通过总线把电信号发送给中断控制器。如果中断的线是激活的，中断控制器就把电信号发送给处理器的某个特定引脚。处理器于是立即停止自己正在做的事，跳到中断处理程序的入口点，进行中断处理</p>
<p>软中断: CPU内部中断，即执行软件中断指令INT或遇到软件陷阱而产生的中断，它们的中断类型号已由CPU规定好<br>硬中断: CPU以外的I/O设备产生的中断</p>
<h4 id="P99、P999"><a href="#P99、P999" class="headerlink" title="P99、P999"></a>P99、P999</h4><p>称之为分位数，分位数是将总体的全部数据按从小到大顺序排列后，处于各等分位置的变量值。</p>
<p>例如 p999=4.22124273，代表99.9%的请求响应时间不大于4.22124273ms，p表示:percent 百分比。</p>
<h4 id="脏页"><a href="#脏页" class="headerlink" title="脏页"></a>脏页</h4><p>脏页是Linux内核中的概念，因为硬盘的读写速度远远赶不上内存的速度，系统就把读写比较频繁的数据事先放到内存中，以提高读写速度，这就是高速缓存，Linux是以页作为高速缓存的单位，当进程修改了高速缓存里的数据时，该页就被内核标记为脏页，内核将会在合适的时间把脏页的数据写到磁盘中，以保持高速缓存中的数据同磁盘中的数据是一致的</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://my.oschina.net/alchemystar/blog/1438839" target="_blank" rel="noopener">MySql-两阶段加锁协议</a></li>
<li><a href="https://blog.csdn.net/qq_18535263/article/details/79373878" target="_blank" rel="noopener">QPS和TPS的区别</a></li>
<li><a href="https://blog.csdn.net/Rylan11/article/details/101458344" target="_blank" rel="noopener">metrics 指标分析——你不在意的p99和p999</a></li>
<li><a href="https://blog.csdn.net/fmlfch/article/details/90673116" target="_blank" rel="noopener">linux 脏页</a></li>
<li><a href="https://www.cnblogs.com/jswang/p/9049229.html" target="_blank" rel="noopener">内核态(内核空间)和用户态(用户空间)的区别和联系</a></li>
</ul>
]]></content>
      <categories>
        <category>Distributed</category>
      </categories>
  </entry>
  <entry>
    <title>YARN 中用户日志的管理和使用</title>
    <url>/2019/04/19/yarn-log/</url>
    <content><![CDATA[<p>YARN 中用户日志的管理和使用</p>
<hr>
<h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><p>在以前, 处理应用(application)生成的用户日志已经成了安装 Hadoop 过程中的一个最大痛点.<br>在 Hadoop 1.x, 用户日志被 TaskTracker 留在每个节点上,<br>这样长期来看在各个节点上的用户日志的是难于管理并且也很难被获取和使用到的.<br>YARN 处理日志管理问题的办法是让 NodeManagers(NMs) 提供一个选项,<br>可以在应用结束后将日志文件安全地移到一个分布式文件系统中, 比如 HDFS.</p>
<h4 id="YARN-中日志聚合的详细描写"><a href="#YARN-中日志聚合的详细描写" class="headerlink" title="YARN 中日志聚合的详细描写"></a>YARN 中日志聚合的详细描写</h4><p>NodeManager 就没有像之前 TaskTracker 那样去截断用户日志, 把日志留在每个节点的本地, 它通过提供一个选项可以在应用结束后将日志安全地移动到分布式文件系统中, 比如 HDFS.</p>
<ul>
<li><p>对属于同一个应用(Application), 跑在一个 NM 上的所有容器(containers)的日志都会被聚合并写入一个配置好地址的(很可能是压缩过的)日志文件中.</p>
</li>
<li><p>在当前的实现中, 一个应用一结束就会得到如下:</p>
<ul>
<li><p>应用级别的日志日志目录 (在 HDFS 上体现  /tmp/logs/{user}/logs)</p>
</li>
<li><p>此节点上该应用的所有的容器的日志文件  (/tmp/logs/{user}/logs 目录下以机器节点区分日志文件)</p>
</li>
</ul>
</li>
<li><p>用户能通过 YARN 的命令行工具, 网页端或者直接从文件系统(FS)中来查看这些日志, 没必要只局限于网页端一种形式.</p>
</li>
<li><p>通过将日志文件存储在分布式文件系统中, 比起第一代的实现, 现在可以让日志文件存储更长的时间.</p>
</li>
<li><p>我们不再需要去截断日志文件, 只要日志文件的长度是合理的, 我们就会保存整个日志文件.</p>
</li>
</ul>
<h4 id="用法"><a href="#用法" class="headerlink" title="用法"></a>用法</h4><h5 id="1-网页端访问-Web-UI"><a href="#1-网页端访问-Web-UI" class="headerlink" title="1. 网页端访问(Web UI)"></a>1. 网页端访问(Web UI)</h5><p>通过网页端访问, 需要知道的事情是, 日志的聚合操作用户是感觉不到的.</p>
<p>当 MapReduce 应用还在执行中时, 用户查看日志是通过 ApplicationMaster UI 来访问的, 后者会自动跳转到 NodeManager UI 上.</p>
<p>一旦应用结束, 所有的日志都会归属到 MapReduce 的 JobHistoryServer 中, 此时将由后者来提供日志访问.</p>
<p>对于 非-MapReduce 应用, 由更一般性质的 ApplicationHistoryServer 来提供一样的日志访问服务.</p>
<p><img src="/images/blog/2019-04-19-1.png" alt></p>
<p><img src="/images/blog/2019-04-19-2.png" alt></p>
<p>MR 的作业我用其打开一般看不到实质性的帮助。Spark 的作业打开页面友好。且帮助信息大。</p>
<h5 id="2-命令行访问"><a href="#2-命令行访问" class="headerlink" title="2. 命令行访问"></a>2. 命令行访问</h5><p>个人比较喜欢命令行访问方式。</p>
<p><img src="/images/blog/2019-04-19-3.png" alt></p>
<p>要打印出一个给定应用的所有日志, 命令如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yarn logs -applicationId &lt;application ID&gt; -appOwner &lt;userId&gt;</span><br></pre></td></tr></table></figure>

<p>如果只要一个给定容器的所有日志, 可以如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yarn logs -applicationId &lt;application ID&gt; -appOwner &lt;userId&gt; -containerId &lt;Container ID&gt; -nodeAddress &lt;Node Address&gt;</span><br></pre></td></tr></table></figure>

<p>使用命令行工具的一个显著优势是可以使用常规的 shell 工具, 如 grep, sort 等等, 来过滤出我们想要的信息.</p>
<h4 id="管理"><a href="#管理" class="headerlink" title="管理"></a>管理</h4><h5 id="1-日志相关的一般配置项"><a href="#1-日志相关的一般配置项" class="headerlink" title="1. 日志相关的一般配置项"></a>1. 日志相关的一般配置项</h5><p>yarn.nodemanager.log-dirs: 确定当容器还在执行中时, 容器日志在节点上的存储位置, 默认值是 ${yarn.log.dir}/userlogs.</p>
<ul>
<li><p>一个应用的本地化日志目录是这样的格式 ${yarn.nodemanager.log-dirs}/application_${appid}</p>
</li>
<li><p>一个独立的容器的日志文件夹会在上面的文件夹下, 文件夹命名格式是这样 container_${containerid}</p>
</li>
<li><p>对于 MapReduce 的应用, 每个容器目录下会包含容器生成的三个文件, stderr, stdin 和 syslog</p>
</li>
<li><p>其他的框架可以自行选择放置多少个日志文件, YARN 对文件名的文件数量都没有限制</p>
</li>
</ul>
<p>yarn.log-aggregation-enable: 选择是否启用日志聚合. 如果不启用聚合, NMs 会把日志存储在节点本地(就像第一代所做的那样).</p>
<h5 id="2-日志聚合启用后的相关配置"><a href="#2-日志聚合启用后的相关配置" class="headerlink" title="2. 日志聚合启用后的相关配置"></a>2. 日志聚合启用后的相关配置</h5><p>yarn.nodemanager.remote-app-log-dir: 这是 NMs 将日志聚合后存放在默认的文件系统(一般就是 HDFS 上的)上的地址. 这个地址不应是本地的文件系统, 否则日志服务器会无法提供聚合后日志的能力. 默认值是 /tmp/logs.</p>
<p>yarn.nodemanager.remote-app-log-dir-suffix: 日志目录会这样创建 {yarn.nodemanager.remote-app-log-dir}/${user}/{thisParam}, 默认值是 logs.</p>
<p>yarn.log-aggregation.retain-seconds: 配置多久后聚合后的日志文件被删除, 默认是7天，如果配置成 -1 或者一个负值就不会删除聚合日志.</p>
<p>yarn.log-aggregation.retain-check-interval-seconds: 确定多长时间去检查一次聚合日志的留存情况以执行日志的删除. 如果设置为 0 或者负值, 那这个值就会用聚合日志的留存时间的十分之一来自动配置, 默认值是 -1.</p>
<h5 id="3-日志聚合未启用的相关配置"><a href="#3-日志聚合未启用的相关配置" class="headerlink" title="3. 日志聚合未启用的相关配置"></a>3. 日志聚合未启用的相关配置</h5><p>yarn.nodemanager.log.retain-seconds: 保存在本地节点的日志的留存时间, 默认值是 10800.</p>
<p>yarn.nodemanager.log.deletion-threads-count: 确定 NMs 启动的进程数去删除本地的日志文件.</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://segmentfault.com/a/1190000007466722#articleHeader5" target="_blank" rel="noopener">在 YARN 中简化用户日志的管理和使用</a></li>
<li><a href="https://www.jianshu.com/p/26429616b441" target="_blank" rel="noopener">查看YARN任务日志的几种方式</a></li>
</ul>
]]></content>
      <categories>
        <category>Yarn</category>
      </categories>
  </entry>
  <entry>
    <title>PostMan 的基本使用汇总</title>
    <url>/2019/04/10/postman-post-put/</url>
    <content><![CDATA[<p>PostMan 发送 JSON 格式的 POST/PUT 请求</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>用 POSTMAN 发送 JSON 格式的 POST/PUT 请求</p>
<h4 id="设置Header"><a href="#设置Header" class="headerlink" title="设置Header"></a>设置Header</h4><p>Content-Type  application/json</p>
<p><img src="/images/blog/2019-04-10-5.png" alt></p>
<h4 id="设置Body"><a href="#设置Body" class="headerlink" title="设置Body"></a>设置Body</h4><p>选择 raw</p>
<p><img src="/images/blog/2019-04-10-6.png" alt></p>
<h4 id="PUT-请求"><a href="#PUT-请求" class="headerlink" title="PUT 请求"></a>PUT 请求</h4><p>把请求方式改为 PUT即可</p>
<p><img src="/images/blog/2019-04-10-7.png" alt></p>
<h4 id="GET-请求"><a href="#GET-请求" class="headerlink" title="GET 请求"></a>GET 请求</h4><p>GET 请求选择是 Params 面板填写参数</p>
<p><img src="/images/blog/2019-04-10-8.png" alt></p>
<h4 id="登录鉴权"><a href="#登录鉴权" class="headerlink" title="登录鉴权"></a>登录鉴权</h4><p>有时候需要登陆鉴权的需求。 这里不重复写了。移步 落落殿下的 <a href="https://blog.csdn.net/qq_42512064/article/details/81034744" target="_blank" rel="noopener">postman登录鉴权，获取token后进行其他接口测试</a></p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/rziqq/article/details/77715085#commentBox" target="_blank" rel="noopener">用POSTMAN发送JSON格式的POST请求</a></li>
<li><a href="https://blog.csdn.net/qq_42512064/article/details/81034744" target="_blank" rel="noopener">postman登录鉴权，获取token后进行其他接口测试</a></li>
</ul>
]]></content>
      <categories>
        <category>Tools</category>
      </categories>
  </entry>
  <entry>
    <title>Curl 模拟 GET\POST 请求</title>
    <url>/2019/04/10/linux-curl/</url>
    <content><![CDATA[<p>Curl 模拟 GET\POST 请求，以及 curl post 上传文件</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>一般情况下，我们调试数据接口，都会使用一个 Postman 的工具。</p>
<p>我在本机的测试环境下使用该工具来模拟 HTTP 请求。</p>
<p>今天遇到一个棘手的问题。刚把开发的 jar 包发布到线上服务器上，结果前端报后端 404。</p>
<p>这就奇了怪了，看了下本地代码发现是有接口的，为了验证下，想模拟下 HTTP 请求。 </p>
<p>发现 Postman 在线上 Linux 用不了。得换个命令行方式。</p>
<p>百度搜索了下发现在命令行中，使用 Curl 这个工具完全可以满足轻量的调试要求。</p>
<h4 id="GET-请求"><a href="#GET-请求" class="headerlink" title="GET 请求"></a>GET 请求</h4><p>curl 命令 + 请求接口的地址。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl http:&#x2F;&#x2F;localhost:9999&#x2F;api&#x2F;daizhige&#x2F;article</span><br></pre></td></tr></table></figure>

<p>如上，我们就可以请求到我们的数据了，如果想看到详细的请求信息，我们可以加上 -v 参数</p>
<p><img src="/images/blog/2019-04-10-4.png" alt></p>
<p>有使用 Get 方式请求需要带参数。如 <a href="http://localhost:8088/dw?a=b&amp;c=d。" target="_blank" rel="noopener">http://localhost:8088/dw?a=b&amp;c=d。</a></p>
<p>在命令行执行后报错说 c 没有传参 （本人踩的大坑）</p>
<p>查阅资料后发现对 &amp; 要进行转义</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl http:&#x2F;&#x2F;localhost:8088&#x2F;dw?a&#x3D;b\&amp;c&#x3D;d</span><br></pre></td></tr></table></figure>

<h4 id="POST-请求"><a href="#POST-请求" class="headerlink" title="POST 请求"></a>POST 请求</h4><p>可以用 -X POST 来申明我们的请求方法，用 -d 参数，来传送我们的参数。</p>
<p><strong>同理可以用 -X PUT 和 -X DELETE 来指定另外的请求方法。</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl http:&#x2F;&#x2F;localhost:9999&#x2F;api&#x2F;daizhige&#x2F;article -X POST -d &quot;title&#x3D;comewords&amp;content&#x3D;articleContent&quot;</span><br></pre></td></tr></table></figure>

<p>如上，这就是一个普通的 Post 请求。</p>
<p>但是，一般我们的接口都是 JSON 格式的，这也没有问题。我们可以用 -H 参数来申明请求的 header</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl http:&#x2F;&#x2F;localhost:9999&#x2F;api&#x2F;daizhige&#x2F;article -X POST -H &quot;Content-Type:application&#x2F;json&quot; -d &#39;&quot;title&quot;:&quot;comewords&quot;,&quot;content&quot;:&quot;articleContent&quot;&#39;</span><br></pre></td></tr></table></figure>

<p>so, 我们还可以用 -H 来设置更多的 header 比如，用户的 token 之类的。</p>
<p>同样，可以用 -v 来查看详情。</p>
<h4 id="POST-上传文件"><a href="#POST-上传文件" class="headerlink" title="POST 上传文件"></a>POST 上传文件</h4><p>上面的两种请求，都是只传输字符串，我们在测试上传接口的时候，会要求传输文件，其实这个对于 curl 来说，也是小菜一碟。</p>
<p>用 -F “file=@<strong>FILE_PATH</strong>“ 的请示，传输文件即可。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl http:&#x2F;&#x2F;localhost:8000&#x2F;api&#x2F;v1&#x2F;upimg -F &quot;file&#x3D;@&#x2F;Users&#x2F;fungleo&#x2F;Downloads&#x2F;401.png&quot; -H &quot;token: 222&quot; -v</span><br></pre></td></tr></table></figure>

<p>更多 curl 的使用方法，以及参数说明，可以在系统中输入 man curl 来进行查看。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/fungleo/article/details/80703365" target="_blank" rel="noopener">curl 模拟 GET\POST 请求，以及 curl post 上传文件</a></li>
<li><a href="https://www.cnblogs.com/alfred0311/p/7988648.html" target="_blank" rel="noopener">curl 命令模拟 HTTP GET/POST 请求</a></li>
</ul>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>Google 三驾马车</title>
    <url>/2019/04/05/google-paper/</url>
    <content><![CDATA[<p>Google 三驾马车</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>谷歌是开启大数据世界的公司，它的三个论文横空出世影响了后续的大数据之路</p>
<p>作为一个入行者一定要读</p>
<h4 id="收藏"><a href="#收藏" class="headerlink" title="收藏"></a>收藏</h4><p><a href="https://mp.weixin.qq.com/s/eQ6BwLU9Twi_O9_99Ltviw" target="_blank" rel="noopener">GFS架构启示 Google File System</a><br><a href="https://mp.weixin.qq.com/s/HqJvoDbNQ9abq4vNqsRgkQ" target="_blank" rel="noopener">Google MapReduce到底解决什么问题？</a><br><a href="https://mp.weixin.qq.com/s/O-9msY6FpCseo7PgMPYEqg" target="_blank" rel="noopener">Google MapReduce有啥巧妙优化？</a><br><a href="https://mp.weixin.qq.com/s/D6dc3nX_3bQc0kFiH-7shA" target="_blank" rel="noopener">Google MapReduce架构设计</a><br><a href="https://mp.weixin.qq.com/s/jPbnjQ3FGGh_OuFCgzQzlw" target="_blank" rel="noopener">Google BigTable到底解决什么问题？</a></p>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title>SpringBoot 启动方式</title>
    <url>/2019/04/04/springboot-start/</url>
    <content><![CDATA[<p>SpringBoot 启动方式</p>
<hr>
<h4 id="IDEA"><a href="#IDEA" class="headerlink" title="IDEA"></a>IDEA</h4><p>启动类必须包含 @SpringBootApplication 这个注解</p>
<p>其他类目录要么跟Application同级，要么在其下级目录</p>
<p><img src="/images/blog/2019-04-04-1.png" alt></p>
<h4 id="MAVEN"><a href="#MAVEN" class="headerlink" title="MAVEN"></a>MAVEN</h4><p>进入到项目的pom.xml 层级，输入图中命令即可</p>
<p><img src="/images/blog/2019-04-04-2.png" alt></p>
<h4 id="JAR"><a href="#JAR" class="headerlink" title="JAR"></a>JAR</h4><p>maven 打包</p>
<p><img src="/images/blog/2019-04-04-3.png" alt></p>
<p>编译成功后进入target 目录下可以看到有个jar包</p>
<p><img src="/images/blog/2019-04-04-4.png" alt></p>
<p>jar 命令启动jar包</p>
<p><img src="/images/blog/2019-04-04-5.png" alt></p>
]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
  </entry>
  <entry>
    <title>HBase Snapshot 源码分析 1</title>
    <url>/2019/04/04/hbase-snapshot-source/</url>
    <content><![CDATA[<p>Snapshot是很多存储系统和数据库系统都支持的功能。</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>主要是以作者 clark010 文章为核心写的。 <a href="https://www.jianshu.com/p/74942f500a13" target="_blank" rel="noopener">原HBase Snapshot - 1 - 简介</a> 。</p>
<p>参考众多文章汇总的文章。无意商用和抄袭。只是个人学习的总结。</p>
<p>具体参考请看文末尾的链接。</p>
<h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><p>HBase 的 Snapshot 及 Restore 都不涉及文件的移动和拷贝，操作耗时在秒级别；</p>
<p>因为底层依赖的 HDFS FileSystem 不支持硬链接，所以 HBase 自己实现了一套FileLink的逻辑，每次snapshot都只是进行文件的link而不实际移动数据，</p>
<p>而这保证了snapshot操作的效率（虽然不可避免的需要进行一下内存flush，但基本最耗时的消耗都在flush这一下）。</p>
<h4 id="Snapshot相关存储路径"><a href="#Snapshot相关存储路径" class="headerlink" title="Snapshot相关存储路径"></a>Snapshot相关存储路径</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;[hbase-root]</span><br><span class="line">    |__ .hbase-snapshot  存储所有snapshot的元信息</span><br><span class="line">        |__ .tmp&#x2F;  snapshot的workDir，临时数据存放</span><br><span class="line">        |__ [snapshot name]</span><br><span class="line">            |__ .snapshotinfo  snapshot的元信息</span><br><span class="line">            |__ data.manifest  snapshot相关hfile的元信息</span><br><span class="line">    |__ data</span><br><span class="line">        |__ [namespace]  </span><br><span class="line">            |__ [table]</span><br><span class="line">                |__ .tabledesc</span><br><span class="line">                    |__ .tableinfo.[id]</span><br><span class="line">                |__ [encode region]</span><br><span class="line">                    |__ .regioninfo</span><br><span class="line">                    |__ [column family]</span><br><span class="line">                        |__ [HFile &#x2F; Link Files] HBase底层支持HFile及链接文件</span><br><span class="line">                        |__ .links-[regionName]  back reference，用于快速的删除无用的引用文件</span><br><span class="line">                            |__ [ref files]</span><br><span class="line">                    |__ .....</span><br><span class="line">    |__ archive</span><br><span class="line">        |__ data</span><br><span class="line">            |__ [namespace]</span><br><span class="line">                |__ [table]</span><br><span class="line">                    |__ [encode region]</span><br><span class="line">                        |__ [column family]</span><br><span class="line">                            |__ [HFile &#x2F; Link Files] </span><br><span class="line">    |__ WALs &#x2F; oldWALs &#x2F; ......</span><br></pre></td></tr></table></figure>

<h4 id="主要Java类"><a href="#主要Java类" class="headerlink" title="主要Java类"></a>主要Java类</h4><p>Client：</p>
<blockquote>
<ul>
<li>HBaseAdmin  - 入口类</li>
</ul>
</blockquote>
<p>Master：</p>
<blockquote>
<ul>
<li>MasterRpcServices  - 接收Client端的RPC请求</li>
<li>SnapshotManager</li>
<li>EnabledTableSnapshotHandler extends TableSnapshotHandler  - 在线表</li>
<li>DisabledTableSnapshotHandler extends TableSnapshotHandler  - 离线表</li>
<li>ProcedureCoordinator  - 用于提交分布式snapshot事务</li>
<li>Procedure</li>
<li>ZKProcedureCoordinatorRpcs extends ProcedureCoordinator</li>
<li>SnapshotFileCache  - 缓存snapshot引用文件，用于判断文件是否deletable</li>
<li>SnapshotFileCleaner - 清理snapshot文件线程</li>
</ul>
</blockquote>
<p>RegionServer：</p>
<blockquote>
<ul>
<li>RegionServerSnapshotManager - 监控分布式任务，并创建管理具体子任务</li>
<li>FlushSnapshotSubprocedure</li>
<li>RegionSnapshotTask - FlushSnapshotSubprocedure内部类</li>
<li>HRegion - 调用snapshot接口，处理具体的snapshot任务</li>
<li>SnapshotManifest - Utility class to help read/write the Snapshot Manifest</li>
<li>SnapshotManifestV2/SnapshotManifestV1 - SnapshotManifest内存数据结构</li>
</ul>
</blockquote>
<hr>
<p>参考链接</p>
<ul>
<li><a href="http://hbasefly.com/2017/09/17/hbase-snapshot/" target="_blank" rel="noopener">HBase原理 – 分布式系统中snapshot是怎么玩的？</a></li>
<li><a href="https://www.jianshu.com/p/74942f500a13" target="_blank" rel="noopener">原HBase Snapshot - 1 - 简介</a></li>
<li><a href="https://www.jianshu.com/p/e1a2baa790d1" target="_blank" rel="noopener">原HBase Snapshot- 2 -Snapshot源码分析</a></li>
<li><a href="https://www.cnblogs.com/cenyuhai/p/3712943.html" target="_blank" rel="noopener">hbase源码系列（七）Snapshot的过程</a></li>
<li><a href="https://www.cnblogs.com/cenyuhai/p/3721247.html" target="_blank" rel="noopener">hbase源码系列（八）从Snapshot恢复表</a></li>
<li><a href="https://ppg.iteye.com/blog/1888453" target="_blank" rel="noopener">HBase Snapshot 源码 流程</a></li>
<li><a href="https://www.cnblogs.com/foxmailed/p/3914117.html" target="_blank" rel="noopener">HBase Snapshot原理和实现</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>HBase 表增加 Snappy 压缩</title>
    <url>/2019/04/03/hbase-snappy/</url>
    <content><![CDATA[<p>HBase 表增加 Snappy 压缩</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>公司有业务要上 HBase。 数据量较大。要保存30天内的日志数据。因此觉得应该对该表增加 Snappy 压缩</p>
<h4 id="验证-snappy-是否正常安装"><a href="#验证-snappy-是否正常安装" class="headerlink" title="验证 snappy 是否正常安装"></a>验证 snappy 是否正常安装</h4><p>snappy 的介绍和安装这里不介绍了。感兴趣的朋友可以读下这篇博客 <a href="https://www.cnblogs.com/shitouer/archive/2013/01/14/2859475.html" target="_blank" rel="noopener">Hadoop HBase 配置 安装 Snappy 终极教程</a></p>
<p>CDH中，直接安装了snappy的库，所以直接用了。</p>
<p>查看 Hadoop 是否支持压缩</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;hadoop checknative</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/2019-07-01-6.png" alt></p>
<p>检查当前HBase是否支持压缩</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;hbase org.apache.hadoop.util.NativeLibraryChecker</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/2019-07-01-7.png" alt></p>
<p>找某个文件，对其进行压缩测试。</p>
<p>我这里就拿 HBase log 日志试试手。 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;var&#x2F;log&#x2F;hbase</span><br><span class="line">hbase org.apache.hadoop.hbase.util.CompressionTest hbase-cmf-hbase-MASTER-massive-dataset-new-004.log.out  snappy</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/2019-04-03-1.png" alt></p>
<p>如果正确安装 snappy 则会显示 SUCCESS 字眼 </p>
<h4 id="增加-snappy-压缩"><a href="#增加-snappy-压缩" class="headerlink" title="增加 snappy 压缩"></a>增加 snappy 压缩</h4><h5 id="1-创建表时指定压缩算法"><a href="#1-创建表时指定压缩算法" class="headerlink" title="1. 创建表时指定压缩算法"></a>1. 创建表时指定压缩算法</h5><p>create ‘test’, {NAME =&gt; ‘info’, VERSIONS =&gt; 1, COMPRESSION =&gt; ‘snappy’}</p>
<h5 id="2-创建表后指定或修改压缩算法"><a href="#2-创建表后指定或修改压缩算法" class="headerlink" title="2. 创建表后指定或修改压缩算法"></a>2. 创建表后指定或修改压缩算法</h5><p>虽然不 disable 也能修改表结构，但是为了降低影响，建议先 disable。 修改完之后在 enable。</p>
<p>注意: 如果表较大，disable需要一些时间，请耐心等待。</p>
<p>修改表定义</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">alter &#39;mytable&#39;, &#123;NAME&#x3D;&gt;&#39;cf&#39;,COMPRESSION &#x3D;&gt; &#39;snappy&#39;&#125;</span><br></pre></td></tr></table></figure>

<h4 id="使压缩生效"><a href="#使压缩生效" class="headerlink" title="使压缩生效"></a>使压缩生效</h4><p>表中此前的数据，还没有压缩，如果要让整个表的数据都压缩，需要对表进行 major compact 。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">major_compact &#39;mytable&#39;</span><br></pre></td></tr></table></figure>

<p>注意: 如果表的数据较多，该操作需要较长时间，所以尽量选择一个不忙的时间，避免对服务造成影响。</p>
<h4 id="压缩比"><a href="#压缩比" class="headerlink" title="压缩比"></a>压缩比</h4><p>打开该表所在某个 RS 的 Web 界面，在 Region Name 找到表所在行，看 Storefile Metrics 中的 </p>
<p>storefileUncompressedSize , storefileSize 。 就可以看得，压缩前后容量。</p>
<p><img src="/images/blog/2019-04-03-2.png" alt></p>
<p>假设Region 数据如下</p>
<p>storefileUncompressedSizeMB=160, storefileSizeMB=66</p>
<p>则压缩比算出来为 compressionRatio=0.4125</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blackwing.iteye.com/blog/1942037" target="_blank" rel="noopener">HBase表增加snappy压缩</a></li>
<li><a href="https://www.cnblogs.com/wxyidea/p/9347362.html" target="_blank" rel="noopener">HBase启用压缩</a></li>
<li><a href="https://www.cnblogs.com/shitouer/p/hbase-table-alter-compression-type-and-snappy-compression-compare.html" target="_blank" rel="noopener">HBase修改压缩格式及Snappy压缩实测分享</a></li>
<li><a href="https://www.cnblogs.com/tyoyi/p/4538830.html" target="_blank" rel="noopener">HBase使用压缩存储（snappy）</a></li>
<li><a href="https://blog.csdn.net/u014414323/article/details/81170607" target="_blank" rel="noopener">第十五记·HBase压缩、HBase与Hive集成详解</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>HBase Snapshots 机制</title>
    <url>/2019/03/24/Introduction-hbase-snapshots/</url>
    <content><![CDATA[<p>Apache HBase 快照机制。</p>
<hr>
<h4 id="什么是快照"><a href="#什么是快照" class="headerlink" title="什么是快照"></a>什么是快照</h4><p>快照是一组元数据信息，允许管理员返回到表的先前状态。<br>快照不是表的副本; 它只是一个文件名列表，不会复制数据。<br>完整快照还原意味着您将返回到先前的“表状态”，即恢复以前的数据，从而丢失自快照以来所做的任何更改。</p>
<h4 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h4><blockquote>
<ul>
<li>拍摄快照：此操作尝试在指定的表上拍摄快照。如果区域在平衡，拆分或合并期间移动，则操作可能会失败。</li>
<li>克隆快照：此操作使用相同的模式创建新表，并在指定的快照中显示相同的数据。此操作的结果是一个新的全功能表，可以在不影响原始表或快照的情况下进行修改。</li>
<li>还原快照：此操作将表架构和数据恢复为快照状态。（注意：此操作会丢弃自拍摄快照以来所做的任何更改。）</li>
<li>删除快照：此操作从系统中删除快照，释放非共享磁盘空间，而不会影响任何克隆或其他快照。</li>
<li>导出快照：此操作将快照数据和元数据复制到另一个群集。该操作仅涉及HDFS，因此与 HMaster 或 RS 没有交互，所以 HBase 群集可以处于停止状态。</li>
</ul>
</blockquote>
<h4 id="零拷贝快照，还原，克隆"><a href="#零拷贝快照，还原，克隆" class="headerlink" title="零拷贝快照，还原，克隆"></a>零拷贝快照，还原，克隆</h4><p>快照和 CopyTable / ExportTable 之间的主要区别在于快照操作仅写入元数据。没有涉及大量数据副本。</p>
<p>HBase的主要设计原则之一是，一旦文件被写入，它将永远不会被修改。拥有不可变文件意味着快照只跟踪快照操作时使用的文件，并且在压缩期间，快照负责通知系统不应删除该文件，而应将其归档。</p>
<p>同样的原则适用于克隆或还原操作。由于文件是不可变的，因此创建一个新表，只使用快照引用的文件的“链接”。</p>
<p>导出快照是唯一需要数据副本的操作，因为其他群集没有数据文件。</p>
<h4 id="导出快照-vs-复制-导出表"><a href="#导出快照-vs-复制-导出表" class="headerlink" title="导出快照 vs 复制/导出表"></a>导出快照 vs 复制/导出表</h4><p>除了复制/导出作业与快照之间提供的更好的一致性保证之外，</p>
<p>导出快照和复制/导出表之间的主要区别在于 ExportSnapshot 在HDFS级别运行。这意味着主服务器和区域服务器不参与此操作。</p>
<p>因此，在扫描过程中创建的对象数量不会创建不必要的数据高速缓存，不会触发其他GC暂停。</p>
<p>对HBase集群的性能影响源于 DataNode 所经历的额外网络和磁盘工作负载。</p>
<h4 id="HBase-Shell"><a href="#HBase-Shell" class="headerlink" title="HBase Shell"></a>HBase Shell</h4><p>通过检查 hbase-site.xml 文件中 hbase.snapshot.enabled 中的属性是否设置为true，确认已启用快照支持。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 要获取指定表的快照，请使用该snapshot命令。（不执行文件复制）</span><br><span class="line">hbase&gt; snapshot ‘tableName’, ‘snapshotName’</span><br><span class="line"> </span><br><span class="line"># 要列出所有快照，请使用该list_snapshot命令。它将显示快照名称，源表以及创建日期和时间。</span><br><span class="line">hbase&gt; list_snapshots</span><br><span class="line">SNAPSHOT               TABLE + CREATION TIME</span><br><span class="line"> TestSnapshot          TestTable (Mon Feb 25 21:13:49 +0000 2013)</span><br><span class="line"></span><br><span class="line"># 要删除快照，请使用该delete_snapshot命令。删除快照不会影响克隆表或其他后续快照。</span><br><span class="line">hbase&gt; delete_snapshot &#39;snapshotName&#39;</span><br><span class="line"></span><br><span class="line"># 要从指定的快照（克隆）创建新表，请使用该clone_snapshot命令。不会执行任何数据副本，因此您最终不会使用两倍的空间来存储相同的数据。</span><br><span class="line">hbase&gt; clone_snapshot &#39;snapshotName&#39;, &#39;newTableName&#39;</span><br><span class="line"></span><br><span class="line"># 要使用指定的快照内容替换当前表模式&#x2F;数据，请使用该restore_snapshot 命令。</span><br><span class="line"># 用快照恢复数据，它需要先禁用表，再进行恢复</span><br><span class="line">hbase&gt; disable &#39;tableName&#39; </span><br><span class="line">hbase&gt; restore_snapshot &#39;snapshotName&#39;</span><br><span class="line"></span><br><span class="line"># 要将现有快照导出到另一个群集，请使用该ExportSnapshot工具。导出不会影响RegionServers工作负载，它在HDFS级别工作，您必须指定HDFS位置（另一个群集的hbase.rootdir）。</span><br><span class="line"># 该操作要用hbase的账户执行，并且在hdfs当中要有hbase的账户建立的临时目录（hbase.tmp.dir参数控制）</span><br><span class="line"># 采用16个mappers来把一个名为MySnapshot的快照复制到一个名为srv2的集群当中</span><br><span class="line"># DN在拷贝数据的时候需要额外的带宽以及IO负载，ExportSnapshot也针对这个问题设置了参数-bandwidth来限制带宽的使用。</span><br><span class="line">hbase org.apache.hadoop.hbase.snapshot.ExportSnapshot -snapshot SnapshotName -copy-to hdfs:&#x2F;&#x2F;&#x2F;srv2:8082&#x2F;hbase -mappers 16 -bandwidth  1024</span><br></pre></td></tr></table></figure>


<h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><p>HBase 数据文件一旦落到磁盘之后就不再允许更新删除等原地修改操作，如果想更新删除的话可以追加写入新文件。<br>（HBase中根本没有更新接口，删除命令也是追加写入）</p>
<p>利用 HBase 已经落盘的文件不会更改的特点。只需要对快照表对应的所有 HFile 文件创建好指针即可。</p>
<p><img src="/images/blog/2019-04-04-6.png" alt></p>
<p>Snapshot 的流程分为4个步骤</p>
<ol>
<li><p>对该表添加全局锁，不允许任何数据的写入、更新和删除</p>
</li>
<li><p>将 memstore 中的数据 flush 到 HFile 文件中 (可选)</p>
</li>
<li><p>为该表涉及的各个 region 中所有 HFile 文件创建引用指针，并记录到 snapshot manifest 文件中</p>
</li>
<li><p>HMaster 将所有的 region 的 snapshot 文件进行汇总形成总 snapshot manifest 文件. </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">snapshot &#39;mytable&#39;, &#39;snapshot123&#39;, &#123;SKIP_FLUSH&#x3D;true&#125;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>可以选择是否跳过 flush</p>
<h4 id="两阶段提交"><a href="#两阶段提交" class="headerlink" title="两阶段提交"></a>两阶段提交</h4><p>关于两阶段提交。大家可以看看这篇博文<a href="https://mp.weixin.qq.com/s/kXtliHCqtrTjlWFm1aHtVQ" target="_blank" rel="noopener">分布式基础，啥是两阶段提交？</a></p>
<p>HBase 采用两阶段提交的方式来保证 Snapshot 的原子性，要么成功，要么失败。</p>
<p>两阶段提交分为 prepare 阶段和 commit 阶段。</p>
<ol>
<li><p>prepare 阶段：HMaster 在 ZK 的”/hbase/online-snapshot/acquired”下创建一个’acquired-snapshotname’节点，并在此节点上写入snapshot相关信息（snapshot表信息）。<br>所有regionserver监测到这个节点之后，根据 acquired-snapshotname 节点携带的 snapshot 表信息查看当前 RS 上是否存在目标表，如果不存在，就忽略该命令。<br>如果存在，遍历目标表中的所有 region，分别针对每个 region 执行 snapshot 操作，注意此处 snapshot 操作的结果并没有写入最终文件夹，而是写入临时文件夹。<br>RS 执行完成之后会在 acquired-snapshotname 节点下新建一个子节点 acquired-snapshotname/nodex，表示 nodex 节点完成了该 RS 上所有相关 region 的 snapshot 准备工作。</p>
</li>
<li><p>commit 阶段：一旦所有 RS 都完成了 snapshot 的 prepared 工作，即都在 /hbase/online-snapshot/acquired/acquired-snapshotname 节点下新建了对应子节点，<br>HMaster 就认为 snapshot 的准备工作完全完成。HMaster 会新建一个新的节点 /hbase/online-snapshot/reached/reached-snapshotname，表示发送一个 commit 命令给参与的 RS。<br>所有 RS 监测到 reached-snapshotname 节点之后，执行snapshot commit操作，commit 操作非常简单，只需要将 prepare 阶段生成的结果从临时文件夹移动到最终文件夹即可。<br>执行完成之后在 reached-snapshotname 节点下新建子节点 reached-snapshotname/nodex，表示节点nodex完成snapshot工作。</p>
</li>
</ol>
<p>如果所有参与的 regionserver 都在 /reached-snapshotname 下创建的子节点，则 HMaster 确认快照创建已经成功。如果一定时间内，reached-snapshotname 下的子节点没有满足条件或者 prepare 阶段中<br>/acquired-snapshot 下的子节点不满足条件，则会进入第3个 abort 阶段</p>
<ol start="3">
<li>abort阶段：HMaster 会认为快照创建超时，进行回滚操作。此时 HMaster 会在ZK上创建 /hbase/online-snapshot/abort/abort-snapshotname 节点，所有 RS 监听到会清理临时 snapshot 在临时文件夹中的生成结果。</li>
</ol>
<h4 id="核心实现"><a href="#核心实现" class="headerlink" title="核心实现"></a>核心实现</h4><p>Snapshot 两个原子性操作 </p>
<ul>
<li>每个 region 真正实现 snapshot</li>
<li>HMaster 又是如何汇总所有 region snapshot 结果</li>
</ul>
<h5 id="region-如何实现-snapshot"><a href="#region-如何实现-snapshot" class="headerlink" title="region 如何实现 snapshot ?"></a>region 如何实现 snapshot ?</h5><p>在基本原理一节提到过 snapshot 不会真正拷贝数据，而是使用指针引用的方式创建一系列元数据。</p>
<p>那元数据具体是什么样的元数据呢？实际上snapshot的整个流程基本如下:</p>
<p><img src="/images/blog/2019-06-21-1.png" alt></p>
<p>分别对应 Debug 日志中如下片段:</p>
<p><img src="/images/blog/2019-06-21-2.png" alt></p>
<p><img src="/images/blog/2019-06-21-3.png" alt></p>
<p>注意：region 生成的 snapshot manifest 文件是临时文件，生成目录在 /hbase/.hbase-snapshot/.tmp 下，一般因为 snapshot 过程特别快，<br>所以很难看到单个 region 生成的 snapshot 文件。</p>
<h5 id="HMaster-如何汇总所有-region-snapshot-的结果？"><a href="#HMaster-如何汇总所有-region-snapshot-的结果？" class="headerlink" title="HMaster 如何汇总所有 region snapshot 的结果？"></a>HMaster 如何汇总所有 region snapshot 的结果？</h5><p>HMaster 会在所有 region 完成 snapshot 之后执行一个汇总操作（consolidate），将所有 region snapshot manifest 汇总成一个单独 manifest。</p>
<p>汇总后的 snapshot 文件是可以在 HDFS 目录下看到的，路径为: /hbase/.hbase-snapshot/{snapshotname}/data.manifest。(snapshotname 为快照名字)</p>
<p>注意，snapshot 目录下有3个文件，如图所示:</p>
<p><img src="/images/blog/2019-06-21-4.png" alt></p>
<p>其中 .snapshotinfo 为 snapshot 基本信息，包含待 snapshot 的表名称以及 snapshot 名；</p>
<p>data.manifest 为 snapshot 执行后生成的元数据信息，<br>即 snapshot 结果信息。可以使用 hadoop dfs -cat /hbase/.hbase-snapshot/{snapshotname}/data.manifest 查看</p>
<p><img src="/images/blog/2019-06-21-5.png" alt></p>
<h4 id="快照的使用示例"><a href="#快照的使用示例" class="headerlink" title="快照的使用示例"></a>快照的使用示例</h4><h5 id="1-更改表名"><a href="#1-更改表名" class="headerlink" title="1. 更改表名"></a>1. 更改表名</h5><p>因为 HBase 中没有 Rename 命令, 所以更改表名比较复杂。重命名主要通过 HBase 的快照功能。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 停止表继续插入</span><br><span class="line">hbase shell&gt; disable &#39;tableName&#39;</span><br><span class="line"></span><br><span class="line"># 制作快照</span><br><span class="line">hbase shell&gt; snapshot &#39;tableName&#39;, &#39;tableSnapshot&#39;</span><br><span class="line"></span><br><span class="line"># 克隆快照为新的名字</span><br><span class="line">hbase shell&gt; clone_snapshot &#39;tableSnapshot&#39;, &#39;newTableName&#39;</span><br><span class="line"></span><br><span class="line"># 删除快照</span><br><span class="line">hbase shell&gt; delete_snapshot &#39;tableSnapshot&#39;</span><br><span class="line"></span><br><span class="line"># 删除原来表</span><br><span class="line">hbase shell&gt; drop &#39;tableName&#39;</span><br></pre></td></tr></table></figure>

<h5 id="2-恢复表"><a href="#2-恢复表" class="headerlink" title="2. 恢复表"></a>2. 恢复表</h5><p>从用户/应用程序错误中恢复表。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 制作快照</span><br><span class="line">hbase shell&gt; snapshot &#39;tableName&#39;, &#39;tableSnapshot&#39;</span><br><span class="line"></span><br><span class="line"># 下线表</span><br><span class="line">hbase shell&gt; disable &#39;tableName&#39;</span><br><span class="line"></span><br><span class="line"># 还原快照</span><br><span class="line">hbase shell&gt; restore_snapshot &#39;tableSnapshot&#39;</span><br><span class="line"></span><br><span class="line"># 上线表</span><br><span class="line">hbase shell&gt; enable &#39;tableName&#39;</span><br></pre></td></tr></table></figure>

<h5 id="3-表迁移"><a href="#3-表迁移" class="headerlink" title="3. 表迁移"></a>3. 表迁移</h5><p>HBase Snapshot 可以在对 RS 影响很小的情况下创建快照、将快照复制到另一个集群。</p>
<p>由于导出快照在 HDFS 级别运行，因此不会像 CopyTable 那样减慢主 HBase 群集的速度。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 在源表上创建快照。</span><br><span class="line"># hbase snapshot create -n $SnapshotName -t $TableName</span><br><span class="line">hbase shell&gt; snapshot &#39;tableName&#39;, &#39;tableSnapshot&#39;</span><br><span class="line"></span><br><span class="line"># 将snapshot拷贝到目标集群的HDFS上。</span><br><span class="line">hbase org.apache.hadoop.hbase.snapshot.ExportSnapshot -snapshot $SnapshotName -copy-from $SrcSnapshotPath -copy-to $DstSnapshotPath</span><br><span class="line"></span><br><span class="line"># 在目标集群恢复snapshot。</span><br><span class="line">hbase shell&gt; restore_snapshot &#39;tableSnapshot&#39;</span><br></pre></td></tr></table></figure>

<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.cloudera.com/blog/2013/03/introduction-to-apache-hbase-snapshots/" target="_blank" rel="noopener">Introduction to Apache HBase Snapshots</a></li>
<li><a href="https://blog.csdn.net/zfy1355/article/details/47605021" target="_blank" rel="noopener">hbase 更改表名</a></li>
<li><a href="https://mp.weixin.qq.com/s/QHxrscg5J3-qzpOxU3pMfQ" target="_blank" rel="noopener">HBase数据导入工具总结</a></li>
<li><a href="https://www.cnblogs.com/cenyuhai/p/3309033.html" target="_blank" rel="noopener">Hbase 学习（十） HBase Snapshots</a></li>
<li><a href="https://www.jianshu.com/p/8d091591d872" target="_blank" rel="noopener">玩转HBase快照</a></li>
<li><a href="http://hbasefly.com/2017/09/17/hbase-snapshot/" target="_blank" rel="noopener">HBase原理 – 分布式系统中snapshot是怎么玩的？</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>HBase 编译</title>
    <url>/2019/03/21/hbase-patch-maven/</url>
    <content><![CDATA[<p>HBase 源码调试后编译</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>说真的，一直说看源码看源码，但是压根不知道怎么入坑。</p>
<p>去官网下载源码下来就难道对着 API 文档一直看？？</p>
<p>好像不是很实用吧。估计大家看着看着就放弃了。</p>
<p>这里做点有成就感的事。</p>
<h4 id="Patch"><a href="#Patch" class="headerlink" title="Patch"></a>Patch</h4><p>有时候我们下载的版本存在一些 bug。 这时就需要打补丁文件了。</p>
<p>可以到官网查看你当前版本需要打的补丁</p>
<p><img src="/images/blog/2019-03-21-1.png" alt></p>
<p>在这里，你可以看到你所在版本修复的补丁。</p>
<p><img src="/images/blog/2019-03-21-2.png" alt></p>
<p>根据编号。去 Apache 的 <a href="http://issues.apache.org/jira/" target="_blank" rel="noopener">JIRA</a> 找到对应的 issue。</p>
<p>这里我们以 HBASE-21592 为例。可以看到它有好几个 patch 文件。</p>
<p><img src="/images/blog/2019-03-21-3.png" alt></p>
<p>我们把 master 的 patch 文件下载下来。并利用 IDEA 打入 HBase 源码中。</p>
<p>如何使用 IDEA 生成 patch 和使用 patch。请参考 <a href="https://blog.csdn.net/lx_yoyo/article/details/75453708" target="_blank" rel="noopener">资料1</a></p>
<h4 id="编译、打包"><a href="#编译、打包" class="headerlink" title="编译、打包"></a>编译、打包</h4><p>对代码进行修改之后，想将其验证。这时就需要编译、打包了。</p>
<p>在项目下执行 mvn clean package -DskipTests=true assembly:single</p>
<p>成功后，在 hbase-1.2.9/hbase-assembly/target 目录下有目标tar包。</p>
<p><img src="/images/blog/2019-03-21-4.png" alt></p>
<p>hbase-1.2.9-bin.tar.gz 即为二进制tar包。将其部署即可。</p>
<p>但是大部分场景是针对已经运行的 HBase 集群。 因此这里将采取替换 jar 包的方式来实现。</p>
<p>假设修改的是 client 模块的代码。打包编译后可以在该模块的 target 目录下看到一个 jar 包</p>
<p><img src="/images/blog/2019-03-21-5.png" alt></p>
<p>将其替换到线上的 HBase 集群的 lib 包下。</p>
<p>并重启 HBase 集群。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/rkjava/article/details/48789571" target="_blank" rel="noopener">HBase1.1.2编译</a></li>
<li><a href="https://blog.csdn.net/xuguokun1986/article/details/50755269" target="_blank" rel="noopener">hbase编译打包</a></li>
<li><a href="https://www.cnblogs.com/bigfanofcpp/archive/2012/05/03/2871855.htm" target="_blank" rel="noopener">Hbase测试&amp;打补丁（HBASE-5415.patch）</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>Linux 实现文件的去重</title>
    <url>/2019/03/21/linux-remove-duplication/</url>
    <content><![CDATA[<p>Linux 实现文件的去重</p>
<hr>
<h4 id="转载出处"><a href="#转载出处" class="headerlink" title="转载出处"></a>转载出处</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">作者：大数据技术宅</span><br><span class="line">链接：https:&#x2F;&#x2F;www.cnblogs.com&#x2F;followees&#x2F;p&#x2F;7845575.html</span><br><span class="line">来源：博客园</span><br></pre></td></tr></table></figure>

<h4 id="实现文件的去重"><a href="#实现文件的去重" class="headerlink" title="实现文件的去重"></a>实现文件的去重</h4><h5 id="1-两个文件的交集，并集"><a href="#1-两个文件的交集，并集" class="headerlink" title="1. 两个文件的交集，并集"></a>1. 两个文件的交集，并集</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 取出两个文件的并集(重复的行只保留一份)</span><br><span class="line"></span><br><span class="line">cat file1 file2 | sort | uniq &gt; file3</span><br><span class="line"></span><br><span class="line"># 取出两个文件的交集(只留下同时存在于两个文件中的文件)</span><br><span class="line"></span><br><span class="line">cat file1 file2 | sort | uniq -d &gt; file3</span><br><span class="line"></span><br><span class="line"># 删除交集，留下其他的行</span><br><span class="line"></span><br><span class="line">cat file1 file2 | sort | uniq -u &gt; file3</span><br></pre></td></tr></table></figure>

<h5 id="2-两个文件合并"><a href="#2-两个文件合并" class="headerlink" title="2. 两个文件合并"></a>2. 两个文件合并</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 一个文件在上，一个文件在下</span><br><span class="line"></span><br><span class="line">cat file1 file2 &gt; file3</span><br><span class="line"></span><br><span class="line"># 一个文件在左，一个文件在右</span><br><span class="line"></span><br><span class="line">paste file1 file2 &gt; file3</span><br></pre></td></tr></table></figure>

<h5 id="3-一个文件去掉重复的行"><a href="#3-一个文件去掉重复的行" class="headerlink" title="3. 一个文件去掉重复的行"></a>3. 一个文件去掉重复的行</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sort file | uniq &gt; file1</span><br><span class="line"></span><br><span class="line">注意：重复的多行记为一行，也就是说这些重复的行还在，只是全部省略为一行！</span><br><span class="line"></span><br><span class="line">sort file | uniq –u &gt; file1</span><br><span class="line"></span><br><span class="line">上面的命令可以把重复的行全部去掉，也就是文件中的非重复行！</span><br></pre></td></tr></table></figure>


]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>HBase Shell</title>
    <url>/2019/03/19/hbase-shell-client/</url>
    <content><![CDATA[<p>介绍最基本的HBase Shell的使用命令。</p>
<hr>
<h4 id="使用-HBase-Shell-访问"><a href="#使用-HBase-Shell-访问" class="headerlink" title="使用 HBase Shell 访问"></a>使用 HBase Shell 访问</h4><p>如果在 HBase 本身的机器上的话，找到 HBase bin 目录下启动 bin/hbase shell 即可</p>
<p>在本地搭建访问 HBase 集群的 Shell 客户端的话，需要配置一下 hbase-site.xml</p>
<p>官网下载二进制包即可。</p>
<p><img src="/images/blog/2019-03-19-1.png" alt></p>
<p>wget <a href="http://mirrors.tuna.tsinghua.edu.cn/apache/hbase/hbase-1.2.11/hbase-1.2.11-bin.tar.gz" target="_blank" rel="noopener">http://mirrors.tuna.tsinghua.edu.cn/apache/hbase/hbase-1.2.11/hbase-1.2.11-bin.tar.gz</a> </p>
<p>修改 conf/hbase-site.xml 文件，添加集群的 ZK 地址。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>100.73.12.12<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">description</span>&gt;</span>ZooKeeper 地址<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>zookeeper.znode.parent<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">description</span>&gt;</span>ZooKeeper 中 HBase 的根 znode。<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>访问集群通过如下命令就可以访问集群了。</p>
<p>bin/hbase shell</p>
<h4 id="HBase-Shell-入门"><a href="#HBase-Shell-入门" class="headerlink" title="HBase Shell 入门"></a>HBase Shell 入门</h4><h5 id="1-展示HBase-Shell的帮助信息"><a href="#1-展示HBase-Shell的帮助信息" class="headerlink" title="1. 展示HBase Shell的帮助信息"></a>1. 展示HBase Shell的帮助信息</h5><p>help命令提供了很多基本的命令和对应的使用方法，当你忘记一些基本用法的时候，记得输入help来查看。</p>
<p><code>hbase(main):001:0&gt; help</code></p>
<p>如果要查看具体的命令帮助的话在后面加个命令名称</p>
<p><code>hbase(main):002:0&gt; help &#39;list&#39;</code></p>
<h5 id="2-基本命令"><a href="#2-基本命令" class="headerlink" title="2. 基本命令"></a>2. 基本命令</h5><p>使用create命令来创建一张新的表，在创建的时候你必须输入表的名称和ColumnFamily的名称</p>
<p>使用list命令来查询已经创建的表。</p>
<p>往表里面插入记录。在hbase中，往表里面写一行记录的命令叫做put</p>
<p>查询表中的所有数据。scan是一种访问HBase数据的方式，它非常的灵活，你可以用它来扫描全表，也可以用它查询固定范围</p>
<p>查询单条记录。使用get来查询单条记录</p>
<p>禁用一张表。使用disable命令能够禁用一张表，使用enable命令能够取消禁用，恢复禁用的表。</p>
<p>删除一张表。使用drop命令，这是一个危险的操作，使用的时候请务必小心。</p>
<p>退出HBase Shell。输入quit命令就可以离开HBase Shell环境了。</p>
<p>需要详细的介绍者请自行百度。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://help.aliyun.com/document_detail/52056.html?spm=a2c4g.11186623.6.555.3f7457bdKfIVRV" target="_blank" rel="noopener">阿里云使用 Shell 访问</a></li>
<li><a href="https://help.aliyun.com/document_detail/52057.html?spm=a2c4g.11186623.6.554.4f553294ENHqYE" target="_blank" rel="noopener">阿里云HBase Shell 入门</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>HBase 源码调试</title>
    <url>/2019/03/19/hbase-src-debug/</url>
    <content><![CDATA[<p>为了更深入的了解HBase的工作流程，对源码进行跟踪调试是一个很好的办法。</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>为了更深入的了解HBase的工作流程，对源码进行跟踪调试是一个很好的办法。</p>
<h4 id="下载源码"><a href="#下载源码" class="headerlink" title="下载源码"></a>下载源码</h4><p>这里使用的版本是社区版的 1.2.6</p>
<p>下载网址 <a href="http://archive.apache.org/dist/hbase/" target="_blank" rel="noopener">http://archive.apache.org/dist/hbase/</a></p>
<h4 id="编译HBase源码"><a href="#编译HBase源码" class="headerlink" title="编译HBase源码"></a>编译HBase源码</h4><p>网上很多教程说需要先编译Hadoop源码才能编译HBase，但是本人亲测，如果只是在单机节点测试，即使用standalone模式是不需要事先编译Hadoop源码的。</p>
<p>在编译源码前，需要准备JDK，Maven等工具。请自行安装</p>
<p>jdk，版本1.8</p>
<p>maven ，版本是3.3.9</p>
<p>下载记得添加环境变量。</p>
<p>准备好这些工具就可以开始编译了，直接去HBase源文件所在的目录。</p>
<p>执行下方命令编译</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mvn clean package -DskipTests&#x3D;true</span><br><span class="line"></span><br><span class="line"># 忽略</span><br><span class="line">## mvn clean install -DskipTests&#x3D;true</span><br></pre></td></tr></table></figure>

<p>编译成功后仍然在当前目录执行mvn idea:idea，这样是为了生成idea所需要的.project文件。</p>
<p>两次都需要看到 BUILD SUCCESS 才行。</p>
<p><img src="/images/blog/2019-03-19-2.png" alt></p>
<p>接下来就是在idea里引入HBase。这一project即可。</p>
<h4 id="HMaster"><a href="#HMaster" class="headerlink" title="HMaster"></a>HMaster</h4><p>Debug Configuration设置</p>
<p>为了能在单机节点上运行HBase，我们需要在conf/hbase-site.xml下设置相关数据的存储目录。<br>即在hbase-site.xml里加入以下配置信息。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///Users/tu/Public/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.defaults.for.version.skip<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>其中，hbase.rootdir的value是用来存放HBase数据的目录，你不需要事先创建hbase目录，<br>只需要事先保证/Users/tu/Public目录存在就行。当然，你也可以自己配置成别的目录。</p>
<p>hbase.defaults.for.version.skip 是避免出现后文中说的异常情况。</p>
<p>HBase架构里有一个HMaster是负责管理整个集群的。所以我们程序的起点也就在 HMaster 这个类里面</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">org.apache.hadoop.hbase.master.HMaster</span><br><span class="line"></span><br><span class="line">-Dlog4j.configuration&#x3D;file:&#x2F;Users&#x2F;tu&#x2F;IdeaProjects&#x2F;hbase-1.2.6&#x2F;conf&#x2F;log4j.properties</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/2019-03-19-3.png" alt></p>
<p>HMaster 应该就可以启动了。当在 Console 里没看见错误，就意味着 HMaster 成功启动了，你也可以在浏览器中输入<a href="http://localhost:16010" target="_blank" rel="noopener">http://localhost:16010</a> 来验证，</p>
<p>类似的，我们也可以把 arguments 改成 stop，新增一个让 HMaster 结束的 JAVA Application。</p>
<p><img src="/images/blog/2019-03-19-4.png" alt></p>
<p>查看 Web UI 验证下</p>
<p><img src="/images/blog/2019-04-26-1.png" alt></p>
<h4 id="Shell"><a href="#Shell" class="headerlink" title="Shell"></a>Shell</h4><p>当然，光有一个HMaster我们无法操控HBase，所以接下来是HBase shell的配置。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">org.jruby.Main</span><br><span class="line"></span><br><span class="line">-Dlog4j.configuration&#x3D;file:&#x2F;Users&#x2F;tu&#x2F;IdeaProjects&#x2F;hbase-1.2.6&#x2F;conf&#x2F;log4j.properties</span><br><span class="line">-Dhbase.ruby.sources&#x3D;&#x2F;Users&#x2F;tu&#x2F;IdeaProjects&#x2F;hbase-1.2.6&#x2F;hbase-shell&#x2F;src&#x2F;main&#x2F;ruby</span><br><span class="line"> </span><br><span class="line">&#x2F;Users&#x2F;tu&#x2F;IdeaProjects&#x2F;hbase-1.2.6&#x2F;bin&#x2F;hirb.rb</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/2019-03-19-5.png" alt></p>
<p>这是hbase shell 也可以运行了。</p>
<p>最好在文本编辑器里写好命令后再粘贴过来，如果直接在 console 中写会出现回退bug。</p>
<p><img src="/images/blog/2019-03-19-6.png" alt></p>
<h4 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h4><p>运行HMaster，报错：hbase-default.xml file seems to be for and old version of HBase</p>
<p><img src="/images/blog/2019-03-19-7.png" alt></p>
<p>这种情况是因为 hbase-default.xml 中的 hbase.defaults.for.version 配置项在打包时没有被正常替换成maven指定的版本号，</p>
<p>具体自己可以解开hbase-*.jar打开hbase-default.xml进行验证。</p>
<p>我这里运用了多种方式解决。没有一个好像特别有效。都是多种方式折腾后才避免。</p>
<p>第一种是重新 maven 编译了一次源码。</p>
<p>第二种是根据参考链接的提示配置了 hbase.defaults.for.version.skip 参数。</p>
<p>第三种修改 hbase-default.xml 中关于HBase默认版本号的配置项。</p>
<p>vim hbase-common/src/main/resources/hbase-default.xml</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property skipInDoc&#x3D;&quot;true&quot;&gt;</span><br><span class="line">    &lt;name&gt;hbase.defaults.for.version&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;1.2.6&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;description&gt;This defaults file was compiled for version $&#123;project.version&#125;. This variable is used</span><br><span class="line">    to make sure that a user doesn&#39;t have an old version of hbase-default.xml on the</span><br><span class="line">    classpath.&lt;&#x2F;description&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>

<p>然后重新 maven 编译一下源码</p>
<p>mvn clean install -DskipTests=true</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/huoshanbaofa123/article/details/75008193" target="_blank" rel="noopener">HBase 在linux环境下本地编译及调试</a></li>
<li><a href="https://www.cnblogs.com/superhedantou/p/5567787.html" target="_blank" rel="noopener">hbase本地调试环境搭建</a></li>
<li><a href="https://www.cnblogs.com/panfeng412/archive/2012/07/22/hbase-exception-hbase-default-xml-file-seems-to-be-for-and-old-version-of-HBase.html" target="_blank" rel="noopener">HBase异常：hbase-default.xml file seems to be for and old version of HBase的解决方法</a></li>
<li><a href="https://www.cnblogs.com/pekkle/p/10465654.html" target="_blank" rel="noopener">转 HBase异常：hbase-default.xml file seems to be for an old version of HBase</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>HBase Q&amp;A</title>
    <url>/2019/03/18/hbase-Q&amp;A/</url>
    <content><![CDATA[<p>记录 HBase 常见的面试问题。</p>
<hr>
<h4 id="HFile"><a href="#HFile" class="headerlink" title="HFile"></a>HFile</h4><blockquote>
<ul>
<li>Q: HFile 从 V1 版本升级到 V2 版本最重要的两个因素是什么？</li>
<li>A: 1. 随着 KeyVlue 增多，布隆过滤器位数组越来越大，V2 在设计上将位数组拆分成多个独立的位数组。2. V2 支持多级索引结构，多级索引可以只加载部分索引，降低内存使用空间。</li>
</ul>
</blockquote>
<h4 id="HBase-生态圈"><a href="#HBase-生态圈" class="headerlink" title="HBase 生态圈"></a>HBase 生态圈</h4><blockquote>
<ul>
<li>Q: HBase 于 Hive 的区别</li>
<li>A: Hive 适合用来对一段时间内的数据进行分析查询，例如，用来计算趋势或者网站的日志。<br>Hive 不应该用来进行实时的查询（Hive 的设计目的，也不是支持实时的查询）。<br>因为它需要很长时间才可以返回结果；HBase 则非常适合用来进行大数据的实时查询，<br>例如 Facebook 用 HBase 进行消息和实时的分析。<br>对于 Hive 和 HBase 的部署来说，也有一些区别，Hive 一般只要有 Hadoop 便可以工作。<br>而 HBase 则还需要 Zookeeper 的帮助（Zookeeper，是一个用来进行分布式协调的服务，<br>这些服务包括配置服务，维护元信息和命名空间服务）。再而，HBase 本身只提供了 Java 的 API 接口，<br>并不直接支持 SQL 的语句查询，而 Hive 则可以直接使用 HQL（一种类 SQL 语言）。<br>如果想要在 HBase 上使用 SQL，则需要联合使用 Apache Phonenix，<br>或者联合使用 Hive 和 HBase。但是和上面提到的一样，如果集成使用 Hive 查询 HBase 的数据，<br>则无法绕过 MapReduce (还支持Spark)，那么实时性还是有一定的损失。<br>Phoenix 加 HBase 的组合则不经过 MapReduce 的框架，<br>因此当使用 Phoneix 加 HBase 的组成，实时性上会优于 Hive 加 HBase 的组合。<br>最后再提下 Hive 和 HBase 所使用的存储层，<br>默认情况下 Hive 和 HBase 的存储层都是 HDFS。<br>但是 HBase 也支持直接使用本机的文件系统。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>Q: HBase是否支持表关联？</li>
<li>A: 官方给出的答案是干脆的，那就是“不支持”。如果想实现数据之间的关联，就必须自己去实现了，这是挑选 NoSQL 数据库必须付出的代价。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>Q: HBase 与相关竞品的区别 (Kudu、Cassandra、Accumulo)</li>
<li>A: </li>
</ul>
</blockquote>
<h4 id="读写"><a href="#读写" class="headerlink" title="读写"></a>读写</h4><blockquote>
<ul>
<li>Q: HBase 是否支持 ACID？</li>
<li>A: ACID 就是 Atomicity（原子性）、Consistency（一致性）、Isolation（隔离性）、Durability（持久性）的首字母缩写，ACID 是事务正确执行的保证，HBase 支持行级的 ACID。<br>读写并发通过 MVCC 机制保障。写写并发通过写锁实现。</li>
</ul>
</blockquote>
<h4 id="Compaction"><a href="#Compaction" class="headerlink" title="Compaction"></a>Compaction</h4><blockquote>
<ul>
<li>Q: HBase 什么时候会删除多余的版本</li>
<li>A: 第一种是刷新数据到磁盘的时候，第二种是执行大合并的时候。用 RAW =&gt; true 的时候，可以获得所有版本的数据。<br>表的保留的最大版本数为3。<br>1 执行4次put，不刷新磁盘，紧接着scan，不论版本数，将会返回4版本的数据。<br>2 执行4次put，刷到磁盘，紧接着执行scan，将会返回3个版本的数据<br>3 执行4次put，刷到磁盘，再执行4次put，并刷新到磁盘，紧接着执行scan，将会返回6个版本数<br>4 执行4次put，刷到磁盘，再执行4次put，并刷新到磁盘，再进行大合并，紧接着scan，将会返回3个版本数</li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>Java 正则表达式的简单使用</title>
    <url>/2019/03/17/regex/</url>
    <content><![CDATA[<p>Java 正则表达式的简单使用</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>Java 正则表达式的简单使用</p>
<h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Pattern pattern &#x3D; Pattern.compile(&quot;&gt;(application_[\s\S]*?)&lt;&#x2F;a&gt;&quot;,([\s\S]*?),&quot;&lt;br&quot;);</span><br><span class="line">Matcher matcher &#x3D; pattern.matcher(znode);</span><br><span class="line"></span><br><span class="line">while (matcher.find()) &#123;</span><br><span class="line">    System.out.println(matcher.group(0));</span><br><span class="line">    System.out.println(matcher.group(1));</span><br><span class="line">    System.out.println(matcher.group(2));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>find 方法</p>
<p>public boolean find() 尝试查找与该模式匹配的输入序列的下一个子序列。</p>
<p>此方法从匹配器区域的开头开始，如果该方法的前一次调用成功了并且从那时开始匹配器没有被重置，则从以前匹配操作没有匹配到的第一个字符开始。<br>如果匹配成功，则可以通过 start、end 和 group 方法获取更多信息。</p>
<blockquote>
<ul>
<li>matcher.group() 返回匹配到的子字符串</li>
<li>matcher.start() 返回匹配到的子字符串在字符串中的索引位置.</li>
<li>matcher.end() 返回匹配到的子字符串的最后一个字符在字符串中的索引位置.</li>
</ul>
</blockquote>
<p>group(0) 表示当前这一次匹配到的所有字符子串。 group(1) 表示当前这一次匹配到的第一个字符子串。<br>同理group(2) 表示当前这一次匹配到的第一个字符子串。第几个字串对应的是 Pattern.compile 中括号的位置</p>
<h4 id="查找不包含某些字符串的"><a href="#查找不包含某些字符串的" class="headerlink" title="查找不包含某些字符串的"></a>查找不包含某些字符串的</h4><p>这里正则查找不包含 huidu、rd 的字符串</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">^(?!.*huidu*)(?!.*rd).*$</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/2019-04-10-1.png" alt></p>
<p><img src="/images/blog/2019-04-10-2.png" alt></p>
<p><img src="/images/blog/2019-04-10-3.png" alt></p>
<h4 id="Matcher的find-和matches-方法不同"><a href="#Matcher的find-和matches-方法不同" class="headerlink" title="Matcher的find()和matches()方法不同"></a>Matcher的find()和matches()方法不同</h4><p>find()方法在部分匹配时和完全匹配时返回true,匹配不上返回false。</p>
<p>matches()方法只有在完全匹配时返回true,匹配不上和部分匹配都返回false。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/dujianxiong/article/details/83146376" target="_blank" rel="noopener">正则查找不包含某些字符串的行</a></li>
</ul>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title>JournalNode 不同步</title>
    <url>/2019/03/16/out-sync-journalnode/</url>
    <content><![CDATA[<p>JournalNode 不同步</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>这里记录之前处理的一次 JournalNode 不同步。</p>
<p>应处理的太急，没有对现场进行截图。</p>
<p>下次再遇到，再补上截图吧。</p>
<h4 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h4><p>在启动 HDFS 之后，HDFS 发出 JournalNode 不同步的警告。</p>
<p>经过检查，可以看见存放同步文件的目录 /data/dfs/jn 目录找不到，没有这种目录。</p>
<p>这时我们从其他正常的 JournalNode 上把目录 copy 过来即可。</p>
<p>如果有目录的话，那就移走。反正已经不完整了。但是别删了，防止需要回滚。</p>
<p>使用scp直接拷贝就可以了</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mv jn &#x2F;tmp </span><br><span class="line">scp -r root@100.xxx.1:&#x2F;data&#x2F;dfs&#x2F;jn .&#x2F;</span><br><span class="line">chown -R hdfs:hdfs jn</span><br></pre></td></tr></table></figure>

<p>查看博客有人提示到说 scp 时间不应该超过 journalnode_sync_status_startup_tolerance 参数设置的时间</p>
<p>同时还提到在 scp 要考虑同步数据时的带宽限制。dfs.image.transfer.bandwidthPerSec参数。</p>
<p><img src="/images/blog/2019-03-16-1.png" alt></p>
<p><img src="/images/blog/2019-03-16-2.png" alt></p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="http://www.voidcn.com/article/p-auelsbhf-bee.html" target="_blank" rel="noopener">Hadoop:CDH 5–不同步的 JournalNode</a></li>
</ul>
]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title>MySQL 5.7 安装</title>
    <url>/2019/03/14/mysql-install/</url>
    <content><![CDATA[<p>在 Linux 环境下手动安装 mysql 5.7 二进制包</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>MySql 是数据库的明星。基本大小厂都在用</p>
<p>作为一个运维人员，MySql 的安装是必须掌握的。</p>
<h4 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h4><p>Mysql 版本： 5.7.18</p>
<p>mysql 二进制包到mysql 官网下载即可</p>
<h5 id="1-解压"><a href="#1-解压" class="headerlink" title="1. 解压"></a>1. 解压</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建 MySql 目录</span><br><span class="line">mkdir &#x2F;opt&#x2F;mysql</span><br><span class="line"># 解压 Mysql 二进制包</span><br><span class="line">tar -zxvf mysql**** -C &#x2F;opt&#x2F;mysql</span><br><span class="line">cd &#x2F;opt&#x2F;mysql</span><br><span class="line">mv mysql... base</span><br></pre></td></tr></table></figure>

<h5 id="2-目录"><a href="#2-目录" class="headerlink" title="2. 目录"></a>2. 目录</h5><blockquote>
<ul>
<li>mysql安装目录：/opt/mysql/base</li>
<li>数据目录：/opt/mysql/data/13307</li>
<li>配置文件：/opt/mysql/my.cnf.13307</li>
<li>binlog目录：/opt/mysql/binlogw/13307</li>
<li>mysql 临时目录： /opt/mysql/base/mysql_tmp</li>
</ul>
</blockquote>
<p>按照上面目录创建目录。没有的就创建，有的就忽略。</p>
<h5 id="3-生成my-cnf-配置文件"><a href="#3-生成my-cnf-配置文件" class="headerlink" title="3. 生成my.cnf 配置文件"></a>3. 生成my.cnf 配置文件</h5><p>编辑修改 my.cnf.13307 文件<br>vim /opt/mysql/my.cnf.13307</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[mysql]</span><br><span class="line"># DO NOT CHANGE</span><br><span class="line">port &#x3D; 13307</span><br><span class="line">default-character-set &#x3D; utf8mb4</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line"># DO NOT CHANGE</span><br><span class="line">server_id &#x3D; 313432</span><br><span class="line">basedir &#x3D; &#x2F;opt&#x2F;mysql&#x2F;base</span><br><span class="line">datadir &#x3D; &#x2F;opt&#x2F;mysql&#x2F;data&#x2F;13307</span><br><span class="line">socket &#x3D; &#x2F;opt&#x2F;mysql&#x2F;data&#x2F;13307&#x2F;mysql_13307.sock</span><br><span class="line">port &#x3D; 13307</span><br><span class="line">log-bin &#x3D; &#x2F;opt&#x2F;mysql&#x2F;binlogw&#x2F;13307&#x2F;mysql-bin</span><br><span class="line">tmpdir &#x3D; &#x2F;opt&#x2F;mysql&#x2F;base&#x2F;mysql_tmp</span><br><span class="line">skip-name-resolve &#x3D; 1</span><br><span class="line">max_allowed_packet &#x3D; 64M</span><br><span class="line">default_storage_engine &#x3D; InnoDB</span><br><span class="line">character_set_server &#x3D; utf8mb4</span><br><span class="line">skip-external-locking &#x3D; 1</span><br><span class="line">table_open_cache_instances &#x3D; 32</span><br><span class="line">back_log &#x3D; 1500</span><br><span class="line">wait_timeout &#x3D; 3600</span><br><span class="line">interactive_timeout &#x3D; 3600</span><br><span class="line">default-time-zone &#x3D; &#39;+8:00&#39;</span><br><span class="line">explicit_defaults_for_timestamp &#x3D; 1</span><br><span class="line">lower_case_table_names &#x3D; 1</span><br><span class="line">symbolic-links &#x3D; 0</span><br><span class="line">secure_file_priv &#x3D; &#39;&#39;</span><br></pre></td></tr></table></figure>

<h5 id="4-修改权限"><a href="#4-修改权限" class="headerlink" title="4. 修改权限"></a>4. 修改权限</h5><p>添加 mysql 用户，并修改 mysql 所在目录的权限</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">useradd mysql</span><br><span class="line"></span><br><span class="line">chown -R mysql:mysql &#x2F;opt&#x2F;mysql</span><br></pre></td></tr></table></figure>

<h4 id="初始化实例"><a href="#初始化实例" class="headerlink" title="初始化实例"></a>初始化实例</h4><p>进入base目录下初始化实例</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;opt&#x2F;mysql&#x2F;base</span><br><span class="line">.&#x2F;bin&#x2F;mysqld --defaults-file&#x3D;&#x2F;opt&#x2F;mysql&#x2F;my.cnf.13307 --user&#x3D;mysql --initialize</span><br></pre></td></tr></table></figure>

<p><strong>去 mysql-error.log 查看实例随机密码，搜索最后一个password 保留即可</strong></p>
<p><img src="/images/blog/2019-03-14-1.png" alt></p>
<p>路径为 /opt/mysql/data/13307/mysql-error.log</p>
<p><strong>在/opt/mysql/data/13307下如果只有mysql-error.log 一个文件，说明初始化实例报错了，<br>查看mysql-error.log错误日志根据报错信息处理即可，处理完之后得把/opt/mysql/data/13307下的文件都清空，否者会数据目录不为空的错误</strong></p>
<h4 id="起mysql实例"><a href="#起mysql实例" class="headerlink" title="起mysql实例"></a>起mysql实例</h4><p>进入 base 目录下起 mysql 实例</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;opt&#x2F;mysql&#x2F;base</span><br><span class="line">.&#x2F;bin&#x2F;mysqld_safe --defaults-file&#x3D;&#x2F;opt&#x2F;mysql&#x2F;my.cnf.13307 --user&#x3D;mysql &amp;</span><br><span class="line"></span><br><span class="line"># 查看是否有mysql 进程</span><br><span class="line"></span><br><span class="line">ps -ef | grep mysqld</span><br></pre></td></tr></table></figure>

<p>如果启动mysql 失败， 查看 mysql-error.log 错误日志处理即可。</p>
<h4 id="连接mysql"><a href="#连接mysql" class="headerlink" title="连接mysql"></a>连接mysql</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 如果不进入bin目录起，可能会报root密码过期错误</span><br><span class="line">cd &#x2F;opt&#x2F;mysql&#x2F;base    </span><br><span class="line">.&#x2F;bin&#x2F;mysql -S &#x2F;opt&#x2F;mysql&#x2F;data&#x2F;13307&#x2F;mysql_13307.sock  -u root -p -A</span><br></pre></td></tr></table></figure>

<p>输入初始化实例步骤获得的随机密码</p>
<p>连接成功后需要修改root密码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SET PASSWORD &#x3D; PASSWORD(&#39;your_password&#39;);</span><br></pre></td></tr></table></figure>

<h4 id="修改环境变量"><a href="#修改环境变量" class="headerlink" title="修改环境变量"></a>修改环境变量</h4><p>全局环境变量</p>
<p>vim /etc/profile</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export MYSQL_HOME&#x3D;&#x2F;opt&#x2F;mysql&#x2F;base</span><br><span class="line">export PATH&#x3D;$&#123;MYSQL_HOME&#125;&#x2F;bin:$PATH</span><br></pre></td></tr></table></figure>













]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
  </entry>
  <entry>
    <title>HDFS 块故障</title>
    <url>/2019/03/14/hdfs-block-fault/</url>
    <content><![CDATA[<p>发现测试集群HDFS报丢失块的告警。 对块进行修复</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>发现测试集群HDFS报丢失块的告警。 </p>
<p><img src="/images/blog/2019-03-14-4.png" alt></p>
<p>如果块损坏过多，超过设置的百分比，NameNode 会进入安全模式。需要退出安全模式处理</p>
<h4 id="处理"><a href="#处理" class="headerlink" title="处理"></a>处理</h4><p>解决方法：</p>
<ul>
<li>如果文件不重要，可以直接删除此文件；或删除后重新复制一份到集群中</li>
<li>如果不能删除，需要从上面命令中找到发生在哪台机器上，然后到此机器上查看日志。</li>
</ul>
<p>先确认集群有没有datanode宕机</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo -u hdfs hdfs dfsadmin -report</span><br></pre></td></tr></table></figure>

<p>查看丢失块的信息</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 需要hdfs权限权限操作。如果没有hdfs 账号，则在命令前加 sudo -u hdfs ，以hdfs权限运行该命令</span><br><span class="line"></span><br><span class="line"># 检查整个文件系统，检查哪些数据块丢失了</span><br><span class="line">hdfs fsck &#x2F;</span><br><span class="line"></span><br><span class="line"># 查看损坏块的信息</span><br><span class="line">hdfs fsck -list-corruptfileblocks   </span><br><span class="line"></span><br><span class="line"># 查看损坏块的信息  -locations位置信息  -blocks各个块的信息   -files显示文件信息，包括文件名称，大小，块数量和健康状况（是否有缺失的块）</span><br><span class="line">hdfs fsck &lt;path&gt; -locations -blocks -files</span><br></pre></td></tr></table></figure>
<p><img src="/images/blog/2019-03-14-5.png" alt></p>
<p><img src="/images/blog/2019-03-14-6.png" alt></p>
<p>当然，这只是数据允许丢失的情况下可以使用的一种简单粗暴的方法<code>hdfs fsck / -delete</code>, 是删除所有损坏的块的数据文件，会导致数据彻底丢失</p>
<p>生产上还是无法使用这种直接删数据的方法的</p>
<p>那么生产上应该怎么处理这种情况呢？</p>
<ol>
<li>首先<code>hdfs fsck -list-corruptfileblocks</code>找到数据块的位置</li>
<li><code>hdfs debug recoverLease [-path &lt;path&gt;] [-retries &lt;num-retries&gt;]</code>用这个命令恢复上面路径丢失的数据块，最后一个参数是重试次数</li>
</ol>
<blockquote>
<p>自动修复<br>hdfs当然会自动修复损坏的数据块，当数据块损坏后，DN节点执⾏directoryscan（datanode进行内存和磁盘数据集块校验）操作之前，都不会发现损坏；也就是directoryscan操作校验是间隔6h<br>dfs.datanode.directoryscan.interval : 21600</p>
<p>在DN向NN进⾏blockreport前，都不会恢复数据块;也就是blockreport操作是间隔6h<br>dfs.blockreport.intervalMsec : 21600000<br>最终当NN收到blockreport才会进⾏恢复操作<br>生产中倾向于使用手动修复的方法去修复损坏的数据块。</p>
</blockquote>
<p>具体的实操请转移这里阅读<a href="https://ruozedata.github.io/2019/06/06/%E7%94%9F%E4%BA%A7HDFS%20Block%E6%8D%9F%E5%9D%8F%E6%81%A2%E5%A4%8D%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E6%80%9D%E8%80%83%E9%A2%98)/" target="_blank" rel="noopener">生产HDFS Block损坏恢复最佳实践(含思考题)</a>，注意地方：</p>
<ul>
<li>删除块和meta文件之后要重启HDFS，模拟损坏效果，不重启的话直接fsck检查看不到块损坏</li>
</ul>
<h4 id="HDFS-block丢失过多进入安全模式（Safe-mode）的解决方法"><a href="#HDFS-block丢失过多进入安全模式（Safe-mode）的解决方法" class="headerlink" title="HDFS block丢失过多进入安全模式（Safe mode）的解决方法"></a>HDFS block丢失过多进入安全模式（Safe mode）的解决方法</h4><p>前面介绍的是简单的块损坏。 后面遇到一次丢失块比例高达 13% 的测试环境。</p>
<p>由于系统断电，内存不足等原因导致 DN 丢失超过设置的丢失百分比，系统自动进入安全模式。</p>
<p>解决办法就是把宕机 DN 重启恢复回来，然后观察是否能够恢复。</p>
<p>不然只能退出安全模式。清理损坏的块。<strong>注意: 这种方式会出现数据丢失，损坏的block会被删掉</strong> 请慎重</p>
<p>退出安全模式操作如下</p>
<p><img src="/images/blog/2019-03-14-7.png" alt></p>
<p>离开安全模式后使用<code>hdfs dfsadmin -safemode get</code>查看，<code>Safe mode is OFF</code> 为安全模式关闭，<code>Safe mode is ON</code>为安全模式开启</p>
<p>如果无法离开安全模式，那就强制离开<code>hdfs dfsadmin -safemode forceExit</code></p>
<p>执行健康检查，删除损坏掉的block <code>hdfs fsck / -delete</code></p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/mnasd/article/details/82143653" target="_blank" rel="noopener">查看修复HDFS中丢失的块 &amp;HDFS block丢失过多进入安全模式（safe mode）的解决方法</a></li>
<li><a href="https://www.cnblogs.com/admln/p/5822004.html" target="_blank" rel="noopener">查看修复HDFS中丢失的块</a></li>
</ul>
<ul>
<li><a href="https://blog.csdn.net/qq_32641659/article/details/88243255" target="_blank" rel="noopener">hadoop之hdfs生产数据块损坏修复方法</a></li>
<li><a href="https://blog.csdn.net/eryehong/article/details/95167059" target="_blank" rel="noopener">【学习笔记】Hadoop之HDFS Block损坏恢复最佳实践(含思考题)</a></li>
</ul>
]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title>Hive 重新配置元数据库</title>
    <url>/2019/03/14/hive-memstore/</url>
    <content><![CDATA[<p>Hive 重新配置元数据库</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>有一套测试环境的 CDH 太久没维护了。各种组件都已经宕掉了。</p>
<p>作为一个合格的运维人员肯定得负责帮开发工程师整起来</p>
<p>在修复 Hive 的过程中，一直报 Hive Metastore canary创建数据库失败 。</p>
<p>通过定位发现是元数据库 mysql 宕了。凭借自己一点点mysql 知识准备把它启动起来。结果失败了。。。。。</p>
<p>后面找到负责搭建这个 mysql 的测试人员让其修复。。他折腾了一会说搞不定了。让找 DBA 看看。。。</p>
<p>觉得太麻烦了。因为是测试环境，元数据丢弃。所以想重新配置一个元数据库。</p>
<h4 id="修复"><a href="#修复" class="headerlink" title="修复"></a>修复</h4><p>先搭建一套 mysql 数据库。我使用的mysql 版本为 5.7.18</p>
<p>搭建完成后需要新建数据库hive，新建 hive 连接的账号 和密码。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create databases hive</span><br><span class="line"></span><br><span class="line">grant all privileges on *.* to &#39;root&#39;@&#39;%&#39; identified by &#39;your_passwd&#39;;</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/2019-03-14-2.png" alt></p>
<p>按图配置对应的参数</p>
<p>接着给 hive 添加 mysql 驱动包  </p>
<p>路径  /opt/cloudera/parcels/CDH/lib/hive/mysql-connector-java-5.1.35-bin.jar</p>
<p>修改 hive 的参数</p>
<p>关闭 严格的 Hive Metastore 架构验证</p>
<p>打开 自动创建和升级 Hive Metastore 数据库架构</p>
<p><img src="/images/blog/2019-03-14-2.png" alt></p>
<p>最后到mysql中的hive数据库执行 alter database hive character set latin1;</p>
<p>重启 hive 即可恢复</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>遇到问题，多看日志，根据日志来处理。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/jim110/article/details/44907745" target="_blank" rel="noopener">hive 启动问题记录 及解决方法</a></li>
<li><a href="https://blog.csdn.net/asas1314/article/details/50098453" target="_blank" rel="noopener">hive中遇到的几个问题</a></li>
</ul>
]]></content>
      <categories>
        <category>Hive</category>
      </categories>
  </entry>
  <entry>
    <title>Hadoop + HBase 单机安装部署</title>
    <url>/2019/03/14/apache-hadoop/</url>
    <content><![CDATA[<p>单机搭建部署 Hadoop 、HBase</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>之前一直是使用 CDH 。 准备入坑源码。所以想搭建一套 Apache 的来玩玩。</p>
<p>选择的版本：</p>
<blockquote>
<ul>
<li>Hadoop: 2.7.7</li>
<li>HBase: 1.2.11</li>
<li>Linux: CentOs 6.5</li>
<li>JDK: jdk1.8.0_161</li>
</ul>
</blockquote>
<h4 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h4><p>安装jdk，配置环境变量。 这个网上很多，自行百度。</p>
<p>Hadoop、HBase tar包去官方网站下载即可</p>
<p>下载之后将其解压。</p>
<p><img src="/images/blog/2019-03-14-8.png" alt></p>
<h4 id="配置-Hadoop-环境变量"><a href="#配置-Hadoop-环境变量" class="headerlink" title="配置 Hadoop 环境变量"></a>配置 Hadoop 环境变量</h4><h5 id="1-配置hadoop-env-sh、yarn-env-sh"><a href="#1-配置hadoop-env-sh、yarn-env-sh" class="headerlink" title="1. 配置hadoop-env.sh、yarn-env.sh"></a>1. 配置hadoop-env.sh、yarn-env.sh</h5><p>在Hadoop安装目录下 etc/hadoop</p>
<p>在 hadoop-env.sh、yarn-env.sh  </p>
<p>加入 export JAVA_HOME=/usr/local/jdk1.8.0_161（jdk安装路径）</p>
<p>保存退出</p>
<h5 id="2-配置HADOOP-HOME"><a href="#2-配置HADOOP-HOME" class="headerlink" title="2. 配置HADOOP_HOME"></a>2. 配置HADOOP_HOME</h5><p>修改全局环境变量</p>
<p>在 /etc/profile </p>
<p>export HADOOP_HOME=/opt/hadoop-2.7.3 # Hadoop的安装路径</p>
<p>export PATH=$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$PATH</p>
<h4 id="配置基本相关xml"><a href="#配置基本相关xml" class="headerlink" title="配置基本相关xml"></a>配置基本相关xml</h4><h5 id="core-site-xml"><a href="#core-site-xml" class="headerlink" title="core-site.xml"></a>core-site.xml</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;fs.defaultFS&lt;&#x2F;name&gt;</span><br><span class="line">                &lt;value&gt;hdfs:&#x2F;&#x2F;localhost:8020&lt;&#x2F;value&gt;</span><br><span class="line">        &lt;&#x2F;property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;hadoop.tmp.dir&lt;&#x2F;name&gt;</span><br><span class="line">                &lt;value&gt;&#x2F;opt&#x2F;hadoop&#x2F;tmp&lt;&#x2F;value&gt; </span><br><span class="line">                &lt;description&gt;namenode上本地的hadoop临时文件夹&lt;&#x2F;description&gt;</span><br><span class="line">        &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>

<h5 id="hdfs-site-xml"><a href="#hdfs-site-xml" class="headerlink" title="hdfs-site.xml"></a>hdfs-site.xml</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">              &lt;name&gt;dfs.replication&lt;&#x2F;name&gt;</span><br><span class="line">              &lt;value&gt;1&lt;&#x2F;value&gt;</span><br><span class="line">              &lt;description&gt;数据需要备份的数量，不能大于集群的机器数量，默认为3&lt;&#x2F;description&gt;</span><br><span class="line">        &lt;&#x2F;property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">             &lt;name&gt;dfs.namenode.name.dir&lt;&#x2F;name&gt;</span><br><span class="line">             &lt;value&gt;&#x2F;opt&#x2F;hadoop&#x2F;dfs&#x2F;nn&lt;&#x2F;value&gt;</span><br><span class="line">　　　　　　 &lt;description&gt;namenode上存储hdfs名字空间元数据&lt;&#x2F;description&gt;</span><br><span class="line">        &lt;&#x2F;property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">             &lt;name&gt;dfs.datanode.data.dir&lt;&#x2F;name&gt;</span><br><span class="line">             &lt;value&gt;&#x2F;opt&#x2F;hadoop&#x2F;dfs&#x2F;dn&lt;&#x2F;value&gt;</span><br><span class="line">             &lt;description&gt;datanode上数据块的物理存储位置&lt;&#x2F;description&gt;</span><br><span class="line">        &lt;&#x2F;property&gt;</span><br><span class="line">        &lt;property&gt; </span><br><span class="line">　　　　　　　　&lt;name&gt;dfs.namenode.secondary.http-address&lt;&#x2F;name&gt; </span><br><span class="line">　　　　　　　　&lt;value&gt;localhost:9001&lt;&#x2F;value&gt;</span><br><span class="line">              &lt;description&gt;SecondaryNameNode HTTP 端口。如端口为 0，服务器将在自由端口启动。&lt;&#x2F;description&gt;</span><br><span class="line">        &lt;&#x2F;property&gt;  </span><br><span class="line">        &lt;property&gt; </span><br><span class="line">              &lt;name&gt;dfs.webhdfs.enabled&lt;&#x2F;name&gt;</span><br><span class="line">              &lt;value&gt;true&lt;&#x2F;value&gt; </span><br><span class="line">              &lt;description&gt;设置为true，可以在浏览器中IP+port查看&lt;&#x2F;description&gt;</span><br><span class="line">        &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>

<h5 id="mapred-site-xml"><a href="#mapred-site-xml" class="headerlink" title="mapred-site.xml"></a>mapred-site.xml</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">        &lt;property&gt; </span><br><span class="line">　　　　　　&lt;name&gt;mapreduce.framework.name&lt;&#x2F;name&gt; </span><br><span class="line">　　　　　　&lt;value&gt;yarn&lt;&#x2F;value&gt; </span><br><span class="line">          &lt;description&gt;mapreduce运用了yarn框架，设置name为yarn&lt;&#x2F;description&gt;</span><br><span class="line">　　　　&lt;&#x2F;property&gt; </span><br><span class="line">　　　　&lt;property&gt; </span><br><span class="line">　　　　　　&lt;name&gt;mapreduce.jobhistory.address&lt;&#x2F;name&gt; </span><br><span class="line">　　　　　　&lt;value&gt;localhost:10020&lt;&#x2F;value&gt; </span><br><span class="line">          &lt;description&gt;历史服务器,查看Mapreduce作业记录&lt;&#x2F;description&gt;</span><br><span class="line">　　　　&lt;&#x2F;property&gt; </span><br><span class="line">　　　　&lt;property&gt; </span><br><span class="line">　　　　　　&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;&#x2F;name&gt; </span><br><span class="line">　　　　　　&lt;value&gt;localhost:19888&lt;&#x2F;value&gt;</span><br><span class="line">          &lt;description&gt;MapReduce JobHistory Web 应用程序 HTTP 端口&lt;&#x2F;description&gt;</span><br><span class="line">　　　　&lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>

<h5 id="yarn-site-xml"><a href="#yarn-site-xml" class="headerlink" title="yarn-site.xml"></a>yarn-site.xml</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">        &lt;property&gt; </span><br><span class="line">　　　　　　&lt;name&gt;yarn.nodemanager.aux-services&lt;&#x2F;name&gt; </span><br><span class="line">　　　　　　&lt;value&gt;mapreduce_shuffle&lt;&#x2F;value&gt; </span><br><span class="line">          &lt;description&gt;NodeManager上运行的附属服务，用于运行mapreduce&lt;&#x2F;description&gt;</span><br><span class="line">　　　　&lt;&#x2F;property&gt; </span><br><span class="line">　　　　&lt;property&gt; </span><br><span class="line">　　　　　　&lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;&#x2F;name&gt; </span><br><span class="line">　　　　　　&lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;&#x2F;value&gt; </span><br><span class="line">          &lt;description&gt;MapReduce JobHistory Web 应用程序 HTTP 端口&lt;&#x2F;description&gt;</span><br><span class="line">　　　　&lt;&#x2F;property&gt; </span><br><span class="line">　　　　&lt;property&gt; </span><br><span class="line">　　　　　　&lt;name&gt;yarn.resourcemanager.address&lt;&#x2F;name&gt; </span><br><span class="line">　　　　　　&lt;value&gt;localhost:8032&lt;&#x2F;value&gt; </span><br><span class="line">          &lt;description&gt;ResourceManager 对客户端暴露的地址&lt;&#x2F;description&gt;</span><br><span class="line">　　　　&lt;&#x2F;property&gt; </span><br><span class="line">　　　　&lt;property&gt; </span><br><span class="line">　　　　　　&lt;name&gt;yarn.resourcemanager.scheduler.address&lt;&#x2F;name&gt; </span><br><span class="line">　　　　　　&lt;value&gt;localhost:8030&lt;&#x2F;value&gt; </span><br><span class="line">          &lt;description&gt;ResourceManager 对ApplicationMaster暴露的地址&lt;&#x2F;description&gt;</span><br><span class="line">　　　　&lt;&#x2F;property&gt; </span><br><span class="line">　　　　&lt;property&gt; </span><br><span class="line">　　　　　　&lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;&#x2F;name&gt;  </span><br><span class="line">　　　　　　&lt;value&gt;localhost:8031&lt;&#x2F;value&gt; </span><br><span class="line">          &lt;description&gt;ResourceManager 对NodeManager暴露的地址&lt;&#x2F;description&gt;</span><br><span class="line">　　　　&lt;&#x2F;property&gt; </span><br><span class="line">　　　　&lt;property&gt;</span><br><span class="line">　　　　　　&lt;name&gt;yarn.resourcemanager.admin.address&lt;&#x2F;name&gt;   </span><br><span class="line">　　　　　　&lt;value&gt;localhost:8033&lt;&#x2F;value&gt; </span><br><span class="line">          &lt;description&gt;ResourceManager 对管理员暴露的地址&lt;&#x2F;description&gt;</span><br><span class="line">　　　　&lt;&#x2F;property&gt; </span><br><span class="line">　　　　&lt;property&gt;</span><br><span class="line">　　　　　　&lt;name&gt;yarn.resourcemanager.webapp.address&lt;&#x2F;name&gt; </span><br><span class="line">           &lt;value&gt;localhost:8088&lt;&#x2F;value&gt; </span><br><span class="line">           &lt;description&gt;ResourceManager 对外web暴露的地址，可在浏览器查看&lt;&#x2F;description&gt;</span><br><span class="line">　　　　&lt;&#x2F;property&gt; </span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>

<h4 id="Hadoop的启动与停止"><a href="#Hadoop的启动与停止" class="headerlink" title="Hadoop的启动与停止"></a>Hadoop的启动与停止</h4><h5 id="1-格式化namenode"><a href="#1-格式化namenode" class="headerlink" title="1. 格式化namenode"></a>1. 格式化namenode</h5><p>bin/hadoop namenode -format</p>
<p><img src="/images/blog/2019-04-27-1.png" alt></p>
<p>出现 successfully formatted 即可</p>
<h5 id="2-启动-HDFS、YARN"><a href="#2-启动-HDFS、YARN" class="headerlink" title="2. 启动 HDFS、YARN"></a>2. 启动 HDFS、YARN</h5><p>一起启动</p>
<p>sbin/start-all.sh</p>
<p>也可以分开启动</p>
<p>sbin/start-dfs.sh</p>
<p>sbin/start-yarn.sh</p>
<h5 id="3-验证"><a href="#3-验证" class="headerlink" title="3. 验证"></a>3. 验证</h5><p>在命令行输入 jps 可以看到启动的 Java 进程，接着在看看是否有 HDFS、Yarn 的角色</p>
<p><img src="/images/blog/2019-03-14-9.png" alt></p>
<p>在浏览器中输入http://{ip}:50070 查看 NameNode 状态。 ip 为你安装的机器 IP</p>
<h5 id="4-停止"><a href="#4-停止" class="headerlink" title="4. 停止"></a>4. 停止</h5><p>停止hadoop，进入Hadoop目录下，输入命令：</p>
<p>sbin/stop-all.sh。</p>
<p>同样也可以分开停止的。</p>
<p>sbin/stop-dfs.sh</p>
<p>sbin/stop-yarn.sh</p>
<h4 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h4><h5 id="1-配置HADOOP-HOME"><a href="#1-配置HADOOP-HOME" class="headerlink" title="1. 配置HADOOP_HOME"></a>1. 配置HADOOP_HOME</h5><p>在 /etc/profile 中加入</p>
<p>export HBASE_HOME=/opt/hbase-1.2.11</p>
<p>export PATH=$HBASE_HOME/bin:$PATH</p>
<h5 id="2-hbase-site-xml"><a href="#2-hbase-site-xml" class="headerlink" title="2. hbase-site.xml"></a>2. hbase-site.xml</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">        &lt;property&gt; </span><br><span class="line">　　　　　　 &lt;name&gt;hbase.rootdir&lt;&#x2F;name&gt; </span><br><span class="line">　　　　　　 &lt;value&gt;hdfs:&#x2F;&#x2F;localhost:8020&#x2F;hbase&lt;&#x2F;value&gt;</span><br><span class="line">　　　　　　 &lt;!-- 端口要和Hadoop的fs.defaultFS端口一致--&gt;</span><br><span class="line">           &lt;!-- hbase存放数据目录 --&gt;</span><br><span class="line">　　　　&lt;&#x2F;property&gt; </span><br><span class="line">　　　　&lt;property&gt; </span><br><span class="line">　　　　　   &lt;name&gt;hbase.cluster.distributed&lt;&#x2F;name&gt; &lt;!-- 是否分布式部署 --&gt;</span><br><span class="line">　　　　　　 &lt;value&gt;false&lt;&#x2F;value&gt; </span><br><span class="line">　　　　&lt;&#x2F;property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;hbase.tmp.dir&lt;&#x2F;name&gt;</span><br><span class="line">                &lt;value&gt;&#x2F;opt&#x2F;hadoop&#x2F;hbase&#x2F;tmp&lt;&#x2F;value&gt;</span><br><span class="line">                &lt;description&gt;hbase临时文件目录&lt;&#x2F;description&gt;</span><br><span class="line">        &lt;&#x2F;property&gt;</span><br><span class="line">　　　　&lt;property&gt; </span><br><span class="line">　　　　　　 &lt;name&gt;hbase.zookeeper.quorum&lt;&#x2F;name&gt; &lt;!-- list of  zookooper --&gt;</span><br><span class="line">　　　　　　 &lt;value&gt;localhost&lt;&#x2F;value&gt; </span><br><span class="line">　　　　&lt;&#x2F;property&gt; 　　　 </span><br><span class="line">　　　　&lt;property&gt;</span><br><span class="line">　　　　　　 &lt;name&gt;hbase.zookeeper.property.dataDir&lt;&#x2F;name&gt; </span><br><span class="line">　　　　　　 &lt;value&gt;&#x2F;opt&#x2F;hadoop&#x2F;hbase&#x2F;zookeeper&lt;&#x2F;value&gt;</span><br><span class="line">            &lt;description&gt;zookooper配置、日志等的存储位置&lt;&#x2F;description&gt;</span><br><span class="line">　　　　&lt;&#x2F;property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">             &lt;name&gt;hbase.master.port&lt;&#x2F;name&gt;</span><br><span class="line">             &lt;value&gt;16000&lt;&#x2F;value&gt;</span><br><span class="line">        &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>

<h5 id="3-hbase-env-sh"><a href="#3-hbase-env-sh" class="headerlink" title="3. hbase-env.sh"></a>3. hbase-env.sh</h5><p>使用 HBase 自带的zookeeper</p>
<p>export HBASE_MANAGES_ZK=true </p>
<p>指定 JDK</p>
<p>export JAVA_HOME=/usr/local/jdk1.8.0_161（jdk安装路径）</p>
<h4 id="启动与停止Hbase"><a href="#启动与停止Hbase" class="headerlink" title="启动与停止Hbase"></a>启动与停止Hbase</h4><p>在Hadoop已经启动成功的基础上，输入 start-hbase.sh ，过几秒钟便启动完成。</p>
<p>输入jps命令查看进程是否启动成功，若 机器 上出现 HMaster 即可。 (不知道为什么。我这里没有出现 HQuorumPeer，不知道是不是版本的原因)</p>
<p>输入 hbase 可以看到支持的命令服务。 zkcli 是进入 HBase 自带的 zk 的命令</p>
<p>输入 hbase shell 命令 进入 HBase 命令模式</p>
<p>在浏览器中输入http://{ip}:16010 就可以在界面上看到 HBase 的配置了</p>
<p>当要停止hbase时输入stop-hbase.sh，过几秒后hbase就会被停止了。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>这只是简易版的。生产环境的要求肯定不止这些。参考者请注意。</p>
<p>这里有个地方没弄懂。尝试过配置 HBase Master 的端口为 16000。 发现未生效。端口是随机的。。。<br>没想明白为什么。等看下源码找找原因。</p>
<p>附上 HBase 端口设置参数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase-site.xml</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.master.port&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;16000&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.master.info.port&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;16010&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.regionserver.port&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;16201&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.regionserver.info.port&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;16301&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>

<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.cnblogs.com/lzxlfly/p/7221890.html" target="_blank" rel="noopener">Hadoop2.7.3+Hbase-1.2.6完全分布式安装部署</a></li>
<li><a href="https://www.cnblogs.com/jing1996/p/8038219.html" target="_blank" rel="noopener">Hadoop2.7.4安装配置</a></li>
</ul>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title>Spark BulkLoad 写入 HBase</title>
    <url>/2019/03/14/spark-bulkload/</url>
    <content><![CDATA[<p>使用 Spark 将数据以 bulkload 的方式写入 HBase</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>因历史遗留问题，有之前的开发写了 mapreduce 版的 bulkload 代码。</p>
<p>数据量太大。要跑3天才能跑完。 因此想将其改成 spark 的来试试。</p>
<p>之前没写过 spark 代码，都是现学现卖。同时记录自己所踩的坑。</p>
<h4 id="为何要-BulkLoad-导入？"><a href="#为何要-BulkLoad-导入？" class="headerlink" title="为何要 BulkLoad 导入？"></a>为何要 BulkLoad 导入？</h4><p>在初始化 HBase 表时，有时候需要大量导入初始数据。 最能想到的方式是一条一条数据写入。</p>
<p>可是这样的话，HBase会block写入，频繁进行flush，split，compact等大量IO操作，</p>
<p>并对HBase节点的稳定性造成一定的影响，GC时间过长，响应变慢，导致节点超时退出，并引起一系列连锁反应。因此在大数据量写入时效率低下。</p>
<p><img src="/images/blog/2019-03-14-10.png" alt></p>
<p>使用 Bulk Load 方式由于利用了 HBase 的数据信息是按照特定格式存储在 HDFS 里的这一特性。</p>
<p>直接在 HDFS 中生成持久化的 HFile 数据格式文件，然后完成巨量数据快速入库的操作。</p>
<p>配合 MapReduce 完成这样的操作，不占用 Region 资源，不会产生巨量的写入 I/O。所以需要较少的 CPU 和网络资源。</p>
<p>概括来说：BulkLoad 方式是绕过了 Write to WAL，Write to MemStore及Flush to disk的过程，减少了对集群资源的消耗，适合大批量数据导入。</p>
<h4 id="MapReduce-版本"><a href="#MapReduce-版本" class="headerlink" title="MapReduce 版本"></a>MapReduce 版本</h4><p>按照HBase存储数据按照HFile格式存储在HDFS的原理，使用MapReduce直接生成HFile格式的数据文件，然后在通过RegionServer将HFile数据文件移动到相应的Region上去。</p>
<p>将数据源准备好，上传到HDFS进行存储。我这里的数据来源是 Hive 表。查看原同事写的 MapReduce 版的是传入 Hive 表的路径</p>
<p>HFile文件的生成，可以使用MapReduce来进行实现，然后在程序中读取HDFS上的数据源，进行自定义封装，组装RowKey。</p>
<p>然后将封装后的数据在回写到HDFS上，以HFile的形式存储到HDFS指定的目录中。</p>
<p>大家可以参考哥不是小萝莉的博客。<a href="https://www.cnblogs.com/smartloli/p/9501887.html" target="_blank" rel="noopener">HBase BulkLoad批量写入数据实战</a></p>
<p>还有过往记忆的博客。<a href="https://www.iteblog.com/archives/1889.html" target="_blank" rel="noopener">通过BulkLoad快速将海量数据导入到HBase</a></p>
<p>两篇博客都写的挺好的。</p>
<h4 id="Spark-版本"><a href="#Spark-版本" class="headerlink" title="Spark 版本"></a>Spark 版本</h4><p>在 Spark 上通过 BulkLoad 快速将海量数据导入到 HBase</p>
<p>批量导数据到 HBase 又可以分为两种：</p>
<blockquote>
<ul>
<li>生成 HFiles，然后批量导数据</li>
<li>直接将数据批量导入到HBase中</li>
</ul>
</blockquote>
<p>具体代码可以参看过往记忆的博客。<a href="https://www.iteblog.com/archives/1891.html" target="_blank" rel="noopener">在Spark上通过BulkLoad快速将海量数据导入到HBase</a></p>
<p>我这里使用的是第一种方式写的代码。 把两种方式都记录下来是下次可以尝试第二种方式实现。</p>
<h5 id="1-批量将-HFiles-导入-HBase"><a href="#1-批量将-HFiles-导入-HBase" class="headerlink" title="1. 批量将 HFiles 导入 HBase"></a>1. 批量将 HFiles 导入 HBase</h5><p>现在我们来介绍如何批量将数据写入到HBase中，主要分为两步：</p>
<blockquote>
<ol>
<li>先生成 HFiles</li>
<li>使用 org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles 将事先生成 HFile 导入到 HBase 中。</li>
</ol>
</blockquote>
<h5 id="2-直接-BulkLoad-数据到-HBase"><a href="#2-直接-BulkLoad-数据到-HBase" class="headerlink" title="2. 直接 BulkLoad 数据到 HBase"></a>2. 直接 BulkLoad 数据到 HBase</h5><p>这种方法不需要事先在HDFS上生成Hfiles，而是直接将数据批量导入到HBase中。</p>
<h4 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h4><h5 id="1-路径没有权限"><a href="#1-路径没有权限" class="headerlink" title="1. 路径没有权限"></a>1. 路径没有权限</h5><p>保存 HFiles 的路径没有权限写入。把路径权限改成777，或者用所属账号提交运行，又或者改成自己能写入的路径。</p>
<h5 id="2-doBulkLoad-卡着"><a href="#2-doBulkLoad-卡着" class="headerlink" title="2. doBulkLoad 卡着"></a>2. doBulkLoad 卡着</h5><p>成功在 HDFS 上生成了 HFiles 文件，但是在 doBulkLoad 过程卡着不动，一度以为是资源问题，可是把我数据量缩小到只有3条数据都跑不动。</p>
<p>不应该是资源问题。后面怀疑是不是 HFiles 文件不符合规范导致卡着不动。</p>
<p>尝试直接使用 bulkload 命令可以直接导入数据 hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /tmp/pres person</p>
<p>说明生成的 HFiles 是可行的。同事说可能是路径权限还是不够，让我将其路径递归设置为 777 。 </p>
<p>再次运行代码到卡着不动时，手动执行递归修改权限命令 hadoop fs -chmod -R 777 /tmp/pres 。程序成功运行完成，并且 HBase 表可以查到数据了。</p>
<p>这里造成的原因不清楚，不知道是不是自己集群做了什么设置。</p>
<h5 id="3-临时目录存在"><a href="#3-临时目录存在" class="headerlink" title="3. 临时目录存在"></a>3. 临时目录存在</h5><p>HFiles 生成的目录应该要求事先不存在的，如果事先存在，应删除。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val fs:FileSystem &#x3D;  FileSystem.get(hdfsCf)</span><br><span class="line">if(fs.exists(new Path(tmpHdfs))) &#123;</span><br><span class="line">  fs.delete(new Path(tmpHdfs), true)    &#x2F;&#x2F; true 为递归删除</span><br><span class="line">  println(&quot;Delete Success&quot;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="4-key-有序"><a href="#4-key-有序" class="headerlink" title="4. key 有序"></a>4. key 有序</h5><p>这里真的是踩了大坑，大坑啊。 最先谷歌出来的博客说是 rowkey 有序即可。</p>
<p>但是我的代码明明已经对其排序了。sortByKey 后还是报错。</p>
<p><img src="/images/blog/2019-03-14-11.png" alt></p>
<p>后面对比了报错的原因才发现，不只是 rowkey 有序，cf、qualifer 也要有序。</p>
<h5 id="5-超过-32-个-HFiles"><a href="#5-超过-32-个-HFiles" class="headerlink" title="5. 超过 32 个 HFiles"></a>5. 超过 32 个 HFiles</h5><p>java.io.IOException: Trying to load more than 32 hfiles to one family of one region</p>
<p><img src="/images/blog/2019-03-15-1.png" alt></p>
<p>doBulkLoad 超过了 32 个 HFiles。 </p>
<p>HBaseConfiguration 设置参数。 两种方式都行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val conf &#x3D; HBaseConfiguration.create()</span><br><span class="line">conf.set(LoadIncrementalHFiles.MAX_FILES_PER_REGION_PER_FAMILY, &quot;1024&quot;)</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; conf.setInt(&quot;hbase.mapreduce.bulkload.max.hfiles.perRegion.perFamily&quot;, 1024);</span><br></pre></td></tr></table></figure>

<p>同时在 saveAsNewAPIHadoopFile 时要使用 conf</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">result.saveAsNewAPIHadoopFile(tmpHdfs, classOf[ImmutableBytesWritable], classOf[KeyValue], classOf[HFileOutputFormat2], conf)</span><br></pre></td></tr></table></figure>

<p>如果在直接使用 bulkload 命令时也报这个错误的话，给命令加个参数 -Dhbase.mapreduce.bulkload.max.hfiles.perRegion.perFamily=1024</p>
<p>hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles -Dhbase.mapreduce.bulkload.max.hfiles.perRegion.perFamily=1024   /tmp/pres person</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>真不容易。各种踩坑，各种填坑才完成的。</p>
<p>主要参考来自 过往的记忆的两篇文章，和哥不是萝莉的文章。<a href="https://www.iteblog.com/archives/1891.html" target="_blank" rel="noopener">资料1</a> 、<a href="https://www.cnblogs.com/smartloli/p/9501887.html" target="_blank" rel="noopener">资料2</a>、<a href="https://www.iteblog.com/archives/1889.html" target="_blank" rel="noopener">资料3</a>、<a href="https://mp.weixin.qq.com/s/fF1AXS958CS46vuJ0gMj5A" target="_blank" rel="noopener">资料5</a></p>
<p>Fayson 大神的 bulkload 好像比较高级。表示没看懂，有兴趣的大佬可以自己观摩。<a href="https://mp.weixin.qq.com/s/-Id4NIwMA1JtwAD1dXheAA" target="_blank" rel="noopener">资料4</a></p>
<hr>
<p>参考链接</p>
<blockquote>
<ul>
<li><a href="https://www.jianshu.com/p/b6c5a5ba30af" target="_blank" rel="noopener">Spark读写HBase之使用Spark自带的API以及使用Bulk Load将大量数据导入HBase</a></li>
<li><a href="https://segmentfault.com/a/1190000009762041" target="_blank" rel="noopener">Spark通过bulkLoad对HBase快速导入</a></li>
<li><a href="https://www.2cto.com/net/201710/692437.html" target="_blank" rel="noopener">使用Spark通过Bulkload的方式导数据到HBase</a></li>
<li><a href="https://blog.csdn.net/u012719230/article/details/81456141" target="_blank" rel="noopener">使用spark将数据以bulkload的方式写入HBase时报错</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>Scala 递归修改 HDFS 路径权限</title>
    <url>/2019/03/07/scala-system/</url>
    <content><![CDATA[<p>通过scala代码可以直接调用JVM的系统功能或者OS的系统功能或者OS的shell命令，这可以极大的简化外部功能的实现</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>写了一个 spark 的文件操作的代码。遇到一个权限问题。 </p>
<p>HDFS 提供 API 修改某个路径的权限。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> fs: <span class="type">FileSystem</span> =  <span class="type">FileSystem</span>.get(sc.hadoopConfiguration)</span><br><span class="line">fs.setPermission(<span class="keyword">new</span> <span class="type">Path</span>(<span class="string">"/user/hadoop/data"</span>),<span class="keyword">new</span> <span class="type">FsPermission</span>(<span class="string">"777"</span>))</span><br></pre></td></tr></table></figure>

<p>但是没有递归修改某个路径的权限。</p>
<p><img src="/images/blog/2019-03-07-1.png" alt></p>
<p>最先想法是通过递归的方式去一个个路径改权限。</p>
<p>同事给了一个 idea ，说可以试试代码调用外部命令来直接使用 hadoop fs -chmod -R 777 path 的方式来实现。</p>
<h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><p>scala外部命令工作的原理：</p>
<p>通过scala代码可以直接调用JVM的系统功能或者OS的系统功能或者OS的shell命令，这可以极大的简化外部功能的实现，因为这种工作方式实际上是复用JVM和OS本身提供的功能，作为scala本身是直接把结果拿过来，其实这是代码模块化和软件复用的一种表现。</p>
<p>scala是基于JVM进程的，scala程序运行的时候会运行在JVM进程中，而JVM进程是OS的一个普通进程，通过JVM可以直接和OS进行交互，而OS有例如启动进程等功能，所以scala程序可以通过JVM去调用外部的功能。</p>
<h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><p>spark-shell</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys.process._</span><br><span class="line"></span><br><span class="line"><span class="string">"ls -al"</span>!</span><br></pre></td></tr></table></figure>

<p>代码</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys.process._</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * scala外部命令</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CMD</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="string">"hadoop fs -chmod -R 777 /user/hadoop/data"</span>!</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">val</span> tmp = <span class="type">Process</span>(<span class="string">s""</span><span class="string">"ls"</span><span class="string">""</span>).!!</span><br><span class="line">    </span><br><span class="line">    println(tmp)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="/images/blog/2019-03-13-1.png" alt></p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/u013063153/article/details/53502788" target="_blank" rel="noopener">scala外部命令工作的原理和使用示例</a></li>
<li><a href="https://www.cnblogs.com/maxigang/p/10033159.html" target="_blank" rel="noopener">Spark中直接操作HDFS</a></li>
<li><a href="https://www.cnblogs.com/heml/p/6186109.html" target="_blank" rel="noopener">spark读取hdfs上的文件和写入数据到hdfs上面</a></li>
<li><a href="https://www.cnblogs.com/zuizui1204/p/9543928.html" target="_blank" rel="noopener">scala中可以执行外部命令Process</a></li>
<li><a href="https://fuliang.iteye.com/blog/1127449" target="_blank" rel="noopener">使用scala.sys.process包和系统交互</a></li>
</ul>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
  </entry>
  <entry>
    <title>Maven 打包 Scala 项目</title>
    <url>/2019/03/05/scala-maven/</url>
    <content><![CDATA[<p>记 Spark 打包放到 Hadoop 集群运行踩坑</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>因近期有写 使用 Spark 通过 BulkLoad 快速导入数据到 HBase 的需求。</p>
<p>在写过程中用 maven 打包一直失败。导致我无法测试自己的代码</p>
<p>折腾了一晚上终于踩坑完毕了</p>
<h4 id="开发环境"><a href="#开发环境" class="headerlink" title="开发环境"></a>开发环境</h4><p>操作系统：MAC</p>
<p>Scala：2.10.5</p>
<p>Maven: 3.3.9</p>
<p>Spark: 1.6.3</p>
<h4 id="构建"><a href="#构建" class="headerlink" title="构建"></a>构建</h4><p>在 IDEA 上新建 scala 项目。怎么构建的就不细说了。网上很多。<a href="https://www.iteblog.com/archives/1947.html" target="_blank" rel="noopener">如何在Java Maven工程中编写Scala代码</a></p>
<p>目录结构如图所示。记得给 scala 文件夹加上 source ，让其变成蓝色</p>
<p><img src="/images/blog/2019-03-05-1.png" alt></p>
<p>新建一个 object </p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Hive2HBase</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    print(<span class="string">"lihm"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>修改 pom.xml</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">sourceDirectory</span>&gt;</span>src/main/scala<span class="tag">&lt;/<span class="name">sourceDirectory</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-tools<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-scala-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>testCompile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">scalaVersion</span>&gt;</span>2.10.5<span class="tag">&lt;/<span class="name">scalaVersion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">args</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">arg</span>&gt;</span>-target:jvm-1.5<span class="tag">&lt;/<span class="name">arg</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">args</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line">scalaVersion 我猜是 打包编译时指定的 scala 版本</span><br><span class="line"></span><br><span class="line">这种 xml 配置打包的话是不会将依赖打成jar包的</span><br><span class="line"></span><br><span class="line">在 plugins 下追加一个 plugin</span><br><span class="line"></span><br><span class="line">```xml</span><br><span class="line"><span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appendAssemblyId</span>&gt;</span>false<span class="tag">&lt;/<span class="name">appendAssemblyId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br></pre></td></tr></table></figure>


<h4 id="打包"><a href="#打包" class="headerlink" title="打包"></a>打包</h4><p>在项目的顶级目录下执行下方命令打包  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mvn clean scala:compile compile package -DskipTests&#x3D;true</span><br><span class="line"></span><br><span class="line"># 忽略此命令</span><br><span class="line"># -Dscala.version 表示你打包时的编译版本。建议跟自己本机安装的相同</span><br><span class="line"># mvn clean package -Dscala.version&#x3D;2.10.5</span><br></pre></td></tr></table></figure>

<p>scala:compile 表示编译 scala 文件</p>
<p>compile 表示编译 Java 文件</p>
<p>-DskipTests=true 表示跳过编译时的测试</p>
<p><img src="/images/blog/2019-03-06-1.png" alt></p>
<p>打包完成之后可以在 项目的 target 文件夹下看到打包好的 jar 文件</p>
<p><img src="/images/blog/2019-03-05-2.png" alt></p>
<h4 id="示例运行"><a href="#示例运行" class="headerlink" title="示例运行"></a>示例运行</h4><p>将编译好的 jar 包上传至服务器，使用spark-submit提交</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export HADOOP_USER_NAME&#x3D;hbase</span><br><span class="line">spark-submit  --class Hive2HBase \</span><br><span class="line">--master yarn-client \</span><br><span class="line">--driver-memory 2g \</span><br><span class="line">--executor-cores 1 \</span><br><span class="line">--num-executors 200 \</span><br><span class="line">--executor-memory 2g \</span><br><span class="line">--queue root.line.etl \</span><br><span class="line">bulkLoadTool-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>

<p>–driver-memory 指定driver的内存</p>
<p>–class 是指定运行的主类。如果有路径，就把路径补全</p>
<p>–num-executors 是指定 executor 个数</p>
<p>–executor-memory 每个 executor 的内存</p>
<p>–queue 是指定提交时的队列。如果你的 yarn 没有资源队列限制就可以去掉。</p>
<p><img src="/images/blog/2019-03-05-3.png" alt></p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>最先开始打包失败时一度以为时maven 的问题。配置 maven 就折腾了好久</p>
<p>最后实在没办法了，怀疑是不是打包命令的问题。找到一个 <a href="https://github.com/davidB/scala-maven-plugin/issues/122" target="_blank" rel="noopener">github issue</a> 看到有人建议 -Dscala.version 。<br>尝试后发现可行。才踩坑完毕</p>
<p>再记：</p>
<p>忽略的那个命令为最先成功的命令，接着开发过程中我发现该命令报错。</p>
<p>接着又找到一个能编译打包并且成功的命令。</p>
<p>先用着，如果后续弄明白了再补上。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/-Id4NIwMA1JtwAD1dXheAA" target="_blank" rel="noopener">使用Spark通过BulkLoad快速导入数据到HBase</a></li>
<li><a href="https://www.jianshu.com/p/d4f426ad11ba" target="_blank" rel="noopener">java与scala混合编程打包(maven构建)</a></li>
<li><a href="https://www.cnblogs.com/zhoudayang/p/5027307.html" target="_blank" rel="noopener">scala和maven整合实践</a></li>
<li><a href="https://www.jianshu.com/p/279bcf76039b" target="_blank" rel="noopener">Maven打包Scala项目</a></li>
<li><a href="http://www.voidcn.com/article/p-trnffley-p.html" target="_blank" rel="noopener">maven-assembly-plugin把java工程打包成为一个可执行的jar包</a></li>
<li><a href="http://outofmemory.cn/code-snippet/8181/maven-assembly-plugin-build-dependency-into-one-jar" target="_blank" rel="noopener">使用maven的maven-assembly-plugin将jar包依赖打包到一个jar文件中</a></li>
<li><a href="https://tonglin.iteye.com/blog/556449#bc2390011" target="_blank" rel="noopener">关于创建可执行的jar文件</a></li>
<li><a href="https://www.cnblogs.com/wkrbky/p/6352188.html" target="_blank" rel="noopener">Maven常用命令</a></li>
</ul>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
  </entry>
  <entry>
    <title>BigTable 学习笔记(转)</title>
    <url>/2019/03/03/read-bigtable/</url>
    <content><![CDATA[<p>谷歌的三架马车之一 BigTable。HBase 其实算是 BigTable 的开源版。 </p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>一直准备读一下 BigTable 论文的。这两天算是静下心来读了一遍。<br>附上论文地址 <a href="http://static.googleusercontent.com/media/research.google.com/zh-CN//archive/bigtable-osdi06.pdf?spm=a2c4e.11153940.blogcont28400.14.7657411dcArZE7&file=bigtable-osdi06.pdf" target="_blank" rel="noopener">BigTable</a></p>
<p>我读的是 alex 翻译版的 <a href="https://blog.csdn.net/three_man/article/details/44409027" target="_blank" rel="noopener">BigTable</a></p>
<p>建议在对 HBase 有了一定了解的情况下再来读 BigTable ，不然读起来有点晦涩难懂。很不爽。</p>
<p>此文章为转载的。只在其中一点点部分加了自己的理解和分析。</p>
<p>原文链接 <a href="https://blog.csdn.net/baigoohao/article/details/50878645" target="_blank" rel="noopener">论文学习笔记：BigTable</a></p>
<h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><ul>
<li>设计目标：可靠的处理PB级别的数据，适用性广泛、可扩展、高性能和高可用性。</li>
<li>很多方面Bigtable和数据库类似，其也使用了数据库很多实现策略，但是Bigtable提供了和这些系统完全不同的接口。Bigtable不支持完整的关系数据模型，但为用户提供了一种简单的数据模型，用户可以动态控制数据的分布和格式</li>
</ul>
<h4 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h4><p>Bigtable是一个稀疏的、分布式的、持久化存储的多维排序Map（Key=&gt;Value）。Map的索引（Key）是行关键字、列关键字和时间戳，Map的值（Value）都是未解析的Byte数组：</p>
<ul>
<li>Key (row:string, col:string, time:int64) =&gt; Value (string)</li>
</ul>
<p>下图是Bigtable存储网页信息的一个例子</p>
<p><img src="/images/blog/2019-03-03-1.png" alt></p>
<ul>
<li>行：”com.cn.www”为网页的URL</li>
<li>列：”contents:”为网页的文档内容，”anchor:”为网页的锚链接文本（anchor:为列族，包含2列cnnsi.com和my.look.ca）</li>
<li>时间戳：t3、t5、t6、t8和t9均为时间戳</li>
</ul>
<h4 id="1-行"><a href="#1-行" class="headerlink" title="1. 行"></a>1. 行</h4><blockquote>
<ul>
<li>行和列关键字都为字符串类型，目前支持最大64KB，但一般10~100个字节就足够了</li>
<li>对同一个行关键字的读写操作都是原子的，这里类似Mysql的行锁，锁粒度并没有达到列级别</li>
<li>Bigtable通过行关键字的字典序来组织数据，表中每行都可以动态分区。每个分区叫做一个”Tablet”，故Tablet是数据分布和负载均衡调整的最小单位。这样做的好处是读取行中很少几列数据的效率很高，而且可以有效的利用数据的位置相关性（局部性原理）</li>
</ul>
</blockquote>
<h4 id="2-列族"><a href="#2-列族" class="headerlink" title="2. 列族"></a>2. 列族</h4><blockquote>
<ul>
<li>列关键字组成的集合叫做”列族”，列族是访问控制的基本单位，存放在同一列族的数据通常都属于同一类型。</li>
<li>一张表列族不能太多（最多几百个），且很少改变，但列却可以有无限多</li>
<li>列关键字的命名语法：列族:限定词。</li>
<li>访问控制、磁盘和内存的使用统计都是在列族层面进行的</li>
</ul>
</blockquote>
<h4 id="3-时间戳"><a href="#3-时间戳" class="headerlink" title="3. 时间戳"></a>3. 时间戳</h4><blockquote>
<ul>
<li>在Bigtable中，表的每个数据项都可包含同一数据的不同版本，不同版本通过时间戳来索引（64位整型，可精确到毫秒）</li>
<li>为了减轻各版本数据的管理负担，每个列族有2个设置参数，可通过这2个参数可以对废弃版本数据进行自动垃圾收集，用户可以指定只保存最后n个版本数据。(相当于HBase中的 version 和 TTL)</li>
</ul>
</blockquote>
<h4 id="API"><a href="#API" class="headerlink" title="API"></a>API</h4><p>在表操作方面，提供建表、删表、建列族、删列族，以及修改集群、表和列族元数据（如访问权限等）等基本API。</p>
<p>一个例子：</p>
<p><img src="/images/blog/2019-03-03-2.png" alt></p>
<p>在数据操作方面，提供写入、删除、读取、遍历等基础API。</p>
<p>一个例子：</p>
<p><img src="/images/blog/2019-03-03-3.png" alt></p>
<p>根据具体需求，Bigtable还开发出支持一些其他的特性，比如：1 支持单行上的事务处理，2 允许把数据项做整数计数器 3 允许用户在Bigtable服务器地址空间上执行脚本程序</p>
<h4 id="基础构件"><a href="#基础构件" class="headerlink" title="基础构件"></a>基础构件</h4><p>Bigtable是建立在其他几个Google基础构件上的，有GFS、SSTable、Chubby等。</p>
<h5 id="1-基础存储相关"><a href="#1-基础存储相关" class="headerlink" title="1. 基础存储相关"></a>1. 基础存储相关</h5><ul>
<li>Bigtable使用GFS存储日志文件和数据文件，集群通常运行在共享机器池（cloud）中，依靠集群管理系统做任务调度、资源管理和机器监控等</li>
</ul>
<h5 id="2-数据文件格式相关"><a href="#2-数据文件格式相关" class="headerlink" title="2. 数据文件格式相关"></a>2. 数据文件格式相关</h5><ul>
<li><p>Bigtable的内部储存文件为Google SSTable格式的，SSTable是一个持久化、排序的、不可更改的Map结构。(感觉像 HBase 的 HFile)</p>
</li>
<li><p>从内部看，SSTable是一系列的数据块，并通过块索引定位，块索引在打开SSTable时加载到内存中，用于快速查找到指定的数据块</p>
</li>
</ul>
<h5 id="3-分布式同步相关"><a href="#3-分布式同步相关" class="headerlink" title="3. 分布式同步相关"></a>3. 分布式同步相关</h5><ul>
<li><p>Bigtable还依赖一个高可用的、序列化的分布式锁服务组件Chubby（类zookeeper）。</p>
</li>
<li><p>Chubby服务维护5个活动副本，其中一个选为Master并处理请求，并通过Paxos算法来保证副本一致性。另外Chubby提供一个名字空间，提供对Chubby文件的一致性缓存等</p>
</li>
<li><p>Bigtable使用Chubby来完成几个任务，比如：1 确保任意时间只有一个活动Master副本，2 存储数据的自引导指令位置，3 查找Tablet服务器信息等 4 存储访问控制列表等</p>
</li>
</ul>
<h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><p>Bigtable包括3个主要的组件：链接到用户程序的库，1个Master服务器和多个Tablet服务器。Tablet服务器可根据工作负载动态增减</p>
<p>Master服务器：为Tablet服务器分配Tablets，对Tablet服务器进行负载均衡，检测Tablet服务器的增减等</p>
<p>Tablet服务器：管理一个Tablets集合（十到上千个Tablet），并负责它们的读写操作。与一般Single-Master类型的分布式存储系统类似，客户端可直接和Tablet服务器通信并进行读写，故Master的负载并不大</p>
<p>初始情况下，每个表只含一个Tablet，随着表数据的增长，它会被自动分割成多个Tablet，使得每个Table一般为100~200MB</p>
<h5 id="1-Tablet的位置信息"><a href="#1-Tablet的位置信息" class="headerlink" title="1. Tablet的位置信息"></a>1. Tablet的位置信息</h5><p>使用三层的、类B+树的结构存储Tablet的位置信息，如下图所示：</p>
<p><img src="/images/blog/2019-03-03-4.png" alt></p>
<ul>
<li><p>第一层为存储于Chubby中的Root Tablet位置信息。Root Tablet包含了一个特殊的METADATA表里所有的Tablet的位置信息，METADATA表的每个Tablet包含了一个用户Tablet的集合。</p>
</li>
<li><p>Root Tablet实际上是METADATA表的第一个Tablet，只不过对它的处理比较特殊 — Root Tablet永远不会被分割 — 这就保证了Tablet的位置信息存储结构不会超过三层。</p>
</li>
<li><p>在MetaData表内，每个Tablet的位置信息都存储在一个行关键字下，这个行关键字由Tablet所在表的标识符和最后一行编码而成</p>
</li>
<li><p>MetaData表每一行都存储约1KB内存数据，即在一个128MB的MetaData表中，采用这种3层存储结构，可标识2^34个Tablet地址</p>
</li>
<li><p>用户程序使用的库会缓存Tablet的位置信息，如果某个Tablet位置信息没有缓存或缓存失效，那么客户端会在树状存储结构中递归查询。故通常会通过预取Tablet地址来减少访问开销</p>
</li>
</ul>
<h5 id="2-Tablet的分配"><a href="#2-Tablet的分配" class="headerlink" title="2. Tablet的分配"></a>2. Tablet的分配</h5><ul>
<li><p>在任何时刻，一个Tablet只能分配给一个Tablet服务器，这个由Master来控制分配（一个Tablet没分配，而一个Tablet服务器用足够空闲空间，则Master会发给该Tablet服务器装载请求）</p>
</li>
<li><p>Bigtable通过Chubby跟踪Tablet服务器的状态。当Tablet服务器启动时，会在Chubby注册文件节点并获得其独占锁，当Tablet服务器失效或关闭时，会释放这个独占锁。</p>
</li>
<li><p>当Tablet服务器不提供服务时，Master会通过轮询Chubby上Tablet服务器文件锁的状态检查出来，确认后会删除其在Chubby注册的节点，使其不再提供服务。最后Master会重新分配这个Tablet服务器上的Tablet到其他未分配的Tablet集合内</p>
</li>
<li><p>当集群管理系统启动一个Master服务器之后，这个Master会执行以下步骤：</p>
<ul>
<li><p>1 从Chubby获取一个唯一的Master锁，保证Chubby只有一个Master实例</p>
</li>
<li><p>2 扫描Chubby上的Tablet文件锁目录，获取当前运行的Tablet服务器列表</p>
</li>
<li><p>3 和所有Tablet服务器通信，获取每个Tablet服务器上的Tablet分配信息</p>
</li>
<li><p>4 扫描MetaData表获取所有Tablet集合，如果发现有还没分配的Tablet，就会将其加入未分配Tablet集合等待分配</p>
</li>
</ul>
</li>
</ul>
<p>可能会遇到一种复杂的情况：</p>
<p>在METADATA表的Tablet还没有被分配之前是不能够扫描它的。<br>因此，在开始扫描之前（步骤4），如果在第三步的扫描过程中发现Root Tablet还没有分配，<br>Master服务器就把Root Tablet加入到未分配的Tablet集合。<br>这个附加操作确保了Root Tablet会被分配。<br>由于Root Tablet包括了所有METADATA的Tablet的名字，因此Master服务器扫描完Root Tablet以后，<br>就得到了所有的METADATA表的Tablet的名字了。 </p>
<p>保存现有Tablet的集合只有在以下事件发生时才会改变：</p>
<blockquote>
<ul>
<li>建立了一个新表或者删除了一个旧表、</li>
<li>两个Tablet被合并了</li>
<li>或者一个Tablet被分割成两个小的Tablet。</li>
</ul>
</blockquote>
<p>Master服务器可以跟踪记录所有这些事件，因为除了最后一个事件外的两个事件都是由它启动的。<br>Tablet分割事件需要特殊处理，因为它是由Tablet服务器启动。</p>
<p>在分割操作完成之后，Tablet服务器通过在METADATA表中记录新的Tablet的信息来提交这个操作；<br>当分割操作提交之后，Tablet服务器会通知Master服务器。<br>如果分割操作已提交的信息没有通知到Master服务器（可能两个服务器中有一个宕机了），<br>Master服务器在要求Tablet服务器装载已经被分割的子表的时候会发现一个新的Tablet。<br>通过对比METADATA表中Tablet的信息，Tablet服务器会发现Master服务器要求其装载的Tablet并不完整，<br>因此，Tablet服务器会重新向Master服务器发送通知信息。</p>
<h5 id="3-Tablet的服务"><a href="#3-Tablet的服务" class="headerlink" title="3. Tablet的服务"></a>3. Tablet的服务</h5><p><img src="/images/blog/2019-03-03-5.png" alt></p>
<ul>
<li><p>如图所示，Tablet的持久化状态信息保存在GFS上。更新操作会提交Redo日志，更新操作分2类：</p>
<ul>
<li><p>最近提交的更新操作会存放在一个排序缓存中，称为memtable</p>
</li>
<li><p>较早提交的更新操作会存放在SSTable中，落地在GFS上</p>
</li>
</ul>
</li>
<li><p>Tablet的恢复：Tablet服务器从MetaData中读取这个Tablet的元数据，元数据里面就包含了组成这个Tablet的SSTable和RedoPoint，然后通过重复RedoPoint之后的日志记录来重建（类似Mysql的binlog）</p>
</li>
<li><p>对Tablet服务器写操作：首先检查操作格式正确性和权限（从Chubby拉取权限列表）。之后有效的写记录会提交日志，也支持批量提交，最后写入的内容插入memtable内</p>
</li>
<li><p>对Tablet服务器读操作：也首先检查格式和权限，之后有效的读操作在一系列SSTable和memtable合并的视图内执行（都按字典序排序，可高效生成合并视图）</p>
</li>
</ul>
<h5 id="4-Compactions"><a href="#4-Compactions" class="headerlink" title="4. Compactions"></a>4. Compactions</h5><ul>
<li><p>当memtable增大达到一个门限值时，这个memtable会转换为SSTable并创建新的memtable，这个过程称为Minor Compaction。</p>
</li>
<li><p>Minor Compaction过程为了减少Tablet服务器使用的内存，以及在灾难恢复时减少从提交日志读取的数据量</p>
</li>
<li><p>如果Minor Compaction过程不断进行下去，SStable数量会过多而影响读操作合并多个SSTable，所以Bigtable会定期合并SStable文件来限制其数量，这个过程称为Major Compaction。</p>
</li>
<li><p>除此之外，Major Compaction过程生产的新SStable不会包含已删除的数据，帮助Bigtable来回收已删除的资源</p>
</li>
</ul>
<p>Minor Compaction 在 HBase 的有很大不同。初学者要注意。</p>
<h4 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h4><h5 id="1-局部性群族"><a href="#1-局部性群族" class="headerlink" title="1. 局部性群族"></a>1. 局部性群族</h5><ul>
<li><p>用户可将多个列族组合成一个局部性群族，Tablet中每个局部性群族都会生产一个SSTable，将通常不会一起访问的分割成不同局部性群族，可以提高读取操作的效率</p>
</li>
<li><p>此外，可以局部性群族为单位专门设定一些调优参数，如是否存储于内存等</p>
</li>
</ul>
<h5 id="2-压缩"><a href="#2-压缩" class="headerlink" title="2. 压缩"></a>2. 压缩</h5><ul>
<li><p>用户可以控制一个局部性群族的SSTable是否压缩</p>
</li>
<li><p>很多用户使用”两遍可定制“的压缩方式：第一遍采用Bentley and Mcllroy（大扫描窗口内常见长字符串压缩），第二遍采用快速压缩算法（小扫描窗口内重复数据），这种方式压缩速度达到100<del>200MB/s，解压速度达到400</del>1000MB/s，空间压缩比达到10:1</p>
</li>
</ul>
<h5 id="3-缓存"><a href="#3-缓存" class="headerlink" title="3. 缓存"></a>3. 缓存</h5><ul>
<li><p>Tablet服务器使用二级缓存策略来提高读操作性能。两级的缓存针对性不同：</p>
</li>
<li><p>第一级缓存为扫描缓存：缓存Tablet服务器通过SSTable接口获取的Key-Value对（时间局部性）</p>
</li>
<li><p>第二季缓存为块缓存：缓存从GFS读取的SSTable块（空间局部性）</p>
</li>
</ul>
<h5 id="4-布隆过滤器"><a href="#4-布隆过滤器" class="headerlink" title="4. 布隆过滤器"></a>4. 布隆过滤器</h5><ul>
<li><p>一个读操作必须读取构成Tablet状态的所有SSTable数据，故如果这些SSTable不在内存便需多次访问磁盘</p>
</li>
<li><p>我们允许用户使用一个Bloom过滤器来查询SStable是否包含指定的行和列数据，付出少量Bloom过滤器内存存储代价，换来显著减少访问磁盘次数</p>
</li>
<li><p>请参考Google黑板报 <a href="http://googlechinablog.com/2007/07/bloom-filter.html" target="_blank" rel="noopener">布隆过滤器</a></p>
</li>
</ul>
<h5 id="5-Commit日志实现"><a href="#5-Commit日志实现" class="headerlink" title="5. Commit日志实现"></a>5. Commit日志实现</h5><ul>
<li><p>如果每个Tablet操作的Commit日志单独写一个文件，会导致日志文件数过多，写入GFS会产生大量的磁盘Seek操作而产生负面影响</p>
</li>
<li><p>优化：设置为每个Tablet服务器写一个公共的日志文件，里面混合了各个Tablet的修改日志。</p>
</li>
<li><p>这个优化显著提高普通操作性能，却让恢复工作复杂化。当一台Tablet服务器挂了，需要将其上面的tablet均匀恢复到其他Tablet服务器，则其他服务器都得读取完整的Commit日志。为了避免多次读Commit日志，我们将日志按关键字排序(table, row, log_seq)，让同一个Tablet的操作日志连续存放</p>
</li>
</ul>
<h5 id="6-Tablet恢复提速"><a href="#6-Tablet恢复提速" class="headerlink" title="6. Tablet恢复提速"></a>6. Tablet恢复提速</h5><ul>
<li>Master转移Tablet时，源Tablet服务器会对这个Tablet做一次Minor Compaction，减少Tablet服务器日志文件没有归并的记录，从而减少了恢复时间</li>
</ul>
<h5 id="7-利用不变性"><a href="#7-利用不变性" class="headerlink" title="7. 利用不变性"></a>7. 利用不变性</h5><ul>
<li>在使用Bigtable时，除了SSTable缓存外其他部分产生的SSTable都是不变的，可以利用这个不变性对系统简化</li>
</ul>
<h4 id="性能评估"><a href="#性能评估" class="headerlink" title="性能评估"></a>性能评估</h4><ul>
<li><p>实验设计：N台Tablet服务器集群（N=1、50、250、500…），每台Tablet服务器1G内存，数据写入一个含1786台机器的GFS集群。使用N台Client产生工作负载，这些机器都连入一个两层树状网络，根节点带宽约100~200Gbps。</p>
</li>
<li><p>一共有6组基准测试：序列写、随机写、序列读、随机读、随机读（内存）和扫描，测试结果如下图所示：</p>
</li>
</ul>
<p><img src="/images/blog/2019-03-03-6.png" alt></p>
<p>测试均为读/写1000字节value的数据，图1显示了1/50/250/500台Tablet服务器，每台服务器的每秒操作次数，图2曲线显示随着Tablet服务器数量增加，所有服务器的每秒操作次数总和</p>
<ul>
<li><p>对于图1单个Tablet服务器性能维度，有下面几个特点：</p>
<ul>
<li><p>随机读性能最慢，这是因为每个随机读操作都要通过网络从GFS集群拉回64KB（1块）数据到Tablet服务器</p>
</li>
<li><p>随机读（内存）性能很快，因为这些读操作的数据都从Tablet服务器的内存读取</p>
</li>
<li><p>序列读性能好于随机读，因为每次从GFS取出64KB数据，这些数据会缓存，序列读很多落到同个块上而减少GFS读取次数</p>
</li>
<li><p>写操作比读操作高，因为写操作实质上为Tablet服务器直接把写入内容追加到Commit日志文件尾部（随机写和序列写性能相近的原因），最后再采用批量提交的方式写入GFS</p>
</li>
<li><p>扫描的性能最高，因为Client的每一次RPC调用都会返回大量value数据，抵消了RPC调用消耗</p>
</li>
</ul>
</li>
<li><p>对于图2Tablet服务器集群性能维度，有下面几个特点：</p>
<ul>
<li><p>随着Tablet服务器的增加，系统整体吞吐量有了梦幻般的增加，之所以会有这样的性能提升，主要是因为基准测试的瓶颈是单台Tablet服务器的CPU</p>
</li>
<li><p>尽管如此，性能的增加也不是线性的，这是由于多台Tablet服务器间负载不均衡造成的</p>
</li>
<li><p>随机读的性能提升最小，还是由于每个1000字节value的读操作都会导致一个64KB块的网络传输，消耗了网络的共享带宽</p>
</li>
</ul>
</li>
</ul>
<h4 id="实际应用"><a href="#实际应用" class="headerlink" title="实际应用"></a>实际应用</h4><p>截止到2006年，Google内部一共运行了388个非测试的Bigtable集群，约24500台Tablet服务器，这些应用以及应用数据大致如下：</p>
<p><img src="/images/blog/2019-03-03-7.png" alt></p>
<p>如上图所示，可以了解到Google分析，Google地图，Google个性化查询等应用的Bigtable使用情况</p>
<h4 id="经验教训"><a href="#经验教训" class="headerlink" title="经验教训"></a>经验教训</h4><ul>
<li><p>很多类型的错误都会导致大型分布式系统受损，而不仅仅是网络中断等“常规”错误。我们使用修改协议来解决这些问题（容错性），如在RPC机制中加入Checksum等</p>
</li>
<li><p>需要在彻底了解一个新特性如何使用后，再决定添加这个新特性是否是重要的。</p>
</li>
<li><p>系统级的监控对Bigtable非常重要，能有效跟踪集群状态、检查引发集群高时延的潜在因素等</p>
</li>
<li><p>简单的设计和编码给维护和调试带来了巨大的好处</p>
</li>
</ul>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/baigoohao/article/details/50878645" target="_blank" rel="noopener">论文学习笔记：BigTable</a></li>
<li><a href="https://blog.csdn.net/three_man/article/details/44409027" target="_blank" rel="noopener">BigTable论文翻译</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>HBase BlockCache 读缓存</title>
    <url>/2019/02/27/hbase-blockcache/</url>
    <content><![CDATA[<p>BlockCache也称为读缓存，HBase会将一次文件查找的Block块缓存到Cache中，<br>以便后续同一请求或者邻近数据查找请求直接从内存中获取，避免昂贵的IO操作，重要性不言而喻。</p>
<hr>
<h4 id="HBase-缓存"><a href="#HBase-缓存" class="headerlink" title="HBase 缓存"></a>HBase 缓存</h4><p>HBase提供了2种类型的缓存结构：MemStore &amp; BlockCache。其中 MemStore 是写缓存，BlockCache 是读缓存。</p>
<p>MemStore： HBase写数据首先写入MemStore之中，并同时写入HLog，待满足一定条件后将MemStore中数据刷到磁盘，可以很大提升HBase的写性能。<br>而且对读也很有提升，如果没有MemStore，读取刚写入的数据需要从文件中通过I/O查找。</p>
<p>BlockCache: HBase会将一次文件查找的Block块缓存到Cache中，以便后续同一请求或者相邻数据查找请求，可以直接从内存中获取，避免昂贵的IO操作。</p>
<p><strong>读取数据时，首先到memestore上读数据，找不到再到blockcahce上找数据，再查不到则到磁盘查找，并把读入的数据同时放入blockcache。</strong></p>
<h4 id="Block-Cache"><a href="#Block-Cache" class="headerlink" title="Block Cache"></a>Block Cache</h4><p>BlockCache 是 RegionServer 级别的，一个 RegionServer 只有一个 BlockCache，在RegionServer启动的时候完成Block Cache的初始化工作。</p>
<p>HBase提供了两种不同的BlockCache实现，用于缓存从HDFS读出的数据。这两种分别为：</p>
<ul>
<li>默认的，存在于堆内存的（on-heap）LruBlockCache</li>
<li>存在堆外内存的（off-heap）BucketCache</li>
</ul>
<p><strong>当blockcache达到heapsize * hfile.block.cache.size * 0.85时，会启用淘汰机制。(有待查看源码验证)</strong></p>
<h4 id="缓存策略"><a href="#缓存策略" class="headerlink" title="缓存策略"></a>缓存策略</h4><p>常用的 BlockCache 包括 LruBlockCache，以及 CombinedBlockCache（LruBlockCache + BucketCache）。</p>
<p>使用缓存有以下三个策略，有多种配置缓存的机制：</p>
<blockquote>
<ul>
<li>LruBlockCache 缓存机制：把元数据和列族信息缓存在Java堆内存中。如果 BucketCache 机制没有启动时，默认是启动 LruBlockCache 的；</li>
<li>CombinedBlockCache 缓存机制：运用 LruBlockCache 和 BucketCache 两个缓存：当BucketCache启用时，INDEX/BLOOM块会保存于LRUBlockCache的堆内存，数据块（DATA blocks）会一直保存于BucketCache。这时启动 BucketCache 缓存机制后默认的操作；</li>
<li>一级和二级缓存机制 (Raw L1+L2)：这个机制把元数据和列族信息缓存在LruBlockCache (一级缓存)，然后从 LruBlockCache 读取数据缓存在 BucketCache (二级缓存)，如果要启动这个缓存机制，要先在 hbase-site.xml 中配置 hbase.bucketcache.combinedcache.enabled=false，这个参数默认是 true；</li>
</ul>
</blockquote>
<p><strong>注意：从HBase 2.0.0 开始，L1与L2的概念便被弃用。第三种缓存方式只能在hbase 2.0.0之前，可以设置。</strong></p>
<p>补充：<br>CombinedBlockCache是一个LRUBlockCache和BucketCache的混合体。<br>BucketCache是阿里贡献的。LRUBlockCache中主要存储Index Block和Bloom Block，<br>而将Data Block存储在BucketCache中。<br>因此一次随机读需要首先在LRUBlockCache中查到对应的Index Block，然后再到BucketCache查找对应数据块</p>
<h5 id="1-开启缓存"><a href="#1-开启缓存" class="headerlink" title="1. 开启缓存"></a>1. 开启缓存</h5><blockquote>
<ul>
<li>当 BLOCKCACHE = false 和 IN_MEMORY = false，这意味著没有缓存；</li>
<li>当 BLOCKCACHE = true 和 IN_MEMORY = false，这意味著使用 最近使用原则 Least Recently Used (LRU) 缓存；</li>
<li>当 BLOCKCACHE = true 和 IN_MEMORY = true，这意味著缓存度是最長久的，有优先级别来缓存数据；</li>
</ul>
</blockquote>
<h5 id="2-禁用缓存"><a href="#2-禁用缓存" class="headerlink" title="2. 禁用缓存"></a>2. 禁用缓存</h5><p>可以在每一个列族上禁用读取缓存，使用 HBase Shell 来将读取时不需要缓存的列族 BLOCKCACHE 参数设置为 false，<br>使用 Java APi 在 scan 和 get 操作时使用 setCacheBlocks(false) 方法来禁用缓存，<br>但注意是的我们不能禁用 metadata 的缓存，因为元数据信息会频繁地被使用，<strong>就算禁用了也回加载到缓存中</strong>。<br>那应该在什么情况下可以禁用缓存，如果数据只是使用一次，不用反覆检索或者查找就不需要使用缓存。</p>
<p>永远不能禁用META块的缓存。由于<a href="https://issues.apache.org/jira/browse/HBASE-4683" target="_blank" rel="noopener">HBASE-4683</a>始终缓存索引和bloom块，<br>因此即使禁用BlockCache，也会缓存META块。</p>
<h4 id="LRUBlockCache"><a href="#LRUBlockCache" class="headerlink" title="LRUBlockCache"></a>LRUBlockCache</h4><p>HBase默认的BlockCache实现方案。Block数据块都存储在 JVM heap内，由JVM进行垃圾回收管理。<br>其使用一个ConcurrentHashMap管理BlockKey到Block的映射关系，<br>缓存Block只需要将BlockKey和对应的Block放入该HashMap中，查询缓存就根据BlockKey从HashMap中获取即可。</p>
<p>同时该方案采用严格的LRU淘汰算法，当Block Cache总量达到一定阈值之后就会启动淘汰机制，最近最少使用的Block会被置换出来。<br>在具体的实现细节方面，需要关注三点：</p>
<h5 id="1-缓存分层策略"><a href="#1-缓存分层策略" class="headerlink" title="1. 缓存分层策略"></a>1. 缓存分层策略</h5><p>HBase在LRU缓存基础上，采用了缓存分层设计，将整个BlockCache分为三个部分：Single、Mutile和In-Memory。</p>
<blockquote>
<ul>
<li>Single：当我们只有一次读取的数据，这个级别的数据块是第一时间就会被挤出去</li>
<li>Mutile：读取多次数据的缓存，这个级别的数据块是当块中没有 SINGLE 级别的数据才会被挤出去</li>
<li>In-Memory：对列族属性中的 IN_MEMEORY 设置为 true，这个级别的数据块是最后才会被挤出去，Catalog 表是默认启动了 IN_MEMORY 表的特性；</li>
</ul>
</blockquote>
<p>将内存从逻辑上分为了三块, 分别占到整个BlockCache大小的25%、50%、25%。</p>
<p>需要特别注意的是，<br><font color="#CD5C5C"><br>HBase系统元数据存放在InMemory区，因此设置数据属性InMemory = true需要非常谨慎，<br>确保此列族数据量很小且访问频繁，否则有可能会将hbase.meta元数据挤出内存，严重影响所有业务性能。<br></font></p>
<h5 id="2-LRU淘汰算法实现"><a href="#2-LRU淘汰算法实现" class="headerlink" title="2. LRU淘汰算法实现"></a>2. LRU淘汰算法实现</h5><p>系统在每次cache block时将BlockKey和Block放入HashMap后都会检查BlockCache总量是否达到阈值，如果达到阈值，就会唤醒淘汰线程对Map中的Block进行淘汰。<br>系统设置三个MinMaxPriorityQueue队列，分别对应上述三个分层，每个队列中的元素按照最近最少被使用排列，系统会优先poll出最近最少使用的元素，将其对应的内存释放。<br>可见，三个分层中的Block会分别执行LRU淘汰算法进行淘汰。</p>
<h5 id="3-LRU方案优缺点"><a href="#3-LRU方案优缺点" class="headerlink" title="3. LRU方案优缺点"></a>3. LRU方案优缺点</h5><p>LRU方案使用JVM提供的HashMap管理缓存，简单有效。<br>但随着数据从single-access区晋升到mutil-access区，基本就伴随着对应的内存对象从young区到old区 ，<br>晋升到old区的Block被淘汰后会变为内存垃圾，最终由CMS回收掉（Conccurent Mark Sweep，一种标记清除算法），<br>然而这种算法会带来大量的内存碎片，碎片空间一直累计就会产生臭名昭著的Full GC。<br>尤其在大内存条件下，一次Full GC很可能会持续较长时间，甚至达到分钟级别。<br>大家知道Full GC是会将整个进程暂停的（称为stop-the-wold暂停），<br>因此长时间Full GC必然会极大影响业务的正常读写请求。BucketCache方案才会横空出世。</p>
<h4 id="BucketCache"><a href="#BucketCache" class="headerlink" title="BucketCache"></a>BucketCache</h4><p>BucketCache 大家自行阅读范欣欣的博客学习 <a href="http://hbasefly.com/2016/04/26/hbase-blockcache-2/" target="_blank" rel="noopener" title="HBase BlockCache系列 － 探求BlockCache实现机制">HBase BlockCache系列 － 探求BlockCache实现机制</a></p>
<h4 id="BucketCache-工作模式"><a href="#BucketCache-工作模式" class="headerlink" title="BucketCache 工作模式"></a>BucketCache 工作模式</h4><p>BucketCache默认有三种工作模式：heap、offheap 和 file；</p>
<p>这三种工作模式在内存逻辑组织形式以及缓存流程上都是相同的。但是对应的最终存储介质不一样，也可以说对应的 IOEngine 不一样。</p>
<blockquote>
<p>heap 模式表示这些 Bucket 是从 JVM Heap 中申请，offheap 模式使用 DirectByteBuffer 技术实现堆外内存存储管理，而 file 模式使用类似SSD的高速缓存文件存储数据块。</p>
</blockquote>
<p>其中 heap 和 offheap 都是用内存作为最终存储介质，内存分配查询也都使用 Java NIO ByteBuffer 技术。</p>
<p>heap模式分配内存调用的是 ByteBuffer.allocate 方法，从JVM提供的heap区分配</p>
<p>offheap调用的是 ByteBuffer.allocateDirect() 方法，直接从操作系统分配。</p>
<blockquote>
<p>ByteBuffer.allocateDirect(cap); 进行内存申请的时候，会调用: DirectByteBuffer(int cap)构造函数</p>
</blockquote>
<p>这两种内存分配模式会对HBase性能产生一定影响，最大的是GC,和heap相比，offheap模式因为内存属于操作系统，所以基本不会产生CMS GC,也就在任何情况下都不会因为内存碎片导致触发Full GC</p>
<p>除此之外，在内存分配以及读取方面，两者性能也有不同，比如内存分配时heap模式需要先从操作系统分了配内存然后再拷贝到JVM Heap，相比offheap直接从操作系统分配内存更耗时；但是反过来</p>
<p>读取缓存是heap模式可以直接从JVM读取，而offheap需要首先从操作系统拷贝JVM heap在读取，后者显得更耗时</p>
<p>file模式和前面两者不同，它使用Fussion-IO或者SSD等作为存储介质，相比昂贵的内存，这样可以提供更大的存储容量，因此可以极大地提升缓存命中率。</p>
<h4 id="BucketCache-配置"><a href="#BucketCache-配置" class="headerlink" title="BucketCache 配置"></a>BucketCache 配置</h4><p>BucketCache配置使用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hbase.bucketcache.ioengine&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;offheap&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hfile.block.cache.size&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;0.2&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hbase.bucketcache.size&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;4196&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>

<p>hbase.bucketcache.size 在1.0 之前表示要提供给缓存的总堆内存大小的百分比，1.0 之后是BucketCache的总容量（兆字节）。默认值：0</p>
<p><img src="/images/blog/2019-02-27-1.png" alt></p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>因水平有限。大部分内容都是 copy 大神们的文章。我这只是把自己的理解汇总到一起。无意商业和彰显。</p>
<p>HBase 的官方文档 <a href="http://hbase.apache.org/book.html#block.cache" target="_blank" rel="noopener">BlockCache</a> 也是挺不错的资料。</p>
<p>找到一个翻译的中文博客，也可以参阅一下。<a href="https://www.cnblogs.com/zackstang/p/10061379.html" target="_blank" rel="noopener" title="HBase Block Cache（块缓存）">HBase Block Cache（块缓存)</a></p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="http://www.cnblogs.com/jcchoiling/p/7360110.html" target="_blank" rel="noopener">第五章：大数据 の HBase 进阶</a></li>
<li><a href="http://hbasefly.com/2016/04/26/hbase-blockcache-2/" target="_blank" rel="noopener" title="HBase BlockCache系列 － 探求BlockCache实现机制">HBase BlockCache系列 － 探求BlockCache实现机制</a></li>
<li><a href="https://blog.csdn.net/yx_keith/article/details/79138974" target="_blank" rel="noopener">hbase缓存机制</a></li>
<li><a href="https://www.cnblogs.com/zackstang/p/10061379.html" target="_blank" rel="noopener" title="HBase Block Cache（块缓存）">HBase Block Cache（块缓存</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/27639937" target="_blank" rel="noopener">1-游走HBase的Block Cache</a></li>
<li><a href="http://hbasefly.com/2016/04/08/hbase-blockcache-1/" target="_blank" rel="noopener">HBase BlockCache系列 – 走进BlockCache</a></li>
<li><a href="http://hbasefly.com/2016/05/06/hbase-blockcache-3/" target="_blank" rel="noopener">HBase BlockCache系列 － 性能对比测试报告</a></li>
<li><a href="http://hbasefly.com/2016/06/18/hbase-practise-ram/" target="_blank" rel="noopener">HBase最佳实践－内存规划</a></li>
<li><a href="https://blog.csdn.net/zhanglh046/article/details/78517812" target="_blank" rel="noopener">HBase之缓存</a></li>
<li><a href="https://blog.csdn.net/liuxiao723846/article/details/89814005" target="_blank" rel="noopener">DirectByteBuffer堆外内存申请、回收</a></li>
<li><a href="https://blog.csdn.net/shenshouniu/article/details/84577050" target="_blank" rel="noopener">HBase LRUBlockCache与BucketCache二级缓存机制原理剖析与参数调优-OLAP商业环境实战</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>订阅Apache ServiceComb的邮件列表</title>
    <url>/2019/02/26/subscriber-apache/</url>
    <content><![CDATA[<p>订阅Apache ServiceComb的邮件列表</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>邮件列表（Mail List）是一个成熟开源社区的“枢纽”。<br>通常来说，开源社区的提问与解答、技术讨论、事务决策等都通过邮件列表来承载。<br>邮件列表异步、广播的特性，也非常适合开源社区的沟通交流。</p>
<p>想要更好的融入社区，加入邮件列表是个不错的选择。</p>
<p>那么，如何订阅Apache ServiceComb的邮件列表呢？</p>
<p>说句实话，当初小白时，真不知道怎么操作。</p>
<h4 id="发送订阅邮件"><a href="#发送订阅邮件" class="headerlink" title="发送订阅邮件"></a>发送订阅邮件</h4><p>这里我以 HBase 举例子。在 HBase 官网可以看到邮件联系列表 <a href="http://hbase.apache.org/mailing-lists.html" target="_blank" rel="noopener">Mailing Lists</a></p>
<p>根据自己的需要选择自己要加入的邮件列表。</p>
<p><img src="/images/blog/2019-02-26-4.png" alt></p>
<p>这里我以订阅 HBase 的 Issue 为例子。</p>
<p>打开自己的邮箱，新建邮件，向 <a href="mailto:issues-subscribe@hbase.apache.org">issues-subscribe@hbase.apache.org</a> 发送一封邮件（邮件主题和内容任意）</p>
<p><img src="/images/blog/2019-02-26-1.png" alt></p>
<h4 id="接收确认邮件"><a href="#接收确认邮件" class="headerlink" title="接收确认邮件"></a>接收确认邮件</h4><p>执行完第一步之后，您将收到一封来自<a href="mailto:issues-help@hbase.apache.org">issues-help@hbase.apache.org</a>的确认邮件，邮件内容如下图所示。</p>
<p>（如果长时间未能收到，请确认该邮件是否已被拦截，或已经被自动归入“订阅邮件”、“垃圾邮件”、“推广邮件”等文件夹）</p>
<p><img src="/images/blog/2019-02-26-2.png" alt></p>
<h4 id="回复确认邮件"><a href="#回复确认邮件" class="headerlink" title="回复确认邮件"></a>回复确认邮件</h4><p>​针对上一步接收到的邮件, 直接回复该邮件确认即可。</p>
<h4 id="接收欢迎邮件"><a href="#接收欢迎邮件" class="headerlink" title="接收欢迎邮件"></a>接收欢迎邮件</h4><p>完成第三步之后，将会受到一封标题为Welcome to <a href="mailto:issues@hbase.apache.org">issues@hbase.apache.org</a>!的欢迎邮件。至此，订阅邮件列表的工作已经完成了，社区的动态都会通过邮件的方式通知您。</p>
<p><img src="/images/blog/2019-02-26-2.png" alt></p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="http://servicecomb.apache.org/cn/developers/subscribe-mail-list/" target="_blank" rel="noopener">如何订阅Apache ServiceComb的邮件列表</a></li>
<li><a href="https://mp.weixin.qq.com/s/7uotoQfqUezLzaABJPR9Zg" target="_blank" rel="noopener">Apache贡献 如何从小白成长为 Apache Committer?</a></li>
</ul>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title>HTTP 超文本传输协议</title>
    <url>/2019/02/24/http/</url>
    <content><![CDATA[<p>HTTP协议是客户端和服务器交互的一种通迅的格式。</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>从一个公众号文章了解  <a href="https://mp.weixin.qq.com/s/StqqafHePlBkWAPQZg3NrA" target="_blank" rel="noopener">一个故事讲完https</a></p>
<h4 id="HTTP"><a href="#HTTP" class="headerlink" title="HTTP"></a>HTTP</h4><p>HTTP是HyperText Transfer Protocol的缩写，中文翻译为超文本传输协议。他是一种用于分布式、协作式和超媒体信息系统的应用层协议。HTTP是万维网的数据通信的基础。</p>
<p>说的简单点，其实HTTP协议主要就是用来进行客户端和服务器之间进行通信的标准协议。HTTP主要规定了客户端如何与服务器建立链接、客户端如何从服务器请求数据、服务器如何响应请求，以及最后连接如何关闭。</p>
<p>是不是很懵逼，我也是这样的。</p>
<p>话不多说，直接上干货。<a href="https://mp.weixin.qq.com/s/CI1PVEkjmk5u8gHHvqhgUw" target="_blank" rel="noopener">漫话：如何给女朋友解释什么是HTTP</a>。<br>前提是得有个女朋友，哈哈，开玩笑的。</p>
<h4 id="HTTP协议的迭代"><a href="#HTTP协议的迭代" class="headerlink" title="HTTP协议的迭代"></a>HTTP协议的迭代</h4><p>HTTP协议主要的版本有3个，分别是HTTP/1.0、HTTP/1.1和HTTP/2。</p>
<p>PS: 据国际互联网工程任务组（The Internet Engineering Task Force，简称 IETF ）消息，HTTP-over-QUIC 实验性协议将被重命名为 HTTP/3，并有望成为 HTTP 协议的第三个正式版本。</p>
<p><a href="https://mp.weixin.qq.com/s/bobcUDUg9nXlJatLiVaaoQ" target="_blank" rel="noopener">HTTP2和HTTPS来不来了解一下？</a></p>
<h4 id="HTTP-各版本的区别"><a href="#HTTP-各版本的区别" class="headerlink" title="HTTP 各版本的区别"></a>HTTP 各版本的区别</h4><h5 id="1-0-与-1-1"><a href="#1-0-与-1-1" class="headerlink" title="1.0 与 1.1"></a>1.0 与 1.1</h5><p>HTTP/1.1相较于 HTTP/1.0 协议的区别</p>
<ul>
<li><p>长连接:<br>HTTP 1.1支持长连接（PersistentConnection）和请求的流水线（Pipelining）处理，在一个TCP连接上可以传送多个HTTP请求和响应，减少了建立和关闭连接的消耗和延迟，在HTTP1.1中默认开启Connection： keep-alive，一定程度上弥补了HTTP1.0每次请求都要创建连接的缺点。</p>
</li>
<li><p>缓存处理:<br>在HTTP1.0中主要使用header里的If-Modified-Since,Expires来做为缓存判断的标准，HTTP1.1则引入了更多的缓存控制策略例如Entity tag，If-Unmodified-Since, If-Match, If-None-Match等更多可供选择的缓存头来控制缓存策略。</p>
</li>
<li><p>新增状态码:<br>在HTTP1.1中新增了24个错误状态响应码，如409（Conflict）表示请求的资源与资源的当前状态发生冲突；410（Gone）表示服务器上的某个资源被永久性的删除。</p>
</li>
</ul>
<p>等</p>
<h5 id="2-0-与-1-x"><a href="#2-0-与-1-x" class="headerlink" title="2.0 与 1.x"></a>2.0 与 1.x</h5><p>HTTP2.0和HTTP1.X相比的新特性</p>
<ul>
<li><p>多路复用:<br>多个请求可同时在一个连接上并行执行。</p>
</li>
<li><p>二进制分帧:<br>HTTP/2在 应用层(HTTP/2)和传输层(TCP or UDP)之间增加一个二进制分帧层。在不改动 HTTP/1.x 的语义、方法、状态码、URI 以及首部字段的情况下, 解决了HTTP1.1 的性能限制，改进传输性能，实现低延迟和高吞吐量。在二进制分帧层中， HTTP/2 会将所有传输的信息分割为更小的消息和帧（frame）,并对它们采用二进制格式的编码 ，其中 HTTP1.x 的首部信息会被封装到 HEADER frame，而相应的 Request Body 则封装到 DATA frame 里面。</p>
</li>
<li><p>首部压缩:<br>HTTP/1.1并不支持 HTTP 首部压缩，为此 SPDY 和 HTTP/2 应运而生， SPDY 使用的是通用的DEFLATE 算法，而 HTTP/2 则使用了专门为首部压缩而设计的 HPACK 算法。 </p>
</li>
<li><p>服务端推送:<br>服务端推送是一种在客户端请求之前发送数据的机制。在 HTTP/2 中，服务器可以对客户端的一个请求发送多个响应。Server Push 让 HTTP1.x 时代使用内嵌资源的优化手段变得没有意义；如果一个请求是由你的主页发起的，服务器很可能会响应主页内容、logo 以及样式表，因为它知道客户端会用到这些东西。这相当于在一个 HTML 文档内集合了所有的资源，不过与之相比，服务器推送还有一个很大的优势：可以缓存！也让在遵循同源的情况下，不同页面之间可以共享缓存资源成为可能。</p>
</li>
</ul>
<h4 id="HTTPS"><a href="#HTTPS" class="headerlink" title="HTTPS"></a>HTTPS</h4><p>HTTPS是Hypertext Transfer Protocol Secure的缩写，翻译为超文本传输安全协议。HTTPS经由HTTP进行通信，但利用SSL/TLS来加密数据包。PS: 现在都用TLS协议了</p>
<p>先明确一点：HTTP和HTTPS是两个不同的协议。</p>
<p>HTTP的URL是由“http://”起始与默认使用端口80，而HTTPS的URL则是由“https://”起始与默认使用端口443。</p>
<p>什么对称加密，非对称加密。数字证书和 CA 认证之前被搞得头晕眼花的。</p>
<p>基础：</p>
<ul>
<li><p>对称加密</p>
<blockquote>
<ul>
<li>加密和解密都是用同一个密钥</li>
</ul>
</blockquote>
</li>
<li><p>非对称加密</p>
<blockquote>
<ul>
<li>加密用公开的密钥，解密用私钥</li>
<li>私钥只有自己知道，公开的密钥大家都知道</li>
</ul>
</blockquote>
</li>
<li><p>数字签名</p>
<blockquote>
<ul>
<li>验证传输的内容是对方发送的数据</li>
<li>发送的数据没有被篡改过</li>
</ul>
</blockquote>
</li>
<li><p>数字证书（Certificate Authority）简称CA</p>
<blockquote>
<ul>
<li>认证机构证明是真实的服务器发送的数据。</li>
</ul>
</blockquote>
<p>数字签名不懂的可以阅读阮一峰老师的博文<a href="http://www.ruanyifeng.com/blog/2011/08/what_is_a_digital_signature.html" target="_blank" rel="noopener">数字签名是什么？</a></p>
</li>
</ul>
<p>HTTPS采用的是混合方式加密。</p>
<blockquote>
<ul>
<li>用户向web服务器发起一个安全连接的请求</li>
<li>服务器返回经过CA认证的数字证书，证书里面包含了服务器的公钥,认证机构对服务器公钥的数字签名，服务器的基本信息。</li>
<li>用户拿到数字证书，用自己浏览器内置的”证书管理器”，查看解开数字证书的公钥是否存在。不存在，说明证书来源可疑。</li>
<li>用户用CA的公钥对证书中的数字签名解密，得到哈希值，对证书中的服务器公钥进行哈希值计算，两个哈希值对比，如果相同，则证书合法，得到正确的服务器公钥。</li>
<li>用户用服务器的公钥加密一个对称加密算法的密钥(即随机数)，同时Hash算法使用该随机数对该消息进行哈希值计算。消息与哈希值一同与传给web服务器。</li>
<li>因为只有服务器有私钥可以解密，所以不用担心中间人拦截这个加密的密钥</li>
<li>服务器拿到这个加密的密钥，用私钥解密获取密钥(即随机数)，使用随机数和Hash算法对该消息加密。验证的到的校验值是否与客户端发来的一致。如果一致则说明消息未被篡改，可以信任。</li>
<li>最后再使用对称加密算法，和用户完成接下来的网络通信。</li>
</ul>
</blockquote>
<h4 id="吐槽"><a href="#吐槽" class="headerlink" title="吐槽"></a>吐槽</h4><p>自己压根也写不出来。把看到的博客记录下来，用于回看。</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/CI1PVEkjmk5u8gHHvqhgUw" target="_blank" rel="noopener">漫话：如何给女朋友解释什么是HTTP</a></li>
<li><a href="https://mp.weixin.qq.com/s/bobcUDUg9nXlJatLiVaaoQ" target="_blank" rel="noopener">HTTP2和HTTPS来不来了解一下？</a></li>
<li><a href="http://www.ruanyifeng.com/blog/2011/08/what_is_a_digital_signature.html" target="_blank" rel="noopener">数字签名是什么？</a></li>
<li><a href="http://www.ruanyifeng.com/blog/2006/12/notes_on_cryptography.html" target="_blank" rel="noopener">密码学笔记</a></li>
<li><a href="https://mp.weixin.qq.com/s/CXRCihXc0RduUl5jXtmRPQ" target="_blank" rel="noopener">HTTP就是这么简单</a></li>
<li><a href="https://mp.weixin.qq.com/s/cRRoa34QvHm7Dx6OEbAPLQ" target="_blank" rel="noopener">HTTP常见面试题</a></li>
<li><a href="https://www.jianshu.com/p/9db57e761255" target="_blank" rel="noopener">什么是数字签名和证书？</a></li>
<li><a href="https://www.jianshu.com/p/650ad90bf563" target="_blank" rel="noopener">简单粗暴系列之HTTPS原理</a></li>
<li><a href="https://www.cnblogs.com/Paul-watermelon/p/10467662.html" target="_blank" rel="noopener">HTTP2.0的多路复用和HTTP1.X中的长连接复用区别</a></li>
</ul>
]]></content>
      <categories>
        <category>Network</category>
      </categories>
  </entry>
  <entry>
    <title>CAP 原理</title>
    <url>/2019/02/21/cap/</url>
    <content><![CDATA[<p>CAP 定理是分布式系统的基本定理，也是理解分布式系统的起点。</p>
<hr>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>在学习微服务概念时看到 CAP 概念。因此学习了解并记录。</p>
<p><img src="/images/blog/2019-02-21-1.png" alt></p>
<p>分布式系统（distributed system）正变得越来越重要，大型网站几乎都是分布式的，大数据体系也是分布式结构的。</p>
<p>分布式系统的最大难点，就是各个节点的状态如何同步。</p>
<p>CAP 定理是这方面的基本定理，也是理解分布式系统的起点。</p>
<p>CAP理论是架构师在设计分布式系统过程中，处理数据一致性问题时必须考虑的基石级理论。</p>
<h4 id="分布式系统"><a href="#分布式系统" class="headerlink" title="分布式系统"></a>分布式系统</h4><p>分布式系统非常关注三个指标</p>
<h5 id="1-数据一致性"><a href="#1-数据一致性" class="headerlink" title="1. 数据一致性"></a>1. 数据一致性</h5><p>数据“强一致性”，是希望系统只读到最新写入的数据。</p>
<p>通过单点串行化的方式，就能够达到这个效果。</p>
<p><a href="https://lihuimintu.github.io/2019/04/24/uniformity/" target="_blank" rel="noopener">一致性模型</a></p>
<h5 id="2-系统可用性"><a href="#2-系统可用性" class="headerlink" title="2. 系统可用性"></a>2. 系统可用性</h5><p>如果系统每运行100个时间单位，会有1个时间单位无法提供服务，则说系统的可用性是99%。</p>
<p>可用性和可靠性是比较容易搞混的两个指标，以一台取款机为例：</p>
<blockquote>
<ul>
<li>正确的输入，能够取到正确的钱，表示系统可靠</li>
<li>取款机7*24小时提供服务，表示系统可用</li>
</ul>
</blockquote>
<p>保证系统高可用的方法是：</p>
<blockquote>
<ul>
<li>冗余</li>
<li>故障自动转移</li>
</ul>
</blockquote>
<h5 id="3-节点连通性与扩展性"><a href="#3-节点连通性与扩展性" class="headerlink" title="3. 节点连通性与扩展性"></a>3. 节点连通性与扩展性</h5><p>分布式系统，往往有多个节点，每个节点之间，都不是完全独立的，<br>需要相互通信，当发生节点无法联通时，数据是否还能保持一致，<br>系统要如何进行容错处理，是需要考虑的。</p>
<h4 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h4><p>CAP定理是对上述分布式系统的三个特性进行归纳。</p>
<blockquote>
<ul>
<li>Consistency 一致性</li>
<li>Availability 可用性</li>
<li>Partition Tolerance 分区容忍性</li>
</ul>
</blockquote>
<p>等等。好像根本理解不了啊。</p>
<p>没关系可以参看阮一峰老师的文章 <a href="http://www.ruanyifeng.com/blog/2018/07/cap.html" target="_blank" rel="noopener">CAP 定理的含义</a> 先有个概念印象。</p>
<p>看完之后好像懂了，有种是懂非懂的感觉。</p>
<p>没关系，本人结自己理解在叙述一遍</p>
<p><strong>CAP理论核心思想是任何基于网络的数据共享系统最多只能满足两者，不可能三者兼顾。</strong></p>
<blockquote>
<ul>
<li>Consistency: 一致性指”all nodes see the same data at the same time”，<br>即更新操作成功并返回客户端完成后，所有节点在同一时间的数据完全一致。等同于所有节点拥有数据的最新版本。</li>
<li><em>这里对应一致性模型中的强一致性*</em></li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>Availability: 可用性指”Reads and writes always succeed”，<br>意思是只要收到用户的读写请求，每一个非故障的节点就必须在有限的时间内给出回应。<br>用户都一定会收到回应数据，不会收到服务错误，但不保证用户收到的数据一定是最新的数据。</li>
<li><em>两个指标”有限的时间内”，”给出回应”*</em></li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>Partition tolerance: 分区容忍性具体指”the system continues to operate despite arbitrary message loss or failure of part of the system”<br>即分布式系统容忍因网络故障出现分区，分区之间网络不可达的情况。仍然需要能够保证对外提供满足一致性和可用性的服务，除非是整个网络环境都发生了故障。<br>特殊的原因导致这些子网络出现网络不连通的状况，但各个子网络的内部网络是正常的，从而导致整个系统的网络环境被切分成了若干个孤立的区域。</li>
</ul>
</blockquote>
<p>“<br>提高分区容忍性的办法就是一个数据项复制到多个节点上，那么出现分区之后，这一数据项就可能分布到各个区里。分区容忍就提高了。然而，要把数据复制到多个节点，就会带来一致性的问题，就是多个节点上面的数据可能是不一致的。要保证一致，每次写操作就都要等待全部节点写成功，而这等待又会带来可用性的问题。<br>“</p>
<p><img src="/images/blog/2019-06-18-1.png" alt></p>
<p>如图，ClientA 可以发送指令到 Server 并且设置更新X的值，Client1 从Server读取该值，在单点情况下，即没有网络分区的情况下，或者通过简单的事务机制，可以保证Client 1 读到的始终是最新的值，不存在一致性的问题。 </p>
<p>如果在系统中增加一组节点，即设计为分布式系统，Write操作可能在 Server B 上成功，在 Server C 上失败，这时候对于 Client 1 和 Client 2，就会读取到不一致的值，出现不一致。如果要保持x值的一致性，Write 操作必须同时失败，降低系统的可用性。<br>如果 ClientA 写 ServerB 操作的时候，让 ServerB 向 ServerC 发送一条消息，要求 ServerC 的 X 也改成 1。即保证每次写操作就都要等待全部节点写成功，那这样”有限时间”过长会降低系统的可用性。</p>
<p>因此设计分布式数据系统，就是在一致性和可用性之间取一个平衡。</p>
<h4 id="强一致很难怎么办？"><a href="#强一致很难怎么办？" class="headerlink" title="强一致很难怎么办？"></a>强一致很难怎么办？</h4><p>在通常的分布式系统中，为了保证数据的高可用，通常会将数据保留多个副本(replica)，<br>网络分区是既成的现实，于是只能在可用性和一致性两者间做出选择。</p>
<p>CAP理论关注的是绝对情况下，在工程上，可用性和一致性并不是完全对立，<br>我们关注的往往是如何在保持相对一致性的前提下，提高系统的可用性。</p>
<p>单点串行化，虽然能保证“强一致”，但对系统的并发性能，以及高可用有较大影响，<br>互联网的玩法，更多的是“最终一致性”，短期内未必读到最新的数据，<br>但在一个可接受的时间窗口之后，能够读到最新的数据。</p>
<p>当然，并不是完全不管数据的一致性。<br>牺牲一致性，只是不再要求关系型数据库中的强一致性，</p>
<p>例如大多数web应用，其实并不需要强一致性，因此牺牲一致性而换取高可用性。达到最终一致性即可，<br>考虑到客户体验，这个最终一致的时间窗口，要尽可能的对用户透明，也就是需要保障“用户感知到的一致性”。</p>
<p>用阮一峰老师的例子来说明：<br>举例来说，发布一张网页到 CDN，多个服务器有这张网页的副本。后来发现一个错误，需要更新网页，这时只能每个服务器都更新一遍。<br>一般来说，网页的更新不是特别强调一致性。短时期内，一些用户拿到老版本，另一些用户拿到新版本，问题不会特别大。当然，所有人最终都会看到新版本。所以，这个场合就是可用性高于一致性。</p>
<p>通常是通过数据的多份异步复制来实现系统的高可用和数据的最终一致性的，<br>“用户感知到的一致性”的时间窗口则取决于数据复制到一致状态的时间。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>分布式系统本身就强调的是不挂掉。因此设计时架构师能够选择的只有C或者A，要么保证数据一致性（保证数据绝对正确），<br>要么保证可用性（保证是系统服务可用）。</p>
<ul>
<li><p>CAP可以理解为一致性，可用性，联通与扩展性</p>
</li>
<li><p>CAP三者只能取其二</p>
</li>
<li><p>最常见的实践是AP + 最终一致性</p>
</li>
</ul>
<hr>
<p>参考链接</p>
<ul>
<li><a href="http://www.ruanyifeng.com/blog/2018/07/cap.html" target="_blank" rel="noopener">CAP 定理的含义</a></li>
<li><a href="https://blog.csdn.net/guitar___/article/details/80656681" target="_blank" rel="noopener">CAP原理这样理解最简单</a></li>
<li><a href="https://www.jdon.com/bigdata/how-to-understand-cap.html" target="_blank" rel="noopener">如何正确理解CAP理论？</a></li>
</ul>
]]></content>
      <categories>
        <category>Distributed</category>
      </categories>
  </entry>
  <entry>
    <title>SpringBoot 企业邮箱发送邮件</title>
    <url>/2019/02/20/springboot-sendEmail/</url>
    <content><![CDATA[<p>以腾讯企业邮箱为例，讲解如何用 spring-boot 发送邮件</p>
<hr>
<h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><p>因公司内部平台需要用公家的企业邮箱账号发送邮件给运维人员。<br>发邮件是一个很常见的功能，代码本身并不复杂，有坑的地方主要在于各家邮件厂家的设置。故此记录下来。<br>下面以腾讯企业邮箱为例，讲解如何用spring-boot发送邮件。</p>
<h4 id="添加依赖项"><a href="#添加依赖项" class="headerlink" title="添加依赖项"></a>添加依赖项</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;!-- SpringBoot Mail --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spring-boot-starter-mail&lt;&#x2F;artifactId&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>

<h4 id="application-yml配置"><a href="#application-yml配置" class="headerlink" title="application.yml配置"></a>application.yml配置</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spring:</span><br><span class="line">  mail:</span><br><span class="line">    host: smtp.exmail.qq.com  # 邮箱服务器地址</span><br><span class="line">    username: xxxx@puscene.com # 这里填写企业邮箱</span><br><span class="line">    password: **************** # 这里填写企业邮箱登录密码</span><br><span class="line">    properties:</span><br><span class="line">      mail:</span><br><span class="line">        smtp:</span><br><span class="line">          auth: true</span><br><span class="line">          socketFactory:</span><br><span class="line">            class: javax.net.ssl.SSLSocketFactory</span><br><span class="line">            fallback: false</span><br><span class="line">            port: 465</span><br></pre></td></tr></table></figure>
<p>企业邮箱如果未开启安全登录，就不需要授权码了，直接填写登录密码即可。如果开启了安全登录，参考下图：</p>
<p><img src="/images/blog/2019-02-20-1.png" alt></p>
<p>则password这里，需要填写客户端专用密码</p>
<h4 id="发送代码示例"><a href="#发送代码示例" class="headerlink" title="发送代码示例"></a>发送代码示例</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.springframework.mail.SimpleMailMessage;</span><br><span class="line"><span class="keyword">import</span> org.springframework.scheduling.annotation.Async;</span><br><span class="line"><span class="keyword">import</span> org.springframework.stereotype.Service;</span><br><span class="line"><span class="keyword">import</span> org.springframework.beans.factory.annotation.Autowired;</span><br><span class="line"><span class="keyword">import</span> org.springframework.beans.factory.annotation.Value;</span><br><span class="line"><span class="keyword">import</span> org.springframework.mail.javamail.JavaMailSender;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> tu</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2019-02-20 15:47</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span> 企业邮箱发送邮件</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Service</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SendMailUtil</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> JavaMailSender mailSender; <span class="comment">//框架自带的</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Value</span>(<span class="string">"$&#123;spring.mail.username&#125;"</span>)  <span class="comment">//发送人的邮箱  比如155156641XX@163.com</span></span><br><span class="line">    <span class="keyword">private</span> String from;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Async</span>  <span class="comment">//意思是异步调用这个方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sendMail</span><span class="params">(String title, String url, String email)</span> </span>&#123;</span><br><span class="line">        SimpleMailMessage message = <span class="keyword">new</span> SimpleMailMessage();</span><br><span class="line">        message.setFrom(from); <span class="comment">// 发送人的邮箱</span></span><br><span class="line">        message.setSubject(title); <span class="comment">//标题</span></span><br><span class="line">        message.setTo(email); <span class="comment">//发给谁  对方邮箱</span></span><br><span class="line">        message.setText(url); <span class="comment">//内容</span></span><br><span class="line">        System.out.println(<span class="string">"lihm"</span>);</span><br><span class="line">        mailSender.send(message); <span class="comment">//发送</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注意: </p>
<p>最开始我用单元测试的方式运行发送代码。邮箱未收到邮件。<br>后面运行 Application 之后再测试才收到邮件</p>
<h4 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h4><p>个人邮箱的发送可以参看<a href="https://www.jianshu.com/p/19fb209c22c7" target="_blank" rel="noopener">Springboot 快速实现邮件发送功能</a> </p>
<p>附上常见邮箱服务器地址:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">smtp.163.com</span><br><span class="line">smtp.qq.com</span><br></pre></td></tr></table></figure>


<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.cnblogs.com/yjmyzz/p/send-mail-using-spring-boot.html" target="_blank" rel="noopener">spring-boot 速成(10) -【个人邮箱/企业邮箱】发送邮件</a></li>
<li><a href="https://www.jianshu.com/p/19fb209c22c7" target="_blank" rel="noopener">Springboot 快速实现邮件发送功能</a></li>
<li><a href="https://www.cnblogs.com/LUA123/p/5575134.html" target="_blank" rel="noopener">Java + 腾讯企业邮箱 + javamail + SSL 发送邮件</a></li>
<li><a href="https://blog.csdn.net/mcb520wf/article/details/80196804" target="_blank" rel="noopener">Springboot实现发送邮箱</a></li>
</ul>
]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
  </entry>
  <entry>
    <title>HBase 表数据迁移</title>
    <url>/2019/02/20/hbase-copytable/</url>
    <content><![CDATA[<p>使用 CopyTable 同步 HBase 数据</p>
<hr>
<h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><p>迁移 Kylin需要将 metadata 表进行迁移。</p>
<h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>CopyTable 是 HBase 提供的一个数据同步工具，可以用于同步表的部分或全部数据。<br>CopyTable 可以将表的一部分或全部复制到同一个集群或另一个集群。目标表必须首先存在。</p>
<h4 id="命令示例"><a href="#命令示例" class="headerlink" title="命令示例"></a>命令示例</h4><p>抄自官方</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;hbase org.apache.hadoop.hbase.mapreduce.CopyTable </span><br><span class="line">[--starttime&#x3D;X] [--endtime&#x3D;Y] [--new.name&#x3D;NEW] [--peer.adr&#x3D;ADR] tablename</span><br></pre></td></tr></table></figure>

<p>CopyTable常用选项说明如下：</p>
<ul>
<li>startrow 开始行。</li>
<li>stoprow 停止行。</li>
<li>starttime 时间戳（版本号）的最小值。</li>
<li>endtime 时间戳的最大值。如果不指定starttime，endtime不起作用。</li>
<li>peer.adr 目标集群的地址。格式为：hbase.zookeeer.quorum:hbase.zookeeper.client.port:zookeeper.znode.parent</li>
<li>families 要同步的列族。多个列族用逗号分隔。</li>
<li>all.cells 删除标记也进行同步。</li>
</ul>
<p>更多参数参见<a href="http://hbase.apache.org/book.html?spm=a2c4e.11153940.blogcont176546.18.330a3d9aX9KqDF#copy.table" target="_blank" rel="noopener">官方文档</a></p>
<p>抄自阿里云</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;hbase org.apache.hadoop.hbase.mapreduce.CopyTable </span><br><span class="line">-Dhbase.client.scanner.caching&#x3D;200 </span><br><span class="line">-Dmapreduce.local.map.tasks.maximum&#x3D;16 </span><br><span class="line">-Dmapred.map.tasks.speculative.execution&#x3D;false </span><br><span class="line">--peer.adr&#x3D;$ZK_IP1,$ZK_IP2,$ZK_IP3:&#x2F;hbase $TABLE_NAME</span><br></pre></td></tr></table></figure>

<blockquote>
<ul>
<li>对于单机运行的情况，需要指定mapreduce.local.map.tasks.maximum参数，表示并行执行的最大map个数。不指定的话默认是1，所有任务都是串行执行的。</li>
<li>hbase.client.scanner.caching建议设置为大于100的数。这个数越大，使用的内存越多，但是会减少scan与服务端的交互次数，对提升读性能有帮助。</li>
<li>mapred.map.tasks.speculative.execution建议设置为false，避免因预测执行机制导致数据写两次。</li>
</ul>
</blockquote>
<p>注意执行前应该先创建目标表目标表的表名和列簇名需与原来保持一致。<br>强烈建议根据数据的分布情况对目标表进行预分裂，这样能够提高写入速度。</p>
<p>性能数据可以阅读第一个参考链接</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://yq.aliyun.com/articles/176546" target="_blank" rel="noopener">使用CopyTable同步HBase数据</a></li>
<li><a href="https://www.cnblogs.com/yingjie2222/p/6016771.html" target="_blank" rel="noopener">表数据迁移（可以指定时间戳将数据导出方法）</a></li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
  </entry>
  <entry>
    <title>JVM 配置GC日志</title>
    <url>/2019/02/19/gcLog/</url>
    <content><![CDATA[<p>JVM 配置GC日志</p>
<hr>
<h4 id="开启GC日志"><a href="#开启GC日志" class="headerlink" title="开启GC日志"></a>开启GC日志</h4><p>多种方法都能开启GC的日志功能，其中包括：使用-verbose:gc或-XX:+PrintGC这两个标志中的任意一个能创建基本的GC日志<br>（这两个日志标志实际上互为别名，默认情况下的GC日志功能是关闭的）<br>使用-XX:+PrintGCDetails标志会创建更详细的GC日志</p>
<p>推荐使用-XX:+PrintGCDetails标志（这个标志默认情况下也是关闭的）；<br>通常情况下使用基本的GC日志很难诊断垃圾回收时发生的问题。</p>
<h4 id="开启GC时间提示"><a href="#开启GC时间提示" class="headerlink" title="开启GC时间提示"></a>开启GC时间提示</h4><p>除了使用详细的GC日志，我们还推荐使用-XX:+PrintGCTimeStamps或者-XX:+PrintGCDateStamps，<br>便于我们更精确地判断几次GC操作之间的时间。这两个参数之间的差别在于时间戳是相对于0（依据JVM启动的时间）的值，<br>而日期戳（date stamp）是实际的日期字符串。由于日期戳需要进行格式化，所以它的效率可能会受轻微的影响，<br>不过这种操作并不频繁，它造成的影响也很难被我们感知。</p>
<h4 id="指定GC日志路径"><a href="#指定GC日志路径" class="headerlink" title="指定GC日志路径"></a>指定GC日志路径</h4><p>默认情况下GC日志直接输出到标准输出，不过使用-Xloggc:filename标志也能修改输出到某个文件。<br>除非显式地使用-PrintGCDetails标志，否则使用-Xloggc会自动地开启基本日志模式。<br>使用日志循环（Log rotation）标志可以限制保存在GC日志中的数据量；<br>对于需要长时间运行的服务器而言，这是一个非常有用的标志，否则累积几个月的数据很可能会耗尽服务器的磁盘。</p>
<h4 id="开启日志滚动输出"><a href="#开启日志滚动输出" class="headerlink" title="开启日志滚动输出"></a>开启日志滚动输出</h4><p>通过-XX:+UseGCLogfileRotation  -XX:NumberOfGCLogfiles=N  -XX:GCLogfileSize=N标志可以控制日志文件的循环。</p>
<p>默认情况下，UseGCLogfileRotation标志是关闭的。它负责打开或关闭GC日志滚动记录功能的。要求必须设置 -Xloggc参数<br>开启UseGCLogfileRotation标志后，默认的文件数目是0（意味着不作任何限制），默认的日志文件大小是0（同样也是不作任何限制）。</p>
<p>因此，为了让日志循环功能真正生效，我们必须为所有这些标志设定值。<br>需要注意的是：</p>
<ul>
<li>The size of the log file at which point the log will be rotated, must be &gt;= 8K. 设置滚动日志文件的大小，必须大于8k。<br>当前写日志文件大小超过该参数值时，日志将写入下一个文件</li>
<li>设置滚动日志文件的个数，必须大于等于1</li>
<li>必须设置 -Xloggc 参数</li>
</ul>
<h4 id="开启语句"><a href="#开启语句" class="headerlink" title="开启语句"></a>开启语句</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-XX:+PrintGCDetails  -XX:+PrintGCDateStamps  </span><br><span class="line">-Xloggc:&#x2F;var&#x2F;log&#x2F;hbase&#x2F;gc-regionserver-hbase.log   </span><br><span class="line">-XX:+UseGCLogFileRotation  </span><br><span class="line">-XX:NumberOfGCLogFiles&#x3D;10  -XX:GCLogFileSize&#x3D;512k</span><br></pre></td></tr></table></figure>

<h4 id="参数详解"><a href="#参数详解" class="headerlink" title="参数详解"></a>参数详解</h4><p>-XX:+PrintGCDetails<br>输出GC的详细日志</p>
<p>-XX:+PrintGCDateStamps<br>输出GC的日期戳</p>
<p>-Xloggc:/var/log/hbase/gc-regionserver-hbase.log<br>GC日志输出的路径</p>
<p>-XX:+UseGCLogFileRotation<br>打开GC日志滚动记录功能</p>
<p>-XX:NumberOfGCLogFiles<br>设置滚动日志文件的个数，必须大于等于1<br>日志文件命名策略是，<filename>.0, <filename>.1, …, <filename>.n-1，其中n是该参数的值</filename></filename></filename></p>
<p>-XX:GCLogFileSize<br><code>The size of the log file at which point the log will be rotated, must be &gt;= 8K.</code><br>设置滚动日志文件的大小，必须大于8k<br>当前写日志文件大小超过该参数值时，日志将写入下一个文件</p>
<h4 id="其他有用参数"><a href="#其他有用参数" class="headerlink" title="其他有用参数"></a>其他有用参数</h4><p>-XX:+PrintGCApplicationStoppedTime<br>打印GC造成应用暂停的时间</p>
<p>-XX:+PrintHeapAtGC<br>在进行GC的前后打印出堆的信息</p>
<p>-XX:+PrintTenuringDistribution<br>在每次新生代 young GC时,输出幸存区中对象的年龄分布</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://www.jianshu.com/p/dfdf80286c73" target="_blank" rel="noopener">jvm 设置滚动记录GC日志</a></li>
<li><a href="https://segmentfault.com/a/1190000009415136" target="_blank" rel="noopener">GC日志参数</a></li>
<li><a href="https://mp.weixin.qq.com/s/aGT31AQyH7NRqnRGArE2eg" target="_blank" rel="noopener">假笨说参数-GC日志其实也支持滚动输出的</a></li>
<li><a href="https://www.jianshu.com/p/e634955f3bbb" target="_blank" rel="noopener">jvm-对象年龄(-XX:+PrintTenuringDistribution)</a></li>
</ul>
]]></content>
      <categories>
        <category>JVM</category>
      </categories>
  </entry>
  <entry>
    <title>SkipList 跳表</title>
    <url>/2019/02/18/skipList/</url>
    <content><![CDATA[<p>SkipList 跳表</p>
<hr>
<h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>skiplist本质上也是一种查找结构，用于解决算法中的查找问题（Searching），即根据给定的key，快速查到它所在的位置（或者对应的value）。<br>这种数据结构是由William Pugh发明的，最早出现于他在1990年发表的论文<a href="https://www.cl.cam.ac.uk/research/srg/netos/papers/2001-caslists.pdf" target="_blank" rel="noopener">《Skip Lists: A Probabilistic Alternative to Balanced Trees》</a>。有兴趣的可以阅读一下。</p>
<h4 id="有序表的搜索"><a href="#有序表的搜索" class="headerlink" title="有序表的搜索"></a>有序表的搜索</h4><p>入坑先阅读帅地的公众号文章 <a href="https://mp.weixin.qq.com/s/-ogyFJd9Jz4jROPfK8W6Vw" target="_blank" rel="noopener">《以后有面试官问你「跳跃表」，你就把这篇文章扔给他》</a> </p>
<p>写的挺不错的文章。文章末尾附有代码</p>
<p>跳表具有如下性质：</p>
<blockquote>
<ul>
<li>由很多层结构组成</li>
<li>每一层都是一个有序的链表</li>
<li>最底层的链表包含所有元素</li>
<li>如果一个元素出现在 Level i 的链表中，则它在 Level i 之下的链表也都会出现。</li>
<li>每个节点包含两个指针，一个指向同一链表中的下一个元素，一个指向下面一层的元素。</li>
</ul>
</blockquote>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>下面我们使用一些通用的标准对skiplis进行一下简单的评价：</p>
<ol>
<li><p>是否支持范围查找<br>因为是有序结构，所以能够很好的支持范围查找。</p>
</li>
<li><p>集合是否能够随着数据的增长而自动扩展<br>可以，因为核心数据结构是链表，所以是可以很好的支持数据的不断增长的</p>
</li>
<li><p>读写性能如何<br>因为从宏观上可以做到一次排除一半的数据，并且在写入时也没有进行其他额外的数据查找性工作，所以对于skiplist来说，其读写的时间复杂度都是O(log n)</p>
</li>
<li><p>是否面向磁盘结构<br>磁盘要求顺序写，顺序读，一次读写必须是一整块的数据。而对于skiplist来说，查询中每一次从高层跳跃到底层的操作，都会对应一次磁盘随机读，而skiplist的层数从宏观上来看一定是O(log n)层。因此也就对应了O(log n)次磁盘随机读。<br>因此这个数据结构不适合于磁盘结构。</p>
</li>
<li><p>并行指标<br>终于来到这个指标了， skiplist的并行指标是非常好的，只要不是在同一个目标插入点插入数据，所有插入都可以并行进行，而就算在同一个插入点，插入本身也可以使用无锁自旋来提升写入效率。因此skiplist是个并行度非常高的数据结构。</p>
</li>
<li><p>内存占用<br>与平衡二叉树的内存消耗基本一致。</p>
</li>
</ol>
<h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><p>在Java的API中已经有了实现：分别是</p>
<blockquote>
<ul>
<li>ConcurrentSkipListMap. 在功能上对应HashTable、HashMap、TreeMap。 在并发环境下，Java也提供ConcurrentHashMap这样的类来完成hashmap功能。</li>
<li>ConcurrentSkipListSet .  在功能上对应HashSet.<br>HashMap是基于散列表实现的，时间复杂度平均能达到O(1)。ConcurrentSkipListMap是基于跳表实现的，时间复杂度平均能达到O(log n)。</li>
</ul>
</blockquote>
<p>ConcurrentHashMap不能排序，容器类中可以排序的Map和Set是TreeMap和TreeSet，但它们不是线程安全的。Java并发包中与TreeMap/TreeSet对应的并发版本是ConcurrentSkipListMap和ConcurrentSkipListSet<br>有兴趣的可以阅读<a href="https://cloud.tencent.com/developer/article/1033615" target="_blank" rel="noopener">《并发容器 - 基于SkipList的Map和Set / 计算机程序的思维逻辑》</a></p>
<p>最近学习HBase源码时发现HRegion在sotre管理上用到了跳表数据结构ConcurrentSkipListMap</p>
<h4 id="延伸"><a href="#延伸" class="headerlink" title="延伸"></a>延伸</h4><p>skiplist里面一个最大的创新点，就是引入了一个新条件：概率。与传统的根据临近元素的来决定是否上推的avl或红黑树相比。Skiplist则使用概率这个完全不需要依托集合内其他元素的因素来决定这个元素是否要上推。<br>这种方式的最大好处，就是可以让每一次的插入都变得更“独立”，而不需要依托于其他元素插入的结果。 这样就能够让冲突只发生在数据真正写入的那一步操作上，而我们已经在前面的文章里面知道了，对于链表来说，数据的写入是能够做到无锁的写入新数据的，于是，利用skiplist,就能成功的做到无锁的有序平衡“树”（多层级）结构。</p>
<p>为了阐述清楚如何能够做到并发写，我们需要先对什么叫”一致性的写”，进行一下说明。<br>一般的人理解数据的一致性写的定义可能是：如果写成功了你就让我看到，而如果没写成功，你就不让我看到呗。<br>但实际上这个定义在计算机里面是无法操作的，因为我们之前也提到过，计算机其实就是个打字机，一次只能进行一个操作，针对复杂的操作，只能通过加锁来实现一致性。但加锁本身的代价又很大，这就形成了个悖论，如何能够既保证性能，又能够实现一致性呢？<br>这时候就需要我们对一致性的定义针对多线程环境进行一下完善：在之前的定义，我们是把写入的过程分为两个时间点的，一个时间点是调用写入接口前，另一个时间点是调用写入接口后。但其实在多线程环境下，应该分为三个时间点，第一个是调用写入接口前，第二个是调用写入接口，但还未返回结果的那段时间，第三个是调用写入接口，返回结果后。<br>然后我们来看看，针对这三个时间点应该如何选择，才能保证数据的一致性：<br>对于第一个时间点，因为还没有调用写入接口，所以所有线程（包含调用写入的线程）都不应该能够从这个映射中读取到待写入的数据。<br>第二个时间点，也就是写入操作过程中，我们需要能够保证：如果数据已经被其他线程看到过了，那么再这个时间点之后的所有时间点，数据应该都能够被其他线程看到，也就是说不能出现先被看到但又被删掉的情况。<br>第三个时间点，这个写入的操作应该能够被所有人看到。</p>
<p>已经定义好了一致性的规范，下面就来看看这个无锁并发的skiplist是如何处理好并发一致性的。<br>首先我们需要先了解一下链表是如何能够做到无锁写入的：<br>对于链表类的数据结构来说，如果想做到无锁，主要就是解决以下的问题，如何能够让当前线程知道，目前要插入新元素的位置，是否有其他人正在插入？ 如果有的话，那么就自旋等待，如果没有，那么就插入。利用这个原理，把原来的多步指针变更操作利用compare and set的方式转换为一个伪原子操作。这样就可以有效的减少锁导致的上下文切换开销，在争用不频繁的情况下，极大的提升性能。（这只是思路，关于linkedlist的无锁编程细节，可以参照A pragmatic implementation of non-blocking linked lists，这篇文章）<br>利用上面链表的无锁写入，我们就能够保证，数据在每一个level内的写是保证无锁写入的。并且，因为每一次有新的数据写入的时候其他尝试写入的线程也都能感知的到，所以这些并行写入的数据可以通过不断相互比较的方式来了解到，自己这个要写入的数据与其他并行写入的数据之间的大小关系，从而可以动态的进行调整以保证在每一层内，数据都是绝对有序的。<br>同一个level的一致性解决了，那么不同level之间的一致性是如何得到解决的呢？这就与我们刚才定义的一致性规范紧密相关了。因为数据的写入是从低层级开始，一层一层的往更高的层级推送的。而数据读取的时候，则是从最高层级往下读取的。又因为数据是绝对有序的，那么我们就一定可以认为，只要最低层级（level0）内存在了的数据，那么他就一定能够被所有线程看到。而如果在上推的过程中出现了任何异常，其实都是没关系的，因为上推的唯一目的在于加快检索速度，所以就算因为异常没有上推，也只是降低了查询的效率，对数据的可见性完全没有影响。<br>这个设计确实是非常的巧妙~</p>
<p>无锁完成多线程链表写入的算法，有兴趣的可以看看 cas （使用cas 实现无锁的skiplist）<br><a href="https://blog.csdn.net/microzzzf/article/details/78782123" target="_blank" rel="noopener">C++版</a></p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/fw0124/article/details/42780679" target="_blank" rel="noopener">SkipList 跳表</a></li>
</ul>
]]></content>
      <categories>
        <category>DataStructure</category>
      </categories>
  </entry>
  <entry>
    <title>Git 的基本使用汇总</title>
    <url>/2019/02/16/git/</url>
    <content><![CDATA[<p>参考：<a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000" target="_blank" rel="noopener">廖雪峰git教程</a></p>
<hr>
<h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><p>git使用是必须必须会的，每次遇到就是临时的找命令或者只记住了基础的命令，所以想把它总结下来，之后查的时候也方便。</p>
<h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><h5 id="1-工作区"><a href="#1-工作区" class="headerlink" title="1. 工作区"></a>1. 工作区</h5><p>工作区就是我们平时工作的本地仓库，此时的仓库是没有经过add的。</p>
<h5 id="2-暂存区"><a href="#2-暂存区" class="headerlink" title="2. 暂存区"></a>2. 暂存区</h5><p>暂存区是git add 之后，但未进行提交的部分。此部分为暂存区。如果我们直接执行 git diff 而不加任何参数的时候，比较的就是工作区和暂存区。</p>
<h4 id="设置仓库"><a href="#设置仓库" class="headerlink" title="设置仓库"></a>设置仓库</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">初始化本地仓库</span><br><span class="line">git init</span><br><span class="line"></span><br><span class="line">查看远程仓库地址命令</span><br><span class="line">git remote -v</span><br><span class="line"></span><br><span class="line">删除远程仓库地址</span><br><span class="line">git remote rm origin</span><br><span class="line"></span><br><span class="line">添加远程仓库地址 http:&#x2F;&#x2F;.....</span><br><span class="line">git remote add origin [url] </span><br><span class="line"></span><br><span class="line">查看remote地址，远程分支，还有本地分支与之相对应关系等信息</span><br><span class="line">git remote show origin</span><br><span class="line"></span><br><span class="line">删除那些远程仓库已经不存在的分支</span><br><span class="line">git remote prune origin</span><br><span class="line"></span><br><span class="line">建好 develop 分支的跟踪中央仓库分支</span><br><span class="line">git checkout -b develop origin&#x2F;develop</span><br></pre></td></tr></table></figure>

<h4 id="设置账号"><a href="#设置账号" class="headerlink" title="设置账号"></a>设置账号</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">查看全局账号</span><br><span class="line">git config --global user.name</span><br><span class="line"></span><br><span class="line">查看全局邮箱</span><br><span class="line">git config --global user.email</span><br><span class="line"></span><br><span class="line">设置全局变量的user.name</span><br><span class="line">git config --global user.name &quot;用户名&quot;</span><br><span class="line"></span><br><span class="line">设置全局变量的邮箱</span><br><span class="line">git config --global user.email &quot;邮箱地址&quot;</span><br><span class="line"></span><br><span class="line"># 局部的信息就是去掉上面的 --global</span><br></pre></td></tr></table></figure>

<h4 id="提交commit"><a href="#提交commit" class="headerlink" title="提交commit"></a>提交commit</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">提交变化文件到缓存区</span><br><span class="line">git add .</span><br><span class="line"></span><br><span class="line">提交 commit</span><br><span class="line">git commit -m &quot;描述&quot;</span><br><span class="line"></span><br><span class="line">修改最近一次 commit 提交的描述</span><br><span class="line">git commit --amend</span><br></pre></td></tr></table></figure>

<h4 id="推送远程仓库"><a href="#推送远程仓库" class="headerlink" title="推送远程仓库"></a>推送远程仓库</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">第一次推送分支</span><br><span class="line">git push -u origin master</span><br><span class="line">git push -u origin develop</span><br><span class="line"></span><br><span class="line">推送主分支</span><br><span class="line">git push origin master</span><br><span class="line"></span><br><span class="line">推送其他分支</span><br><span class="line">git push origin dev</span><br><span class="line"></span><br><span class="line">推送当前分支   </span><br><span class="line"># 不带任何参数的git push，默认只推送当前分支，这叫做simple方式。</span><br><span class="line">git push origin</span><br></pre></td></tr></table></figure>

<h4 id="管理分之"><a href="#管理分之" class="headerlink" title="管理分之"></a>管理分之</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">查看本地分支</span><br><span class="line">git branch</span><br><span class="line"></span><br><span class="line">查看远程分支列表</span><br><span class="line">git branch -r</span><br><span class="line"></span><br><span class="line">查看所有分之</span><br><span class="line">git branch -a</span><br><span class="line"></span><br><span class="line">查看本地分支与远程分支的映射关系</span><br><span class="line">git branch -vv</span><br><span class="line"></span><br><span class="line">建立当前分支与远程分支的映射关系</span><br><span class="line">git branch -u origin&#x2F;master</span><br><span class="line">git branch --set-upstream-to origin&#x2F;addFile</span><br><span class="line"></span><br><span class="line">撤销本地分支与远程分支的映射关系</span><br><span class="line">git branch --unset-upstream</span><br><span class="line"></span><br><span class="line">创建分支</span><br><span class="line">git branch dev</span><br><span class="line"></span><br><span class="line">切换分支</span><br><span class="line">git checkout dev</span><br><span class="line"></span><br><span class="line">创建dev分支，然后切换到dev分支 </span><br><span class="line"># git checkout命令加上-b参数表示创建并切换，相当于以上两条命令</span><br><span class="line">git checkout -b dev</span><br><span class="line"></span><br><span class="line">删除本地分支</span><br><span class="line"># 在master分支下，删除新开的dev分支</span><br><span class="line">git branch -d dev</span><br><span class="line"></span><br><span class="line">删除分支（即使没有合并）</span><br><span class="line">注意：如果dev的更改，push到远程，在GitLab(或者其他git系统)上面进行了merge操作，</span><br><span class="line">但是本地master没有pull最新的代码，会删除不成功，</span><br><span class="line">可以先git pull origin master，或者强制删除</span><br><span class="line">git branch -D dev</span><br><span class="line"></span><br><span class="line">删除远程分支dev</span><br><span class="line">git push origin :dev</span><br><span class="line"></span><br><span class="line">更新分支列表信息</span><br><span class="line"># -p 表示删除不存在的远程跟踪分支</span><br><span class="line">git fetch -p</span><br><span class="line"></span><br><span class="line">清理无效的远程追踪分支</span><br><span class="line">git remote prune origin</span><br></pre></td></tr></table></figure>

<h4 id="commit-回退"><a href="#commit-回退" class="headerlink" title="commit 回退"></a>commit 回退</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">回退上一个版本</span><br><span class="line">git reset --hard HEAD^</span><br><span class="line"></span><br><span class="line"># 上上一个版本就是HEAD^^</span><br><span class="line"># 100个版本写成HEAD~100</span><br><span class="line"></span><br><span class="line">回退到指定版本</span><br><span class="line">git reset --hard 1094a</span><br><span class="line"></span><br><span class="line"># 1094a 是版本号。</span><br><span class="line"># 版本号没必要写全，前几位就可以了，Git会自动去找。</span><br><span class="line"># 当然也不能只写前一两位，因为Git可能会找到多个版本号，就无法确定是哪一个了</span><br></pre></td></tr></table></figure>

<h4 id="commit-历史"><a href="#commit-历史" class="headerlink" title="commit 历史"></a>commit 历史</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">查看分支历史</span><br><span class="line">git log</span><br><span class="line"></span><br><span class="line">简化查看分之历史</span><br><span class="line"># 没有pretty的是，只有commit id 前7位，加pretty的是全部的id</span><br><span class="line">git log --oneline</span><br><span class="line">git log --pretty&#x3D;oneline</span><br><span class="line"></span><br><span class="line">图形式展示分支历史</span><br><span class="line">git log --graph</span><br><span class="line"></span><br><span class="line">简化版图形式展示分支历史</span><br><span class="line">git log --graph --oneline</span><br><span class="line"></span><br><span class="line">简化版图形式展示分支历史附带展示已经修改的文件</span><br><span class="line">git log --graph --oneline --name-only</span><br><span class="line"></span><br><span class="line">使用git reset --hard命令改变了HEAD指向某个版本后后，还可以用git log --reflog查看所有版本。</span><br><span class="line">git log --reflog</span><br></pre></td></tr></table></figure>

<h4 id="合并"><a href="#合并" class="headerlink" title="合并"></a>合并</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">普通合并</span><br><span class="line">git merge dev</span><br><span class="line"></span><br><span class="line">准备合并dev分支，请注意--no-ff参数，表示禁用Fast forward </span><br><span class="line"># 合并分支时，加上--no-ff参数就可以用普通模式合并，合并后的历史有分支，能看出来曾经做过合并</span><br><span class="line">而fast forward合并就看不出来曾经做过合并。</span><br><span class="line">git merge --no-ff -m &quot;merge with no-ff&quot; dev</span><br><span class="line"></span><br><span class="line">合并多次提交</span><br><span class="line">非关键性的提交太多会让版本历史很难看、冗余，所以合并多次提交也是挺有必要的。</span><br><span class="line">同样是使用以上的变基命令，不同的是变基命令打开的文本编辑器里的内容的修改。</span><br><span class="line">将pick修改为squash，可以是多行修改，然后保存退出。</span><br><span class="line">这个操作会将标记为squash的所有提交，都合并到最近的一个祖先提交上。</span><br><span class="line">注意：不能对的第一行commit进行修改，至少保证第一行是接受合并的祖先提交。</span><br><span class="line">-i参数表示进入交互模式。</span><br><span class="line">git rebase -i &lt;commit range&gt;</span><br></pre></td></tr></table></figure>

<h4 id="历史"><a href="#历史" class="headerlink" title="历史"></a>历史</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">记录你的每一次命令</span><br><span class="line">git reflog</span><br><span class="line"></span><br><span class="line">查看本地工作区、暂存区中文件的修改状态</span><br><span class="line">git status</span><br><span class="line"></span><br><span class="line">帮助</span><br><span class="line">git help </span><br><span class="line"></span><br><span class="line">帮助特定的命令</span><br><span class="line">git help diff</span><br></pre></td></tr></table></figure>

<h4 id="文件对比"><a href="#文件对比" class="headerlink" title="文件对比"></a>文件对比</h4><p>在我们不指定专门的文件夹的时候，git diff 默认比较的是整个项目 git 目录。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">工作区与暂存区比较</span><br><span class="line">git diff</span><br><span class="line"></span><br><span class="line">工作区与 HEAD ( 当前工作分支) 比较</span><br><span class="line">git diff HEAD</span><br><span class="line"></span><br><span class="line">工作区比较上次的提交</span><br><span class="line">git diff HEAD^</span><br><span class="line"></span><br><span class="line">工作区比较上两次提交，于是有了，git diff HEAD~n 是比较上n次提交与现在工作区间的关系</span><br><span class="line">git diff HEAD~2</span><br><span class="line"></span><br><span class="line">工作区与暂存区文件比较</span><br><span class="line">git diff filename</span><br><span class="line"></span><br><span class="line">工作区比较特定提交</span><br><span class="line">git diff commitId</span><br><span class="line"></span><br><span class="line">工作区与特定提交文件进行比较</span><br><span class="line">git diff commitId filepath </span><br><span class="line"></span><br><span class="line">暂存区与 HEAD 比较</span><br><span class="line">git diff --cached</span><br><span class="line"></span><br><span class="line">暂存区比较 HEAD 的某个文件差异</span><br><span class="line">git diff --cached _posts&#x2F;blog&#x2F;2019-02-16-git.md</span><br><span class="line"></span><br><span class="line">当前分支与 branchName 分支进行比较</span><br><span class="line">git diff branchName</span><br><span class="line"></span><br><span class="line">当前分支的文件与branchName 分支的文件进行比较</span><br><span class="line">git diff branchName filepath</span><br><span class="line"></span><br><span class="line">查看某两个版本之间的差异</span><br><span class="line">git diff ffd98b291e0caa6c33575c1ef465eae661ce40c9 b8e7b00c02b95b320f14b625663fdecf2d63e74c</span><br><span class="line"></span><br><span class="line">查看某两个版本的某个文件之间的差异</span><br><span class="line">git diff ffd98b291e0caa6c33575c1ef465eae661ce40c9:filename b8e7b00c02b95b320f14b625663fdecf2d63e74c:filename</span><br></pre></td></tr></table></figure>

<h4 id="合并多次-commit"><a href="#合并多次-commit" class="headerlink" title="合并多次 commit"></a>合并多次 commit</h4><p>针对多次 commit 但是未 push 到远程仓库的情况</p>
<p>使用 <code>git log --oneline</code> 命令查看提交历史</p>
<p>想要合并前四个 commit，执行 git rebase -i HEAD~4 对最近的 4 个 commit 进行 rebase 操作</p>
<p>进入编辑界面，把要保留的 commit 使用 pick，其他的使用 squash 命令，或者根据命令提示选择自己想用的命令。最上面的 commit 肯定是得保留的，<br>所以是第一个得是 pick</p>
<p><img src="/images/blog/2019-07-22-1.png" alt> </p>
<p>squash 和 fixup 区别是 squash 会将该 commit 的注释添加到上一个 commit 注释中，fixup 是放弃当前 commit 的注释。</p>
<p>保存退出，Git 自动进入另一个界面，此时就可以写合并之后 commit 的信息了。</p>
<p>将信息修改后保存退出，可以看到成功的命令。</p>
<p>重新查看提交提交历史，会发现这些 commit 已经合并了，整个提交历史简洁了很多</p>
<h4 id="删除某次-commit"><a href="#删除某次-commit" class="headerlink" title="删除某次 commit"></a>删除某次 commit</h4><p>同样的，利用 git 压缩 rebase 指令来删除某个 commit，过程和以上是类似的；</p>
<p>将需要删除的 commit 设置操作指令 drop ，保存退出即可。</p>
<h4 id="Merge-Request"><a href="#Merge-Request" class="headerlink" title="Merge Request"></a>Merge Request</h4><p>如何通过 GitLab 的 Merge Request（合并请求）进行代码审查以及我们遵循的现有代码审查最佳实践来改进工作流程。</p>
<h4 id="Commit-日志规范"><a href="#Commit-日志规范" class="headerlink" title="Commit 日志规范"></a>Commit 日志规范</h4><p>提交信息一定要认真填写！</p>
<p>建议参考规范: <font color="#2177b8"> <code>&lt;type&gt;(scope): &lt;subject&gt;</code></font></p>
<p>比如: fix(首页模块): 修复弹窗 JS Bug</p>
<p><code>type</code> 表示 动作类型，可分为:</p>
<ul>
<li>fix: 修复 xxx Bug</li>
<li>feat: 新增 xxx 功能</li>
<li>test: 调试 xxx 功能</li>
<li>style: 变更 xxx 代码格式或注释</li>
<li>docs: 变更 xxx 文档</li>
<li>refactor: 重构 xxx 功能或方法</li>
</ul>
<p><code>scope</code> 表示 影响范围，可分为: 模块、类库、方法等</p>
<p><code>subject</code> 表示 简短描述，最好不要超过 60 个字，如果有相关 Bug 的 Jira 号，建议在描述中加上</p>
<p>规范这东西不是一成不变的，供参考</p>
<hr>
<p>参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/csdlwzy/article/details/83379546" target="_blank" rel="noopener">Git使用rebase命令合并多次commit</a></li>
<li><a href="https://blog.csdn.net/al_assad/article/details/81145856" target="_blank" rel="noopener">Git 合并多次 commit、删除某次 commit</a></li>
<li><a href="https://www.cnblogs.com/pinefantasy/articles/6287147.html" target="_blank" rel="noopener">【Git系列】git rebase详解</a></li>
<li><a href="https://blog.csdn.net/LJFPHP/article/details/81741931" target="_blank" rel="noopener">git fetch -p 获取远程仓库的新分支以及删除远程仓库已删除的分支</a></li>
<li><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000" target="_blank" rel="noopener">廖雪峰git教程</a></li>
<li><a href="https://blog.csdn.net/qq_16885135/article/details/52777871" target="_blank" rel="noopener">git远程删除分支后，本地git branch -a 依然能看到的解决办法。</a></li>
<li><a href="https://www.jianshu.com/p/198915ac64ba" target="_blank" rel="noopener">用GitLab的Merge Request做代码评审</a></li>
<li><a href="https://www.cnblogs.com/myqianlan/p/4195994.html" target="_blank" rel="noopener">Git基本命令和GitFlow工作流</a></li>
<li><a href="https://www.jianshu.com/p/526eb3eec83e" target="_blank" rel="noopener">git本地分支与远程分支关联与解除关联</a></li>
<li><a href="https://mp.weixin.qq.com/s/6d6r-Z86mxvuUfFtT2cetg" target="_blank" rel="noopener">Git 分支设计规范</a></li>
</ul>
]]></content>
      <categories>
        <category>Tools</category>
      </categories>
  </entry>
  <entry>
    <title>CDH 更改 hostname</title>
    <url>/2019/02/12/cdh-modify-hostname/</url>
    <content><![CDATA[<p>修改 hostname</p>
<hr>
<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>修改 CDH 的 hostname</p>
<h3 id="操作系统"><a href="#操作系统" class="headerlink" title="操作系统"></a>操作系统</h3><p>CentOS7.2</p>
<hr>
<h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><h4 id="停止-CDH"><a href="#停止-CDH" class="headerlink" title="停止 CDH"></a>停止 CDH</h4><p>在 CM 页面上停止 Cluster 集群和 Clouder Management Service</p>
<h4 id="停止-CM"><a href="#停止-CM" class="headerlink" title="停止 CM"></a>停止 CM</h4><p>先把所有主机的 agent 停止。接着停止 server</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 停止</span><br><span class="line">systemctl stop cloudera-scm-agent</span><br><span class="line">systemctl stop cloudera-scm-server</span><br><span class="line"></span><br><span class="line"># 确认是否停止</span><br><span class="line">systemctl status cloudera-scm-agent</span><br><span class="line">systemctl status cloudera-scm-server</span><br></pre></td></tr></table></figure>

<h4 id="修改ClouderaManger资源库表修改数据库的hostname"><a href="#修改ClouderaManger资源库表修改数据库的hostname" class="headerlink" title="修改ClouderaManger资源库表修改数据库的hostname"></a>修改ClouderaManger资源库表修改数据库的hostname</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 获取密码</span><br><span class="line">grep password &#x2F;etc&#x2F;cloudera-scm-server&#x2F;db.properties</span><br><span class="line"></span><br><span class="line"># 登录 （mysql 的自行查阅）</span><br><span class="line">psql -h localhost -p 7432 -U scm  </span><br><span class="line"></span><br><span class="line"># 查询表</span><br><span class="line">select host_id, host_identifier, name, ip_address from hosts;</span><br><span class="line"></span><br><span class="line"># 修改表</span><br><span class="line">update HOSTS set name&#x3D;&#39;hostname&#39; where host_id&#x3D;1;</span><br><span class="line"></span><br><span class="line"># 退出</span><br><span class="line">\q</span><br></pre></td></tr></table></figure>

<h4 id="修改主机-hostname"><a href="#修改主机-hostname" class="headerlink" title="修改主机 hostname"></a>修改主机 hostname</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 直接修改命令</span><br><span class="line">hostname yz-JDB-106-38-11</span><br><span class="line"></span><br><span class="line"># 查看修改后的hostname</span><br><span class="line">hostname</span><br></pre></td></tr></table></figure>

<h4 id="修改host"><a href="#修改host" class="headerlink" title="修改host"></a>修改host</h4><p>更改host，复制到所有主机上</p>
<h4 id="启动-CM"><a href="#启动-CM" class="headerlink" title="启动 CM"></a>启动 CM</h4><p>所有主机启动agent</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 启动</span><br><span class="line">systemctl start cloudera-scm-server</span><br><span class="line">systemctl start cloudera-scm-agent</span><br></pre></td></tr></table></figure>

<h4 id="处理"><a href="#处理" class="headerlink" title="处理"></a>处理</h4><p>目前只启动 Clouder Management Service</p>
<h5 id="更改hostname服务器包含namenode等主节点的情况"><a href="#更改hostname服务器包含namenode等主节点的情况" class="headerlink" title="更改hostname服务器包含namenode等主节点的情况"></a>更改hostname服务器包含namenode等主节点的情况</h5><p>如果更改hostname服务器不包含namenode就跳过<br>重命名hosts的服务器中有namenode，而且已经启用了高可用</p>
<ol>
<li>只启动zookeeper 集群，此时所有其他服务，特别是HDFS/ZKFC，不能处于运行状态</li>
<li>在某一台zookeeper服务器，执行zookeeper-client</li>
</ol>
<ul>
<li>若集群没有启用kerberos，则直接跳过第二步；若集群配置了kerberos，则按第二步操作</li>
<li>配置zk认证如下：点击“HDFS”服务-&gt;点击“实例”页面-&gt;点击“Failover Controller”角色-&gt;点击“进程”页面；<br>在“hdfs/hdfs.sh [“zkfc”]”程序中，点击“显示”，查看“core-site.xml”，查看ha.zookeeper.auth属性，获取“digest:hdfs-fcs:”后为密码，如TEbW2bgoODa96rO3ZTn7ND5fSOGx0h；<br>执行addauth：addauth digest hdfs-fcs:TEbW2bgoODa96rO3ZTn7ND5fS</li>
<li>验证HA znode是否存在：ls /hadoop-ha</li>
<li>删除HDFS znode：rmr /hadoop-ha/nameservice1</li>
<li>如果没有运行JobTracker的高可用，则删除HA znode：rmr /hadoop-ha</li>
</ul>
<p>3.点击”HDFS”服务-&gt;点击”实例”tab-&gt;选择“操作”-&gt;点击“在zookeeper中初始化HA状态”</p>
<p>4.更新Hive Metastore</p>
<ul>
<li>备份元数据库</li>
<li>点击“Hive”服务-&gt;点击“操作”-&gt;点击“更新Hive Metastore Namenodes”</li>
</ul>
<p><img src="/images/blog/2019-02-13-1.png" alt></p>
<h4 id="更改hostname服务器包含kudu-master等主节点的情况"><a href="#更改hostname服务器包含kudu-master等主节点的情况" class="headerlink" title="更改hostname服务器包含kudu master等主节点的情况"></a>更改hostname服务器包含kudu master等主节点的情况</h4><p>修改 Kudu master hostname 的步骤请参考<a href="https://kudu.apache.org/docs/administration.html#_changing_the_master_hostnames" target="_blank" rel="noopener">官方文档</a></p>
<h4 id="启动其他组件"><a href="#启动其他组件" class="headerlink" title="启动其他组件"></a>启动其他组件</h4>]]></content>
      <categories>
        <category>CDH</category>
      </categories>
  </entry>
</search>
