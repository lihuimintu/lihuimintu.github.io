<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>lihuimintu</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-02-07T13:52:51.547Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>图</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Linux CPU、Memory、IO、Network 分析</title>
    <link href="http://yoursite.com/2020/02/07/CPU-Memory-IO-Network/"/>
    <id>http://yoursite.com/2020/02/07/CPU-Memory-IO-Network/</id>
    <published>2020-02-06T16:00:00.000Z</published>
    <updated>2020-02-07T13:52:51.547Z</updated>
    
    <content type="html"><![CDATA[<p>Linux性能监控 - CPU、Memory、IO、Network</p><hr><h4 id="CPU"><a href="#CPU" class="headerlink" title="CPU"></a>CPU</h4><p>CPU 查看这些重要参数: 中断、上下文切换、可运行队列、CPU 利用率来监测性能。</p><p>每个参数的健康区间:</p><ul><li>CPU利用率: User Time &lt;= 70%，System Time &lt;= 35%，User Time + System Time &lt;= 70%。</li><li>上下文切换: 与CPU利用率相关联，如果CPU利用率状态良好，大量的上下文切换也是可以接受的。</li><li>可运行队列: 每个处理器的可运行队列 &lt;=3 个线程。如双处理器系统的可运行队列里不应该超过6个线程。</li></ul><p>运用到的工具有<a href="https://www.jellythink.com/archives/419" target="_blank" rel="noopener">Linux vmstat命令详解</a>、<a href="https://www.jellythink.com/archives/421" target="_blank" rel="noopener">Linux top命令详解</a>、<a href="https://www.cnblogs.com/mayou18/p/9546431.html" target="_blank" rel="noopener">Linux mpstat-显示各个可用CPU的状态</a></p><p>top 查看 CPU 利用率，如果 User Time 比较高，使用 <a href="https://github.com/oldratlee/useful-scripts/blob/dev-2.x/docs/java.md#-show-busy-java-threads" target="_blank" rel="noopener">show-busy-java-threads</a> 工具排查，其会从所有运行的Java进程中找出最消耗CPU的线程（缺省5个），打印出其线程栈。<a href="https://lihuimintu.github.io/2019/06/03/Occupying-CPU-high/" target="_blank" rel="noopener">Linux 系统 CPU 过高异常排查</a></p><p>如果 System Time 比 User Time 高，以及高频度的上下文切换（cs），说明应用程序进行了大量的系统调用。</p><p>vmstat 输出例子</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----</span><br><span class="line"> r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st</span><br><span class="line"> 3  0 361396 196772  55820 359372    0    0    13    21    1    1  2  0 98  0  0</span><br><span class="line"> 1  0 361392 196524  55820 359616    8    0   236     0  411  527  1  0 90  9  0</span><br><span class="line"> 2  1 361392 196524  55828 359608    0    0     0    48  370  503  1  1 98  0  0</span><br><span class="line"> 4  0 361392 196524  55828 359616    0    0     0     0  442  559  1  0 99  0  0</span><br></pre></td></tr></table></figure><p>需要关注的参数</p><ul><li>r: 当前运行队列中线程的数目，代表线程处于可运行状态，但CPU还未能执行</li><li>b: 等待IO的进程数量；<strong>如果该值一直都很大，说明IO比较繁忙，处理较慢</strong></li><li>in: 每秒中断数</li><li>cs: 每秒上下文切换数</li><li>us: 用户占用CPU的百分比</li><li>sys: 内核占用CPU的百分比</li><li>id: CPU 完全空闲的百分比</li></ul><h4 id="Memory"><a href="#Memory" class="headerlink" title="Memory"></a>Memory</h4><p>分析 vmstat 输出</p><ul><li>si: 每秒从交换区写到内存的大小</li><li>so: 每秒写入交换区的内存大小</li></ul><p>内存够用的时候，这2个值都是0，如果这2个值长期大于0时，系统性能会受到影响，磁盘IO和CPU资源都会被消耗。有时我们看到空闲内存（free）很少的或接近于0时，就认为内存不够用了，不能光看这一点，还要结合si和so，如果free很少，但是si和so也很少（大多时候是0），那么不用担心，系统性能这时不会受到影响的</p><p><a href="https://mp.weixin.qq.com/s/XAhASXPkIELTvwwY8QPXng" target="_blank" rel="noopener">查询进程占用内存情况方法</a></p><h4 id="IO"><a href="#IO" class="headerlink" title="IO"></a>IO</h4><p>通过 <code>vmstat</code> 的输出，重点关注<code>b</code>、<code>bi</code>、<code>bo</code>和<code>wa</code>字段。这几个值变大，都意味着IO的消耗增加。</p><p>对于读请求大的服务器，一般<code>b</code>、<code>bi</code>、<code>wa</code>都会比较大，而对于写入量大的服务器，一般<code>b</code>、<code>bo</code>、<code>wa</code>都会比较大。</p><p>借助<a href="https://www.jellythink.com/archives/438" target="_blank" rel="noopener">Linux iostat命令详解</a>可以查看相关参数</p><ul><li>%iowait: 如果该值较高，表示磁盘存在I/O瓶颈</li><li>await: 平均每次设备I/O操作的等待时间 (毫秒)，一般地，系统I/O响应时间应该低于5ms，如果大于 10ms就比较大了</li><li>%util：一秒中有百分之多少的时间用于I/O操作，即被IO消耗的CPU百分比，一般地，如果该参数是100%表示设备已经接近满负荷运行了</li></ul><p>借助<code>pidstat -d 1</code>定位出导致瓶颈的进程，参阅<a href="https://www.jellythink.com/archives/444" target="_blank" rel="noopener">Linux pidstat命令详解</a></p><p>现在定位到进程级别了，可能需要知道这个进程到底打开了哪些文件，借助<code>lsof -p 20711</code>命令列出指定20711进程打开的文件列表，参阅<a href="https://www.jellythink.com/archives/449" target="_blank" rel="noopener">Linux lsof命令详解</a></p><h4 id="Network"><a href="#Network" class="headerlink" title="Network"></a>Network</h4><p>网络的监测是所有 Linux 子系统里面最复杂的，这里贴了解到的运维命令</p><p>iftop 命令查看端到端流量</p><p>NetHogs实时监控进程的网络带宽占用情况，参阅<a href="https://lihuimintu.github.io/2019/12/06/Linux-nethogs/" target="_blank" rel="noopener">Linux NetHogs 监控工具</a></p><p>还有些<code>ip</code>，<code>netstat</code>，<code>tcpdump</code>，<code>sar</code> 来分析网络性能问题，参阅<a href="https://www.jellythink.com/archives/486" target="_blank" rel="noopener">Linux性能监测：网络篇</a></p><p><strong>UDP监控</strong></p><p>对于UDP服务，查看所有监听的UDP端口的网络情况</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$  watch netstat -unlp</span><br><span class="line">Every 2.0s: netstat -unlp                                                                                                                      Fri Feb  7 21:36:55 2020</span><br><span class="line"></span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         StatePID&#x2F;Program name</span><br><span class="line">udp        0  0 0.0.0.0:627             0.0.0.0:*                           6813&#x2F;rpcbind</span><br><span class="line">udp        0  0 0.0.0.0:7191            0.0.0.0:*                           8137&#x2F;python2</span><br><span class="line">udp        0  0 0.0.0.0:68              0.0.0.0:*                           780&#x2F;dhclient</span><br><span class="line">udp        0  0 0.0.0.0:111             0.0.0.0:*                           1&#x2F;systemd</span><br><span class="line">udp        0  0 10.0.0.122:123          0.0.0.0:*                           3924&#x2F;ntpd</span><br><span class="line">udp        0  0 127.0.0.1:123           0.0.0.0:*                           3924&#x2F;ntpd</span><br><span class="line">udp        0  0 0.0.0.0:123             0.0.0.0:*                           3924&#x2F;ntpd</span><br><span class="line">udp6   0  0 :::627                  :::*                                6813&#x2F;rpcbind</span><br><span class="line">udp6   0  0 :::7191                 :::*                                8137&#x2F;python2</span><br><span class="line">udp6   0  0 :::111                  :::*                                6813&#x2F;rpcbind</span><br><span class="line">udp6   0  0 :::123                  :::*                                3924&#x2F;ntpd</span><br></pre></td></tr></table></figure><p>对于<code>Recv-Q</code>和<code>Send-Q</code>两个指标值为0，或者没有长时间大于0的数值是比较正常的。</p><p>对于UDP服务，查看丢包情况（网卡收到了，但是应用层没有处理过来造成的丢包）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ netstat -su</span><br><span class="line">...</span><br><span class="line">Udp:</span><br><span class="line">    14706185 packets received</span><br><span class="line">    272545 packets to unknown port received.</span><br><span class="line">    11026 packet receive errors</span><br><span class="line">    46474392 packets sent</span><br><span class="line">    10964 receive buffer errors</span><br><span class="line">    0 send buffer errors</span><br><span class="line">    InCsumErrors: 62</span><br><span class="line">....</span><br></pre></td></tr></table></figure><p><code>packet receive errors</code> 这一项数值增长了，则表明在丢包。</p><p><strong>TCP监控</strong></p><p>对于TCP服务而言，这个就比较复杂；因为TCP涉及到重传，所以我们就需要重点关注这个重传率。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ netstat -st | grep segments</span><br><span class="line">    689039681 segments received</span><br><span class="line">    864949096 segments send out</span><br><span class="line">    2233493 segments retransmited</span><br><span class="line">    5186 bad segments received.</span><br></pre></td></tr></table></figure><p>查看<code>segments send out</code>和<code>segments retransmited</code>指标，对比一段时间内，这两个指标的增长率就是对应的重传率(<a href="https://www.jellythink.com/archives/486" target="_blank" rel="noopener">Linux性能监测：网络篇</a>文末尾有计算重传率的Shell脚本)，发生重传说明网络传输有丢包，基本上从3个点去定位：客户端网络情况、服务端网络情况、中间链路网络情况。</p><p><strong>网卡吞吐率</strong></p><p>可以通过<code>sar -n DEV 2 3</code>命令来查看</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ sar -n DEV 2 3</span><br><span class="line">Linux 3.10.0-862.14.4.el7.x86_64 (region-0-0-122) 2020年02月07日 _x86_64_(4 CPU)</span><br><span class="line"></span><br><span class="line">21时43分32秒     IFACE   rxpck&#x2F;s   txpck&#x2F;s    rxkB&#x2F;s    txkB&#x2F;s   rxcmp&#x2F;s   txcmp&#x2F;s  rxmcst&#x2F;s</span><br><span class="line">21时43分34秒      eth0    116.50    119.50     11.77     19.72      0.00      0.00      0.00</span><br><span class="line">21时43分34秒        lo     66.00     66.00    144.26    144.26      0.00      0.00      0.00</span><br><span class="line"></span><br><span class="line">21时43分34秒     IFACE   rxpck&#x2F;s   txpck&#x2F;s    rxkB&#x2F;s    txkB&#x2F;s   rxcmp&#x2F;s   txcmp&#x2F;s  rxmcst&#x2F;s</span><br><span class="line">21时43分36秒      eth0    155.00    152.00     19.60     44.66      0.00      0.00      0.00</span><br><span class="line">21时43分36秒        lo     29.00     29.00     94.53     94.53      0.00      0.00      0.00</span><br><span class="line"></span><br><span class="line">21时43分36秒     IFACE   rxpck&#x2F;s   txpck&#x2F;s    rxkB&#x2F;s    txkB&#x2F;s   rxcmp&#x2F;s   txcmp&#x2F;s  rxmcst&#x2F;s</span><br><span class="line">21时43分38秒      eth0    116.00    118.00     11.86     19.80      0.00      0.00      0.00</span><br><span class="line">21时43分38秒        lo     33.00     33.00     68.67     68.67      0.00      0.00      0.00</span><br><span class="line"></span><br><span class="line">平均时间:     IFACE   rxpck&#x2F;s   txpck&#x2F;s    rxkB&#x2F;s    txkB&#x2F;s   rxcmp&#x2F;s   txcmp&#x2F;s  rxmcst&#x2F;s</span><br><span class="line">平均时间:      eth0    129.17    129.83     14.41     28.06      0.00      0.00      0.00</span><br><span class="line">平均时间:        lo     42.67     42.67    102.49    102.49      0.00      0.00      0.00</span><br></pre></td></tr></table></figure><p>将<code>rxkB/s</code>和<code>txkB/s</code>进行相加，得到网卡设备的实际吞吐率，然后再和网卡的硬件指标进行比对即可。</p><p>比如一个网卡的<code>rxkB/s</code>指标为21999.10，<code>txkB/s</code>指标为482.56，那这个网卡的吞吐率大概在<code>22Mbytes/s</code>，即<code>176 Mbits/sec</code>，没有达到<code>1Gbit/sec</code>的硬件上限。</p><hr><p>参考链接</p><ul><li><a href="https://www.cnblogs.com/linuxbug/p/4909980.html" target="_blank" rel="noopener">Linux性能监控 - CPU、Memory、IO、Network</a></li></ul>]]></content>
    
    <summary type="html">
    
      Linux 性能监控
    
    </summary>
    
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
  </entry>
  
  <entry>
    <title>缺页中断算法 - LRU</title>
    <link href="http://yoursite.com/2020/02/05/LRU/"/>
    <id>http://yoursite.com/2020/02/05/LRU/</id>
    <published>2020-02-04T16:00:00.000Z</published>
    <updated>2020-02-05T08:09:48.486Z</updated>
    
    <content type="html"><![CDATA[<p>最近最久未使用置换算法</p><hr><h4 id="缺页中断"><a href="#缺页中断" class="headerlink" title="缺页中断"></a>缺页中断</h4><p>缺页中断就是 CPU 要访问的页不在主存，需要操作系统将其调入主存后再进行访问</p><p>访问的页面不在内存时，会产生一次缺页中断，缺页中断是由于所要访问的页面不存在主内存时触发，属于由硬件所产生的一种特殊的中断，也称之为硬中断。</p><p>缺页本身是一种中断，与软中断一样，需要经过4个处理步骤</p><ol><li>保护CPU现场 </li><li>分析中断原因 </li><li>转入缺页中断处理程序进行处理 </li><li>恢复CPU现场，继续执行</li></ol><p>缺页中断更多可以阅读<a href="https://liam.page/2017/09/01/page-fault/" target="_blank" rel="noopener">程序员的自我修养（七）：内存缺页错误</a></p><h4 id="页面置换算法"><a href="#页面置换算法" class="headerlink" title="页面置换算法"></a>页面置换算法</h4><p>进程运行过程中，如果发生缺页中断，而此时内存中有没有空闲的物理块时，<br>为了能够把所缺的页面装入内存，系统必须从内存中选择一页调出到磁盘的对换区。<br>但此时应该把那个页面换出，则需要根据一定的页面置换算法（Page Replacement Algorithm)来确定。</p><p>页面置换算法有 OPT、FIFO、LRU 三种算法。OPT、FIFO 大家自行阅读<a href="https://blog.csdn.net/Youth_Mr6/article/details/82767332" target="_blank" rel="noopener">缺页中断算法(FIFO,LRU)</a></p><h4 id="LRU"><a href="#LRU" class="headerlink" title="LRU"></a>LRU</h4><p>最近最久未使用置换算法（Least Recently Used）</p><p>置换最近一段时间以来最长时间未访问过的页面。根据程序局部性原理，刚被访问的页面，可能马上又要被访问；而较长时间内没有被访问的页面，可能最近不会被访问。 </p><p>采用固定分配局部置换的策略，假定系统为某进程在内存中分配了3个物理页，页面访问顺序为2、3、2、1、5、2、4、5、3、2、5、2。假定系统未采用预调页策略，即未事先调入任何页面。</p><p><img src="/images/blog/2020-02-05-1.png" alt></p><p>中断次数为7，缺页中断率为7/12*100% = 58.3%</p><h4 id="数据结构实现实现-LRU"><a href="#数据结构实现实现-LRU" class="headerlink" title="数据结构实现实现 LRU"></a>数据结构实现实现 LRU</h4><p>哈希表 + 双向链表</p><p>用哈希表，辅以双向链表记录键值对的信息。所以可以在 <code>O(1)</code> 时间内完成 put 和 get 操作，同时也支持 <code>O(1)</code> 删除第一个添加的节点。</p><p><img src="/images/blog/2020-02-05-2.png" alt></p><p>使用双向链表的一个好处是不需要额外信息删除一个节点，同时可以在常数时间内从头部或尾部插入删除节点。</p><p>一个需要注意的是，在双向链表实现中，这里使用一个伪头部和伪尾部标记界限，这样在更新的时候就不需要检查是否是 null 节点。</p><p><img src="/images/blog/2020-02-05-3.png" alt></p><p>感兴趣可以到 LeetCode 做这道题——<a href="https://leetcode-cn.com/problems/lru-cache/solution/lru-huan-cun-ji-zhi-by-leetcode/" target="_blank" rel="noopener">146. LRU缓存机制</a></p><hr><p>参考链接</p><ul><li><a href="https://blog.csdn.net/Youth_Mr6/article/details/82767332" target="_blank" rel="noopener">缺页中断算法(FIFO,LRU)</a></li><li><a href="https://leetcode-cn.com/problems/lru-cache/solution/lru-huan-cun-ji-zhi-by-leetcode/" target="_blank" rel="noopener">146. LRU缓存机制</a></li></ul>]]></content>
    
    <summary type="html">
    
      最近最久未使用置换算法
    
    </summary>
    
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
  </entry>
  
  <entry>
    <title>Zookeeper 解决脑裂原理</title>
    <link href="http://yoursite.com/2020/02/04/ZK-Split-Brain/"/>
    <id>http://yoursite.com/2020/02/04/ZK-Split-Brain/</id>
    <published>2020-02-03T16:00:00.000Z</published>
    <updated>2020-02-08T08:39:38.997Z</updated>
    
    <content type="html"><![CDATA[<p>这是分布式系统中一个很实际的问题</p><hr><h4 id="转载来源"><a href="#转载来源" class="headerlink" title="转载来源"></a>转载来源</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">作者：1点25</span><br><span class="line">链接：https:&#x2F;&#x2F;juejin.im&#x2F;post&#x2F;5d36c2f25188257f6a209d37</span><br><span class="line">来源: 掘金</span><br></pre></td></tr></table></figure><h4 id="脑裂"><a href="#脑裂" class="headerlink" title="脑裂"></a>脑裂</h4><p>脑裂(split-brain)就是“大脑分裂”，也就是本来一个“大脑”被拆分了两个或多个“大脑”，我们都知道，如果一个人有多个大脑，并且相互独立的话，那么会导致人体“手舞足蹈”，“不听使唤”。<br>脑裂通常会出现在集群环境中，比如ElasticSearch、Zookeeper集群，而这些集群环境有一个统一的特点，就是它们有一个大脑，比如ElasticSearch集群中有Master节点，Zookeeper集群中有Leader节点。</p><h4 id="Zookeeper-集群中的脑裂场景"><a href="#Zookeeper-集群中的脑裂场景" class="headerlink" title="Zookeeper 集群中的脑裂场景"></a>Zookeeper 集群中的脑裂场景</h4><p>对于一个集群，想要提高这个集群的可用性，通常会采用多机房部署，比如现在有一个由6台zkServer所组成的一个集群，部署在了两个机房</p><p><img src="/images/blog/2020-02-04-1.png" alt></p><p>正常情况下，此集群只会有一个Leader，那么如果机房之间的网络断了之后，两个机房内的zkServer还是可以相互通信的，如果<strong>不考虑过半机制</strong>，那么就会出现每个机房内部都将选出一个Leader。</p><p><img src="/images/blog/2020-02-04-2.png" alt></p><p>这就相当于原本一个集群，被分成了两个集群，出现了两个“大脑”，这就是脑裂。<br>对于这种情况，我们也可以看出来，原本应该是统一的一个集群对外提供服务的，现在变成了两个集群同时对外提供服务，如果过了一会，断了的网络突然联通了，那么此时就会出现问题了，两个集群刚刚都对外提供服务了，数据该怎么合并，数据冲突怎么解决等等问题。<br>刚刚在说明脑裂场景时，有一个前提条件就是没有考虑过半机制，所以实际上Zookeeper集群中是不会出现脑裂问题的，而不会出现的原因就跟过半机制有关。</p><h4 id="过半机制"><a href="#过半机制" class="headerlink" title="过半机制"></a>过半机制</h4><p>在领导者选举的过程中，如果某台zkServer获得了超过半数的选票，则此zkServer就可以成为Leader了。</p><p>过半机制的源码实现其实非常简单：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">public class QuorumMaj implements QuorumVerifier &#123;</span><br><span class="line">    private static final Logger LOG &#x3D; LoggerFactory.getLogger(QuorumMaj.class);</span><br><span class="line">    </span><br><span class="line">    int half;</span><br><span class="line">    </span><br><span class="line">    &#x2F;&#x2F; n表示集群中zkServer的个数（准确的说是参与者的个数，参与者不包括观察者节点）</span><br><span class="line">    public QuorumMaj(int n)&#123;</span><br><span class="line">        this.half &#x3D; n&#x2F;2;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; 验证是否符合过半机制</span><br><span class="line">    public boolean containsQuorum(Set&lt;Long&gt; set)&#123;</span><br><span class="line">        &#x2F;&#x2F; half是在构造方法里赋值的</span><br><span class="line">        &#x2F;&#x2F; set.size()表示某台zkServer获得的票数</span><br><span class="line">        return (set.size() &gt; half);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>大家仔细看一下上面方法中的注释，核心代码就是下面两行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">this.half &#x3D; n&#x2F;2;</span><br><span class="line">return (set.size() &gt; half);</span><br></pre></td></tr></table></figure><p>举个简单的例子： 如果现在集群中有5台zkServer，那么half=5/2=2，那么也就是说，领导者选举的过程中至少要有三台zkServer投了同一个zkServer，才会符合过半机制，才能选出来一个Leader。</p><p>那么有一个问题我们想一下，<strong>选举的过程中为什么一定要有一个过半机制验证？</strong>因为这样不需要等待所有zkServer都投了同一个zkServer就可以选举出来一个Leader了，这样比较快，所以叫快速领导者选举算法呗。</p><p>那么再来想一个问题，<strong>过半机制中为什么是大于，而不是大于等于呢？</strong></p><p>这就是跟脑裂问题有关系了，比如回到上文出现脑裂问题的场景：</p><p><img src="/images/blog/2020-02-04-3.png" alt></p><p>当机房中间的网络断掉之后，机房1内的三台服务器会进行领导者选举，但是此时过半机制的条件是set.size() &gt; 3，也就是说至少要4台zkServer才能选出来一个Leader，所以对于机房1来说它不能选出一个Leader，同样机房2也不能选出一个Leader，这种情况下整个集群当机房间的网络断掉后，整个集群将没有Leader。</p><p>而如果过半机制的条件是set.size() &gt;= 3，那么机房1和机房2都会选出一个Leader，这样就出现了脑裂。所以我们就知道了，为什么过半机制中是大于，而不是大于等于。就是为了防止脑裂。</p><p>如果假设我们现在只有5台机器，也部署在两个机房</p><p><img src="/images/blog/2020-02-04-4.png" alt></p><p>此时过半机制的条件是set.size() &gt; 2，也就是至少要3台服务器才能选出一个Leader，此时机房件的网络断开了，对于机房1来说是没有影响的，Leader依然还是Leader，对于机房2来说是选不出来Leader的，此时整个集群中只有一个Leader。</p><p>所以，我们可以总结得出，有了过半机制，对于一个Zookeeper集群，要么没有Leader，要没只有1个Leader，这样就避免了脑裂问题。</p><blockquote><p>Q: 在阅读时我就想如果机房1有1个leader和1个follower,机房2有3个follower, 这种情况呢？<br>A: 每当新leader产生时，会生成一个epoch，这个epoch是递增的，followers如果确认了新的leader存在，知道其epoch，就会拒绝epoch小于现任leader epoch的所有请求。所以旧leader无法被follower认可。</p></blockquote><h4 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">作者：三胖桑</span><br><span class="line">链接：https:&#x2F;&#x2F;blog.csdn.net&#x2F;u013374645&#x2F;article&#x2F;details&#x2F;93140148</span><br><span class="line">标题: 面试题-Zookeeper是如何解决脑裂问题</span><br></pre></td></tr></table></figure><p>解决 Split-Brain 的问题，一般有3种方式</p><ul><li>Quorums（法定人数）: 比如3个节点的集群，Quorums = 2, 也就是说集群可以容忍1个节点失效，这时候还能选举出1个lead，集群还可用。比如4个节点的集群，它的Quorums = 3，Quorums要超过3，相当于集群的容忍度还是1，如果2个节点失效，那么整个集群还是无效的</li><li>Redundant communications: 冗余通信的方式，集群中采用多种通信方式，防止一种通信方式失效导致集群中的节点无法通信。</li><li>Fencing: 共享资源的方式，比如能看到共享资源就表示在集群中，能够获得共享资源的锁的就是Leader，看不到共享资源的，就不在集群中。HDFS NameNode 使用的就是这种方式</li></ul><p>ZooKeeper默认采用了Quorums这种方式，即只有集群中超过半数节点投票才能选举出Leader。这样的方式可以确保leader的唯一性,要么选出唯一的一个leader,要么选举失败。</p><p>在ZooKeeper中Quorums有2个作用:</p><ul><li>集群中最少的节点数用来选举Leader保证集群可用：通知客户端数据已经安全保存，前集群中最少数量的节点数已经保存了该数据。一旦这些节点保存了该数据，客户端将被通知已经安全保存了，可以继续其他任务。而集群中剩余的节点将会最终也保存了该数据。</li><li>假设某个leader假死，其余的followers选举出了一个新的leader。这时，旧的leader复活并且仍然认为自己是leader，这个时候它向其他followers发出写请求也是会被拒绝的。因为每当新leader产生时，会生成一个epoch，这个epoch是递增的，followers如果确认了新的leader存在，知道其epoch，就会拒绝epoch小于现任leader epoch的所有请求。那有没有follower不知道新的leader存在呢，有可能，但肯定不是大多数，否则新leader无法产生。Zookeeper的写也遵循quorum机制，因此，得不到大多数支持的写是无效的，旧leader即使各种认为自己是leader，依然没有什么作用。</li></ul>]]></content>
    
    <summary type="html">
    
      脑裂是什么？Zookeeper是如何解决的？
    
    </summary>
    
    
      <category term="Zookeeper" scheme="http://yoursite.com/categories/Zookeeper/"/>
    
    
  </entry>
  
  <entry>
    <title>CDH 集群中 DN 热换盘处理</title>
    <link href="http://yoursite.com/2020/02/03/DN-Fault-Raid/"/>
    <id>http://yoursite.com/2020/02/03/DN-Fault-Raid/</id>
    <published>2020-02-02T16:00:00.000Z</published>
    <updated>2020-02-06T03:30:13.320Z</updated>
    
    <content type="html"><![CDATA[<p>在集群使用的过程中会遇到数据节点的磁盘故障，在不停数据节点的情况下，如何为数据节点进行热插拔换盘操作</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>HDFS 随着使用时间会出现数据借点磁盘故障现象，网上查阅到的大部分都是停止数据节点，换盘后重启。我司处理的方法是热插拔处理。</p><h4 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h4><p>具体参阅 Fayson 的<a href="https://cloud.tencent.com/developer/article/1158329" target="_blank" rel="noopener">如何在CDH集群中为数据节点热插拔硬盘</a></p><p>Fayson 自己也说操作步骤更类似于加盘操作，磁盘坏掉如果磁盘的盘符未变更则只需要将磁盘格式化挂载在原来的目录下。</p><p>变通方式是先将其取消坏盘挂载，更换新盘后格式化再挂载回来，DN 执行刷新数据目录操作即可。</p><p>这样会出现节点内磁盘不均衡的现象，需要借助节点内平衡 DiskBalancer，更多参考 <a href="https://issues.apache.org/jira/browse/HDFS-1312" target="_blank" rel="noopener">HDFS-1312</a></p><hr><p>参考链接</p><ul><li><a href="https://cloud.tencent.com/developer/article/1158329" target="_blank" rel="noopener">如何在CDH集群中为数据节点热插拔硬盘</a></li><li><a href="https://ilinuxkernel.com/?p=958" target="_blank" rel="noopener">Linux硬盘盘符分配</a></li></ul>]]></content>
    
    <summary type="html">
    
      如何在CDH集群中为数据节点热插拔硬盘
    
    </summary>
    
    
      <category term="CDH" scheme="http://yoursite.com/categories/CDH/"/>
    
    
  </entry>
  
  <entry>
    <title>内存缺页错误</title>
    <link href="http://yoursite.com/2020/02/01/Page-Fault/"/>
    <id>http://yoursite.com/2020/02/01/Page-Fault/</id>
    <published>2020-01-31T16:00:00.000Z</published>
    <updated>2020-02-07T09:19:49.523Z</updated>
    
    <content type="html"><![CDATA[<p>缺页中断就是要访问的页不在主存，需要操作系统将其调入主存后再进行访问。</p><hr><h4 id="转载来源"><a href="#转载来源" class="headerlink" title="转载来源"></a>转载来源</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">作者：Liam Huang</span><br><span class="line">链接：https:&#x2F;&#x2F;liam.page&#x2F;2017&#x2F;09&#x2F;01&#x2F;page-fault&#x2F;</span><br></pre></td></tr></table></figure><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>众所周知，CPU 不能直接和硬盘进行交互。CPU 所作的一切运算，都是通过 CPU 缓存间接与内存进行操作的。若是 CPU 请求的内存数据在物理内存中不存在，那么 CPU 就会报告「缺页错误（Page Fault）」，提示内核产生中断，将所缺的页面装入内存。</p><p>在内核处理缺页错误时，就有可能进行磁盘的读写操作。这样的操作，相对 CPU 的处理是非常缓慢的。因此，发生大量的缺页错误，势必会对程序的性能造成很大影响。因此，在对性能要求很高的环境下，应当尽可能避免这种情况。</p><p>此篇介绍缺页错误本身，并结合一个实际示例作出一些实践分析。这里主要在 Linux 的场景下做讨论；其他现代操作系统，基本也是类似的。</p><h4 id="内存页和缺页错误"><a href="#内存页和缺页错误" class="headerlink" title="内存页和缺页错误"></a>内存页和缺页错误</h4><p>现代 CPU 都支持分段和分页的内存寻址模式。在 Linux 当中，实际起作用的只有分页模式。</p><p>具体来说，分页模式在逻辑上将虚拟内存和物理内存同时等分成固定大小的块。这些块在虚拟内存上称之为「页」，而在物理内存上称之为「页帧」，并交由 CPU 中的 MMU 模块来负责页帧和页之间的映射管理。</p><p>引入分页模式的好处，可以大致概括为两个方面: </p><ul><li>允许虚存空间远大于实际物理内存大小的情况。这是因为，分页之后，操作系统读入磁盘的文件时，无需以文件为单位全部读入，而可以以内存页为单位，分片读入。同时，考虑到 CPU 不可能一次性需要使用整个内存中的数据，因此可以交由特定的算法，进行内存调度: 将长时间不用的页帧内的数据暂存到磁盘上。</li><li>减少了内存碎片的产生。这是因为，引入分页之后，内存的分配管理都是以页大小（通常是 4KiB，扩展分页模式下是 4MiB）为单位的；虚拟内存中的页总是对应物理内存中实际的页帧。这样一来，在虚拟内存空间中，页内连续的内存在物理内存上也一定是连续的，不会产生碎片。</li></ul><h4 id="缺页错误"><a href="#缺页错误" class="headerlink" title="缺页错误"></a>缺页错误</h4><p>当进程在进行一些计算时，CPU 会请求内存中存储的数据。在这个请求过程中，CPU 发出的地址是逻辑地址（虚拟地址），然后交由 CPU 当中的 MMU 单元进行内存寻址，找到实际物理内存上的内容。若是目标虚存空间中的内存页（因为某种原因），在物理内存中没有对应的页帧，那么 CPU 就无法获取数据。这种情况下，CPU 是无法进行计算的，于是它就会报告一个缺页错误（Page Fault）。</p><p>因为 CPU 无法继续进行进程请求的计算，并报告了缺页错误，用户进程必然就中断了。这样的中断称之为缺页中断，因为由 CPU 以外的硬件产生的中断，也称之为硬中断。在报告 Page Fault 之后，进程会从用户态切换到系统态，交由操作系统内核的 Page Fault Handler 处理缺页错误。</p><h4 id="分类和处理"><a href="#分类和处理" class="headerlink" title="分类和处理"></a>分类和处理</h4><p>基本来说，缺页错误可以分为两类: 硬缺页错误（Hard Page Fault）和软缺页错误（Soft Page Fault）。这里，前者又称为主要缺页错误（Major Page Fault）；后者又称为次要缺页错误（Minor Page Fault）。当缺页中断发生后，Page Fault Handler 会判断缺页的类型，进而处理缺页错误，最终将控制权交给用户态代码。</p><p>若是此时物理内存里，已经有一个页帧正是此时 CPU 请求的内存页，那么这是一个软缺页错误；于是，Page Fault Hander 会指示 MMU 建立相应的页帧到页的映射关系。这一操作的实质是进程间共享内存——比如动态库（共享对象），比如 mmap 的文件。</p><p>若是此时物理内存中，没有相应的页帧，那么这就是一个硬缺页错误；于是 Page Fault Hander 会指示 CPU，从已经打开的磁盘文件中读取相应的内容到物理内存，而后交由 MMU 建立这份页帧到页的映射关系。</p><p>不难发现，软缺页错误只是在内核态里轻轻地走了一遭，而硬缺页错误则涉及到磁盘 I/O。因此，处理起来，硬缺页错误要比软缺页错误耗时长得多。这就是为什么我们要求高性能程序必须在对外提供服务时，尽可能少地发生硬缺页错误。</p><blockquote><p>除了硬缺页错误和软缺页错误之外，还有一类缺页错误是因为访问非法内存引起的。前两类缺页错误中，进程尝试访问的虚存地址尚为合法有效的地址，只是对应的物理内存页帧没有在物理内存当中。后者则不然，进程尝试访问的虚存地址是非法无效的地址。比如尝试对 nullptr 解引用，就会访问地址为 0x0 的虚存地址，这是非法地址。此时 CPU 报出无效缺页错误（Invalid Page Fault）。操作系统对无效缺页错误的处理各不相同：Windows 会使用异常机制向进程报告；*nix 则会通过向进程发送 SIGSEGV 信号（11），引发<a href="https://liam.page/2017/05/27/tutorial-to-GDB-taking-ncurses-as-an-example/" target="_blank" rel="noopener">内存转储</a>。</p></blockquote><h4 id="缺页错误的原因"><a href="#缺页错误的原因" class="headerlink" title="缺页错误的原因"></a>缺页错误的原因</h4><p>之前提到，物理内存中没有 CPU 所需的页帧，就会引发缺页错误。这一现象背后的原因可能有很多。</p><p>例如说，进程通过 mmap 系统调用，直接建立了磁盘文件和虚拟内存的映射关系。然而，在 mmap 调用之后，并不会立即从磁盘上读取这一文件。而是在实际需要文件内容时，通过 CPU 触发缺页错误，要求 Page Fault Handler 去将文件内容读入内存。</p><p>又例如说，一个进程启动了很久，但是长时间没有活动。若是计算机处在很高的内存压力下，则操作系统会将这一进程长期未使用的页帧内容，从物理内存转储到磁盘上。这个过程称为换出（swap out）。在 *nix 系统下，用于转储这部分内存内容的磁盘空间，称为交换空间；在 Windows 上，这部分磁盘空间，则被称为虚拟内存，对应磁盘上的文件则称为页面文件。在这个过程中，进程在内存中保存的任意内容，都可能被换出到交换空间：可以是数据内容，也可以是进程的代码段内容。</p><p>Windows 用户看到这里，应该能明白这部分空间为什么叫做「虚拟内存」——因为它于真实的内存条相对，是在硬盘上虚拟出来的一份内存。通过这样的方式，「好像」将内存的容量扩大了。同样，为什么叫「页面文件」也一目了然。因为事实上，文件内保存的就是一个个内存页帧。在 Windows 上经常能观察到「假死」的现象，就和缺页错误有关。这种现象，实际就是长期不运行某个程序，导致程序对应的内存被换出到磁盘；在需要响应时，由于需要从磁盘上读取大量内容，导致响应很慢，产生假死现象。这种现象发生时，若是监控系统硬错误数量，就会发现在短时间内，目标进程产生了大量的硬错误。</p><p>在 Windows XP 流行的年代，有很多来路不明的「系统优化建议」。其中一条就是「扩大页面文件的大小，有助于加快系统速度」。事实上，这种方式只能加大内存「看起来」的容量，却给内存整体（将物理内存和磁盘页面文件看做一个整体）的响应速度带来了巨大的负面影响。因为，尽管容量增大了，但是访问这部分增大的容量时，进程实际上需要先陷入内核态，从磁盘上读取内容做好映射，再继续执行。更有甚者，这些建议会要求「将页面文件分散在多个不同磁盘分区」，并美其名曰「分散压力」。事实上，从页面文件中读取内存页帧本就已经很慢；若是还要求磁盘不断在不同分区上寻址，那就更慢了。可见谣言害死人。</p><h4 id="观察缺页错误"><a href="#观察缺页错误" class="headerlink" title="观察缺页错误"></a>观察缺页错误</h4><p>通过<code>top</code> 命令查看观察软中断和硬中断的 CPU 使用率，hi(hard interrupt) 表示硬中断、si(soft interrupt) 表示软中断</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%Cpu(s): 53.3 us,  1.2 sy,  0.0 ni, 44.6 id,  0.9 wa,  0.0 hi,  0.1 si,  0.0 st</span><br></pre></td></tr></table></figure><p>通过<code>mpstat -P ALL 2</code> 每隔两秒查看下所有核状态信息，其中<code>%irq</code>为硬中断，<code>%soft</code>为软中断，如果硬中断比较高可以看看是不是大量读取磁盘引起的(<code>iostat -d 1</code>每隔1s 查看磁盘读写速度)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">$ mpstat -P ALL 2</span><br><span class="line">Linux 3.10.0-862.14.4.el7.x86_64 (region-0-0-122) 2020年02月07日 _x86_64_(4 CPU)</span><br><span class="line"></span><br><span class="line">16时23分31秒  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle</span><br><span class="line">16时23分33秒  all   52.64    0.00    0.88    0.38    0.00    0.00    0.00    0.00    0.00   46.10</span><br><span class="line">16时23分33秒    0  100.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00</span><br><span class="line">16时23分33秒    1    4.06    0.00    1.52    1.52    0.00    0.00    0.00    0.00    0.00   92.89</span><br><span class="line">16时23分33秒    2   99.50    0.00    0.50    0.00    0.00    0.00    0.00    0.00    0.00    0.00</span><br><span class="line">16时23分33秒    3    5.08    0.00    1.52    0.00    0.00    0.00    0.00    0.00    0.00   93.40</span><br><span class="line"></span><br><span class="line">16时23分33秒  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle</span><br><span class="line">16时23分35秒  all   52.41    0.00    1.01    0.13    0.00    0.13    0.00    0.00    0.00   46.33</span><br><span class="line">16时23分35秒    0  100.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00</span><br><span class="line">16时23分35秒    1    4.57    0.00    2.54    0.51    0.00    0.00    0.00    0.00    0.00   92.39</span><br><span class="line">16时23分35秒    2  100.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00</span><br><span class="line">16时23分35秒    3    3.55    0.00    2.03    0.51    0.00    0.51    0.00    0.00    0.00   93.40</span><br><span class="line"></span><br><span class="line">16时23分35秒  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle</span><br><span class="line">16时23分37秒  all   54.74    0.00    1.52    0.13    0.00    0.25    0.00    0.00    0.00   43.36</span><br><span class="line">16时23分37秒    0   99.50    0.00    0.50    0.00    0.00    0.00    0.00    0.00    0.00    0.00</span><br><span class="line">16时23分37秒    1    9.28    0.00    2.06    0.00    0.00    0.52    0.00    0.00    0.00   88.14</span><br><span class="line">16时23分37秒    2  100.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00</span><br><span class="line">16时23分37秒    3    8.16    0.00    3.57    0.00    0.00    0.51    0.00    0.00    0.00   87.76</span><br></pre></td></tr></table></figure><p><code>dstat</code>可以查看 CPU 软硬中断次数，hiq、siq分别为硬中断和软中断次数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ dstat</span><br><span class="line">You did not select any stats, using -cdngy by default.</span><br><span class="line">----total-cpu-usage---- -dsk&#x2F;total- -net&#x2F;total- ---paging-- ---system--</span><br><span class="line">usr sys idl wai hiq siq| read  writ| recv  send|  in   out | int   csw</span><br><span class="line"> 62   1  36   0   0   0|  84k  355k|   0     0 |   0     0 |5766  4315</span><br><span class="line"> 52   1  47   0   0   0|   0   232k|  21k   54k|   0     0 |5279  4004</span><br><span class="line"> 52   1  47   0   0   0|   0    56k|  15k   22k|   0     0 |5327  4045</span><br></pre></td></tr></table></figure><p>ps 是一个强大的命令，我们可以用 -o 选项指定希望关注的项目。比如</p><ul><li>min_flt: 进程启动至今软缺页中断数量</li><li>maj_flt: 进程启动至今硬缺页中断数量</li><li>cmd: 执行的命令</li><li>args: 执行的命令的参数（从 $0$ 开始）</li><li>uid: 执行命令的用户的 ID</li><li>gid: 执行命令的用户所在组的 ID</li></ul><p>因此，我们可以用 <code>ps -o min_flt,maj_flt,cmd,args,uid,gid 14434</code> 来观察进程号为 14434 的进程的缺页错误</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ps -o min_flt,maj_flt,cmd,args,uid,gid 14434</span><br><span class="line">   MINFL  MAJFL CMD                         COMMAND                       UID   GID</span><br><span class="line">  2413550   355 &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_181-clou &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_181-clou   981   978</span><br></pre></td></tr></table></figure><p>结合 <code>watch</code> 命令，则可关注进程当前出发缺页中断的状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">watch -n 1 --difference &quot;ps -o min_flt,maj_flt,cmd,args,uid,gid 14434&quot;</span><br></pre></td></tr></table></figure><p>你还可以结合 <code>sort</code> 命令，动态观察产生缺页错误最多的几个进程。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ watch -n 1 &quot;ps -eo min_flt,maj_flt,cmd,args,uid,gid | sort -nrk1 | head -n 8&quot;</span><br><span class="line">Every 1.0s: ps -eo min_flt,maj_flt,cmd,args,uid,gid | sort -nrk1 | head -n 8                                                                   Fri Feb  7 17:18:43 2020</span><br><span class="line"></span><br><span class="line">437395316 262 &#x2F;usr&#x2F;bin&#x2F;python2 &#x2F;opt&#x2F;cloud &#x2F;usr&#x2F;bin&#x2F;python2 &#x2F;opt&#x2F;cloud     0     0</span><br><span class="line">69510242   75 &#x2F;bin&#x2F;bash &#x2F;opt&#x2F;cloudera&#x2F;cm- &#x2F;bin&#x2F;bash &#x2F;opt&#x2F;cloudera&#x2F;cm-   981   978</span><br><span class="line">23151349 1164 &#x2F;usr&#x2F;local&#x2F;aegis&#x2F;aegis_clie &#x2F;usr&#x2F;local&#x2F;aegis&#x2F;aegis_clie     0     0</span><br><span class="line">5772594  1197 &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;systemd-jo &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;systemd-jo     0     0</span><br><span class="line">2902146    46 &#x2F;usr&#x2F;sbin&#x2F;irqbalance --fore &#x2F;usr&#x2F;sbin&#x2F;irqbalance --fore     0     0</span><br><span class="line">2413598   355 &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_181-clou &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_181-clou   981   978</span><br><span class="line">1591327   619 &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_181-clou &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_181-clou   997   995</span><br><span class="line">1588418   197 &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;systemd -- &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;systemd --     0     0</span><br></pre></td></tr></table></figure><h4 id="一个硬缺页错误导致的问题"><a href="#一个硬缺页错误导致的问题" class="headerlink" title="一个硬缺页错误导致的问题"></a>一个硬缺页错误导致的问题</h4><p>我司的某一高性能服务采取了 mmap 的方式，从磁盘加载大量数据。由于调研测试需要，多名组内成员共享一台调研机器。现在的问题是，当共享的人数较多时，新启动的服务进程会在启动时耗费大量时间——以几十分钟计。那么，这是为什么呢？</p><blockquote><p>因为涉及到公司机密，这里不方便给截图。留待以后，做模拟实验后给出。</p></blockquote><p>以 top 命令观察，机器卡顿时，CPU 负载并不高：32 核只有 1.3 左右的 1min 平均负载。但是，iostat 观察到，磁盘正在以 10MiB/s 级别的速度，不断进行读取。由此判断，这种情况下，目标进程一定有大量的 Page Fault 产生。使用上述 <code>watch -n 1 --difference &quot;ps -o min_flt,maj_flt,cmd,args,uid,gid &lt;pid&gt;&quot;</code> 观察，发现目标进程确实有大量硬缺页错误产生，肯定了这一推断。</p><p>然而，诚然进程需要载入大量数据，但是以 mmap 的方式映射，为何会已有大量同类服务存在的情况下，大量读取硬盘呢？这就需要更加深入的分析了。</p><p>事实上，这里隐含了一个非常细小的矛盾。一方面，该服务需要从磁盘加载大量数据；另一方面，该服务对性能要求非常高。我们知道，mmap 只是对文件做了映射，不会在调用 mmap 时立即将文件内容加载进内存。这就导致了一个问题：当服务启动对外提供服务时，可能还有数据未能加载进内存；而这种加载是非常慢的，严重影响服务性能。因此，可以推断，为了解决这个问题，程序必然在 mmap 之后，尝试将所有数据加载进物理内存。</p><p>这样一来，先前遇到的现象就很容易解释了。</p><ul><li>一方面，因为公用机器的人很多，必然造成内存压力大，从而存在大量换出的内存；</li><li>另一方面，新启动的进程，会逐帧地扫描文件；</li><li>这样一来，新启动的进程，就必须在极大的内存压力下，不断逼迫系统将其它进程的内存换出，而后换入自己需要的内存，不断进行磁盘 I/O；</li><li>故此，新启动的进程会耗费大量时间进行不必要的磁盘 I/O。</li></ul>]]></content>
    
    <summary type="html">
    
      缺页错误（Page Fault）
    
    </summary>
    
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
  </entry>
  
  <entry>
    <title>HDFS 处理小文件方法</title>
    <link href="http://yoursite.com/2020/01/28/HDFS-small-file/"/>
    <id>http://yoursite.com/2020/01/28/HDFS-small-file/</id>
    <published>2020-01-27T16:00:00.000Z</published>
    <updated>2020-02-03T10:02:20.443Z</updated>
    
    <content type="html"><![CDATA[<p>Hadoop 中处理小文件</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>HDFS 随着使用时间会存在小文件现象，这里将网上调研的处理方法收集一下</p><h4 id><a href="#" class="headerlink" title></a></h4><p>什么是小文件？小文件是怎么来的？可以参阅<a href="https://mp.weixin.qq.com/s/QdaNFfq37bBlAQtNDGLmVw" target="_blank" rel="noopener">如何在Hadoop中处理小文件</a></p><p>利用 HAR 方法处理小文件可以参阅<a href="https://mp.weixin.qq.com/s/rlM3ZPmWz-ZT72w59lvrRw" target="_blank" rel="noopener">0508-如何使用Hadoop的Archive处理小文件</a></p><p>利用 Hive 方法处理小文件可以参阅<a href="https://mp.weixin.qq.com/s/xw6MjNvpI97m0aygyW1HaA" target="_blank" rel="noopener">如何在Hadoop中处理小文件-续</a>、<a href="https://mp.weixin.qq.com/s/ki2k9fBPco6PwMZEzff60A" target="_blank" rel="noopener">0704-5.16.2-如何使用Hive合并小文件</a></p><p>利用 Impala 方法处理小文件可以参阅<a href="https://mp.weixin.qq.com/s/BTBqUqfbczq08LKRCuC_Ig" target="_blank" rel="noopener">如何使用Impala合并小文件</a></p><p>在有赞使用 SparkSQL 中提到存在小文件现象，采用社区 <a href="https://issues.apache.org/jira/browse/SPARK-24940" target="_blank" rel="noopener">SPARK-24940</a>方式处理，借助 SQL hint 的方式合并小文件。参阅<a href="https://www.iteblog.com/archives/2501.html" target="_blank" rel="noopener">Spark SQL 查询中 Coalesce 和 Repartition 暗示（Hint）</a></p><p>利用 Spark 处理小文件跟 Spark SQL 也是差不多的，通过 Spark 的 coalesce() 方法和 repartition() 方法，设置并行个数可以通过输入文件的总大小和期望输出文件的大小进行预计算而得。</p><hr><p>参考链接</p><ul><li><a href="http://bigdatadecode.club/HDFS-little-file-action.html" target="_blank" rel="noopener">HDFS小文件合并实战</a></li><li><a href="https://juejin.im/post/5c3f2713f265da61285a5b75#heading-8" target="_blank" rel="noopener">SparkSQL 在有赞的实践</a></li><li><a href="https://blog.csdn.net/weixin_37944880/article/details/86694829" target="_blank" rel="noopener">Spark小文件合并</a></li></ul>]]></content>
    
    <summary type="html">
    
      Hadoop 中处理小文件
    
    </summary>
    
    
      <category term="HDFS" scheme="http://yoursite.com/categories/HDFS/"/>
    
    
  </entry>
  
  <entry>
    <title>Hive set 命令使用</title>
    <link href="http://yoursite.com/2020/01/27/Hive-set/"/>
    <id>http://yoursite.com/2020/01/27/Hive-set/</id>
    <published>2020-01-26T16:00:00.000Z</published>
    <updated>2020-02-03T10:02:20.441Z</updated>
    
    <content type="html"><![CDATA[<p>Hive set 命令使用</p><hr><h4 id="转载来源"><a href="#转载来源" class="headerlink" title="转载来源"></a>转载来源</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">作者：lwf006164</span><br><span class="line">链接：https:&#x2F;&#x2F;blog.csdn.net&#x2F;lwf006164&#x2F;article&#x2F;details&#x2F;96754526</span><br><span class="line">来源：Hive篇.set命令使用</span><br></pre></td></tr></table></figure><h4 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h4><p>Hive 命令行下执行 set 命令，仅当前会话有效</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 设置显示当前使用的数据库</span><br><span class="line">set hive.cli.print.current.db&#x3D;true;</span><br><span class="line"></span><br><span class="line">#  设置显示表名</span><br><span class="line">set hive.cli.print.header&#x3D;true;</span><br></pre></td></tr></table></figure><p>Hive 脚本中配置 set 命令，当前机器用户有效   </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 进入编辑模式，在执行 hive 命令进入 shell 命令行的时候默认会加载这个脚本</span><br><span class="line">vi ~&#x2F;.hiverc</span><br><span class="line"></span><br><span class="line"># 将下面的两行命令添加到该文件中</span><br><span class="line"># 设置显示当前使用的数据库</span><br><span class="line">set hive.cli.print.current.db&#x3D;true;</span><br><span class="line"></span><br><span class="line"># 设置显示表名</span><br><span class="line">set hive.cli.print.header&#x3D;true;</span><br></pre></td></tr></table></figure><p>查看 Hive 历史操作命令集可以浏览 <code>~/.hivehistory</code></p>]]></content>
    
    <summary type="html">
    
      Hive set 命令使用
    
    </summary>
    
    
      <category term="Hive" scheme="http://yoursite.com/categories/Hive/"/>
    
    
  </entry>
  
  <entry>
    <title>HDFS 中 atime 与 mtime 解析</title>
    <link href="http://yoursite.com/2020/01/27/HDFS-atime-ctime/"/>
    <id>http://yoursite.com/2020/01/27/HDFS-atime-ctime/</id>
    <published>2020-01-26T16:00:00.000Z</published>
    <updated>2020-02-06T07:55:22.814Z</updated>
    
    <content type="html"><![CDATA[<p>HDFS 中 atime 和 mtime</p><hr><h4 id="转载来源"><a href="#转载来源" class="headerlink" title="转载来源"></a>转载来源</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">作者：混绅士</span><br><span class="line">链接：http:&#x2F;&#x2F;bigdatadecode.club&#x2F;HDFS%E4%B8%ADatime%E5%92%8Cmtime.html</span><br><span class="line">来源：HDFS中atime与mtime解析</span><br></pre></td></tr></table></figure><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>如果不知道 atime、mtime 的建议先了解下 Linux 中 atime 和 mtime，这样有助于学习 HDFS 中 atime 与 mtime</p><h4 id="查看"><a href="#查看" class="headerlink" title="查看"></a>查看</h4><p>转载文章中未介绍如何查看 HDFS atime、mtime，因此说明下。</p><p>跟 Linux 一样，也是使用 stat 命令来查看</p><p>用法: <code>hadoop fs -stat [format] &lt;path&gt; ...</code></p><blockquote><p>以指定的格式打印有关 <path></path> 处的文件/目录的统计信息。格式接受八进制（％a）和符号（％A），字节（％b）文件大小，类型（％F），所有者组名称（％g），名称（％n），块大小（％o），复制（％r），<br>所有者用户名（％u），访问日期（％x，％X）和修改日期（％y，％Y）。％x和％y将UTC日期显示为“yyyy-MM-dd HH:mm:ss”，并且％X和％Y显示自1970年1月1日UTC以来的毫秒数。如果未指定格式，则默认使用％y。 —— <a href="https://www.jianshu.com/p/49d89c197310" target="_blank" rel="noopener">https://www.jianshu.com/p/49d89c197310</a></p></blockquote><p>如查看 HBase 根目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -stat &quot;type:%F perm:%a %u:%g size:%b mtime:%y atime:%x name:%n&quot; &#x2F;hbase</span><br><span class="line">type:directory perm:755 hbase:hbase size:0 mtime:2020-01-14 12:54:42 atime:1970-01-01 00:00:00 name:hbase</span><br></pre></td></tr></table></figure><h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><p>先来了解下 HDFS 中的这 atime 与 mtime 的变化规则</p><p>在看代码之前，先想下 atime 和 mtime 有可能在哪些地方会修改</p><p>hdfs底层api对文件都有哪些操作？</p><p>无非就是读写两种操作，读肯定修改的是atime，写修改的是mtime，是否修改atime还得确认。<br>这里我们还漏掉一种操作，那就是mv。</p><h5 id="atime"><a href="#atime" class="headerlink" title="atime"></a>atime</h5><p>去年写过一篇文章 <a href="http://bigdatadecode.club/HDFS%20read%E8%A7%A3%E6%9E%90.html" target="_blank" rel="noopener">HDFS read解析(一)之Open文件流</a>介绍HDFS读操作流程，这里就不再累赘了，直接贴出关键代码。(有兴趣的同学可以自行查看)</p><blockquote><p>这里还是简单说下读的流程：客户端向NN发起一个读请求，NN将相关的block信息返回给客户端，客户端再与对应的DN建立连接读取信息。<br>  在这个过程中，先与NN交互然后再与DN交互，那么每个文件的atime相关元数据信息都存在NN中，那么atime相关的修改也肯定发生在与NN交互的这个过程中。</p></blockquote><p>从之前的文章中可知入口函数是 <code>FSNamesystem.getBlockLocations</code>，关键代码如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">LocatedBlocks getBlockLocations(String clientMachine, String srcArg,</span><br><span class="line">    long offset, long length) throws IOException &#123;</span><br><span class="line">...</span><br><span class="line">  if (res.updateAccessTime()) &#123;</span><br><span class="line">    String src &#x3D; srcArg;</span><br><span class="line">    writeLock();</span><br><span class="line">    final long now &#x3D; now();</span><br><span class="line">    try &#123;</span><br><span class="line">      final INodesInPath iip &#x3D; dir.resolvePath(pc, src);</span><br><span class="line">      src &#x3D; iip.getPath();</span><br><span class="line">      INode inode &#x3D; iip.getLastINode();</span><br><span class="line">      &#x2F;&#x2F; 再次判断是否可以更新atime</span><br><span class="line">      boolean updateAccessTime &#x3D; inode !&#x3D; null &amp;&amp;</span><br><span class="line">          now &gt; inode.getAccessTime() + getAccessTimePrecision();</span><br><span class="line">      if (!isInSafeMode() &amp;&amp; updateAccessTime) &#123;</span><br><span class="line">        &#x2F;&#x2F; 设置atime</span><br><span class="line">        boolean changed &#x3D; FSDirAttrOp.setTimes(dir,</span><br><span class="line">            inode, -1, now, false, iip.getLatestSnapshotId());</span><br><span class="line">        if (changed) &#123;</span><br><span class="line">          getEditLog().logTimes(src, -1, now);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; catch (Throwable e) &#123;</span><br><span class="line">      LOG.warn(&quot;Failed to update the access time of &quot; + src, e);</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">      writeUnlock(operationName);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>res.updateAccessTime()</code> 决定了是否更新atime，其值是在getBlockLocationsInt中赋值的，代码如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">boolean updateAccessTime &#x3D; isAccessTimeSupported() &amp;&amp; !isInSafeMode()</span><br><span class="line">        &amp;&amp; !iip.isSnapshot()</span><br><span class="line">        &amp;&amp; now &gt; inode.getAccessTime() + getAccessTimePrecision();</span><br></pre></td></tr></table></figure><p>其中关键的因素是<code>isAccessTimeSupported()</code>和<code>getAccessTimePrecision()</code>，这个两个方法都与<code>accessTimePrecision</code>有关，<br>此值是由<code>dfs.namenode.accesstime.precision</code>设置的，默认是3600000。</p><p>当此值大于0，<code>isAccessTimeSupported()</code>返回true，<code>getAccessTimePrecision()</code>得到的值是<code>dfs.namenode.accesstime.precision</code>的值。</p><p>从上述代码中可以看出更新atime的一个条件是<strong>两次读取间隔相隔<code>dfs.namenode.accesstime.precision</code>秒，默认是1小时。</strong></p><p>这里遗留两个问题:</p><ol><li>新建文件时atime如何赋值</li><li>修改文件内容时atime如何赋值</li></ol><p>关于这个两个问题我在下一节在写流程中解答。请继续向下看</p><h5 id="mtime"><a href="#mtime" class="headerlink" title="mtime"></a>mtime</h5><p>同样去年也写过一篇关于写的文章<a href="http://bigdatadecode.club/HDFS%20write%E8%A7%A3%E6%9E%90.html" target="_blank" rel="noopener">HDFS write解析</a>介绍HDFS读操作流程，这里就不再累赘了，直接贴出关键代码。(有兴趣的同学可以自行查看)</p><p>写相关的操作包括create、close和append<br>写文件有两种方式，一种是调用<code>create(Path)</code>方法，另一种是调用<code>append(Path)</code>方法</p><p><strong>create</strong></p><p>通过调用<code>create(Path)</code>，最终会调用<code>FSDirectory.addFile</code>方法，<br>在此方法中会new一个<code>INodeFile</code>，此时会设置 mtime 和 atime 为同一个值，代码如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">INodesInPath addFile(INodesInPath existing, String localName, PermissionStatus</span><br><span class="line">    permissions, short replication, long preferredBlockSize,</span><br><span class="line">    String clientName, String clientMachine)</span><br><span class="line">  throws FileAlreadyExistsException, QuotaExceededException,</span><br><span class="line">    UnresolvedLinkException, SnapshotAccessControlException, AclException &#123;</span><br><span class="line">  long modTime &#x3D; now();</span><br><span class="line">  INodeFile newNode &#x3D; newINodeFile(allocateNewInodeId(), permissions, modTime,</span><br><span class="line">      modTime, replication, preferredBlockSize);</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>有打开一个文件就有关闭一个文件，接下来看下关闭文件时 atime 和 mtime 会有什么变化。</p><p><strong>close</strong></p><p><code>closeFile()</code>在<code>finalizeINodeFileUnderConstruction</code>中调用，在此方法中会设置mtime，看下代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">private void finalizeINodeFileUnderConstruction(String src,</span><br><span class="line">    INodeFile pendingFile, int latestSnapshot) throws IOException &#123;</span><br><span class="line">...</span><br><span class="line">  pendingFile.toCompleteFile(now());</span><br><span class="line">...</span><br><span class="line">  closeFile(src, pendingFile);</span><br><span class="line">...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; INodeFile.java</span><br><span class="line">public INodeFile toCompleteFile(long mtime) &#123;</span><br><span class="line">  Preconditions.checkState(isUnderConstruction(),</span><br><span class="line">      &quot;file is no longer under construction&quot;);</span><br><span class="line">  FileUnderConstructionFeature uc &#x3D; getFileUnderConstructionFeature();</span><br><span class="line">  if (uc !&#x3D; null) &#123;</span><br><span class="line">    assertAllBlocksComplete();</span><br><span class="line">    removeFeature(uc);</span><br><span class="line">    this.setModificationTime(mtime);</span><br><span class="line">  &#125;</span><br><span class="line">  return this;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从代码中可以看出，close文件时只对mtime进行了修改。</p><p><strong>append</strong></p><p>append只是打开一个文件流，并不会修改mtime或者atime，只是在close的时候修改mtime</p><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p><strong>atime</strong></p><ol><li>两次读间隔大于默认的1小时时，更新atime。默认间隔通过<code>dfs.namenode.accesstime.precision</code>控制。</li><li>新建一个文件时atime赋值为当前时间(注意，当关闭一个文件时atime不会修改)</li></ol><p><strong>mtime</strong></p><ol><li>新建一个文件时mtime赋值为当前时间(同时会修改atime)</li><li>关闭一个文件时mtime赋值为当前时间(此时并不会修改atime)</li></ol>]]></content>
    
    <summary type="html">
    
      HDFS 中 atime 与 mtime 解析
    
    </summary>
    
    
      <category term="HDFS" scheme="http://yoursite.com/categories/HDFS/"/>
    
    
  </entry>
  
  <entry>
    <title>Kafka 零拷贝原理</title>
    <link href="http://yoursite.com/2020/01/24/Kafka-Zero-Copy/"/>
    <id>http://yoursite.com/2020/01/24/Kafka-Zero-Copy/</id>
    <published>2020-01-23T16:00:00.000Z</published>
    <updated>2020-02-05T13:37:24.348Z</updated>
    
    <content type="html"><![CDATA[<p>从字面意思理解就是数据不需要来回的拷贝，大大提升了系统的性能</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>“零拷贝”是指计算机操作的过程中，CPU 不需要为数据在内存之间的拷贝消耗资源。而它通常是指计算机在网络上发送文件时，不需要将文件内容拷贝到用户空间（User Space）而直接在内核空间（Kernel Space）中传输到网络的方式。</p><p>零拷贝并不是不需要拷贝，而是减少不必要的拷贝次数。通常是说在 IO 读写过程中</p><h4 id="缓冲区"><a href="#缓冲区" class="headerlink" title="缓冲区"></a>缓冲区</h4><p>缓冲区是所有 I/O 的基础，I/O 讲的无非就是把数据移进或移出缓冲区</p><p>进程执行I/O操作，就是向操作系统发出请求，让它要么把缓冲区的数据排干(写)，要么填充缓冲区(读)</p><p>下面看一个 Java 进程发起读请求加载数据大致的流程图</p><p><img src="/images/blog/2020-01-24-1.png" alt></p><ol><li><p>进程发起读请求之后，内核接收到读请求之后，会先检查内核空间中是否已经存在进程所需要的数据，<br>如果已经存在，则直接把数据拷贝给进程的缓冲区</p></li><li><p>如果内核缓冲区没有命中随即向磁盘控制器发出命令，要求从磁盘读取数据，磁盘控制器把数据直接写入内核read缓冲区，这一步通过DMA完成</p></li><li><p>接下来内核将数据拷贝到进程的缓冲区</p></li></ol><p>DMA 是一种硬件和软件之间的数据传输的技术，且 DMA 进行数据传输的过程中几乎不需要 CPU 参与</p><h4 id="虚拟内存"><a href="#虚拟内存" class="headerlink" title="虚拟内存"></a>虚拟内存</h4><p>所有现代操作系统都使用虚拟内存，使用虚拟的地址取代物理地址，这样有两个好处:</p><ol><li>一个以上的虚拟地址可以指向同一个物理内存地址</li><li>虚拟内存空间可大于实际可用的物理地址</li></ol><p>利用第一条特性可以把内核空间地址和用户空间的虚拟地址映射到同一个物理地址，<br>这样DMA就可以填充对内核和用户空间进程同时可见的缓冲区了，大致如下图所示:</p><p><img src="/images/blog/2020-01-24-3.png" alt></p><h4 id="零拷贝技术"><a href="#零拷贝技术" class="headerlink" title="零拷贝技术"></a>零拷贝技术</h4><p>本人水平有限，零拷贝技术可以阅读下方链接</p><ul><li><a href="https://www.jianshu.com/p/497e7640b57c" target="_blank" rel="noopener">零拷贝的原理及Java实现</a></li><li><a href="https://blog.csdn.net/lzb348110175/article/details/100853071" target="_blank" rel="noopener">Kafka 的零拷贝技术</a></li><li><a href="https://mp.weixin.qq.com/s/otuUvACiVDafGgDl6xNd2A" target="_blank" rel="noopener">面试被问到“零拷贝”！你真的理解吗？</a></li><li><a href="https://mp.weixin.qq.com/s/vaKRVvfUnFjhHzfkwZDbKQ" target="_blank" rel="noopener">图解Kafka的零拷贝技术到底有多牛？</a></li><li><a href="https://blog.csdn.net/ljheee/article/details/99652448" target="_blank" rel="noopener">Kafka零拷贝</a></li></ul><p>Kafka 通过 sendfile 实现的零拷贝 I/O，理想状态下的零拷贝I/O需要接触 DMA 完成。</p>]]></content>
    
    <summary type="html">
    
      Kafka 零拷贝原理
    
    </summary>
    
    
      <category term="Kafka" scheme="http://yoursite.com/categories/Kafka/"/>
    
    
  </entry>
  
  <entry>
    <title>HDFS Federation 发展历程</title>
    <link href="http://yoursite.com/2020/01/22/HDFS-Federation/"/>
    <id>http://yoursite.com/2020/01/22/HDFS-Federation/</id>
    <published>2020-01-21T16:00:00.000Z</published>
    <updated>2020-02-12T07:43:22.162Z</updated>
    
    <content type="html"><![CDATA[<p>HDFS Federation（联邦）相关知识，如 HDFS Router-based Federation</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>在阅读一些 Hadoop3 新特性的文章中，我看到有提”多 NameNode”，然后简单某度搜索到的链接都是提 HDFS Federation（联邦）</p><p><img src="/images/blog/2020-01-22-2.png" alt></p><p>下意识以为”多 NameNode”指的是 Federation 正式在 Hadoop3 上实现可用，<br>在一次面试过程中跟面试官扯 Hadoop3 Federation 新特性，人家反问一句这不是 Hadoop2 就有了吗。。。我瞬间懵了。</p><p>事后回顾到才知道 HDFS RBF 才是 Hadoop3 Router-based Federation 新特性。”多 NameNode” 是指 HDFS HA 支持多 Standby 节点机制 <a href="https://issues.apache.org/jira/browse/HDFS-6440" target="_blank" rel="noopener">HDFS-6440</a></p><p>在配置平衡策略时看到有个”BlockPool”，想到这是跟 Federation 有关，因此想整理一下对这方面的知识程度</p><p><img src="/images/blog/2020-01-22-1.png" alt></p><h4 id="HDFS-Federation"><a href="#HDFS-Federation" class="headerlink" title="HDFS Federation"></a>HDFS Federation</h4><p>对 Federation 学习个人大部分是通过以下链接学习了解，大家可以按链接顺序自行学习。</p><ul><li><a href="https://mp.weixin.qq.com/s/8VrQFIuQUVEa20JfG2EZkQ" target="_blank" rel="noopener">HDFS Federation（联邦）简介</a></li><li><a href="https://mp.weixin.qq.com/s/YCzhYCv-fcbOHfM9wvMchg" target="_blank" rel="noopener">如何通过CM为HDFS启用Federation</a></li><li><a href="https://mp.weixin.qq.com/s/X-8DmvgAsLsFEZADYGklBA" target="_blank" rel="noopener">如何通过CM禁用Federation</a></li><li><a href="https://mp.weixin.qq.com/s/2QDnm54_ObpXMyD952GovQ" target="_blank" rel="noopener">HDFS Router-based Federation</a></li><li><a href="https://mp.weixin.qq.com/s/BPfhDxiSaUkoOSyrMuAHIw" target="_blank" rel="noopener">Router-Based HDFS Federation 在滴滴大数据的应用</a></li><li><a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs-rbf/HDFSRouterFederation.html" target="_blank" rel="noopener">HDFS Router-based Federation 官方文档</a></li></ul><p>由于 Router-based HDFS federation 还算比较新的特性，所以社区分了几个阶段修复或添加了一些新的功能，<br>社区在 2.9 和 3.0 版本中发布 HDFS RBF 方案解决统一命名空间问题，参见 <a href="https://issues.apache.org/jira/browse/HDFS-10467" target="_blank" rel="noopener">HDFS-10467</a>，<br>在 Apache Hadoop 3.2.0 版本修复或添加了一些功能，参见 <a href="https://issues.apache.org/jira/browse/HDFS-12615" target="_blank" rel="noopener">HDFS-12615</a>，<br>以及 Router-based HDFS federation 稳定性相关的 ISSUE <a href="https://issues.apache.org/jira/browse/HDFS-13891" target="_blank" rel="noopener">HDFS-13891</a>，<br>这个 ISSUE 可能会在 Apache Hadoop 3.3.0 版本发布(过往记忆大数据的<a href="https://mp.weixin.qq.com/s/0qY-LMHQT-yGJPh-ybbZUg" target="_blank" rel="noopener">Apache Hadoop 的 HDFS federation 前世今生</a>文末提的。。。不知道会不会发布)。</p><p>小米在<a href="https://mp.weixin.qq.com/s/2QDnm54_ObpXMyD952GovQ" target="_blank" rel="noopener">HDFS Router-based Federation</a>文末提了 Rebalance 社区没有实现跨子集群的迁移。<br>翻看官方文档的确有提:</p><blockquote><p>Some operations are not available in Router-based federation. The Router throws exceptions for those. Examples users may encounter include the following.</p><ul><li>Rename file/folder in two different nameservices.</li><li>Copy file/folder in two different nameservices.</li><li>Write into a file/folder being rebalanced.</li></ul></blockquote><p>但是跨集群的数据平衡已经在做了，可以参见 <a href="https://issues.apache.org/jira/browse/HDFS-13123" target="_blank" rel="noopener">HDFS-13123</a>。<br>小米自己也说内部实现了跨子集群的 Federation Rename，可以在未来将这一特性合并到社区的 Rebalance 方案中，期待ing。</p><hr><p>参考链接</p><ul><li><a href="https://mp.weixin.qq.com/s/CEIKrDZrR6EZIKJ9x2Se0Q" target="_blank" rel="noopener">HDFS3.2升级在滴滴的实践</a></li><li><a href="http://www.voidcn.com/article/p-umrtxeag-pk.html" target="_blank" rel="noopener">HDFS HA支持多Standby节点机制</a></li></ul>]]></content>
    
    <summary type="html">
    
      HDFS Federation 发展历程
    
    </summary>
    
    
      <category term="HDFS" scheme="http://yoursite.com/categories/HDFS/"/>
    
    
  </entry>
  
  <entry>
    <title>HBase 表存储层次</title>
    <link href="http://yoursite.com/2020/01/21/HBase-Table/"/>
    <id>http://yoursite.com/2020/01/21/HBase-Table/</id>
    <published>2020-01-20T16:00:00.000Z</published>
    <updated>2020-02-03T10:02:20.429Z</updated>
    
    <content type="html"><![CDATA[<p>对 HBase 表存储层次梳理</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>HBase 表是由一个或者多个 region 组成，一个 region 由一个或者多个列蔟组成，一个列蔟由一个 store 组成，一个 store 由唯一 memstore 加上一个或者多个 HFile 组成，HFile 又是由 block 组成，而 block 是由多个 cell 组成。</p><h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><h5 id="Region"><a href="#Region" class="headerlink" title="Region"></a>Region</h5><p>Region 类似于数据库的分片和分区的概念，每个 Region 负责一小部分 Rowkey 范围的数据的读写和维护，这样即使是一张巨大的表，由于被切割到不同的 Region，访问起来的时延也很低。<br>Region 包含了对应的起始行到结束行的所有信息。</p><p><img src="/images/blog/2020-01-21-1.png" alt></p><h5 id="Store"><a href="#Store" class="headerlink" title="Store"></a>Store</h5><p>Store 是存储所有写入表中信息的实体，并且当数据需要从表中读取时也将被用到。A Store is the same thing as a ColumnFamily. 一个 Store 等价一个列簇 </p><h5 id="Memstore"><a href="#Memstore" class="headerlink" title="Memstore"></a>Memstore</h5><p>HBase 是基于 LSM-Tree 模型的，所有的数据更新插入操作都首先写入 Memstore 中（同时会顺序写到日志HLog中），达到指定大小之后再将这些修改操作批量写入磁盘，生成一个新的 HFile 文件，这种设计可以极大地提升 HBase 的写入性能。每一个 Store 里面都有一个 Memstore, MemStore 保存对 Store 的内存修改。修改是 Cells / KeyValues。</p><h5 id="StoreFile"><a href="#StoreFile" class="headerlink" title="StoreFile"></a>StoreFile</h5><p>StoreFile 为 HFile 上的一层类封装。A StoreFile is a facade of HFile.</p><h5 id="HFile"><a href="#HFile" class="headerlink" title="HFile"></a>HFile</h5><p>HFile 对应的列蔟。当内存写满必须要刷新到磁盘时，HFile 就会被创建，所以 HFile 存放的是某个时刻的  MemStore 刷写时的快照，一个完整的行的数据可能存放在多个 HFile 里。 随着时间推移，HFile 最终会被压缩（即合并）成大文件。HFile 是 HBase 用来存储数据的文件格式。</p><p>HFile 由不同种类的 block 块组成（索引块和数据块）。HFile 存储在 HDFS 上，因此获得 HDFS 的持久化和多副本的优势。</p><h5 id="Block"><a href="#Block" class="headerlink" title="Block"></a>Block</h5><p>HFile 由 block 组成，不要与 HDFS 的 block 混淆了。一个 HDFS 数据块可以包含很多 HFile block。HFile block 通常在 8kb 和 1MB 之间，默认大小是 64kb(在建表语句中可以通过参数 BlockSize 指定)。如果一个表配置了压缩选项，HBase 仍会产生了 64kb 大小 block，然后压缩 block。根据数据的大小以及压缩格式，压缩后的 block 存储在磁盘的大小也不一样。</p><p>每个 block 的大小可以在创建表列簇的时候通过参数 <code>blocksize ＝&gt; &#39;65535&#39;</code> 进行指定，默认为 64k，大号的 Block 有利于顺序 Scan，小号 Block 利于随机查询，因而需要权衡。</p><p>如果将 block size 配置得很小，将会产生过多的 HFile block 索引，这将会给内存造成压力 （《HBase 实战》P27 《HBase应用架构》P19）</p><h5 id="KeyValue"><a href="#KeyValue" class="headerlink" title="KeyValue"></a>KeyValue</h5><p><img src="/images/blog/2020-01-21-2.png" alt></p><p>KeyValue 是 HBase 数据存储的核心，KeyValue 并不是简单的 KV 数据对，是一个具有复杂元素的结构体</p><p>由 keylength、valuelength、key、value 四个部分组成，其中 Key 又由 Row Length、Row、Column Family Length、Column Family、Column Qualifier、TimeStamp、Key Type 七部分组成。 </p><table><thead><tr><th>字段</th><th>说明</th><th>长度</th></tr></thead><tbody><tr><td>Key Length</td><td>存储Key的长度</td><td>4个字节</td></tr></tbody></table><p>Value Length | 存储Value的长度 | 4个字节</p><p>Row Length | 存储 Row 的长度，即 Rowkey 的长度 |  2个字节</p><p>Row | 存储 Rowkey | Row Length</p><p>Column Family Length | 存储列簇 Column Family 的长度 | 1个字节</p><p>Column Family | 存储 Column Family 实际内容 | Column Family Length</p><p>Column Qualifier | 存储 Column Qualifier 对应的数据 |  -          </p><p>Time Stamp | 存储时间戳 TimeStamp | 8个字节</p><p>Key Type | 存储 Key 类型 KeyType | 1个字节</p><p>Value | 存储单元格 Cell 对应的实际的值Value | Value Length</p><p>“key type” 字段代表不同可能的HBase操作</p><ul><li>Put</li><li>Delete</li><li>DeleteColumn</li><li>DeleteFamily</li></ul><p>KeyValue 保存了这个 value 的所有信息，这也是为什么面向列的原因</p><blockquote><p>Q: Row，Column Family 都有一个 length 标志，为什么 Column Qualifier 没有呢？<br>A: 由于 Key 中其它的字段占用大小已经知道，并且知道整个 Key 的大小，因此没有存储 Column Qualifier 的大小。 Column Qualifier 的 Length 可以由 key length 减去其他 length 得到，这样可以减少一定的存储量。</p></blockquote><hr><p>参考链接</p><ul><li><a href="https://blog.csdn.net/bitcarmanlee/article/details/78979836" target="_blank" rel="noopener">Hbase KeyValue结构详解</a></li><li><a href="https://blog.csdn.net/ping_hu/article/details/77115998" target="_blank" rel="noopener">HBase的KeyValue分析</a></li></ul>]]></content>
    
    <summary type="html">
    
      HBase 表存储层次
    
    </summary>
    
    
      <category term="HBase" scheme="http://yoursite.com/categories/HBase/"/>
    
    
  </entry>
  
  <entry>
    <title>Spark-Submit 参数说明</title>
    <link href="http://yoursite.com/2020/01/20/Spark-submit/"/>
    <id>http://yoursite.com/2020/01/20/Spark-submit/</id>
    <published>2020-01-19T16:00:00.000Z</published>
    <updated>2020-02-03T10:02:20.426Z</updated>
    
    <content type="html"><![CDATA[<p>spark-submit 详细参数说明</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>部分初级开发者需要使用 Spark-submit 提交 spark 作业到 yarn 上，经常问些参数设置的问题，<br>基于此需求梳理下</p><h4 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h4><p>在命令行输入 <code>spark-submit -h</code>，可以看到 spark-submit 的所用参数如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">$ bin&#x2F;spark-submit -h</span><br><span class="line">Usage: spark-submit [options] &lt;app jar | python file | R file&gt; [app arguments]</span><br><span class="line">Usage: spark-submit --kill [submission ID] --master [spark:&#x2F;&#x2F;...]</span><br><span class="line">Usage: spark-submit --status [submission ID] --master [spark:&#x2F;&#x2F;...]</span><br><span class="line">Usage: spark-submit run-example [options] example-class [example args]</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">  --master MASTER_URL         spark:&#x2F;&#x2F;host:port, mesos:&#x2F;&#x2F;host:port, yarn,</span><br><span class="line">                              k8s:&#x2F;&#x2F;https:&#x2F;&#x2F;host:port, or local (Default: local[*]).</span><br><span class="line">  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (&quot;client&quot;) or</span><br><span class="line">                              on one of the worker machines inside the cluster (&quot;cluster&quot;)</span><br><span class="line">                              (Default: client).</span><br><span class="line">  --class CLASS_NAME          Your application&#39;s main class (for Java &#x2F; Scala apps).</span><br><span class="line">  --name NAME                 A name of your application.</span><br><span class="line">  --jars JARS                 Comma-separated list of jars to include on the driver</span><br><span class="line">                              and executor classpaths.</span><br><span class="line">  --packages                  Comma-separated list of maven coordinates of jars to include</span><br><span class="line">                              on the driver and executor classpaths. Will search the local</span><br><span class="line">                              maven repo, then maven central and any additional remote</span><br><span class="line">                              repositories given by --repositories. The format for the</span><br><span class="line">                              coordinates should be groupId:artifactId:version.</span><br><span class="line">  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while</span><br><span class="line">                              resolving the dependencies provided in --packages to avoid</span><br><span class="line">                              dependency conflicts.</span><br><span class="line">  --repositories              Comma-separated list of additional remote repositories to</span><br><span class="line">                              search for the maven coordinates given with --packages.</span><br><span class="line">  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place</span><br><span class="line">                              on the PYTHONPATH for Python apps.</span><br><span class="line">  --files FILES               Comma-separated list of files to be placed in the working</span><br><span class="line">                              directory of each executor. File paths of these files</span><br><span class="line">                              in executors can be accessed via SparkFiles.get(fileName).</span><br><span class="line"></span><br><span class="line">  --conf PROP&#x3D;VALUE           Arbitrary Spark configuration property.</span><br><span class="line">  --properties-file FILE      Path to a file from which to load extra properties. If not</span><br><span class="line">                              specified, this will look for conf&#x2F;spark-defaults.conf.</span><br><span class="line"></span><br><span class="line">  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).</span><br><span class="line">  --driver-java-options       Extra Java options to pass to the driver.</span><br><span class="line">  --driver-library-path       Extra library path entries to pass to the driver.</span><br><span class="line">  --driver-class-path         Extra class path entries to pass to the driver. Note that</span><br><span class="line">                              jars added with --jars are automatically included in the</span><br><span class="line">                              classpath.</span><br><span class="line"></span><br><span class="line">  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).</span><br><span class="line"></span><br><span class="line">  --proxy-user NAME           User to impersonate when submitting the application.</span><br><span class="line">                              This argument does not work with --principal &#x2F; --keytab.</span><br><span class="line"></span><br><span class="line">  --help, -h                  Show this help message and exit.</span><br><span class="line">  --verbose, -v               Print additional debug output.</span><br><span class="line">  --version,                  Print the version of current Spark.</span><br><span class="line"></span><br><span class="line"> Cluster deploy mode only:</span><br><span class="line">  --driver-cores NUM          Number of cores used by the driver, only in cluster mode</span><br><span class="line">                              (Default: 1).</span><br><span class="line"></span><br><span class="line"> Spark standalone or Mesos with cluster deploy mode only:</span><br><span class="line">  --supervise                 If given, restarts the driver on failure.</span><br><span class="line">  --kill SUBMISSION_ID        If given, kills the driver specified.</span><br><span class="line">  --status SUBMISSION_ID      If given, requests the status of the driver specified.</span><br><span class="line"></span><br><span class="line"> Spark standalone and Mesos only:</span><br><span class="line">  --total-executor-cores NUM  Total cores for all executors.</span><br><span class="line"></span><br><span class="line"> Spark standalone and YARN only:</span><br><span class="line">  --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode,</span><br><span class="line">                              or all available cores on the worker in standalone mode)</span><br><span class="line"></span><br><span class="line"> YARN-only:</span><br><span class="line">  --queue QUEUE_NAME          The YARN queue to submit to (Default: &quot;default&quot;).</span><br><span class="line">  --num-executors NUM         Number of executors to launch (Default: 2).</span><br><span class="line">                              If dynamic allocation is enabled, the initial number of</span><br><span class="line">                              executors will be at least NUM.</span><br><span class="line">  --archives ARCHIVES         Comma separated list of archives to be extracted into the</span><br><span class="line">                              working directory of each executor.</span><br><span class="line">  --principal PRINCIPAL       Principal to be used to login to KDC, while running on</span><br><span class="line">                              secure HDFS.</span><br><span class="line">  --keytab KEYTAB             The full path to the file that contains the keytab for the</span><br><span class="line">                              principal specified above. This keytab will be copied to</span><br><span class="line">                              the node running the Application Master via the Secure</span><br><span class="line">                              Distributed Cache, for renewing the login tickets and the</span><br><span class="line">                              delegation tokens periodically.</span><br></pre></td></tr></table></figure><p>相关意义如下:</p><table><thead><tr><th>参数名</th><th>默认值</th><th>参数说明</th></tr></thead><tbody><tr><td>–master</td><td>local[*]</td><td>master 的地址，提交任务到哪里执行</td></tr><tr><td>–deploy-mode</td><td>client</td><td>在本地 (client) 启动 driver 或在 cluster 上启动，默认是 client</td></tr><tr><td>–class</td><td>-</td><td>应用程序的主类，仅针对 java 或 scala 应用</td></tr><tr><td>–name</td><td>-</td><td>应用程序的名称</td></tr><tr><td>–jars</td><td>-</td><td>用逗号分隔的本地 jar 包，设置后，这些 jar 将包含在 driver 和 executor 的 classpath 下</td></tr><tr><td>–packages</td><td>-</td><td>包含在 driver 和 executor 的 classpath 中的 jar 的 maven 坐标</td></tr><tr><td>–exclude-packages</td><td>-</td><td>为了避免冲突 而指定不包含的 package</td></tr><tr><td>–repositories</td><td>-</td><td>逗号分隔的其他远程存储库列表，用于搜索用–packages给定的maven坐标</td></tr><tr><td>–conf PROP=VALUE</td><td>-</td><td>指定 spark 配置属性的值，例如 -conf spark.executor.extraJavaOptions=”-XX:MaxPermSize=256m”</td></tr><tr><td>–properties-file</td><td>-</td><td>加载的配置文件，默认为 conf/spark-defaults.conf</td></tr><tr><td>–driver-memory</td><td>1024M</td><td>Driver内存</td></tr><tr><td>–driver-cores</td><td>1</td><td>Driver 的核数。只能在 client 模式下使用</td></tr><tr><td>–driver-java-options</td><td>-</td><td>传给 driver 的额外的 Java 选项</td></tr><tr><td>–driver-library-path</td><td>-</td><td>传给 driver 的额外的库路径</td></tr><tr><td>–driver-class-path</td><td>-</td><td>传给 driver 的额外的类路径</td></tr><tr><td>–total-executor-cores</td><td>-</td><td>所有 executor 总共的核数。仅仅在 mesos 或者 standalone 下使用</td></tr><tr><td>–num-executors</td><td>2</td><td>启动的 executor 数量。在 yarn 下使用</td></tr><tr><td>–executor-core</td><td>1</td><td>每个 executor 的核数。在 yarn 或者 standalone下使用</td></tr><tr><td>–executor-memory</td><td>1G</td><td>每个 executor 的内存</td></tr><tr><td>–queue</td><td>“default”</td><td>提交到 Yarn 上队列</td></tr></tbody></table><h4 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h4><p>将官方的 example 包使用 client 模式提交到 Yarn 上</p><p><code>spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode client --driver-memory 4g --num-executors 2 --executor-memory 2g --executor-cores 2 spark-examples*.jar 10</code></p><p><code>client</code> 模式输出结果，会在控制台展示</p><p>使用 cluster 模式只需将 <code>--deploy-mode</code> 参数换成 <code>cluster</code>，使用 <code>cluster</code> 模式，控制台上面是没有输出结果的</p><p>需要利用 Yarn 来查看  </p><ul><li><code>yarn logs -applicationId {applicationId} &gt;&gt; yarnApp.log</code> </li><li>Yarn Web UI</li></ul><h4 id="遇到问题"><a href="#遇到问题" class="headerlink" title="遇到问题"></a>遇到问题</h4><p><strong><a href="https://www.cnblogs.com/itboys/p/10579352.html" target="_blank" rel="noopener">Spark代码中设置appName在client模式和cluster模式中不一样问题</a></strong></p><hr><p>参考链接</p><ul><li><a href="https://blog.csdn.net/qq_29303759/article/details/82659185" target="_blank" rel="noopener">spark-submit 详细参数说明</a></li><li><a href="https://blog.csdn.net/MASILEJFOAISEGJIAE/article/details/89317964" target="_blank" rel="noopener">Spark使用示例：分别使用client模式和cluster运行SparkPi程序</a></li></ul>]]></content>
    
    <summary type="html">
    
      spark-submit 详细参数说明
    
    </summary>
    
    
      <category term="Spark" scheme="http://yoursite.com/categories/Spark/"/>
    
    
  </entry>
  
  <entry>
    <title>YARN Memory CPU 配置</title>
    <link href="http://yoursite.com/2020/01/19/Yarn-Container/"/>
    <id>http://yoursite.com/2020/01/19/Yarn-Container/</id>
    <published>2020-01-18T16:00:00.000Z</published>
    <updated>2020-02-09T15:06:39.385Z</updated>
    
    <content type="html"><![CDATA[<p>介绍如何配置 YARN 对内存和CPU的使用</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>以下内容是我学习的理解，主要还是转载武基鹏的<a href="http://blog.itpub.net/30089851/viewspace-2127851/" target="_blank" rel="noopener">YARN的Memory和CPU调优配置详解</a>。</p><p>Hadoop YARN 同时支持内存和 CPU 两种资源的调度</p><p>YARN 作为一个资源调度器，应该考虑到集群里面每一台机子的计算资源，然后根据 application 申请的资源进行分配 Container</p><p>Container 是 YARN 里面资源分配的基本单位，一个 Container 就是一组分配的系统资源，现阶段只包含内存、CPU 系统资源 </p><p>Container 和集群节点的关系是一个节点会运行多个 Container，但一个 Container 不会跨节点</p><p>在 YARN 集群中，平衡内存、CPU、磁盘的资源的很重要的，作为一般建议，每个磁盘和每个核心允许两个容器可以为群集利用率提供最佳平衡。</p><p>确定群集节点的适当 YARN 内存配置时，从可用的硬件资源开始分析  </p><ul><li>RAM（内存量）</li><li>CORES（CPU内核数）</li><li>DISKS（磁盘数）</li></ul><p>关键参数默认值如下表:  </p><table><thead><tr><th>参数</th><th>默认值</th></tr></thead><tbody><tr><td>yarn.nodemanager.resource.memory-mb</td><td>-1 (代表NodeManager占机器总内存80%)</td></tr><tr><td>yarn.scheduler.minimum-allocation-mb</td><td>1024MB</td></tr><tr><td>yarn.scheduler.maximum-allocation-mb</td><td>8192MB</td></tr><tr><td>yarn.nodemanager.resource.cpu-vcores</td><td>-1 (代表值为8个虚拟CPU)</td></tr><tr><td>yarn.scheduler.minimum-allocation-vcores</td><td>1</td></tr><tr><td>yarn.scheduler.maximum-allocation-vcores</td><td>4</td></tr></tbody></table><h4 id="内存配置"><a href="#内存配置" class="headerlink" title="内存配置"></a>内存配置</h4><p>关于内存相关的配置可以参考 hortonwork 公司的文档<a href="https://docs.cloudera.com/HDPDocuments/HDP2/HDP-2.1.1/bk_installing_manually_book/content/rpm-chap1-11.html" target="_blank" rel="noopener">Determine HDP Memory Configuration Settings</a>来配置你的集群</p><p>YARN 可用的内存资源应该要考虑预留内存，预留内存是系统进程和其他Hadoop进程（例如HBase）所需的内存。</p><p>预留内存 = 操作系统内存预留 + HBase 内存预留（如果HBase在同一节点上）</p><h5 id="保留内存建议"><a href="#保留内存建议" class="headerlink" title="保留内存建议"></a>保留内存建议</h5><table><thead><tr><th>每个节点的总内存</th><th>建议的预留系统内存</th><th>建议的保留 HBase 内存</th></tr></thead><tbody><tr><td>4GB</td><td>1GB</td><td>1GB</td></tr><tr><td>8GB</td><td>2GB</td><td>1GB</td></tr><tr><td>16GB</td><td>2GB</td><td>2GB</td></tr><tr><td>24GB</td><td>4GB</td><td>4GB</td></tr><tr><td>48GB</td><td>6GB</td><td>8GB</td></tr><tr><td>64GB</td><td>8GB</td><td>8GB</td></tr><tr><td>72GB</td><td>8GB</td><td>8GB</td></tr><tr><td>96GB</td><td>12GB</td><td>16GB</td></tr><tr><td>128GB</td><td>24GB</td><td>24GB</td></tr><tr><td>256GB</td><td>32GB</td><td>32GB</td></tr><tr><td>512GB</td><td>64GB</td><td>64GB</td></tr></tbody></table><p>下一个计算是确定每个节点允许的最大容器数。可以使用以下公式:</p><blockquote><p>containers = min (2 * CORES, 1.8 * DISKS, (Total available RAM) / MIN_CONTAINER_SIZE)</p><p>说明:<br>CORES 为机器CPU核数<br>DISKS 为机器上挂载的磁盘个数<br>Total available RAM 为机器总内存<br>MIN_CONTAINER_SIZE 是指 container 最小的容量大小</p></blockquote><p><code>MIN_CONTAINER_SIZE</code> 这需要根据具体情况去设置，在较小的内存节点中，<br>最小容器大小也应该较小，下表概述了推荐值: </p><table><thead><tr><th>每台机子可用内存</th><th>container 最小值</th></tr></thead><tbody><tr><td>小于4GB</td><td>256MB</td></tr><tr><td>4GB到8GB之间</td><td>512MB</td></tr><tr><td>8GB到24GB之间</td><td>1024MB</td></tr><tr><td>大于24GB</td><td>2048MB</td></tr></tbody></table><p>每个 container 的平均使用内存大小计算方式为</p><blockquote><p>RAM-per-container = max(MIN_CONTAINER_SIZE, (Total Available RAM) / containers))</p></blockquote><table><thead><tr><th>配置文件</th><th>配置设置</th><th>默认值</th><th>计算值</th></tr></thead><tbody><tr><td>yarn-site.xml</td><td>yarn.nodemanager.resource.memory-mb</td><td>8192MB</td><td>= containers * RAM-per-container</td></tr><tr><td>yarn-site.xml</td><td>yarn.scheduler.minimum-allocation-mb</td><td>1024MB</td><td>= RAM-per-container</td></tr><tr><td>yarn-site.xml</td><td>yarn.scheduler.maximum-allocation-mb</td><td>8192MB</td><td>= containers * RAM-per-container</td></tr></tbody></table><p>举个例子: 对于 128G 内存、32核CPU 的机器，挂载了 7 个磁盘，根据上面的说明，系统保留内存为 24G，<br>不适应 HBase 情况下，系统剩余可用内存为 104G，计算 containers 值如下</p><blockquote><p>containers = min (2 * 32, 1.8 * 7 , (128-24)/2) = min (64, 12.6 , 52) = 13</p></blockquote><p>计算 RAM-per-container 值如下</p><blockquote><p>RAM-per-container = max (2, (124-24)/13) = max (2, 8) = 8</p></blockquote><p>可以使用脚本 <a href="../codeRep/yarn-utils.py">yarn-utils.py</a> 来计算上面的值</p><p>执行下面命令，python 版本为 python2</p><blockquote><p>python2 yarn-utils.py -c 32 -m 128 -d 7 -k False </p></blockquote><p>这样的话，每个container内存为8G，似乎有点多，还是更愿意根据集群使用情况任务将其调整为2G内存，则集群中下面的参数配置值如下</p><table><thead><tr><th>配置文件</th><th>配置设置</th><th>计算值</th></tr></thead><tbody><tr><td>yarn-site.xml</td><td>yarn.nodemanager.resource.memory-mb</td><td>= 52 * 2 =104 G</td></tr><tr><td>yarn-site.xml</td><td>yarn.scheduler.minimum-allocation-mb</td><td>= 2G</td></tr><tr><td>yarn-site.xml</td><td>yarn.scheduler.maximum-allocation-mb</td><td>= 52 * 2 = 104G</td></tr></tbody></table><h4 id="CPU-配置"><a href="#CPU-配置" class="headerlink" title="CPU 配置"></a>CPU 配置</h4><p>YARN中目前的CPU被划分成虚拟CPU（CPU virtual Core），这里的虚拟CPU是YARN自己引入的概念，<br>初衷是，考虑到不同节点的CPU性能可能不同，每个CPU具有的计算能力也是不一样的，<br>比如某个物理CPU的计算能力可能是另外一个物理CPU的2倍，这时候，<br>你可以通过为第一个物理CPU多配置几个虚拟CPU弥补这种差异。<br>用户提交作业时，可以指定每个任务需要的虚拟CPU个数。</p><p>在YARN中，CPU相关配置参数如下</p><ul><li><code>yarn.nodemanager.resource.cpu-vcores</code> 表示能够分配给Container的CPU核数，默认配置为-1，代表值为8个虚拟CPU，推荐该值的设置和物理CPU的核数数量相同，若不够，则需要调小该值。</li><li><code>yarn.scheduler.minimum-allocation-vcores</code> 的默认值为1，表示每个Container容器在处理任务的时候可申请的最少CPU个数为1个。</li><li><code>yarn.scheduler.maximum-allocation-vcores</code> 的默认值为4，表示每个Container容器在处理任务的时候可申请的最大CPU个数为4个。</li></ul><p>在环境上建议留一个给操作系统</p><hr><p>参考链接</p><ul><li><a href="https://blog.csdn.net/qq_25302531/article/details/80623791" target="_blank" rel="noopener">浅谈YARN中Container容器（内存、CPU分配）</a></li><li><a href="http://blog.itpub.net/30089851/viewspace-2127851/" target="_blank" rel="noopener">YARN的Memory和CPU调优配置详解</a></li><li><a href="https://docs.cloudera.com/HDPDocuments/HDP2/HDP-2.1.1/bk_installing_manually_book/content/rpm-chap1-11.html" target="_blank" rel="noopener">9.确定HDP内存配置设置</a></li><li><a href="https://blog.csdn.net/suifeng3051/article/details/48135521" target="_blank" rel="noopener">Yarn 内存分配管理机制及相关参数配置</a></li></ul>]]></content>
    
    <summary type="html">
    
      YARN Memory CPU 配置
    
    </summary>
    
    
      <category term="Yarn" scheme="http://yoursite.com/categories/Yarn/"/>
    
    
  </entry>
  
  <entry>
    <title>记一次 HBase Master is initializing 问题处理</title>
    <link href="http://yoursite.com/2020/01/18/HBase-Master-is-initializing/"/>
    <id>http://yoursite.com/2020/01/18/HBase-Master-is-initializing/</id>
    <published>2020-01-17T16:00:00.000Z</published>
    <updated>2020-02-03T10:02:20.419Z</updated>
    
    <content type="html"><![CDATA[<p>HBase org.apache.hadoop.hbase.PleaseHoldException: Master is initializing 错误处理</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>测试集群上做了jar包替换验证测试，测试完之后还原jar，重启出现进入 hbase shell 中执行命令遇到问题。</p><p><img src="/images/blog/2020-01-18-1.png" alt></p><p><code>org.apache.hadoop.hbase.PleaseHoldException: Master is initializing</code> 代表Master正在初始化中，出现这种错误的原因有很多</p><p>关键还是得看 Master 日志，盲目搜索效率不高</p><p>一开始只是搜索 ERROR 日志，并没有看到很关键的信息，后面看其他博客贴的日志片段是 WARN，进而<br>转换思路看 WARN，一看这个是 meta 表未分配到 RS 上啊，之前处理过，用 hbck2 解决即可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">WARN org.apache.hadoop.hbase.master.HMaster: hbase:meta,,1.1588230740 is NOT online; state&#x3D;&#123;1588230740 state&#x3D;OPENING, </span><br><span class="line">ts&#x3D;1579334053311, server&#x3D;region-168-1-240,16020,1579334037426&#125;; ServerCrashProcedures&#x3D;true. Master startup cannot </span><br><span class="line">progress, in holding-pattern until region onlined.</span><br></pre></td></tr></table></figure><p>hbck2 配置参考<a href="https://lihuimintu.github.io/2019/12/16/hbase-hbck2/" target="_blank" rel="noopener">CDH-HBase 使用 HBCK2 运维</a>即可</p><p>手动去 assign 一下 meta 表即可，hbase:meta 表的 encoded name 是一个时间戳，比如上面日志的 encoded name 就是 <code>1588230740</code> ，<br>具体命令参考下方</p><p><code>hbase org.apache.hbase.HBCK2 assigns -o 1588230740</code></p><p>查看 Master 日志，可以看到成功分配</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">INFO org.apache.hadoop.hbase.master.procedure.MasterProcedureScheduler: Took xlock for pid&#x3D;7, state&#x3D;RUNNABLE:REGION_TRANSITION_QUEUE; AssignProcedure table&#x3D;hbase:meta, region&#x3D;1588230740, override&#x3D;true</span><br><span class="line">INFO org.apache.hadoop.hbase.master.assignment.AssignProcedure: Assigned, not reassigning rit&#x3D;OPEN, location&#x3D;region-168-1-240,16020,1579334037426</span><br><span class="line">INFO org.apache.hadoop.hbase.procedure2.ProcedureExecutor: Finished pid&#x3D;6, state&#x3D;SUCCESS; AssignProcedure table&#x3D;hbase:meta, region&#x3D;1588230740, override&#x3D;true in 160msec</span><br></pre></td></tr></table></figure><p>执行命令验证成功</p><p><img src="/images/blog/2020-01-18-2.png" alt></p><h4 id="吐槽"><a href="#吐槽" class="headerlink" title="吐槽"></a>吐槽</h4><p>之前听范老师说过: 运维三大法办- 监控、日志、源码。还是要细心</p><p>hbck2 的使用可以多看看官方文档说明，学习如何使用</p><hr><p>参考链接</p><ul><li><a href="https://blog.csdn.net/zzf1510711060/article/details/80919660" target="_blank" rel="noopener">HBase org.apache.hadoop.hbase.PleaseHoldException: Master is initializing 错误解决方法</a></li><li><a href="https://lihuimintu.github.io/2019/12/16/hbase-hbck2/" target="_blank" rel="noopener">CDH-HBase 使用 HBCK2 运维</a></li></ul>]]></content>
    
    <summary type="html">
    
      HBase Master is initializing 问题处理
    
    </summary>
    
    
      <category term="HBase" scheme="http://yoursite.com/categories/HBase/"/>
    
    
  </entry>
  
  <entry>
    <title>HBase CMS GC 配置参考</title>
    <link href="http://yoursite.com/2020/01/18/HBase-CMS/"/>
    <id>http://yoursite.com/2020/01/18/HBase-CMS/</id>
    <published>2020-01-17T16:00:00.000Z</published>
    <updated>2020-02-03T10:02:20.416Z</updated>
    
    <content type="html"><![CDATA[<p>HBase发展到当下，对其进行的各种优化从未停止，而 GC 优化更是其中的重中之重</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>HBase发展到当下，对其进行的各种优化从未停止，而 GC 优化更是其中的重中之重</p><p>参考自一下文章:</p><ul><li><a href="http://hbasefly.com/2016/05/21/hbase-gc-1/" target="_blank" rel="noopener">HBase GC的前生今世 – 身世篇</a></li><li><a href="http://hbasefly.com/2016/05/29/hbase-gc-2/" target="_blank" rel="noopener">HBase GC的前生今世 – 演进篇</a></li><li><a href="http://hbasefly.com/2016/08/09/hbase-cms-gc/" target="_blank" rel="noopener">HBase最佳实践－CMS GC调优</a></li><li><a href="https://mp.weixin.qq.com/s/GEwD1B-XqFIudWP_EbGgdQ" target="_blank" rel="noopener">HBase实战：记一次Safepoint导致长时间STW的踩坑之旅</a></li></ul><h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">-Xms32g -Xmx32g -Xmn512m </span><br><span class="line">-XX:+UseParNewGC </span><br><span class="line">-XX:ParallelGCThreads&#x3D;8 </span><br><span class="line">-XX:+UseConcMarkSweepGC</span><br><span class="line"> -XX:+UseCMSCompactAtFullCollection </span><br><span class="line">-XX:CMSFullGCsBeforeCompaction&#x3D;1 </span><br><span class="line">-XX:CMSInitiatingOccupancyFraction&#x3D;65 </span><br><span class="line">-XX:+UseCMSInitiatingOccupancyOnly </span><br><span class="line">-XX:+PrintGC </span><br><span class="line">-XX:+PrintGCDetails </span><br><span class="line">-XX:+PrintGCDateStamps </span><br><span class="line">-XX:+PrintGCApplicationStoppedTime </span><br><span class="line">-XX:+PrintHeapAtGC  </span><br><span class="line">-XX:+PrintTenuringDistribution </span><br><span class="line">-Xloggc:&#x2F;var&#x2F;log&#x2F;hbase&#x2F;gc-regionserver-hbase.log </span><br><span class="line">-XX:+UseGCLogFileRotation  </span><br><span class="line">-XX:NumberOfGCLogFiles&#x3D;10  </span><br><span class="line">-XX:GCLogFileSize&#x3D;100M</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      HBase CMS GC 配置参考
    
    </summary>
    
    
      <category term="HBase" scheme="http://yoursite.com/categories/HBase/"/>
    
    
  </entry>
  
  <entry>
    <title>记一次 Arthas 实操</title>
    <link href="http://yoursite.com/2020/01/17/Arthas/"/>
    <id>http://yoursite.com/2020/01/17/Arthas/</id>
    <published>2020-01-16T16:00:00.000Z</published>
    <updated>2020-02-03T10:02:20.414Z</updated>
    
    <content type="html"><![CDATA[<p>Arthas 是 Alibaba 开源的Java诊断工具，深受开发者喜爱</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>Arthas 是 Alibaba 开源的Java诊断工具，深受开发者喜爱。</p><p>Arthas 官方文档十分详细，本文也参考了官方文档内容，同时在开源在的 Github 的项目里的 Issues 里不仅有问题反馈，更有大量的使用案例，也可以进行学习参考。</p><p>开源地址: <a href="https://github.com/alibaba/arthas" target="_blank" rel="noopener">https://github.com/alibaba/arthas</a></p><p>官方文档: <a href="https://alibaba.github.io/arthas" target="_blank" rel="noopener">https://alibaba.github.io/arthas</a></p><h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p><img src="/images/blog/2020-01-17-1.png" alt></p><p>在使用 Yarn 跑测试任务时，发现中文输出乱码。</p><p>网上搜索到<a href="https://blog.csdn.net/wankunde/article/details/84658467" target="_blank" rel="noopener">hadoop web中查看文件内容乱码解决</a>，<br>感觉可以试试能不能解决问题，但是又不想把 hadoop-common 项目下载下来完整编译打包上传。</p><p>这时就想到 Arthas 好想可以解决这个问题，以前特地过了下基础教程学习使用了一下。但是一直未实战，感觉这次可以试试</p><h4 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h4><p>准备工作，下载相关文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 服务端下载这个类的 CDH6.3.1 的源码</span><br><span class="line">wget https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;cloudera&#x2F;hadoop-common&#x2F;cdh6.3.1&#x2F;hadoop-common-project&#x2F;hadoop-common&#x2F;src&#x2F;main&#x2F;java&#x2F;org&#x2F;apache&#x2F;hadoop&#x2F;http&#x2F;HtmlQuoting.java</span><br><span class="line"></span><br><span class="line"># 下载 Arthas</span><br><span class="line">wget https:&#x2F;&#x2F;alibaba.github.io&#x2F;arthas&#x2F;arthas-boot.jar</span><br></pre></td></tr></table></figure><p>修改 org.apache.hadoop.http.HtmlQuoting </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 导入该类</span><br><span class="line">import java.nio.charset.Charset;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; quoteHtmlChars 方法</span><br><span class="line">public static String quoteHtmlChars(String item) &#123;</span><br><span class="line">    if (item &#x3D;&#x3D; null) &#123;</span><br><span class="line">      return null;</span><br><span class="line">    &#125;</span><br><span class="line">    try &#123;</span><br><span class="line">      byte[] bytes &#x3D; item.getBytes(Charset.defaultCharset().name());</span><br><span class="line">      if (needsQuoting(bytes, 0, bytes.length)) &#123;</span><br><span class="line">        ByteArrayOutputStream buffer &#x3D; new ByteArrayOutputStream();</span><br><span class="line">        try &#123;</span><br><span class="line">          quoteHtmlChars(buffer, bytes, 0, bytes.length);</span><br><span class="line">          return buffer.toString(&quot;UTF-8&quot;);</span><br><span class="line">        &#125; catch (IOException ioe) &#123;</span><br><span class="line">          &#x2F;&#x2F; Won&#39;t happen, since it is a bytearrayoutputstream</span><br><span class="line">          return null;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        return item;</span><br><span class="line">     &#125;</span><br><span class="line">    &#125; catch (IOException zz) &#123;</span><br><span class="line">       return null;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>启动 Arthas</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sudo -u yarn java -jar &#x2F;opt&#x2F;arthas-boot.jar</span><br><span class="line"></span><br><span class="line"># 选择 RM、NM 进程，然后将其相关替换</span><br><span class="line"></span><br><span class="line"># 三板斧</span><br><span class="line">sc  -d *HtmlQuoting # 获取classLoaderHash</span><br><span class="line">mc -c 1963006a &#x2F;opt&#x2F;HbaseSessions.java -d &#x2F;opt</span><br><span class="line">redefine &#x2F;opt&#x2F;org&#x2F;apache&#x2F;hadoop&#x2F;http&#x2F;HtmlQuoting.class </span><br><span class="line"></span><br><span class="line"># 查看是否替换成功</span><br><span class="line">jad org.apache.hadoop.http.HtmlQuoting -c 1963006a</span><br></pre></td></tr></table></figure><p>启动 Spark 任务发现还是中文乱码，并没有解决问题，但是这次实操 Arthas 经历挺好的，熟悉下使用</p><h4 id="遇到问题"><a href="#遇到问题" class="headerlink" title="遇到问题"></a>遇到问题</h4><p><strong>AttachNotSupportedException</strong> <a href="https://github.com/alibaba/arthas/issues/440" target="_blank" rel="noopener">issue-440</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[root@region-168-1-228 ~]# java -jar arthas-boot.jar</span><br><span class="line">[INFO] arthas-boot version: 3.1.7</span><br><span class="line">[INFO] Found existing java process, please choose one and hit RETURN.</span><br><span class="line">* [1]: 13120 org.apache.hadoop.hdfs.server.datanode.DataNode</span><br><span class="line">  [2]: 13617 org.apache.hadoop.yarn.server.nodemanager.NodeManager</span><br><span class="line">  [3]: 13619 org.apache.hadoop.hbase.regionserver.HRegionServer</span><br><span class="line">  [4]: 13620 org.apache.hadoop.yarn.server.resourcemanager.ResourceManager</span><br><span class="line">  [5]: 1124 org.apache.zookeeper.server.quorum.QuorumPeerMain</span><br><span class="line">  [6]: 13118 org.apache.hadoop.hdfs.qjournal.server.JournalNode</span><br><span class="line">  [7]: 44015 com.cloudera.kafka.wrap.Kafka</span><br><span class="line">4</span><br><span class="line">[INFO] Start download arthas from remote server: https:&#x2F;&#x2F;maven.aliyun.com&#x2F;repository&#x2F;public&#x2F;com&#x2F;taobao&#x2F;arthas&#x2F;arthas-packaging&#x2F;3.1.7&#x2F;arthas-packaging-3.1.7-bin.zip</span><br><span class="line">[INFO] Download arthas success.</span><br><span class="line">[INFO] arthas home: &#x2F;root&#x2F;.arthas&#x2F;lib&#x2F;3.1.7&#x2F;arthas</span><br><span class="line">[INFO] Try to attach process 13620</span><br><span class="line">[ERROR] Start arthas failed, exception stack trace: </span><br><span class="line">com.sun.tools.attach.AttachNotSupportedException: Unable to open socket file: target process not responding or HotSpot VM not loaded</span><br><span class="line">        at sun.tools.attach.LinuxVirtualMachine.&lt;init&gt;(LinuxVirtualMachine.java:106)</span><br><span class="line">        at sun.tools.attach.LinuxAttachProvider.attachVirtualMachine(LinuxAttachProvider.java:78)</span><br><span class="line">        at com.sun.tools.attach.VirtualMachine.attach(VirtualMachine.java:250)</span><br><span class="line">        at com.taobao.arthas.core.Arthas.attachAgent(Arthas.java:85)</span><br><span class="line">        at com.taobao.arthas.core.Arthas.&lt;init&gt;(Arthas.java:28)</span><br><span class="line">        at com.taobao.arthas.core.Arthas.main(Arthas.java:123)</span><br><span class="line">[ERROR] attach fail, targetPid: 13620</span><br></pre></td></tr></table></figure><p><img src="/images/blog/2020-01-17-2.png" alt></p><p>根据意见参考，执行 arthas 使用 yarn 用户执行的解决</p><hr><p>参考链接</p><ul><li><a href="https://blog.csdn.net/wankunde/article/details/84658467" target="_blank" rel="noopener">hadoop web中查看文件内容乱码解决</a></li><li><a href="https://stackoverflow.com/questions/11277494/why-java-nio-charset-charsets-compile-error" target="_blank" rel="noopener">why java.nio.charset.Charsets compile error? </a></li></ul>]]></content>
    
    <summary type="html">
    
      记一次 Arthas 实操
    
    </summary>
    
    
      <category term="Tools" scheme="http://yoursite.com/categories/Tools/"/>
    
    
  </entry>
  
  <entry>
    <title>CDH 中使用 Hive on Spark</title>
    <link href="http://yoursite.com/2020/01/15/Hive-On-Spark/"/>
    <id>http://yoursite.com/2020/01/15/Hive-On-Spark/</id>
    <published>2020-01-14T16:00:00.000Z</published>
    <updated>2020-02-03T10:02:20.410Z</updated>
    
    <content type="html"><![CDATA[<p>Content here</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>使用 Hive insert SQL 后查看 Yarn 发现其跑的是 MR 方式</p><p><img src="/images/blog/2020-01-15-1.png" alt></p><p>这里想改用 Spark 引起来缩短 HiveQL 的响应时间</p><p>有两种方式</p><ul><li>SparkSQL</li><li>Hive on Spark</li></ul><p>两种方式都可以，看个人习惯</p><p>Hive on Spark 大体与 SparkSQL 结构类似，只是 SQL 引擎不同，但是计算引擎都是 Spark</p><p>本文主要介绍 Hive on Spark</p><h4 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h4><p>CDH Hive 配置中可以看到有官方的提示配置文档</p><p><img src="/images/blog/2020-01-15-2.png" alt></p><p>要将 Hive 配置为在 Spark 上运行，请执行以下两个步骤</p><ol><li>配置 Hive 依赖项为 Spark 服务</li><li>配置 Hive 客户端以使用 Spark 执行引擎</li></ol><p><strong>配置 Hive 依赖项为 Spark 服务</strong></p><p>按照官方文档操作即可</p><ol><li>在 Cloudera Manager 管理控制台中，转到 Hive 服务</li><li>单击配置选项卡</li><li>搜索 Spark On YARN 服务。要配置 Spark 服务，请选择 Spark 服务名称。要删除依赖项，请选择 none</li><li>点击保存更改。</li><li>进入Spark服务。</li><li>在 HiveServer2 所在的主机上添加 Spark 的 gateway 角色(即客户端)</li><li>重启 Hive、Spark 服务</li></ol><p><strong>配置 Hive 客户端以使用 Spark 执行引擎</strong></p><p>CDH 中的 Hive 支持两个执行引擎: MapReduce 和 Spark</p><p>要配置执行引擎，请执行以下步骤之一</p><p>beeline/hive: 运行 <code>set hive.execution.engine=engine</code> 命令，engine 选项要么wei <code>mr</code> 要么为 <code>spark</code>，<br>默认为 <code>mr</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">set hive.execution.engine&#x3D;spark;</span><br><span class="line"></span><br><span class="line"># 查看当前的设置执行引擎</span><br><span class="line">set hive.execution.engine;</span><br></pre></td></tr></table></figure><p>Cloudera Manager（影响所有查询，不推荐）:</p><ol><li>转到 Hive 服务</li><li>单击配置选项卡</li><li>搜索 “execution”</li><li>将”Default Execution Engine”属性设置为 MapReduce 或 Spark。默认值为 MapReduce</li><li>重启 Hive 服务</li></ol><h4 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h4><p>官方文档中提到性能</p><p><img src="/images/blog/2020-01-15-3.png" alt></p><p>暂未研究，有兴趣的可以自行看看</p><hr><p>参考链接</p><ul><li><a href="https://docs.cloudera.com/documentation/enterprise/latest/topics/admin_hos_oview.html" target="_blank" rel="noopener">在CDH中的Spark上运行Apache Hive</a></li><li><a href="https://docs.cloudera.com/documentation/enterprise/latest/topics/admin_hive_configure.html#concept_i3p_2lv_cv" target="_blank" rel="noopener">Hive执行引擎</a></li></ul>]]></content>
    
    <summary type="html">
    
      Hive on Spark 使用
    
    </summary>
    
    
      <category term="Hive" scheme="http://yoursite.com/categories/Hive/"/>
    
    
  </entry>
  
  <entry>
    <title>Linux block 与 inode</title>
    <link href="http://yoursite.com/2020/01/10/Linux-block-inode/"/>
    <id>http://yoursite.com/2020/01/10/Linux-block-inode/</id>
    <published>2020-01-09T16:00:00.000Z</published>
    <updated>2020-02-03T10:02:20.408Z</updated>
    
    <content type="html"><![CDATA[<p>Linux 磁盘管理 block 与 inode</p><hr><h4 id="详解"><a href="#详解" class="headerlink" title="详解"></a>详解</h4><p><strong>Sector（扇区）与 Block（块）</strong></p><p>1) 硬盘的最小存储单位: sector（扇区），每个扇区储存512字节；操作系统会一次性连续读取多个扇区，即一次性读取多个扇区称为一个block（块）</p><p>2) 文件存取的最小单位: block（块），由多个扇区组成；block的大小常见的有1KB、2KB、4KB，在linux中常见设置为4KB，即连续8个扇区组成一个block</p><p>每个block只能存放一个文件，如果文件的大小比block大，会申请更多的block；如果文件的大小比block小，仍会占用一个block，剩余的空间会浪费</p><p>例：有1万个文件，大小为10B，block为4KB</p><p>理论上占用空间大小：10000 * 10B = 97.656MB</p><p>实际上占用空间大小：10000 * 4KB = 40GB</p><p><strong>superblock、inode 与 block</strong></p><p>操作系统对于文件数据的存放包括两个部分：1文件内容、2权限及文件属性</p><p>在硬盘分区中，还有一个超级区块（superblock）</p><p>1）superblock: 记录文件系统的整体信息，包括inode与block的总量、使用大小、剩余大小以及文件系统的格式与相关信息等</p><p>2）inode: 记录文件的属性、权限，同时会记录该文件的数据所在的block编号</p><p>3）block: 存储文件的内容</p><p><strong>inode 与 block</strong></p><p>每个inode与block都有编号，而每个文件都会占用一个inode，inode内则有文件数据放置的block号码；能够找到文件的inode就可以找到该文件所放置数据的block号码，从而读取文件内容</p><p>在格式化时可以指定默认的inode与block的大小；-b指定默认block值，-I指定默认inode值，例：mkfs.ext4 –b 4096 –I 256 /dev/sdb</p><hr><p>参考链接</p><ul><li><a href="http://www.mamicode.com/info-detail-2435756.html" target="_blank" rel="noopener">Linux磁盘管理（block与inode）</a></li></ul>]]></content>
    
    <summary type="html">
    
      Linux block 与 inode
    
    </summary>
    
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
  </entry>
  
  <entry>
    <title>监控 Yarn 上作业状态</title>
    <link href="http://yoursite.com/2020/01/07/Yarn-App-Monitor/"/>
    <id>http://yoursite.com/2020/01/07/Yarn-App-Monitor/</id>
    <published>2020-01-06T16:00:00.000Z</published>
    <updated>2020-02-03T10:02:20.405Z</updated>
    
    <content type="html"><![CDATA[<p>监控 yarn 上 spark 或者 mr 应用的存活状态</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>有开发人员有疑惑想监控 yarn 上 spark 或者 mr 应用的存活状态。</p><p>实际通过 Yarn 提供的 API 即可做到</p><h4 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h4><p><code>pom.xml</code> 文件，添加相关的配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">&lt;repositories&gt;</span><br><span class="line">    &lt;repository&gt;</span><br><span class="line">        &lt;id&gt;cloudera&lt;&#x2F;id&gt;</span><br><span class="line">        &lt;url&gt;https:&#x2F;&#x2F;repository.cloudera.com&#x2F;artifactory&#x2F;cloudera-repos&#x2F;&lt;&#x2F;url&gt;</span><br><span class="line">    &lt;&#x2F;repository&gt;</span><br><span class="line">&lt;&#x2F;repositories&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;hadoop-common&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;version&gt;3.0.0-cdh6.3.1&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;hadoop-client&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;version&gt;3.0.0-cdh6.3.1&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;hadoop-yarn-api&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;version&gt;3.0.0-cdh6.3.1&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;hadoop-yarn-client&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;version&gt;3.0.0-cdh6.3.1&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;&#x2F;dependencies&gt;</span><br></pre></td></tr></table></figure><p>具体实现代码其实很简单就是，通过 yarnclient 获取 resourcemanager 上 Application 状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.yarn.api.records.ApplicationReport;</span><br><span class="line">import org.apache.hadoop.yarn.api.records.YarnApplicationState;</span><br><span class="line">import org.apache.hadoop.yarn.client.api.YarnClient;</span><br><span class="line">import org.apache.hadoop.yarn.conf.YarnConfiguration;</span><br><span class="line">import org.apache.hadoop.yarn.exceptions.YarnException;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line">import java.util.Collections;</span><br><span class="line">import java.util.Comparator;</span><br><span class="line">import java.util.EnumSet;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">public class client &#123;</span><br><span class="line"></span><br><span class="line">    public static ApplicationReport getAppReport(String applictionName) &#123;</span><br><span class="line">        Configuration conf &#x3D; new YarnConfiguration();</span><br><span class="line">        &#x2F;&#x2F; 修改为 RM 的地址</span><br><span class="line">        conf.set(&quot;yarn.resourcemanager.address&quot;, &quot;master-240:8032&quot;);</span><br><span class="line">        conf.set(&quot;yarn.resourcemanager.admin.address&quot;, &quot;master-240:8033&quot;);</span><br><span class="line">        YarnClient yarnClient &#x3D; YarnClient.createYarnClient();</span><br><span class="line">        yarnClient.init(conf);</span><br><span class="line">        yarnClient.start();</span><br><span class="line">        EnumSet&lt;YarnApplicationState&gt; appStates &#x3D; EnumSet.noneOf(YarnApplicationState.class);</span><br><span class="line">        if (appStates.isEmpty()) &#123;</span><br><span class="line">            appStates.add(YarnApplicationState.RUNNING);</span><br><span class="line">            appStates.add(YarnApplicationState.ACCEPTED);</span><br><span class="line">            appStates.add(YarnApplicationState.SUBMITTED);</span><br><span class="line">            appStates.add(YarnApplicationState.FINISHED);</span><br><span class="line">            appStates.add(YarnApplicationState.FAILED);</span><br><span class="line">            appStates.add(YarnApplicationState.KILLED);</span><br><span class="line">        &#125;</span><br><span class="line">        List&lt;ApplicationReport&gt; appsReports &#x3D; null;</span><br><span class="line">        try &#123;</span><br><span class="line">            &#x2F;&#x2F; 如果不指定YarnApplicationState的话就搜索所有状态 Application</span><br><span class="line">            &#x2F;&#x2F;appsReports &#x3D; yarnClient.getApplications(appStates);</span><br><span class="line">            appsReports &#x3D; yarnClient.getApplications();</span><br><span class="line">&#x2F;&#x2F;            appsReports.sort(new Comparator&lt;ApplicationReport&gt;() &#123;</span><br><span class="line">&#x2F;&#x2F;                @Override</span><br><span class="line">&#x2F;&#x2F;                public int compare(ApplicationReport o1, ApplicationReport o2) &#123;</span><br><span class="line">&#x2F;&#x2F;                    return -o1.getApplicationId().compareTo(o2.getApplicationId());</span><br><span class="line">&#x2F;&#x2F;                &#125;</span><br><span class="line">&#x2F;&#x2F;            &#125;);</span><br><span class="line">&#x2F;&#x2F;            appsReports.sort((ApplicationReport o1, ApplicationReport o2) -&gt; -(o1.getApplicationId().compareTo(o2.getApplicationId())));</span><br><span class="line">&#x2F;&#x2F;            Collections.sort(appsReports, Comparator.comparing(ApplicationReport::getApplicationId).reversed());</span><br><span class="line">            &#x2F;&#x2F; 反转 applicationId</span><br><span class="line">            Comparator&lt;ApplicationReport&gt; comp &#x3D; (ApplicationReport o1, ApplicationReport o2) -&gt; o1.getApplicationId().compareTo(o2.getApplicationId());</span><br><span class="line">            appsReports.sort(comp.reversed());</span><br><span class="line">        &#125; catch (YarnException | IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        assert appsReports !&#x3D; null;</span><br><span class="line">        ApplicationReport aimAppReport &#x3D; null;</span><br><span class="line">        for (ApplicationReport appReport : appsReports) &#123;</span><br><span class="line">            &#x2F;&#x2F; 获取任务</span><br><span class="line">            String appName &#x3D; appReport.getName();</span><br><span class="line">            if (appName.equals(applictionName)) &#123;</span><br><span class="line">                aimAppReport &#x3D; appReport;</span><br><span class="line">                break;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        yarnClient.stop();</span><br><span class="line">        return aimAppReport;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args)&#123;</span><br><span class="line">        ApplicationReport applicationReport &#x3D; getAppReport(&quot;Spark shell&quot;);</span><br><span class="line">        System.out.println(&quot;ApplicationId &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt; &quot;+ applicationReport.getApplicationId());</span><br><span class="line">        System.out.println(&quot;YarnApplicationState &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt; &quot;+ applicationReport.getYarnApplicationState());</span><br><span class="line">        System.out.println(&quot;name &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt; &quot;+ applicationReport.getName());</span><br><span class="line">        System.out.println(&quot;queue &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt; &quot;+ applicationReport.getQueue());</span><br><span class="line">        System.out.println(&quot;queue &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt; &quot;+ applicationReport.getUser());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这样，我们通过app name字段可以获取到存活的 spark 等任务</p><p><img src="/images/blog/2020-01-07-1.png" alt></p><p>客户端连接还可以将 yarn-site.xml 放到 resources 目录下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Configuration conf &#x3D; new YarnConfiguration();</span><br><span class="line">&#x2F;&#x2F; 这里将会加载 yarn 配置，配置地址如果是 hostname 的话，要在 client 机器上 hosts 文件配置好 host ip 映射</span><br><span class="line">conf.addResource(new Path(&quot;src&#x2F;main&#x2F;resources&#x2F;yarn-site.xml&quot;));</span><br></pre></td></tr></table></figure><p>如果是高可用配置的话还可以配置参数的形式访问</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Configuration conf &#x3D; new YarnConfiguration();</span><br><span class="line">conf.set(&quot;yarn.resourcemanager.ha.enabled&quot;, &quot;true&quot;);</span><br><span class="line">&#x2F;&#x2F; 下方三个参数每个 yarn 集群不一样，可以从 yarn-site.xml 中寻找</span><br><span class="line">conf.set(&quot;yarn.resourcemanager.ha.rm-ids&quot;, &quot;rm61,rm54&quot;);</span><br><span class="line">conf.set(&quot;yarn.resourcemanager.address.rm54&quot;, &quot;192.168.1.229:8032&quot;);</span><br><span class="line">conf.set(&quot;yarn.resourcemanager.address.rm61&quot;, &quot;192.168.1.228:8032&quot;);</span><br></pre></td></tr></table></figure><hr><p>参考链接</p><ul><li><a href="https://blog.csdn.net/rlnLo2pNEfx9c/article/details/83353496" target="_blank" rel="noopener">写个yarn的监控</a></li><li><a href="https://blog.csdn.net/qq_29493353/article/details/85788171" target="_blank" rel="noopener">yarn通过客户端提交application</a></li><li><a href="https://www.jianshu.com/p/c1fb266e4809" target="_blank" rel="noopener">YARN HA</a></li></ul>]]></content>
    
    <summary type="html">
    
      监控 Yarn 上作业状态
    
    </summary>
    
    
      <category term="Yarn" scheme="http://yoursite.com/categories/Yarn/"/>
    
    
  </entry>
  
  <entry>
    <title>Spark spark.yarn.jars 使用说明</title>
    <link href="http://yoursite.com/2020/01/06/Spark-yarn-jar/"/>
    <id>http://yoursite.com/2020/01/06/Spark-yarn-jar/</id>
    <published>2020-01-05T16:00:00.000Z</published>
    <updated>2020-02-03T10:02:20.402Z</updated>
    
    <content type="html"><![CDATA[<p>spark 优化将依赖包传入HDFS 使用 spark.yarn.jar </p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>问题描述请转移 <a href="https://blog.csdn.net/weizhonggui/article/details/85240804" target="_blank" rel="noopener">十：WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set,解决案例</a></p><p>启动 Spark 任务时，在没有配置 spark.yarn.archive 或者 spark.yarn.jars 时， 会看到不停地上传jar，非常耗时</p><h4 id="处理"><a href="#处理" class="headerlink" title="处理"></a>处理</h4><p>如果使用了 spark.yarn.archive 配置将会替换 spark.yarn.jars 的配置，所以这里使用<br>spark.yarn.jars 可以大大地减少任务的启动时间，整个处理过程如下。</p><p>上传依赖jar包, <code>/user/spark/jars</code> 为 hfds 上的目录，如果没有自行新建；<br>spark/jars/* 为 spark 服务自带的 jar 包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -put spark&#x2F;jars&#x2F;* &#x2F;user&#x2F;spark&#x2F;jars</span><br></pre></td></tr></table></figure><p>配置spark-defaut.conf，下方 <code>hdfs://reh</code> 为我这 HDFS nameserver 名字，你自行改成你自己的</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.yarn.jars&#x3D;local:&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.3.1-1.cdh6.3.1.p0.1470567&#x2F;lib&#x2F;spark&#x2F;jars&#x2F;*,local:&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.3.1-1.cdh6.3.1.p0.1470567&#x2F;lib&#x2F;spark&#x2F;hive&#x2F;*,hdfs:&#x2F;&#x2F;reh&#x2F;user&#x2F;spark&#x2F;jars&#x2F;*.jar</span><br></pre></td></tr></table></figure><p>注：本地配置local,hdfs标记为hdfs目录即可</p><hr><p>参考链接</p><ul><li><a href="https://blog.csdn.net/weizhonggui/article/details/85240804" target="_blank" rel="noopener">十：WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set,解决案例</a></li><li><a href="https://www.cnblogs.com/yyy-blog/p/11110388.html" target="_blank" rel="noopener">spark优化——依赖包传入HDFS_spark.yarn.jar和spark.yarn.archive的使用</a></li></ul>]]></content>
    
    <summary type="html">
    
      Spark spark.yarn.jars 使用说明
    
    </summary>
    
    
      <category term="Spark" scheme="http://yoursite.com/categories/Spark/"/>
    
    
  </entry>
  
</feed>
