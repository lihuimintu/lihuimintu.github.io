<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>lihuimintu</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-02-03T04:35:39.971Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>图</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>HDFS 处理小文件方法</title>
    <link href="http://yoursite.com/2020/02/01/Page-Fault/"/>
    <id>http://yoursite.com/2020/02/01/Page-Fault/</id>
    <published>2020-01-31T16:00:00.000Z</published>
    <updated>2020-02-03T04:35:39.971Z</updated>
    
    <content type="html"><![CDATA[<p>Hadoop 中处理小文件</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>HDFS 随着使用时间会存在小文件现象，这里将网上调研的处理方法收集一下</p><h4 id><a href="#" class="headerlink" title></a></h4><p>什么是小文件？小文件是怎么来的？可以参阅<a href="https://mp.weixin.qq.com/s/QdaNFfq37bBlAQtNDGLmVw" target="_blank" rel="noopener">如何在Hadoop中处理小文件</a></p><p>利用 HAR 方法处理小文件可以参阅<a href="https://mp.weixin.qq.com/s/rlM3ZPmWz-ZT72w59lvrRw" target="_blank" rel="noopener">0508-如何使用Hadoop的Archive处理小文件</a></p><p>利用 Hive 方法处理小文件可以参阅<a href="https://mp.weixin.qq.com/s/xw6MjNvpI97m0aygyW1HaA" target="_blank" rel="noopener">如何在Hadoop中处理小文件-续</a>、<a href="https://mp.weixin.qq.com/s/ki2k9fBPco6PwMZEzff60A" target="_blank" rel="noopener">0704-5.16.2-如何使用Hive合并小文件</a></p><p>利用 Impala 方法处理小文件可以参阅<a href="https://mp.weixin.qq.com/s/BTBqUqfbczq08LKRCuC_Ig" target="_blank" rel="noopener">如何使用Impala合并小文件</a></p><p>在有赞使用 SparkSQL 中提到存在小文件现象，采用社区 <a href="https://issues.apache.org/jira/browse/SPARK-24940" target="_blank" rel="noopener">SPARK-24940</a>方式处理，借助 SQL hint 的方式合并小文件。参阅<a href="https://www.iteblog.com/archives/2501.html" target="_blank" rel="noopener">Spark SQL 查询中 Coalesce 和 Repartition 暗示（Hint）</a></p><p>利用 Spark 处理小文件跟 Spark SQL 也是差不多的，通过 Spark 的 coalesce() 方法和 repartition() 方法，设置并行个数可以通过输入文件的总大小和期望输出文件的大小进行预计算而得。</p><hr><p>参考链接</p><ul><li><a href="http://bigdatadecode.club/HDFS-little-file-action.html" target="_blank" rel="noopener">HDFS小文件合并实战</a></li><li><a href="https://juejin.im/post/5c3f2713f265da61285a5b75#heading-8" target="_blank" rel="noopener">SparkSQL 在有赞的实践</a></li><li><a href="https://blog.csdn.net/weixin_37944880/article/details/86694829" target="_blank" rel="noopener">Spark小文件合并</a></li></ul>]]></content>
    
    <summary type="html">
    
      Hadoop 中处理小文件
    
    </summary>
    
    
      <category term="HDFS" scheme="http://yoursite.com/categories/HDFS/"/>
    
    
  </entry>
  
  <entry>
    <title>HDFS 处理小文件方法</title>
    <link href="http://yoursite.com/2020/01/28/HDFS-small-file/"/>
    <id>http://yoursite.com/2020/01/28/HDFS-small-file/</id>
    <published>2020-01-27T16:00:00.000Z</published>
    <updated>2020-02-03T04:35:39.971Z</updated>
    
    <content type="html"><![CDATA[<p>Hadoop 中处理小文件</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>HDFS 随着使用时间会存在小文件现象，这里将网上调研的处理方法收集一下</p><h4 id><a href="#" class="headerlink" title></a></h4><p>什么是小文件？小文件是怎么来的？可以参阅<a href="https://mp.weixin.qq.com/s/QdaNFfq37bBlAQtNDGLmVw" target="_blank" rel="noopener">如何在Hadoop中处理小文件</a></p><p>利用 HAR 方法处理小文件可以参阅<a href="https://mp.weixin.qq.com/s/rlM3ZPmWz-ZT72w59lvrRw" target="_blank" rel="noopener">0508-如何使用Hadoop的Archive处理小文件</a></p><p>利用 Hive 方法处理小文件可以参阅<a href="https://mp.weixin.qq.com/s/xw6MjNvpI97m0aygyW1HaA" target="_blank" rel="noopener">如何在Hadoop中处理小文件-续</a>、<a href="https://mp.weixin.qq.com/s/ki2k9fBPco6PwMZEzff60A" target="_blank" rel="noopener">0704-5.16.2-如何使用Hive合并小文件</a></p><p>利用 Impala 方法处理小文件可以参阅<a href="https://mp.weixin.qq.com/s/BTBqUqfbczq08LKRCuC_Ig" target="_blank" rel="noopener">如何使用Impala合并小文件</a></p><p>在有赞使用 SparkSQL 中提到存在小文件现象，采用社区 <a href="https://issues.apache.org/jira/browse/SPARK-24940" target="_blank" rel="noopener">SPARK-24940</a>方式处理，借助 SQL hint 的方式合并小文件。参阅<a href="https://www.iteblog.com/archives/2501.html" target="_blank" rel="noopener">Spark SQL 查询中 Coalesce 和 Repartition 暗示（Hint）</a></p><p>利用 Spark 处理小文件跟 Spark SQL 也是差不多的，通过 Spark 的 coalesce() 方法和 repartition() 方法，设置并行个数可以通过输入文件的总大小和期望输出文件的大小进行预计算而得。</p><hr><p>参考链接</p><ul><li><a href="http://bigdatadecode.club/HDFS-little-file-action.html" target="_blank" rel="noopener">HDFS小文件合并实战</a></li><li><a href="https://juejin.im/post/5c3f2713f265da61285a5b75#heading-8" target="_blank" rel="noopener">SparkSQL 在有赞的实践</a></li><li><a href="https://blog.csdn.net/weixin_37944880/article/details/86694829" target="_blank" rel="noopener">Spark小文件合并</a></li></ul>]]></content>
    
    <summary type="html">
    
      Hadoop 中处理小文件
    
    </summary>
    
    
      <category term="HDFS" scheme="http://yoursite.com/categories/HDFS/"/>
    
    
  </entry>
  
  <entry>
    <title>HDFS 中 atime 与 mtime 解析</title>
    <link href="http://yoursite.com/2020/01/27/HDFS-atime-ctime/"/>
    <id>http://yoursite.com/2020/01/27/HDFS-atime-ctime/</id>
    <published>2020-01-26T16:00:00.000Z</published>
    <updated>2020-02-03T04:35:39.969Z</updated>
    
    <content type="html"><![CDATA[<p>HDFS 中 atime 和 mtime</p><hr><h4 id="转载来源"><a href="#转载来源" class="headerlink" title="转载来源"></a>转载来源</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">作者：混绅士</span><br><span class="line">链接：http:&#x2F;&#x2F;bigdatadecode.club&#x2F;HDFS%E4%B8%ADatime%E5%92%8Cmtime.html</span><br><span class="line">来源：HDFS中atime与mtime解析</span><br></pre></td></tr></table></figure><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4>]]></content>
    
    <summary type="html">
    
      HDFS 中 atime 与 mtime 解析
    
    </summary>
    
    
      <category term="HDFS" scheme="http://yoursite.com/categories/HDFS/"/>
    
    
  </entry>
  
  <entry>
    <title>Hive set 命令使用</title>
    <link href="http://yoursite.com/2020/01/27/Hive-set/"/>
    <id>http://yoursite.com/2020/01/27/Hive-set/</id>
    <published>2020-01-26T16:00:00.000Z</published>
    <updated>2020-02-03T04:35:39.971Z</updated>
    
    <content type="html"><![CDATA[<p>Hive set 命令使用</p><hr><h4 id="转载来源"><a href="#转载来源" class="headerlink" title="转载来源"></a>转载来源</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">作者：lwf006164</span><br><span class="line">链接：https:&#x2F;&#x2F;blog.csdn.net&#x2F;lwf006164&#x2F;article&#x2F;details&#x2F;96754526</span><br><span class="line">来源：Hive篇.set命令使用</span><br></pre></td></tr></table></figure><h4 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h4><p>Hive 命令行下执行 set 命令，仅当前会话有效</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 设置显示当前使用的数据库</span><br><span class="line">set hive.cli.print.current.db&#x3D;true;</span><br><span class="line"></span><br><span class="line">#  设置显示表名</span><br><span class="line">set hive.cli.print.header&#x3D;true;</span><br></pre></td></tr></table></figure><p>Hive 脚本中配置 set 命令，当前机器用户有效   </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 进入编辑模式，在执行 hive 命令进入 shell 命令行的时候默认会加载这个脚本</span><br><span class="line">vi ~&#x2F;.hiverc</span><br><span class="line"></span><br><span class="line"># 将下面的两行命令添加到该文件中</span><br><span class="line"># 设置显示当前使用的数据库</span><br><span class="line">set hive.cli.print.current.db&#x3D;true;</span><br><span class="line"></span><br><span class="line"># 设置显示表名</span><br><span class="line">set hive.cli.print.header&#x3D;true;</span><br></pre></td></tr></table></figure><p>查看 Hive 历史操作命令集可以浏览 <code>~/.hivehistory</code></p>]]></content>
    
    <summary type="html">
    
      Hive set 命令使用
    
    </summary>
    
    
      <category term="Hive" scheme="http://yoursite.com/categories/Hive/"/>
    
    
  </entry>
  
  <entry>
    <title>Kafka 零拷贝原理</title>
    <link href="http://yoursite.com/2020/01/24/Kafka-Zero-Copy/"/>
    <id>http://yoursite.com/2020/01/24/Kafka-Zero-Copy/</id>
    <published>2020-01-23T16:00:00.000Z</published>
    <updated>2020-02-03T04:35:39.969Z</updated>
    
    <content type="html"><![CDATA[<p>从字面意思理解就是数据不需要来回的拷贝，大大提升了系统的性能</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>“零拷贝”是指计算机操作的过程中，CPU 不需要为数据在内存之间的拷贝消耗资源。而它通常是指计算机在网络上发送文件时，不需要将文件内容拷贝到用户空间（User Space）而直接在内核空间（Kernel Space）中传输到网络的方式。</p><p>零拷贝并不是不需要拷贝，而是减少不必要的拷贝次数。通常是说在 IO 读写过程中</p><h4 id="缓冲区"><a href="#缓冲区" class="headerlink" title="缓冲区"></a>缓冲区</h4><p>缓冲区是所有 I/O 的基础，I/O 讲的无非就是把数据移进或移出缓冲区</p><p>进程执行I/O操作，就是向操作系统发出请求，让它要么把缓冲区的数据排干(写)，要么填充缓冲区(读)</p><p>下面看一个 Java 进程发起读请求加载数据大致的流程图</p><p><img src="/images/blog/2020-01-24-1.png" alt>{:height=”80%” width=”80%”}</p><ol><li><p>进程发起读请求之后，内核接收到读请求之后，会先检查内核空间中是否已经存在进程所需要的数据，<br>如果已经存在，则直接把数据拷贝给进程的缓冲区</p></li><li><p>如果内核缓冲区没有命中随即向磁盘控制器发出命令，要求从磁盘读取数据，磁盘控制器把数据直接写入内核read缓冲区，这一步通过DMA完成</p></li><li><p>接下来内核将数据拷贝到进程的缓冲区</p></li></ol><p>DMA 是一种硬件和软件之间的数据传输的技术，且 DMA 进行数据传输的过程中几乎不需要 CPU 参与</p><h4 id="虚拟内存"><a href="#虚拟内存" class="headerlink" title="虚拟内存"></a>虚拟内存</h4><p>所有现代操作系统都使用虚拟内存，使用虚拟的地址取代物理地址，这样有两个好处:</p><ol><li>一个以上的虚拟地址可以指向同一个物理内存地址</li><li>虚拟内存空间可大于实际可用的物理地址</li></ol><p>利用第一条特性可以把内核空间地址和用户空间的虚拟地址映射到同一个物理地址，<br>这样DMA就可以填充对内核和用户空间进程同时可见的缓冲区了，大致如下图所示:</p><p><img src="/images/blog/2020-01-24-3.png" alt>{:height=”80%” width=”80%”}</p><h4 id="零拷贝技术"><a href="#零拷贝技术" class="headerlink" title="零拷贝技术"></a>零拷贝技术</h4><p>零拷贝技术可以阅读下方链接</p><ul><li><a href="https://www.jianshu.com/p/497e7640b57c" target="_blank" rel="noopener">零拷贝的原理及Java实现</a></li><li><a href="https://blog.csdn.net/lzb348110175/article/details/100853071" target="_blank" rel="noopener">Kafka 的零拷贝技术</a></li><li><a href="https://mp.weixin.qq.com/s/otuUvACiVDafGgDl6xNd2A" target="_blank" rel="noopener">面试被问到“零拷贝”！你真的理解吗？</a></li><li><a href="https://mp.weixin.qq.com/s/vaKRVvfUnFjhHzfkwZDbKQ" target="_blank" rel="noopener">图解Kafka的零拷贝技术到底有多牛？</a></li><li><a href="https://blog.csdn.net/ljheee/article/details/99652448" target="_blank" rel="noopener">Kafka零拷贝</a></li></ul>]]></content>
    
    <summary type="html">
    
      Kafka 零拷贝原理
    
    </summary>
    
    
      <category term="Kafka" scheme="http://yoursite.com/categories/Kafka/"/>
    
    
  </entry>
  
  <entry>
    <title>HDFS Federation 发展历程</title>
    <link href="http://yoursite.com/2020/01/22/HDFS-Federation/"/>
    <id>http://yoursite.com/2020/01/22/HDFS-Federation/</id>
    <published>2020-01-21T16:00:00.000Z</published>
    <updated>2020-02-03T04:35:39.968Z</updated>
    
    <content type="html"><![CDATA[<p>HDFS Federation（联邦）相关知识，如 HDFS Router-based Federation</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>在阅读一些 Hadoop3 新特性的文章中，我看到有提”多 NameNode”，然后简单某度搜索到的链接都是提 HDFS Federation（联邦）</p><p><img src="/images/blog/2020-01-22-2.png" alt>{:height=”80%” width=”80%”}</p><p>下意识以为”多 NameNode”指的是 Federation 正式在 Hadoop3 上实现可用，<br>在一次面试过程中跟面试官扯 Hadoop3 Federation 新特性，人家反问一句这不是 Hadoop2 就有了吗。。。我瞬间懵了。</p><p>事后回顾到才知道 HDFS RBF 才是 Hadoop3 Router-based Federation 新特性。”多 NameNode” 是指 HDFS HA 支持多 Standby 节点机制 <a href="https://issues.apache.org/jira/browse/HDFS-6440" target="_blank" rel="noopener">HDFS-6440</a></p><p>在配置平衡策略时看到有个”BlockPool”，想到这是跟 Federation 有关，因此想整理一下对这方面的知识程度</p><p><img src="/images/blog/2020-01-22-1.png" alt>{:height=”80%” width=”80%”}</p><h4 id="Federation"><a href="#Federation" class="headerlink" title="Federation"></a>Federation</h4><p>对 Federation 学习个人大部分是通过以下链接学习了解，大家可以按链接顺序自行学习。</p><ul><li><a href="https://mp.weixin.qq.com/s/8VrQFIuQUVEa20JfG2EZkQ" target="_blank" rel="noopener">HDFS Federation（联邦）简介</a></li><li></li><li><a href="https://mp.weixin.qq.com/s/YCzhYCv-fcbOHfM9wvMchg" target="_blank" rel="noopener">如何通过CM为HDFS启用Federation</a></li><li><a href="https://mp.weixin.qq.com/s/X-8DmvgAsLsFEZADYGklBA" target="_blank" rel="noopener">如何通过CM禁用Federation</a></li><li><a href="https://mp.weixin.qq.com/s/2QDnm54_ObpXMyD952GovQ" target="_blank" rel="noopener">HDFS Router-based Federation</a></li><li><a href="https://mp.weixin.qq.com/s/BPfhDxiSaUkoOSyrMuAHIw" target="_blank" rel="noopener">Router-Based HDFS Federation 在滴滴大数据的应用</a></li><li><a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs-rbf/HDFSRouterFederation.html" target="_blank" rel="noopener">HDFS Router-based Federation 官方文档</a></li></ul><p>由于 Router-based HDFS federation 还算比较新的特性，所以社区分了几个阶段修复或添加了一些新的功能，<br>社区在 2.9 和 3.0 版本中发布 HDFS RBF 方案解决统一命名空间问题，参见 <a href="https://issues.apache.org/jira/browse/HDFS-10467" target="_blank" rel="noopener">HDFS-10467</a>，<br>在 Apache Hadoop 3.2.0 版本修复或添加了一些功能，参见 <a href="https://issues.apache.org/jira/browse/HDFS-12615" target="_blank" rel="noopener">HDFS-12615</a>，<br>以及 Router-based HDFS federation 稳定性相关的 ISSUE <a href="https://issues.apache.org/jira/browse/HDFS-13891" target="_blank" rel="noopener">HDFS-13891</a>，<br>这个 ISSUE 可能会在 Apache Hadoop 3.3.0 版本发布(过往记忆大数据的<a href="https://mp.weixin.qq.com/s/0qY-LMHQT-yGJPh-ybbZUg" target="_blank" rel="noopener">Apache Hadoop 的 HDFS federation 前世今生</a>文末提的。。。不知道会不会发布)。</p><p>小米在<a href="https://mp.weixin.qq.com/s/2QDnm54_ObpXMyD952GovQ" target="_blank" rel="noopener">HDFS Router-based Federation</a>文末提了 Rebalance 社区没有实现跨子集群的迁移。<br>翻看官方文档的确有提:</p><blockquote><p>Some operations are not available in Router-based federation. The Router throws exceptions for those. Examples users may encounter include the following.</p><ul><li>Rename file/folder in two different nameservices.</li><li>Copy file/folder in two different nameservices.</li><li>Write into a file/folder being rebalanced.</li></ul></blockquote><p>但是跨集群的数据平衡已经在做了，可以参见 <a href="https://issues.apache.org/jira/browse/HDFS-13123" target="_blank" rel="noopener">HDFS-13123</a>。<br>小米自己也说内部实现了跨子集群的 Federation Rename，可以在未来将这一特性合并到社区的 Rebalance 方案中，期待ing。</p><hr><p>参考链接</p><ul><li><a href="https://mp.weixin.qq.com/s/CEIKrDZrR6EZIKJ9x2Se0Q" target="_blank" rel="noopener">HDFS3.2升级在滴滴的实践</a></li><li><a href="http://www.voidcn.com/article/p-umrtxeag-pk.html" target="_blank" rel="noopener">HDFS HA支持多Standby节点机制</a></li></ul>]]></content>
    
    <summary type="html">
    
      HDFS Federation 发展历程
    
    </summary>
    
    
      <category term="HDFS" scheme="http://yoursite.com/categories/HDFS/"/>
    
    
  </entry>
  
  <entry>
    <title>HBase 表存储层次</title>
    <link href="http://yoursite.com/2020/01/21/HBase-Table/"/>
    <id>http://yoursite.com/2020/01/21/HBase-Table/</id>
    <published>2020-01-20T16:00:00.000Z</published>
    <updated>2020-02-03T04:35:39.967Z</updated>
    
    <content type="html"><![CDATA[<p>对 HBase 表存储层次梳理</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>HBase 表是由一个或者多个 region 组成，一个 region 由一个或者多个列蔟组成，一个列蔟由一个 store 组成，一个 store 由唯一 memstore 加上一个或者多个 HFile 组成，HFile 又是由 block 组成，而 block 是由多个 cell 组成。</p><h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><h5 id="Region"><a href="#Region" class="headerlink" title="Region"></a>Region</h5><p>Region 类似于数据库的分片和分区的概念，每个 Region 负责一小部分 Rowkey 范围的数据的读写和维护，这样即使是一张巨大的表，由于被切割到不同的 Region，访问起来的时延也很低。<br>Region 包含了对应的起始行到结束行的所有信息。</p><p><img src="/images/blog/2020-01-21-1.png" alt>{:height=”80%” width=”80%”}</p><h5 id="Store"><a href="#Store" class="headerlink" title="Store"></a>Store</h5><p>Store 是存储所有写入表中信息的实体，并且当数据需要从表中读取时也将被用到。A Store is the same thing as a ColumnFamily. 一个 Store 等价一个列簇 </p><h5 id="Memstore"><a href="#Memstore" class="headerlink" title="Memstore"></a>Memstore</h5><p>HBase 是基于 LSM-Tree 模型的，所有的数据更新插入操作都首先写入 Memstore 中（同时会顺序写到日志HLog中），达到指定大小之后再将这些修改操作批量写入磁盘，生成一个新的 HFile 文件，这种设计可以极大地提升 HBase 的写入性能。每一个 Store 里面都有一个 Memstore, MemStore 保存对 Store 的内存修改。修改是 Cells / KeyValues。</p><h5 id="StoreFile"><a href="#StoreFile" class="headerlink" title="StoreFile"></a>StoreFile</h5><p>StoreFile 为 HFile 上的一层类封装。A StoreFile is a facade of HFile.</p><h5 id="HFile"><a href="#HFile" class="headerlink" title="HFile"></a>HFile</h5><p>HFile 对应的列蔟。当内存写满必须要刷新到磁盘时，HFile 就会被创建，所以 HFile 存放的是某个时刻的  MemStore 刷写时的快照，一个完整的行的数据可能存放在多个 HFile 里。 随着时间推移，HFile 最终会被压缩（即合并）成大文件。HFile 是 HBase 用来存储数据的文件格式。</p><p>HFile 由不同种类的 block 块组成（索引块和数据块）。HFile 存储在 HDFS 上，因此获得 HDFS 的持久化和多副本的优势。</p><h5 id="Block"><a href="#Block" class="headerlink" title="Block"></a>Block</h5><p>HFile 由 block 组成，不要与 HDFS 的 block 混淆了。一个 HDFS 数据块可以包含很多 HFile block。HFile block 通常在 8kb 和 1MB 之间，默认大小是 64kb(在建表语句中可以通过参数 BlockSize 指定)。如果一个表配置了压缩选项，HBase 仍会产生了 64kb 大小 block，然后压缩 block。根据数据的大小以及压缩格式，压缩后的 block 存储在磁盘的大小也不一样。</p><p>每个 block 的大小可以在创建表列簇的时候通过参数 <code>blocksize ＝&gt; &#39;65535&#39;</code> 进行指定，默认为 64k，大号的 Block 有利于顺序 Scan，小号 Block 利于随机查询，因而需要权衡。</p><p>如果将 block size 配置得很小，将会产生过多的 HFile block 索引，这将会给内存造成压力 （《HBase 实战》P27 《HBase应用架构》P19）</p><h5 id="KeyValue"><a href="#KeyValue" class="headerlink" title="KeyValue"></a>KeyValue</h5><p><img src="/images/blog/2020-01-21-2.png" alt>{:height=”80%” width=”80%”}</p><p>KeyValue 是 HBase 数据存储的核心，KeyValue 并不是简单的 KV 数据对，是一个具有复杂元素的结构体</p><p>由 keylength、valuelength、key、value 四个部分组成，其中 Key 又由 Row Length、Row、Column Family Length、Column Family、Column Qualifier、TimeStamp、Key Type 七部分组成。 </p><table><thead><tr><th>字段</th><th>说明</th><th>长度</th></tr></thead><tbody><tr><td>Key Length</td><td>存储Key的长度</td><td>4个字节</td></tr></tbody></table><p>Value Length | 存储Value的长度 | 4个字节</p><p>Row Length | 存储 Row 的长度，即 Rowkey 的长度 |  2个字节</p><p>Row | 存储 Rowkey | Row Length</p><p>Column Family Length | 存储列簇 Column Family 的长度 | 1个字节</p><p>Column Family | 存储 Column Family 实际内容 | Column Family Length</p><p>Column Qualifier | 存储 Column Qualifier 对应的数据 |  -          </p><p>Time Stamp | 存储时间戳 TimeStamp | 8个字节</p><p>Key Type | 存储 Key 类型 KeyType | 1个字节</p><p>Value | 存储单元格 Cell 对应的实际的值Value | Value Length</p><p>“key type” 字段代表不同可能的HBase操作</p><ul><li>Put</li><li>Delete</li><li>DeleteColumn</li><li>DeleteFamily</li></ul><p>KeyValue 保存了这个 value 的所有信息，这也是为什么面向列的原因</p><blockquote><p>Q: Row，Column Family 都有一个 length 标志，为什么 Column Qualifier 没有呢？<br>A: 由于 Key 中其它的字段占用大小已经知道，并且知道整个 Key 的大小，因此没有存储 Column Qualifier 的大小。 Column Qualifier 的 Length 可以由 key length 减去其他 length 得到，这样可以减少一定的存储量。</p></blockquote><hr><p>参考链接</p><ul><li><a href="https://blog.csdn.net/bitcarmanlee/article/details/78979836" target="_blank" rel="noopener">Hbase KeyValue结构详解</a></li><li><a href="https://blog.csdn.net/ping_hu/article/details/77115998" target="_blank" rel="noopener">HBase的KeyValue分析</a></li></ul>]]></content>
    
    <summary type="html">
    
      HBase 表存储层次
    
    </summary>
    
    
      <category term="HBase" scheme="http://yoursite.com/categories/HBase/"/>
    
    
  </entry>
  
  <entry>
    <title>Spark-Submit 参数说明</title>
    <link href="http://yoursite.com/2020/01/20/Spark-submit/"/>
    <id>http://yoursite.com/2020/01/20/Spark-submit/</id>
    <published>2020-01-19T16:00:00.000Z</published>
    <updated>2020-02-03T04:35:39.967Z</updated>
    
    <content type="html"><![CDATA[<p>spark-submit 详细参数说明</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>部分初级开发者需要使用 Spark-submit 提交 spark 作业到 yarn 上，经常问些参数设置的问题，<br>基于此需求梳理下</p><h4 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h4><p>在命令行输入 <code>spark-submit -h</code>，可以看到 spark-submit 的所用参数如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">$ bin&#x2F;spark-submit -h</span><br><span class="line">Usage: spark-submit [options] &lt;app jar | python file | R file&gt; [app arguments]</span><br><span class="line">Usage: spark-submit --kill [submission ID] --master [spark:&#x2F;&#x2F;...]</span><br><span class="line">Usage: spark-submit --status [submission ID] --master [spark:&#x2F;&#x2F;...]</span><br><span class="line">Usage: spark-submit run-example [options] example-class [example args]</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">  --master MASTER_URL         spark:&#x2F;&#x2F;host:port, mesos:&#x2F;&#x2F;host:port, yarn,</span><br><span class="line">                              k8s:&#x2F;&#x2F;https:&#x2F;&#x2F;host:port, or local (Default: local[*]).</span><br><span class="line">  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (&quot;client&quot;) or</span><br><span class="line">                              on one of the worker machines inside the cluster (&quot;cluster&quot;)</span><br><span class="line">                              (Default: client).</span><br><span class="line">  --class CLASS_NAME          Your application&#39;s main class (for Java &#x2F; Scala apps).</span><br><span class="line">  --name NAME                 A name of your application.</span><br><span class="line">  --jars JARS                 Comma-separated list of jars to include on the driver</span><br><span class="line">                              and executor classpaths.</span><br><span class="line">  --packages                  Comma-separated list of maven coordinates of jars to include</span><br><span class="line">                              on the driver and executor classpaths. Will search the local</span><br><span class="line">                              maven repo, then maven central and any additional remote</span><br><span class="line">                              repositories given by --repositories. The format for the</span><br><span class="line">                              coordinates should be groupId:artifactId:version.</span><br><span class="line">  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while</span><br><span class="line">                              resolving the dependencies provided in --packages to avoid</span><br><span class="line">                              dependency conflicts.</span><br><span class="line">  --repositories              Comma-separated list of additional remote repositories to</span><br><span class="line">                              search for the maven coordinates given with --packages.</span><br><span class="line">  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place</span><br><span class="line">                              on the PYTHONPATH for Python apps.</span><br><span class="line">  --files FILES               Comma-separated list of files to be placed in the working</span><br><span class="line">                              directory of each executor. File paths of these files</span><br><span class="line">                              in executors can be accessed via SparkFiles.get(fileName).</span><br><span class="line"></span><br><span class="line">  --conf PROP&#x3D;VALUE           Arbitrary Spark configuration property.</span><br><span class="line">  --properties-file FILE      Path to a file from which to load extra properties. If not</span><br><span class="line">                              specified, this will look for conf&#x2F;spark-defaults.conf.</span><br><span class="line"></span><br><span class="line">  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).</span><br><span class="line">  --driver-java-options       Extra Java options to pass to the driver.</span><br><span class="line">  --driver-library-path       Extra library path entries to pass to the driver.</span><br><span class="line">  --driver-class-path         Extra class path entries to pass to the driver. Note that</span><br><span class="line">                              jars added with --jars are automatically included in the</span><br><span class="line">                              classpath.</span><br><span class="line"></span><br><span class="line">  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).</span><br><span class="line"></span><br><span class="line">  --proxy-user NAME           User to impersonate when submitting the application.</span><br><span class="line">                              This argument does not work with --principal &#x2F; --keytab.</span><br><span class="line"></span><br><span class="line">  --help, -h                  Show this help message and exit.</span><br><span class="line">  --verbose, -v               Print additional debug output.</span><br><span class="line">  --version,                  Print the version of current Spark.</span><br><span class="line"></span><br><span class="line"> Cluster deploy mode only:</span><br><span class="line">  --driver-cores NUM          Number of cores used by the driver, only in cluster mode</span><br><span class="line">                              (Default: 1).</span><br><span class="line"></span><br><span class="line"> Spark standalone or Mesos with cluster deploy mode only:</span><br><span class="line">  --supervise                 If given, restarts the driver on failure.</span><br><span class="line">  --kill SUBMISSION_ID        If given, kills the driver specified.</span><br><span class="line">  --status SUBMISSION_ID      If given, requests the status of the driver specified.</span><br><span class="line"></span><br><span class="line"> Spark standalone and Mesos only:</span><br><span class="line">  --total-executor-cores NUM  Total cores for all executors.</span><br><span class="line"></span><br><span class="line"> Spark standalone and YARN only:</span><br><span class="line">  --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode,</span><br><span class="line">                              or all available cores on the worker in standalone mode)</span><br><span class="line"></span><br><span class="line"> YARN-only:</span><br><span class="line">  --queue QUEUE_NAME          The YARN queue to submit to (Default: &quot;default&quot;).</span><br><span class="line">  --num-executors NUM         Number of executors to launch (Default: 2).</span><br><span class="line">                              If dynamic allocation is enabled, the initial number of</span><br><span class="line">                              executors will be at least NUM.</span><br><span class="line">  --archives ARCHIVES         Comma separated list of archives to be extracted into the</span><br><span class="line">                              working directory of each executor.</span><br><span class="line">  --principal PRINCIPAL       Principal to be used to login to KDC, while running on</span><br><span class="line">                              secure HDFS.</span><br><span class="line">  --keytab KEYTAB             The full path to the file that contains the keytab for the</span><br><span class="line">                              principal specified above. This keytab will be copied to</span><br><span class="line">                              the node running the Application Master via the Secure</span><br><span class="line">                              Distributed Cache, for renewing the login tickets and the</span><br><span class="line">                              delegation tokens periodically.</span><br></pre></td></tr></table></figure><p>相关意义如下:</p><table><thead><tr><th>参数名</th><th>默认值</th><th>参数说明</th></tr></thead><tbody><tr><td>–master</td><td>local[*]</td><td>master 的地址，提交任务到哪里执行</td></tr><tr><td>–deploy-mode</td><td>client</td><td>在本地 (client) 启动 driver 或在 cluster 上启动，默认是 client</td></tr><tr><td>–class</td><td>-</td><td>应用程序的主类，仅针对 java 或 scala 应用</td></tr><tr><td>–name</td><td>-</td><td>应用程序的名称</td></tr><tr><td>–jars</td><td>-</td><td>用逗号分隔的本地 jar 包，设置后，这些 jar 将包含在 driver 和 executor 的 classpath 下</td></tr><tr><td>–packages</td><td>-</td><td>包含在 driver 和 executor 的 classpath 中的 jar 的 maven 坐标</td></tr><tr><td>–exclude-packages</td><td>-</td><td>为了避免冲突 而指定不包含的 package</td></tr><tr><td>–repositories</td><td>-</td><td>逗号分隔的其他远程存储库列表，用于搜索用–packages给定的maven坐标</td></tr><tr><td>–conf PROP=VALUE</td><td>-</td><td>指定 spark 配置属性的值，例如 -conf spark.executor.extraJavaOptions=”-XX:MaxPermSize=256m”</td></tr><tr><td>–properties-file</td><td>-</td><td>加载的配置文件，默认为 conf/spark-defaults.conf</td></tr><tr><td>–driver-memory</td><td>1024M</td><td>Driver内存</td></tr><tr><td>–driver-cores</td><td>1</td><td>Driver 的核数。只能在 client 模式下使用</td></tr><tr><td>–driver-java-options</td><td>-</td><td>传给 driver 的额外的 Java 选项</td></tr><tr><td>–driver-library-path</td><td>-</td><td>传给 driver 的额外的库路径</td></tr><tr><td>–driver-class-path</td><td>-</td><td>传给 driver 的额外的类路径</td></tr><tr><td>–total-executor-cores</td><td>-</td><td>所有 executor 总共的核数。仅仅在 mesos 或者 standalone 下使用</td></tr><tr><td>–num-executors</td><td>2</td><td>启动的 executor 数量。在 yarn 下使用</td></tr><tr><td>–executor-core</td><td>1</td><td>每个 executor 的核数。在 yarn 或者 standalone下使用</td></tr><tr><td>–executor-memory</td><td>1G</td><td>每个 executor 的内存</td></tr><tr><td>–queue</td><td>“default”</td><td>提交到 Yarn 上队列</td></tr></tbody></table><h4 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h4><p>将官方的 example 包使用 client 模式提交到 Yarn 上</p><p><code>spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode client --driver-memory 4g --num-executors 2 --executor-memory 2g --executor-cores 2 spark-examples*.jar 10</code></p><p><code>client</code> 模式输出结果，会在控制台展示</p><p>使用 cluster 模式只需将 <code>--deploy-mode</code> 参数换成 <code>cluster</code>，使用 <code>cluster</code> 模式，控制台上面是没有输出结果的</p><p>需要利用 Yarn 来查看  </p><ul><li><code>yarn logs -applicationId {applicationId} &gt;&gt; yarnApp.log</code> </li><li>Yarn Web UI</li></ul><h4 id="遇到问题"><a href="#遇到问题" class="headerlink" title="遇到问题"></a>遇到问题</h4><p><strong><a href="https://www.cnblogs.com/itboys/p/10579352.html" target="_blank" rel="noopener">Spark代码中设置appName在client模式和cluster模式中不一样问题</a></strong></p><hr><p>参考链接</p><ul><li><a href="https://blog.csdn.net/qq_29303759/article/details/82659185" target="_blank" rel="noopener">spark-submit 详细参数说明</a></li><li><a href="https://blog.csdn.net/MASILEJFOAISEGJIAE/article/details/89317964" target="_blank" rel="noopener">Spark使用示例：分别使用client模式和cluster运行SparkPi程序</a></li></ul>]]></content>
    
    <summary type="html">
    
      spark-submit 详细参数说明
    
    </summary>
    
    
      <category term="Spark" scheme="http://yoursite.com/categories/Spark/"/>
    
    
  </entry>
  
  <entry>
    <title>YARN Memory CPU 配置</title>
    <link href="http://yoursite.com/2020/01/19/Yarn-Container/"/>
    <id>http://yoursite.com/2020/01/19/Yarn-Container/</id>
    <published>2020-01-18T16:00:00.000Z</published>
    <updated>2020-02-03T04:35:39.966Z</updated>
    
    <content type="html"><![CDATA[<p>介绍如何配置 YARN 对内存和CPU的使用</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>以下内容是我学习的理解，主要还是转载武基鹏的<a href="http://blog.itpub.net/30089851/viewspace-2127851/" target="_blank" rel="noopener">YARN的Memory和CPU调优配置详解</a>。</p><p>Hadoop YARN 同时支持内存和 CPU 两种资源的调度</p><p>YARN 作为一个资源调度器，应该考虑到集群里面每一台机子的计算资源，然后根据 application 申请的资源进行分配 Container</p><p>Container 是 YARN 里面资源分配的基本单位，一个 Container 就是一组分配的系统资源，现阶段只包含内存、CPU 系统资源 </p><p>Container 和集群节点的关系是一个节点会运行多个 Container，但一个 Container 不会跨节点</p><p>在 YARN 集群中，平衡内存、CPU、磁盘的资源的很重要的，作为一般建议，每个磁盘和每个核心允许两个容器可以为群集利用率提供最佳平衡。</p><p>确定群集节点的适当 YARN 内存配置时，从可用的硬件资源开始分析  </p><ul><li>RAM（内存量）</li><li>CORES（CPU内核数）</li><li>DISKS（磁盘数）</li></ul><p>关键参数默认值如下表:  </p><table><thead><tr><th>参数</th><th>默认值</th></tr></thead><tbody><tr><td>yarn.nodemanager.resource.memory-mb</td><td>-1 (代表NodeManager占机器总内存80%)</td></tr><tr><td>yarn.scheduler.minimum-allocation-mb</td><td>1024MB</td></tr><tr><td>yarn.scheduler.maximum-allocation-mb</td><td>8192MB</td></tr><tr><td>yarn.nodemanager.resource.cpu-vcores</td><td>-1 (代表值为8个虚拟CPU)</td></tr><tr><td>yarn.scheduler.minimum-allocation-vcores</td><td>1</td></tr><tr><td>yarn.scheduler.maximum-allocation-vcores</td><td>4</td></tr></tbody></table><h4 id="内存配置"><a href="#内存配置" class="headerlink" title="内存配置"></a>内存配置</h4><p>关于内存相关的配置可以参考 hortonwork 公司的文档<a href="https://docs.cloudera.com/HDPDocuments/HDP2/HDP-2.1.1/bk_installing_manually_book/content/rpm-chap1-11.html" target="_blank" rel="noopener">Determine HDP Memory Configuration Settings</a>来配置你的集群</p><p>YARN 可用的内存资源应该要考虑预留内存，预留内存是系统进程和其他Hadoop进程（例如HBase）所需的内存。</p><p>预留内存 = 操作系统内存预留 + HBase 内存预留（如果HBase在同一节点上）</p><h5 id="保留内存建议"><a href="#保留内存建议" class="headerlink" title="保留内存建议"></a>保留内存建议</h5><table><thead><tr><th>每个节点的总内存</th><th>建议的预留系统内存</th><th>建议的保留 HBase 内存</th></tr></thead><tbody><tr><td>4GB</td><td>1GB</td><td>1GB</td></tr><tr><td>8GB</td><td>2GB</td><td>1GB</td></tr><tr><td>16GB</td><td>2GB</td><td>2GB</td></tr><tr><td>24GB</td><td>4GB</td><td>4GB</td></tr><tr><td>48GB</td><td>6GB</td><td>8GB</td></tr><tr><td>64GB</td><td>8GB</td><td>8GB</td></tr><tr><td>72GB</td><td>8GB</td><td>8GB</td></tr><tr><td>96GB</td><td>12GB</td><td>16GB</td></tr><tr><td>128GB</td><td>24GB</td><td>24GB</td></tr><tr><td>256GB</td><td>32GB</td><td>32GB</td></tr><tr><td>512GB</td><td>64GB</td><td>64GB</td></tr></tbody></table><p>下一个计算是确定每个节点允许的最大容器数。可以使用以下公式:</p><blockquote><p>containers = min (2<em>CORES, 1.8</em>DISKS, (Total available RAM) / MIN_CONTAINER_SIZE)</p><p>说明:<br>CORES 为机器CPU核数<br>DISKS 为机器上挂载的磁盘个数<br>Total available RAM 为机器总内存<br>MIN_CONTAINER_SIZE 是指 container 最小的容量大小</p></blockquote><p><code>MIN_CONTAINER_SIZE</code> 这需要根据具体情况去设置，在较小的内存节点中，<br>最小容器大小也应该较小，下表概述了推荐值: </p><table><thead><tr><th>每台机子可用内存</th><th>container 最小值</th></tr></thead><tbody><tr><td>小于4GB</td><td>256MB</td></tr><tr><td>4GB到8GB之间</td><td>512MB</td></tr><tr><td>8GB到24GB之间</td><td>1024MB</td></tr><tr><td>大于24GB</td><td>2048MB</td></tr></tbody></table><p>每个 container 的平均使用内存大小计算方式为</p><blockquote><p>RAM-per-container = max(MIN_CONTAINER_SIZE, (Total Available RAM) / containers))</p></blockquote><table><thead><tr><th>配置文件</th><th>配置设置</th><th>默认值</th><th>计算值</th></tr></thead><tbody><tr><td>yarn-site.xml</td><td>yarn.nodemanager.resource.memory-mb</td><td>8192MB</td><td>= containers * RAM-per-container</td></tr><tr><td>yarn-site.xml</td><td>yarn.scheduler.minimum-allocation-mb</td><td>1024MB</td><td>= RAM-per-container</td></tr><tr><td>yarn-site.xml</td><td>yarn.scheduler.maximum-allocation-mb</td><td>8192MB</td><td>= containers * RAM-per-container</td></tr></tbody></table><p>举个例子: 对于 128G 内存、32核CPU 的机器，挂载了 7 个磁盘，根据上面的说明，系统保留内存为 24G，<br>不适应 HBase 情况下，系统剩余可用内存为 104G，计算 containers 值如下</p><blockquote><p>containers = min (2<em>32, 1.8</em> 7 , (128-24)/2) = min (64, 12.6 , 51) = 13</p></blockquote><p>计算 RAM-per-container 值如下</p><blockquote><p>RAM-per-container = max (2, (124-24)/13) = max (2, 8) = 8</p></blockquote><p>可以使用脚本 <a href="../codeRep/yarn-utils.py">yarn-utils.py</a> 来计算上面的值</p><p>执行下面命令，python 版本为 python2</p><blockquote><p>python2 yarn-utils.py -c 32 -m 128 -d 7 -k False </p></blockquote><p>这样的话，每个container内存为8G，似乎有点多，还是更愿意根据集群使用情况任务将其调整为2G内存，则集群中下面的参数配置值如下</p><table><thead><tr><th>配置文件</th><th>配置设置</th><th>计算值</th></tr></thead><tbody><tr><td>yarn-site.xml</td><td>yarn.nodemanager.resource.memory-mb</td><td>= 52 * 2 =104 G</td></tr><tr><td>yarn-site.xml</td><td>yarn.scheduler.minimum-allocation-mb</td><td>= 2G</td></tr><tr><td>yarn-site.xml</td><td>yarn.scheduler.maximum-allocation-mb</td><td>= 52 * 2 = 104G</td></tr></tbody></table><h4 id="CPU-配置"><a href="#CPU-配置" class="headerlink" title="CPU 配置"></a>CPU 配置</h4><p>YARN中目前的CPU被划分成虚拟CPU（CPU virtual Core），这里的虚拟CPU是YARN自己引入的概念，<br>初衷是，考虑到不同节点的CPU性能可能不同，每个CPU具有的计算能力也是不一样的，<br>比如某个物理CPU的计算能力可能是另外一个物理CPU的2倍，这时候，<br>你可以通过为第一个物理CPU多配置几个虚拟CPU弥补这种差异。<br>用户提交作业时，可以指定每个任务需要的虚拟CPU个数。</p><p>在YARN中，CPU相关配置参数如下</p><ul><li><code>yarn.nodemanager.resource.cpu-vcores</code> 表示能够分配给Container的CPU核数，默认配置为-1，代表值为8个虚拟CPU，推荐该值的设置和物理CPU的核数数量相同，若不够，则需要调小该值。</li><li><code>yarn.scheduler.minimum-allocation-vcores</code> 的默认值为1，表示每个Container容器在处理任务的时候可申请的最少CPU个数为1个。</li><li><code>yarn.scheduler.maximum-allocation-vcores</code> 的默认值为4，表示每个Container容器在处理任务的时候可申请的最大CPU个数为4个。</li></ul><p>在环境上建议留一个给操作系统</p><hr><p>参考链接</p><ul><li><a href="https://blog.csdn.net/qq_25302531/article/details/80623791" target="_blank" rel="noopener">浅谈YARN中Container容器（内存、CPU分配）</a></li><li><a href="http://blog.itpub.net/30089851/viewspace-2127851/" target="_blank" rel="noopener">YARN的Memory和CPU调优配置详解</a></li><li><a href="https://docs.cloudera.com/HDPDocuments/HDP2/HDP-2.1.1/bk_installing_manually_book/content/rpm-chap1-11.html" target="_blank" rel="noopener">9.确定HDP内存配置设置</a></li><li><a href="https://blog.csdn.net/suifeng3051/article/details/48135521" target="_blank" rel="noopener">Yarn 内存分配管理机制及相关参数配置</a></li></ul>]]></content>
    
    <summary type="html">
    
      YARN Memory CPU 配置
    
    </summary>
    
    
      <category term="Yarn" scheme="http://yoursite.com/categories/Yarn/"/>
    
    
  </entry>
  
  <entry>
    <title>记一次 HBase Master is initializing 问题处理</title>
    <link href="http://yoursite.com/2020/01/18/HBase-Master-is-initializing/"/>
    <id>http://yoursite.com/2020/01/18/HBase-Master-is-initializing/</id>
    <published>2020-01-17T16:00:00.000Z</published>
    <updated>2020-02-03T04:35:39.965Z</updated>
    
    <content type="html"><![CDATA[<p>HBase org.apache.hadoop.hbase.PleaseHoldException: Master is initializing 错误处理</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>测试集群上做了jar包替换验证测试，测试完之后还原jar，重启出现进入 hbase shell 中执行命令遇到问题。</p><p><img src="/images/blog/2020-01-18-1.png" alt>{:height=”80%” width=”80%”}</p><p><code>org.apache.hadoop.hbase.PleaseHoldException: Master is initializing</code> 代表Master正在初始化中，出现这种错误的原因有很多</p><p>关键还是得看 Master 日志，盲目搜索效率不高</p><p>一开始只是搜索 ERROR 日志，并没有看到很关键的信息，后面看其他博客贴的日志片段是 WARN，进而<br>转换思路看 WARN，一看这个是 meta 表未分配到 RS 上啊，之前处理过，用 hbck2 解决即可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">WARN org.apache.hadoop.hbase.master.HMaster: hbase:meta,,1.1588230740 is NOT online; state&#x3D;&#123;1588230740 state&#x3D;OPENING, </span><br><span class="line">ts&#x3D;1579334053311, server&#x3D;region-168-1-240,16020,1579334037426&#125;; ServerCrashProcedures&#x3D;true. Master startup cannot </span><br><span class="line">progress, in holding-pattern until region onlined.</span><br></pre></td></tr></table></figure><p>hbck2 配置参考<a href="https://lihuimintu.github.io/2019/12/16/hbase-hbck2/" target="_blank" rel="noopener">CDH-HBase 使用 HBCK2 运维</a>即可</p><p>手动去 assign 一下 meta 表即可，hbase:meta 表的 encoded name 是一个时间戳，比如上面日志的 encoded name 就是 <code>1588230740</code> ，<br>具体命令参考下方</p><p><code>hbase org.apache.hbase.HBCK2 assigns -o 1588230740</code></p><p>查看 Master 日志，可以看到成功分配</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">INFO org.apache.hadoop.hbase.master.procedure.MasterProcedureScheduler: Took xlock for pid&#x3D;7, state&#x3D;RUNNABLE:REGION_TRANSITION_QUEUE; AssignProcedure table&#x3D;hbase:meta, region&#x3D;1588230740, override&#x3D;true</span><br><span class="line">INFO org.apache.hadoop.hbase.master.assignment.AssignProcedure: Assigned, not reassigning rit&#x3D;OPEN, location&#x3D;region-168-1-240,16020,1579334037426</span><br><span class="line">INFO org.apache.hadoop.hbase.procedure2.ProcedureExecutor: Finished pid&#x3D;6, state&#x3D;SUCCESS; AssignProcedure table&#x3D;hbase:meta, region&#x3D;1588230740, override&#x3D;true in 160msec</span><br></pre></td></tr></table></figure><p>执行命令验证成功</p><p><img src="/images/blog/2020-01-18-2.png" alt>{:height=”80%” width=”80%”}</p><h4 id="吐槽"><a href="#吐槽" class="headerlink" title="吐槽"></a>吐槽</h4><p>之前听范老师说过: 运维三大法办- 监控、日志、源码。还是要细心</p><p>hbck2 的使用可以多看看官方文档说明，学习如何使用</p><hr><p>参考链接</p><ul><li><a href="https://blog.csdn.net/zzf1510711060/article/details/80919660" target="_blank" rel="noopener">HBase org.apache.hadoop.hbase.PleaseHoldException: Master is initializing 错误解决方法</a></li><li><a href="https://lihuimintu.github.io/2019/12/16/hbase-hbck2/" target="_blank" rel="noopener">CDH-HBase 使用 HBCK2 运维</a></li></ul>]]></content>
    
    <summary type="html">
    
      HBase Master is initializing 问题处理
    
    </summary>
    
    
      <category term="HBase" scheme="http://yoursite.com/categories/HBase/"/>
    
    
  </entry>
  
  <entry>
    <title>HBase CMS GC 配置参考</title>
    <link href="http://yoursite.com/2020/01/18/HBase-CMS/"/>
    <id>http://yoursite.com/2020/01/18/HBase-CMS/</id>
    <published>2020-01-17T16:00:00.000Z</published>
    <updated>2020-02-03T04:35:39.964Z</updated>
    
    <content type="html"><![CDATA[<p>HBase发展到当下，对其进行的各种优化从未停止，而 GC 优化更是其中的重中之重</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>HBase发展到当下，对其进行的各种优化从未停止，而 GC 优化更是其中的重中之重</p><p>参考自一下文章:</p><ul><li><a href="http://hbasefly.com/2016/05/21/hbase-gc-1/" target="_blank" rel="noopener">HBase GC的前生今世 – 身世篇</a></li><li><a href="http://hbasefly.com/2016/05/29/hbase-gc-2/" target="_blank" rel="noopener">HBase GC的前生今世 – 演进篇</a></li><li><a href="http://hbasefly.com/2016/08/09/hbase-cms-gc/" target="_blank" rel="noopener">HBase最佳实践－CMS GC调优</a></li><li><a href="https://mp.weixin.qq.com/s/GEwD1B-XqFIudWP_EbGgdQ" target="_blank" rel="noopener">HBase实战：记一次Safepoint导致长时间STW的踩坑之旅</a></li></ul><h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">-Xms32g -Xmx32g -Xmn512m </span><br><span class="line">-XX:+UseParNewGC </span><br><span class="line">-XX:ParallelGCThreads&#x3D;8 </span><br><span class="line">-XX:+UseConcMarkSweepGC</span><br><span class="line"> -XX:+UseCMSCompactAtFullCollection </span><br><span class="line">-XX:CMSFullGCsBeforeCompaction&#x3D;1 </span><br><span class="line">-XX:CMSInitiatingOccupancyFraction&#x3D;65 </span><br><span class="line">-XX:+UseCMSInitiatingOccupancyOnly </span><br><span class="line">-XX:+PrintGC </span><br><span class="line">-XX:+PrintGCDetails </span><br><span class="line">-XX:+PrintGCDateStamps </span><br><span class="line">-XX:+PrintGCApplicationStoppedTime </span><br><span class="line">-XX:+PrintHeapAtGC  </span><br><span class="line">-XX:+PrintTenuringDistribution </span><br><span class="line">-Xloggc:&#x2F;var&#x2F;log&#x2F;hbase&#x2F;gc-regionserver-hbase.log </span><br><span class="line">-XX:+UseGCLogFileRotation  </span><br><span class="line">-XX:NumberOfGCLogFiles&#x3D;10  </span><br><span class="line">-XX:GCLogFileSize&#x3D;100M</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      HBase CMS GC 配置参考
    
    </summary>
    
    
      <category term="HBase" scheme="http://yoursite.com/categories/HBase/"/>
    
    
  </entry>
  
  <entry>
    <title>记一次 Arthas 实操</title>
    <link href="http://yoursite.com/2020/01/17/Arthas/"/>
    <id>http://yoursite.com/2020/01/17/Arthas/</id>
    <published>2020-01-16T16:00:00.000Z</published>
    <updated>2020-02-03T04:35:39.958Z</updated>
    
    <content type="html"><![CDATA[<p>Arthas 是 Alibaba 开源的Java诊断工具，深受开发者喜爱</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>Arthas 是 Alibaba 开源的Java诊断工具，深受开发者喜爱。</p><p>Arthas 官方文档十分详细，本文也参考了官方文档内容，同时在开源在的 Github 的项目里的 Issues 里不仅有问题反馈，更有大量的使用案例，也可以进行学习参考。</p><p>开源地址: <a href="https://github.com/alibaba/arthas" target="_blank" rel="noopener">https://github.com/alibaba/arthas</a></p><p>官方文档: <a href="https://alibaba.github.io/arthas" target="_blank" rel="noopener">https://alibaba.github.io/arthas</a></p><h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p><img src="/images/blog/2020-01-17-1.png" alt>{:height=”80%” width=”80%”}</p><p>在使用 Yarn 跑测试任务时，发现中文输出乱码。</p><p>网上搜索到<a href="https://blog.csdn.net/wankunde/article/details/84658467" target="_blank" rel="noopener">hadoop web中查看文件内容乱码解决</a>，<br>感觉可以试试能不能解决问题，但是又不想把 hadoop-common 项目下载下来完整编译打包上传。</p><p>这时就想到 Arthas 好想可以解决这个问题，以前特地过了下基础教程学习使用了一下。但是一直未实战，感觉这次可以试试</p><h4 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h4><p>准备工作，下载相关文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 服务端下载这个类的 CDH6.3.1 的源码</span><br><span class="line">wget https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;cloudera&#x2F;hadoop-common&#x2F;cdh6.3.1&#x2F;hadoop-common-project&#x2F;hadoop-common&#x2F;src&#x2F;main&#x2F;java&#x2F;org&#x2F;apache&#x2F;hadoop&#x2F;http&#x2F;HtmlQuoting.java</span><br><span class="line"></span><br><span class="line"># 下载 Arthas</span><br><span class="line">wget https:&#x2F;&#x2F;alibaba.github.io&#x2F;arthas&#x2F;arthas-boot.jar</span><br></pre></td></tr></table></figure><p>修改 org.apache.hadoop.http.HtmlQuoting </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 导入该类</span><br><span class="line">import java.nio.charset.Charset;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; quoteHtmlChars 方法</span><br><span class="line">public static String quoteHtmlChars(String item) &#123;</span><br><span class="line">    if (item &#x3D;&#x3D; null) &#123;</span><br><span class="line">      return null;</span><br><span class="line">    &#125;</span><br><span class="line">    try &#123;</span><br><span class="line">      byte[] bytes &#x3D; item.getBytes(Charset.defaultCharset().name());</span><br><span class="line">      if (needsQuoting(bytes, 0, bytes.length)) &#123;</span><br><span class="line">        ByteArrayOutputStream buffer &#x3D; new ByteArrayOutputStream();</span><br><span class="line">        try &#123;</span><br><span class="line">          quoteHtmlChars(buffer, bytes, 0, bytes.length);</span><br><span class="line">          return buffer.toString(&quot;UTF-8&quot;);</span><br><span class="line">        &#125; catch (IOException ioe) &#123;</span><br><span class="line">          &#x2F;&#x2F; Won&#39;t happen, since it is a bytearrayoutputstream</span><br><span class="line">          return null;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        return item;</span><br><span class="line">     &#125;</span><br><span class="line">    &#125; catch (IOException zz) &#123;</span><br><span class="line">       return null;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>启动 Arthas</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sudo -u yarn java -jar &#x2F;opt&#x2F;arthas-boot.jar</span><br><span class="line"></span><br><span class="line"># 选择 RM、NM 进程，然后将其相关替换</span><br><span class="line"></span><br><span class="line"># 三板斧</span><br><span class="line">sc  -d *HtmlQuoting # 获取classLoaderHash</span><br><span class="line">mc -c 1963006a &#x2F;opt&#x2F;HbaseSessions.java -d &#x2F;opt</span><br><span class="line">redefine &#x2F;opt&#x2F;org&#x2F;apache&#x2F;hadoop&#x2F;http&#x2F;HtmlQuoting.class </span><br><span class="line"></span><br><span class="line"># 查看是否替换成功</span><br><span class="line">jad org.apache.hadoop.http.HtmlQuoting -c 1963006a</span><br></pre></td></tr></table></figure><p>启动 Spark 任务发现还是中文乱码，并没有解决问题，但是这次实操 Arthas 经历挺好的，熟悉下使用</p><h4 id="遇到问题"><a href="#遇到问题" class="headerlink" title="遇到问题"></a>遇到问题</h4><p><strong>AttachNotSupportedException</strong> <a href="https://github.com/alibaba/arthas/issues/440" target="_blank" rel="noopener">issue-440</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[root@region-168-1-228 ~]# java -jar arthas-boot.jar</span><br><span class="line">[INFO] arthas-boot version: 3.1.7</span><br><span class="line">[INFO] Found existing java process, please choose one and hit RETURN.</span><br><span class="line">* [1]: 13120 org.apache.hadoop.hdfs.server.datanode.DataNode</span><br><span class="line">  [2]: 13617 org.apache.hadoop.yarn.server.nodemanager.NodeManager</span><br><span class="line">  [3]: 13619 org.apache.hadoop.hbase.regionserver.HRegionServer</span><br><span class="line">  [4]: 13620 org.apache.hadoop.yarn.server.resourcemanager.ResourceManager</span><br><span class="line">  [5]: 1124 org.apache.zookeeper.server.quorum.QuorumPeerMain</span><br><span class="line">  [6]: 13118 org.apache.hadoop.hdfs.qjournal.server.JournalNode</span><br><span class="line">  [7]: 44015 com.cloudera.kafka.wrap.Kafka</span><br><span class="line">4</span><br><span class="line">[INFO] Start download arthas from remote server: https:&#x2F;&#x2F;maven.aliyun.com&#x2F;repository&#x2F;public&#x2F;com&#x2F;taobao&#x2F;arthas&#x2F;arthas-packaging&#x2F;3.1.7&#x2F;arthas-packaging-3.1.7-bin.zip</span><br><span class="line">[INFO] Download arthas success.</span><br><span class="line">[INFO] arthas home: &#x2F;root&#x2F;.arthas&#x2F;lib&#x2F;3.1.7&#x2F;arthas</span><br><span class="line">[INFO] Try to attach process 13620</span><br><span class="line">[ERROR] Start arthas failed, exception stack trace: </span><br><span class="line">com.sun.tools.attach.AttachNotSupportedException: Unable to open socket file: target process not responding or HotSpot VM not loaded</span><br><span class="line">        at sun.tools.attach.LinuxVirtualMachine.&lt;init&gt;(LinuxVirtualMachine.java:106)</span><br><span class="line">        at sun.tools.attach.LinuxAttachProvider.attachVirtualMachine(LinuxAttachProvider.java:78)</span><br><span class="line">        at com.sun.tools.attach.VirtualMachine.attach(VirtualMachine.java:250)</span><br><span class="line">        at com.taobao.arthas.core.Arthas.attachAgent(Arthas.java:85)</span><br><span class="line">        at com.taobao.arthas.core.Arthas.&lt;init&gt;(Arthas.java:28)</span><br><span class="line">        at com.taobao.arthas.core.Arthas.main(Arthas.java:123)</span><br><span class="line">[ERROR] attach fail, targetPid: 13620</span><br></pre></td></tr></table></figure><p><img src="/images/blog/2020-01-17-2.png" alt>{:height=”80%” width=”80%”}</p><p>根据意见参考，执行 arthas 使用 yarn 用户执行的解决</p><hr><p>参考链接</p><ul><li><a href="https://blog.csdn.net/wankunde/article/details/84658467" target="_blank" rel="noopener">hadoop web中查看文件内容乱码解决</a></li><li><a href="https://stackoverflow.com/questions/11277494/why-java-nio-charset-charsets-compile-error" target="_blank" rel="noopener">why java.nio.charset.Charsets compile error? </a></li></ul>]]></content>
    
    <summary type="html">
    
      记一次 Arthas 实操
    
    </summary>
    
    
      <category term="Tools" scheme="http://yoursite.com/categories/Tools/"/>
    
    
  </entry>
  
  <entry>
    <title>CDH 中使用 Hive on Spark</title>
    <link href="http://yoursite.com/2020/01/15/Hive-On-Spark/"/>
    <id>http://yoursite.com/2020/01/15/Hive-On-Spark/</id>
    <published>2020-01-14T16:00:00.000Z</published>
    <updated>2020-02-03T04:35:39.957Z</updated>
    
    <content type="html"><![CDATA[<p>Content here</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>使用 Hive insert SQL 后查看 Yarn 发现其跑的是 MR 方式</p><p><img src="/images/blog/2020-01-15-1.png" alt>{:height=”80%” width=”80%”}</p><p>这里想改用 Spark 引起来缩短 HiveQL 的响应时间</p><p>有两种方式</p><ul><li>SparkSQL</li><li>Hive on Spark</li></ul><p>两种方式都可以，看个人习惯</p><p>Hive on Spark 大体与 SparkSQL 结构类似，只是 SQL 引擎不同，但是计算引擎都是 Spark</p><p>本文主要介绍 Hive on Spark</p><h4 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h4><p>CDH Hive 配置中可以看到有官方的提示配置文档</p><p><img src="/images/blog/2020-01-15-2.png" alt>{:height=”80%” width=”80%”}</p><p>要将 Hive 配置为在 Spark 上运行，请执行以下两个步骤</p><ol><li>配置 Hive 依赖项为 Spark 服务</li><li>配置 Hive 客户端以使用 Spark 执行引擎</li></ol><p><strong>配置 Hive 依赖项为 Spark 服务</strong></p><p>按照官方文档操作即可</p><ol><li>在 Cloudera Manager 管理控制台中，转到 Hive 服务</li><li>单击配置选项卡</li><li>搜索 Spark On YARN 服务。要配置 Spark 服务，请选择 Spark 服务名称。要删除依赖项，请选择 none</li><li>点击保存更改。</li><li>进入Spark服务。</li><li>在 HiveServer2 所在的主机上添加 Spark 的 gateway 角色(即客户端)</li><li>重启 Hive、Spark 服务</li></ol><p><strong>配置 Hive 客户端以使用 Spark 执行引擎</strong></p><p>CDH 中的 Hive 支持两个执行引擎: MapReduce 和 Spark</p><p>要配置执行引擎，请执行以下步骤之一</p><p>beeline/hive: 运行 <code>set hive.execution.engine=engine</code> 命令，engine 选项要么wei <code>mr</code> 要么为 <code>spark</code>，<br>默认为 <code>mr</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">set hive.execution.engine&#x3D;spark;</span><br><span class="line"></span><br><span class="line"># 查看当前的设置执行引擎</span><br><span class="line">set hive.execution.engine;</span><br></pre></td></tr></table></figure><p>Cloudera Manager（影响所有查询，不推荐）:</p><ol><li>转到 Hive 服务</li><li>单击配置选项卡</li><li>搜索 “execution”</li><li>将”Default Execution Engine”属性设置为 MapReduce 或 Spark。默认值为 MapReduce</li><li>重启 Hive 服务</li></ol><h4 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h4><p>官方文档中提到性能</p><p><img src="/images/blog/2020-01-15-3.png" alt>{:height=”80%” width=”80%”}</p><p>暂未研究，有兴趣的可以自行看看</p><hr><p>参考链接</p><ul><li><a href="https://docs.cloudera.com/documentation/enterprise/latest/topics/admin_hos_oview.html" target="_blank" rel="noopener">在CDH中的Spark上运行Apache Hive</a></li><li><a href="https://docs.cloudera.com/documentation/enterprise/latest/topics/admin_hive_configure.html#concept_i3p_2lv_cv" target="_blank" rel="noopener">Hive执行引擎</a></li></ul>]]></content>
    
    <summary type="html">
    
      Hive on Spark 使用
    
    </summary>
    
    
      <category term="Hive" scheme="http://yoursite.com/categories/Hive/"/>
    
    
  </entry>
  
  <entry>
    <title>Linux block 与 inode</title>
    <link href="http://yoursite.com/2020/01/10/Linux-block-inode/"/>
    <id>http://yoursite.com/2020/01/10/Linux-block-inode/</id>
    <published>2020-01-09T16:00:00.000Z</published>
    <updated>2020-02-03T04:35:39.956Z</updated>
    
    <content type="html"><![CDATA[<p>Linux 磁盘管理 block 与 inode</p><hr><h4 id="详解"><a href="#详解" class="headerlink" title="详解"></a>详解</h4><p><strong>Sector（扇区）与 Block（块）</strong></p><p>1) 硬盘的最小存储单位: sector（扇区），每个扇区储存512字节；操作系统会一次性连续读取多个扇区，即一次性读取多个扇区称为一个block（块）</p><p>2) 文件存取的最小单位: block（块），由多个扇区组成；block的大小常见的有1KB、2KB、4KB，在linux中常见设置为4KB，即连续8个扇区组成一个block</p><p>每个block只能存放一个文件，如果文件的大小比block大，会申请更多的block；如果文件的大小比block小，仍会占用一个block，剩余的空间会浪费</p><p>例：有1万个文件，大小为10B，block为4KB</p><p>理论上占用空间大小：10000 * 10B = 97.656MB</p><p>实际上占用空间大小：10000 * 4KB = 40GB</p><p><strong>superblock、inode 与 block</strong></p><p>操作系统对于文件数据的存放包括两个部分：1文件内容、2权限及文件属性</p><p>在硬盘分区中，还有一个超级区块（superblock）</p><p>1）superblock: 记录文件系统的整体信息，包括inode与block的总量、使用大小、剩余大小以及文件系统的格式与相关信息等</p><p>2）inode: 记录文件的属性、权限，同时会记录该文件的数据所在的block编号</p><p>3）block: 存储文件的内容</p><p><strong>inode 与 block</strong></p><p>每个inode与block都有编号，而每个文件都会占用一个inode，inode内则有文件数据放置的block号码；能够找到文件的inode就可以找到该文件所放置数据的block号码，从而读取文件内容</p><p>在格式化时可以指定默认的inode与block的大小；-b指定默认block值，-I指定默认inode值，例：mkfs.ext4 –b 4096 –I 256 /dev/sdb</p><hr><p>参考链接</p><ul><li><a href="http://www.mamicode.com/info-detail-2435756.html" target="_blank" rel="noopener">Linux磁盘管理（block与inode）</a></li></ul>]]></content>
    
    <summary type="html">
    
      Linux block 与 inode
    
    </summary>
    
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
  </entry>
  
  <entry>
    <title>监控 Yarn 上作业状态</title>
    <link href="http://yoursite.com/2020/01/07/Yarn-App-Monitor/"/>
    <id>http://yoursite.com/2020/01/07/Yarn-App-Monitor/</id>
    <published>2020-01-06T16:00:00.000Z</published>
    <updated>2020-02-03T04:35:39.956Z</updated>
    
    <content type="html"><![CDATA[<p>监控 yarn 上 spark 或者 mr 应用的存活状态</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>有开发人员有疑惑想监控 yarn 上 spark 或者 mr 应用的存活状态。</p><p>实际通过 Yarn 提供的 API 即可做到</p><h4 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h4><p><code>pom.xml</code> 文件，添加相关的配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">&lt;repositories&gt;</span><br><span class="line">    &lt;repository&gt;</span><br><span class="line">        &lt;id&gt;cloudera&lt;&#x2F;id&gt;</span><br><span class="line">        &lt;url&gt;https:&#x2F;&#x2F;repository.cloudera.com&#x2F;artifactory&#x2F;cloudera-repos&#x2F;&lt;&#x2F;url&gt;</span><br><span class="line">    &lt;&#x2F;repository&gt;</span><br><span class="line">&lt;&#x2F;repositories&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;hadoop-common&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;version&gt;3.0.0-cdh6.3.1&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;hadoop-client&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;version&gt;3.0.0-cdh6.3.1&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;hadoop-yarn-api&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;version&gt;3.0.0-cdh6.3.1&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;hadoop-yarn-client&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;version&gt;3.0.0-cdh6.3.1&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;&#x2F;dependencies&gt;</span><br></pre></td></tr></table></figure><p>具体实现代码其实很简单就是，通过 yarnclient 获取 resourcemanager 上 Application 状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.yarn.api.records.ApplicationReport;</span><br><span class="line">import org.apache.hadoop.yarn.api.records.YarnApplicationState;</span><br><span class="line">import org.apache.hadoop.yarn.client.api.YarnClient;</span><br><span class="line">import org.apache.hadoop.yarn.conf.YarnConfiguration;</span><br><span class="line">import org.apache.hadoop.yarn.exceptions.YarnException;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line">import java.util.Collections;</span><br><span class="line">import java.util.Comparator;</span><br><span class="line">import java.util.EnumSet;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">public class client &#123;</span><br><span class="line"></span><br><span class="line">    public static ApplicationReport getAppReport(String applictionName) &#123;</span><br><span class="line">        Configuration conf &#x3D; new YarnConfiguration();</span><br><span class="line">        &#x2F;&#x2F; 修改为 RM 的地址</span><br><span class="line">        conf.set(&quot;yarn.resourcemanager.address&quot;, &quot;master-240:8032&quot;);</span><br><span class="line">        conf.set(&quot;yarn.resourcemanager.admin.address&quot;, &quot;master-240:8033&quot;);</span><br><span class="line">        YarnClient yarnClient &#x3D; YarnClient.createYarnClient();</span><br><span class="line">        yarnClient.init(conf);</span><br><span class="line">        yarnClient.start();</span><br><span class="line">        EnumSet&lt;YarnApplicationState&gt; appStates &#x3D; EnumSet.noneOf(YarnApplicationState.class);</span><br><span class="line">        if (appStates.isEmpty()) &#123;</span><br><span class="line">            appStates.add(YarnApplicationState.RUNNING);</span><br><span class="line">            appStates.add(YarnApplicationState.ACCEPTED);</span><br><span class="line">            appStates.add(YarnApplicationState.SUBMITTED);</span><br><span class="line">            appStates.add(YarnApplicationState.FINISHED);</span><br><span class="line">            appStates.add(YarnApplicationState.FAILED);</span><br><span class="line">            appStates.add(YarnApplicationState.KILLED);</span><br><span class="line">        &#125;</span><br><span class="line">        List&lt;ApplicationReport&gt; appsReports &#x3D; null;</span><br><span class="line">        try &#123;</span><br><span class="line">            &#x2F;&#x2F; 如果不指定YarnApplicationState的话就搜索所有状态 Application</span><br><span class="line">            &#x2F;&#x2F;appsReports &#x3D; yarnClient.getApplications(appStates);</span><br><span class="line">            appsReports &#x3D; yarnClient.getApplications();</span><br><span class="line">&#x2F;&#x2F;            appsReports.sort(new Comparator&lt;ApplicationReport&gt;() &#123;</span><br><span class="line">&#x2F;&#x2F;                @Override</span><br><span class="line">&#x2F;&#x2F;                public int compare(ApplicationReport o1, ApplicationReport o2) &#123;</span><br><span class="line">&#x2F;&#x2F;                    return -o1.getApplicationId().compareTo(o2.getApplicationId());</span><br><span class="line">&#x2F;&#x2F;                &#125;</span><br><span class="line">&#x2F;&#x2F;            &#125;);</span><br><span class="line">&#x2F;&#x2F;            appsReports.sort((ApplicationReport o1, ApplicationReport o2) -&gt; -(o1.getApplicationId().compareTo(o2.getApplicationId())));</span><br><span class="line">&#x2F;&#x2F;            Collections.sort(appsReports, Comparator.comparing(ApplicationReport::getApplicationId).reversed());</span><br><span class="line">            &#x2F;&#x2F; 反转 applicationId</span><br><span class="line">            Comparator&lt;ApplicationReport&gt; comp &#x3D; (ApplicationReport o1, ApplicationReport o2) -&gt; o1.getApplicationId().compareTo(o2.getApplicationId());</span><br><span class="line">            appsReports.sort(comp.reversed());</span><br><span class="line">        &#125; catch (YarnException | IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        assert appsReports !&#x3D; null;</span><br><span class="line">        ApplicationReport aimAppReport &#x3D; null;</span><br><span class="line">        for (ApplicationReport appReport : appsReports) &#123;</span><br><span class="line">            &#x2F;&#x2F; 获取任务</span><br><span class="line">            String appName &#x3D; appReport.getName();</span><br><span class="line">            if (appName.equals(applictionName)) &#123;</span><br><span class="line">                aimAppReport &#x3D; appReport;</span><br><span class="line">                break;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        yarnClient.stop();</span><br><span class="line">        return aimAppReport;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args)&#123;</span><br><span class="line">        ApplicationReport applicationReport &#x3D; getAppReport(&quot;Spark shell&quot;);</span><br><span class="line">        System.out.println(&quot;ApplicationId &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt; &quot;+ applicationReport.getApplicationId());</span><br><span class="line">        System.out.println(&quot;YarnApplicationState &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt; &quot;+ applicationReport.getYarnApplicationState());</span><br><span class="line">        System.out.println(&quot;name &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt; &quot;+ applicationReport.getName());</span><br><span class="line">        System.out.println(&quot;queue &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt; &quot;+ applicationReport.getQueue());</span><br><span class="line">        System.out.println(&quot;queue &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt; &quot;+ applicationReport.getUser());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这样，我们通过app name字段可以获取到存活的 spark 等任务</p><p><img src="/images/blog/2020-01-07-1.png" alt>{:height=”80%” width=”80%”}</p><p>客户端连接还可以将 yarn-site.xml 放到 resources 目录下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Configuration conf &#x3D; new YarnConfiguration();</span><br><span class="line">&#x2F;&#x2F; 这里将会加载 yarn 配置，配置地址如果是 hostname 的话，要在 client 机器上 hosts 文件配置好 host ip 映射</span><br><span class="line">conf.addResource(new Path(&quot;src&#x2F;main&#x2F;resources&#x2F;yarn-site.xml&quot;));</span><br></pre></td></tr></table></figure><p>如果是高可用配置的话还可以配置参数的形式访问</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Configuration conf &#x3D; new YarnConfiguration();</span><br><span class="line">conf.set(&quot;yarn.resourcemanager.ha.enabled&quot;, &quot;true&quot;);</span><br><span class="line">&#x2F;&#x2F; 下方三个参数每个 yarn 集群不一样，可以从 yarn-site.xml 中寻找</span><br><span class="line">conf.set(&quot;yarn.resourcemanager.ha.rm-ids&quot;, &quot;rm61,rm54&quot;);</span><br><span class="line">conf.set(&quot;yarn.resourcemanager.address.rm54&quot;, &quot;192.168.1.229:8032&quot;);</span><br><span class="line">conf.set(&quot;yarn.resourcemanager.address.rm61&quot;, &quot;192.168.1.228:8032&quot;);</span><br></pre></td></tr></table></figure><hr><p>参考链接</p><ul><li><a href="https://blog.csdn.net/rlnLo2pNEfx9c/article/details/83353496" target="_blank" rel="noopener">写个yarn的监控</a></li><li><a href="https://blog.csdn.net/qq_29493353/article/details/85788171" target="_blank" rel="noopener">yarn通过客户端提交application</a></li><li><a href="https://www.jianshu.com/p/c1fb266e4809" target="_blank" rel="noopener">YARN HA</a></li></ul>]]></content>
    
    <summary type="html">
    
      监控 Yarn 上作业状态
    
    </summary>
    
    
      <category term="Yarn" scheme="http://yoursite.com/categories/Yarn/"/>
    
    
  </entry>
  
  <entry>
    <title>Spark spark.yarn.jars 使用说明</title>
    <link href="http://yoursite.com/2020/01/06/Spark-yarn-jar/"/>
    <id>http://yoursite.com/2020/01/06/Spark-yarn-jar/</id>
    <published>2020-01-05T16:00:00.000Z</published>
    <updated>2020-02-01T04:42:46.239Z</updated>
    
    <content type="html"><![CDATA[<p>spark 优化将依赖包传入HDFS 使用 spark.yarn.jar </p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>问题描述请转移 <a href="https://blog.csdn.net/weizhonggui/article/details/85240804" target="_blank" rel="noopener">十：WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set,解决案例</a></p><p>启动 Spark 任务时，在没有配置 spark.yarn.archive 或者 spark.yarn.jars 时， 会看到不停地上传jar，非常耗时</p><h4 id="处理"><a href="#处理" class="headerlink" title="处理"></a>处理</h4><p>如果使用了 spark.yarn.archive 配置将会替换 spark.yarn.jars 的配置，所以这里使用<br>spark.yarn.jars 可以大大地减少任务的启动时间，整个处理过程如下。</p><p>上传依赖jar包, <code>/user/spark/jars</code> 为 hfds 上的目录，如果没有自行新建；<br>spark/jars/* 为 spark 服务自带的 jar 包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -put spark&#x2F;jars&#x2F;* &#x2F;user&#x2F;spark&#x2F;jars</span><br></pre></td></tr></table></figure><p>配置spark-defaut.conf，下方 <code>hdfs://reh</code> 为我这 HDFS nameserver 名字，你自行改成你自己的</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.yarn.jars&#x3D;local:&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.3.1-1.cdh6.3.1.p0.1470567&#x2F;lib&#x2F;spark&#x2F;jars&#x2F;*,local:&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.3.1-1.cdh6.3.1.p0.1470567&#x2F;lib&#x2F;spark&#x2F;hive&#x2F;*,hdfs:&#x2F;&#x2F;reh&#x2F;user&#x2F;spark&#x2F;jars&#x2F;*.jar</span><br></pre></td></tr></table></figure><p>注：本地配置local,hdfs标记为hdfs目录即可</p><hr><p>参考链接</p><ul><li><a href="https://blog.csdn.net/weizhonggui/article/details/85240804" target="_blank" rel="noopener">十：WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set,解决案例</a></li><li><a href="https://www.cnblogs.com/yyy-blog/p/11110388.html" target="_blank" rel="noopener">spark优化——依赖包传入HDFS_spark.yarn.jar和spark.yarn.archive的使用</a></li></ul>]]></content>
    
    <summary type="html">
    
      Spark spark.yarn.jars 使用说明
    
    </summary>
    
    
      <category term="Spark" scheme="http://yoursite.com/categories/Spark/"/>
    
    
  </entry>
  
  <entry>
    <title>Linux shell 命令行参数</title>
    <link href="http://yoursite.com/2020/01/04/Linux-getopts-getopt/"/>
    <id>http://yoursite.com/2020/01/04/Linux-getopts-getopt/</id>
    <published>2020-01-03T16:00:00.000Z</published>
    <updated>2020-02-01T04:42:46.236Z</updated>
    
    <content type="html"><![CDATA[<p>Linux shell 命令行参数解析处理</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>在bash中，可以用以下三种方式来处理命令行参数</p><ul><li>直接处理：使用$1,$2,…,$n进行解析</li><li>getopts：单个字符选项的情况（如：-n 10 -f file.txt等选项）</li><li>getopt：可以处理单个字符选项，也可以处理长选项long-option（如：–prefix=/home等）</li></ul><p>直接处理的方式网上很多，也很简单，直接跳过</p><h4 id="getopts"><a href="#getopts" class="headerlink" title="getopts"></a>getopts</h4><p>getopts 是 bash 的内部命令</p><p>getopts 的使用形式：getopts OPTION_STRING VAR;</p><p>OPTION_STRING 是类似 -u,-p 这种单字符选项</p><p>测试代码:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">while getopts &quot;a:b:c:&quot; opt; do</span><br><span class="line">  case $opt in</span><br><span class="line">    a)</span><br><span class="line">        use&#x3D;$OPTARG</span><br><span class="line">        echo &quot;user is $use&quot; ;;</span><br><span class="line">    b)</span><br><span class="line">        passwd&#x3D;$OPTARG</span><br><span class="line">        echo &quot;passwd is $passwd&quot; ;;</span><br><span class="line">    c)</span><br><span class="line">        #cccc&#x3D;$OPTARG</span><br><span class="line">    echo &quot;this is -c option. OPTARG&#x3D;[$OPTARG] OPTIND&#x3D;[$OPTIND]&quot;</span><br><span class="line">;;</span><br><span class="line">        #echo &quot;cccc is $cccc&quot; ;;</span><br><span class="line">    \?)</span><br><span class="line">        echo &quot;invalid arg&quot; ;;</span><br><span class="line">  esac</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>运行结果:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># sh test.sh -u tom -p 123456</span><br><span class="line">user is tom</span><br><span class="line">passwd is 123456</span><br></pre></td></tr></table></figure><h4 id="getopt"><a href="#getopt" class="headerlink" title="getopt"></a>getopt</h4><p>getopt 是一个外部命令，不是 bash 内置命令，Linux 发行版通常会自带</p><p>老版本的getopt问题较多，增强版getopt比较好用，执行命令getopt -T; echo $?，如果输出4，则代表是增强版的</p><p>测试代码:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">filepath&#x3D;$(cd &#96;dirname $0&#96;; pwd)</span><br><span class="line"></span><br><span class="line">show_usage&#x3D;&quot;args: [-s , -e , -n , -c]\</span><br><span class="line">                                  [--sdate&#x3D;, --edate&#x3D;, --numprocs&#x3D;, --cfile&#x3D;]&quot;</span><br><span class="line"></span><br><span class="line">if [[ -z $@ ]];then</span><br><span class="line">echo $show_usage</span><br><span class="line">exit 0</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">GETOPT_ARGS&#x3D;&#96;getopt -o s:e:n:c: -al sdate:,edate:,numprocs:,cfile: -- &quot;$@&quot;&#96;</span><br><span class="line"></span><br><span class="line">#echo &quot;$GETOPT_ARGS&quot;</span><br><span class="line">eval set -- &quot;$GETOPT_ARGS&quot;</span><br><span class="line"></span><br><span class="line">while [ -n &quot;$1&quot; ]</span><br><span class="line">do</span><br><span class="line">        case &quot;$1&quot; in</span><br><span class="line">                -s|--sdate) sdate&#x3D;$2; echo $sdate; shift 2;;</span><br><span class="line">                -e|--edate) edate&#x3D;$2; echo $edate; shift 2;;</span><br><span class="line">                -n|--numprocs) numprocs&#x3D;$2; echo $numprocs; shift 2;;</span><br><span class="line">                -c|--cfile) cfile&#x3D;$2; echo $cfile; shift 2;;</span><br><span class="line">                --) break ;;</span><br><span class="line">                *) echo $1,$2,$show_usage; break ;;</span><br><span class="line">        esac</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>运行结果:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># bash test2.sh --edate 11.11 -numprocs 123456</span><br><span class="line">11.11</span><br><span class="line">123456</span><br></pre></td></tr></table></figure><hr><p>参考链接</p><ul><li><a href="https://www.jianshu.com/p/6393259f0a13" target="_blank" rel="noopener">linux shell命令行选项与参数用法详解–getopts、getopt</a></li><li><a href="https://www.cnblogs.com/houyongchong/p/8482652.html" target="_blank" rel="noopener">shell getopts用法</a></li><li><a href="https://www.cnblogs.com/tommyjiang/p/10629848.html" target="_blank" rel="noopener">shell之getopt的用法</a></li></ul>]]></content>
    
    <summary type="html">
    
      Linux shell 命令行参数
    
    </summary>
    
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
  </entry>
  
  <entry>
    <title>Maven 项目打成 jar 包运行</title>
    <link href="http://yoursite.com/2020/01/02/mavan-jar/"/>
    <id>http://yoursite.com/2020/01/02/mavan-jar/</id>
    <published>2020-01-01T16:00:00.000Z</published>
    <updated>2020-02-03T04:35:39.955Z</updated>
    
    <content type="html"><![CDATA[<p>普通 Java Maven 项目打成 jar 包方式放服务器上运行</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>动手写了个普通 Java 的 Maven 项目，将其打包放到服务器上运行</p><h4 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h4><p>在 <code>pom.xml</code> 中添加如下配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">&lt;build&gt;</span><br><span class="line">    &lt;pluginManagement&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-assembly-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.5.5&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;archive&gt;</span><br><span class="line">                        &lt;manifest&gt;</span><br><span class="line">                            &lt;!-- 这里是启动类全路径名 --&gt;</span><br><span class="line">                            &lt;mainClass&gt;com.xxx.XXXX&lt;&#x2F;mainClass&gt;</span><br><span class="line">                        &lt;&#x2F;manifest&gt;</span><br><span class="line">                    &lt;&#x2F;archive&gt;</span><br><span class="line">                    &lt;descriptorRefs&gt;</span><br><span class="line">                        &lt;descriptorRef&gt;jar-with-dependencies&lt;&#x2F;descriptorRef&gt;</span><br><span class="line">                    &lt;&#x2F;descriptorRefs&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.1&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;!-- jdk 版本 --&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                    &lt;encoding&gt;utf-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;&#x2F;plugins&gt;</span><br><span class="line">    &lt;&#x2F;pluginManagement&gt;</span><br><span class="line">&lt;&#x2F;build&gt;</span><br></pre></td></tr></table></figure><p>请将 com.xxx.XXXX 改成你项目的主类</p><p>终端下执行<code>mvn clean package assembly:single</code></p><p><img src="/images/blog/2020-01-02-1.png" alt>{:height=”80%” width=”80%”}</p><p>出现如图标志说明打包成功</p><p>可以在target 目录下看到两个 jar 包，带 jar-with-dependencies 后缀的是带依赖的jar包</p><p><img src="/images/blog/2020-01-02-2.png" alt>{:height=”80%” width=”80%”}</p><p>这里选择将其上传到服务器上运行 <code>java -jar cdhAgent-1.0-SNAPSHOT-jar-with-dependencies.jar</code></p><hr><p>参考链接</p><ul><li><a href="https://blog.csdn.net/qq_27252133/article/details/82750599" target="_blank" rel="noopener">使用maven-assembly-plugin插件打包</a></li><li><a href="https://blog.csdn.net/qq_16038125/article/details/60168389" target="_blank" rel="noopener">构建工具-Maven-使用maven-assembly-plugin将依赖打包进jar并指定JDK版本</a></li><li><a href="https://www.cnblogs.com/binhua-liu/p/5604841.html" target="_blank" rel="noopener">如何配置pom.xml用maven打包java工程</a></li></ul>]]></content>
    
    <summary type="html">
    
      Maven 项目打成 jar 包运行
    
    </summary>
    
    
      <category term="Tools" scheme="http://yoursite.com/categories/Tools/"/>
    
    
  </entry>
  
  <entry>
    <title>YARN Fair scheduler</title>
    <link href="http://yoursite.com/2020/01/02/Yarn-Fair-Scheduler/"/>
    <id>http://yoursite.com/2020/01/02/Yarn-Fair-Scheduler/</id>
    <published>2020-01-01T16:00:00.000Z</published>
    <updated>2020-02-03T04:35:39.954Z</updated>
    
    <content type="html"><![CDATA[<p>YARN 默认调度 Fair scheduler</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>工作中这个问题碰到好几次了。调度规则(scheduling rule)，啥是调度策略(scheduling policy)，还有 CDH 本身有个动态资源池的概念，<br>让我学了又忘，忘了又学，这次通过文档方式来给自己一个交代。</p><h4 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h4><p><strong>什么是资源？</strong></p><p>对于一个资源管理系统，首先要确定什么是资源，然后将每种资源量化，最后对量化的资源进行管理。</p><p>YARN对资源的抽象很简单，只有内存和vcore，这两种资源。每个NodeManager节点贡献一定的内存和vcore，由ResourceManager统一管理。</p><h4 id="调度规则"><a href="#调度规则" class="headerlink" title="调度规则"></a>调度规则</h4><p>YARN的三种调度规则(scheduling rule)</p><ul><li>FIFO Scheduler</li><li>Capacity Scheduler</li><li>Fair Scheduler</li></ul><p>调度规则具体可以看<a href="https://www.cnblogs.com/sodawoods-blogs/p/8877197.html" target="_blank" rel="noopener">YARN中FIFO、Capacity以及Fari调度器的详细介绍</a><br>，记得之前有次看到说随着发展，Capacity 与 Fari 区别越来越小了</p><h4 id="调度策略"><a href="#调度策略" class="headerlink" title="调度策略"></a>调度策略</h4><p><a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/FairScheduler.html" target="_blank" rel="noopener">Fair Scheduler支持的调度策略</a></p><blockquote><p>Additionally, the fair scheduler allows setting a different custom policy for each queue to allow sharing the queue’s resources in any which way the user wants. A custom policy can be built by extending org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.SchedulingPolicy. FifoPolicy, FairSharePolicy (default), and DominantResourceFairnessPolicy are built-in and can be readily used.</p></blockquote><p>队列内部的默认调度策略是基于内存的共享策略，也可以配置成 FIFO 和 multi-resource with Dominant Resource Fairness (即DRF)</p><p>对于CDH版本来说有些不同，<a href="https://docs.cloudera.com/documentation/enterprise/latest/topics/cm_mc_resource_pools.html#concept_xkk_l1d_wr__section_utc_gbl_vl" target="_blank" rel="noopener">CDH版本的 Fair Scheduler</a> 默认是采用DRF 策略</p><blockquote><p>Dominant Resource Fairness (DRF) (default) - An extension of fair scheduling for more than one resource. DRF determines CPU and memory resource shares based on the availability of those resources and the job requirements.</p></blockquote><p><img src="/images/blog/2020-01-02-3.png" alt>{:height=”80%” width=”80%”}</p><p>也就是说 CDH 版本的 YARN 默认采用的调度策略是 Fair Scheduler 的 DRF 策略，即基于 vcore 和内存的策略，而不是只基于内存的调度策略。</p><h4 id="抢占"><a href="#抢占" class="headerlink" title="抢占"></a>抢占</h4><p>Capacity Scheduler 与 Fair Scheduler 都可以将集群分为队列且允许抢占机制</p><p>当一个job提交到一个繁忙集群中的空队列时，job并不会马上执行，而是阻塞直到正在运行的job释放系统资源。为了使提交job的执行时间更具预测性（可以设置等待的超时时间），调度器支持抢占。</p><p>抢占就是允许调度器杀掉占用超过其应占份额资源队列的containers，这些containers资源便可被分配到应该享有这些份额资源的队列中。需要注意抢占会降低集群的执行效率，因为被终止的containers需要被重新执行。</p><p>可以阅读下最后三个参考链接</p><hr><p>参考链接</p><ul><li><a href="https://www.jianshu.com/p/ada7f7b58298" target="_blank" rel="noopener">CDH版本的YARN默认调度Fair scheduler 和DRF简述</a></li><li><a href="https://www.cnblogs.com/sodawoods-blogs/p/8877197.html" target="_blank" rel="noopener">YARN中FIFO、Capacity以及Fari调度器的详细介绍</a></li><li><a href="https://www.jianshu.com/p/990094565a9d" target="_blank" rel="noopener">Yarn 队列调度策略</a></li><li><a href="https://blog.csdn.net/suifeng3051/article/details/49508261" target="_blank" rel="noopener">Yarn 调度器Scheduler详解</a></li><li><a href="http://bigdatadecode.club/YARN%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BFair%20Scheduler%20part1.html" target="_blank" rel="noopener">YARN源码分析之Fair Scheduler part1</a></li><li><a href="http://bigdatadecode.club/YARN%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BFair%20Scheduler%20part2.html" target="_blank" rel="noopener">YARN源码分析之Fair Scheduler part2</a></li></ul>]]></content>
    
    <summary type="html">
    
      YARN Fair scheduler
    
    </summary>
    
    
      <category term="Yarn" scheme="http://yoursite.com/categories/Yarn/"/>
    
    
  </entry>
  
  <entry>
    <title>二阶段提交（2PC）</title>
    <link href="http://yoursite.com/2020/01/01/2PC/"/>
    <id>http://yoursite.com/2020/01/01/2PC/</id>
    <published>2019-12-31T16:00:00.000Z</published>
    <updated>2020-02-03T04:35:39.954Z</updated>
    
    <content type="html"><![CDATA[<p>在分布式环境下，所有节点进行事务提交，保持一致性的算法</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>今天是2020年的元旦，在这先祝自己开心快乐</p><p>2PC 在之前学习 HBase Snapshot 时了解过，这次通过文档的记录的方式再过一遍</p><p>本人主要是阅读下方学习了解的</p><ul><li><a href="https://mp.weixin.qq.com/s/kXtliHCqtrTjlWFm1aHtVQ" target="_blank" rel="noopener">分布式基础，啥是两阶段提交？</a></li><li><a href="https://mp.weixin.qq.com/s/QS01KCQ4u2DhNHww_A2Xtw" target="_blank" rel="noopener">漫话：如何给女朋友解释什么是2PC（二阶段提交）？</a></li></ul><h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><p>2PC 是解决分布式系统中数据一致性问题，即分布式事务</p><p><strong>为什么分布式事务为什么难？</strong></p><p>在分布式环境下，每个节点都可以知晓自己操作的成功或者失败，却无法知道其他节点操作的成功或失败。当一个分布式事务跨多个节点时，保持事务的原子性与一致性，是非常困难的。</p><p>举个例子: 在电商网站下单的时候，需要有多个分布式服务同时服务，如支付系统进行支付、红包系统进行红包扣减、库存系统扣减库存、物流系统更新物流信息等。</p><p>但是，如果其中某一个系统在执行过程中失败了，或者由于网络原因没有收到请求，那么，整个系统可能就有不一致的现象了，<br>即：付了钱，扣了红包，但是库存没有扣减</p><h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><p>通过引入一个协调者（Coordinator）来统一掌控所有参与者（Participant）的操作结果，<br>并指示它们是否要把操作结果进行真正的提交（commit）或者回滚（rollback）</p><p>2PC分为两个阶段:</p><ul><li>投票阶段（voting phase）参与者通知协调者，协调者反馈结果</li><li>提交阶段（commit phase）收到参与者的反馈后，协调者再向参与者发出通知，根据反馈情况决定各参与者是否要提交还是回滚</li></ul><h5 id="准备阶段"><a href="#准备阶段" class="headerlink" title="准备阶段"></a>准备阶段</h5><p>准备阶段分为以下三个步骤</p><ol><li>协调者节点向所有参与者节点询问是否可以执行提交操作，并开始等待各参与者节点的响应</li><li>参与者节点执行询问发起为止的所有事务操作，并将Undo信息和Redo信息写入日志 (HBase SnapShot 是将执行操作写入到临时文件，回滚就直接删除，执行就移动到目标目录)</li><li>各参与者节点响应协调者节点发起的询问。如果参与者节点的事务操作实际执行成功，则它返回一个”OK”消息, 如果参与者节点的事务操作实际执行失败，则它返回一个”NO”消息</li></ol><p><img src="/images/blog/2020-01-01-1.png" alt>{:height=”80%” width=”80%”}</p><h5 id="提交阶段"><a href="#提交阶段" class="headerlink" title="提交阶段"></a>提交阶段</h5><p>如果协调者收到了参与者的失败消息或者超时，直接给每个参与者发送回滚消息；否则，发送提交消息；参与者根据协调者的指令执行提交或者回滚操作</p><p>当协调者节点从所有参与者节点获得的相应消息都为”OK”时</p><ol><li>协调者节点向所有参与者节点发出”正式提交”的请求</li><li>参与者节点正式完成操作，并释放在整个事务期间内占用的资源</li><li>参与者节点向协调者节点发送”完成”消息</li><li>协调者节点受到所有参与者节点反馈的”完成”消息后，完成事务</li></ol><p><img src="/images/blog/2020-01-01-2.png" alt>{:height=”80%” width=”80%”}</p><p>如果任一参与者节点在第一阶段返回的响应消息为”NO”时，或者 协调者节点在第一阶段的询问超时之前无法获取所有参与者节点的响应消息时</p><ol><li>协调者节点向所有参与者节点发出”回滚操作”的请求</li><li>参与者节点利用之前写入的Undo信息执行回滚，并释放在整个事务期间内占用的资源</li><li>参与者节点向协调者节点发送”回滚完成”消息</li><li>协调者节点受到所有参与者节点反馈的”回滚完成”消息后，取消事务</li></ol><p><img src="/images/blog/2020-01-01-3.png" alt>{:height=”80%” width=”80%”}</p><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><p>1、同步阻塞问题。执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。</p><p>2、单点故障。由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。（如果是协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题）</p><p>3、数据不一致。在二阶段提交的阶段二中，当协调者向参与者发送commit请求之后，发生了局部网络异常或者在发送commit请求过程中协调者发生了故障，这回导致只有一部分参与者接受到了commit请求。而在这部分参与者接到commit请求之后就会执行commit操作。但是其他部分未接到commit请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据部一致性的现象。</p><p>4、二阶段无法解决的问题：协调者再发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。</p><h4 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h4><ul><li><a href="https://mp.weixin.qq.com/s/lxc-JvDRkoR9tXLpsTqGeA" target="_blank" rel="noopener">漫话：如何给女朋友解释什么是3PC？</a></li></ul><hr><p>参考链接</p><ul><li><a href="https://mp.weixin.qq.com/s/kXtliHCqtrTjlWFm1aHtVQ" target="_blank" rel="noopener">分布式基础，啥是两阶段提交？</a></li><li><a href="https://mp.weixin.qq.com/s/QS01KCQ4u2DhNHww_A2Xtw" target="_blank" rel="noopener">漫话：如何给女朋友解释什么是2PC（二阶段提交）？</a></li></ul>]]></content>
    
    <summary type="html">
    
      在分布式环境下，所有节点进行事务提交，保持一致性的算法
    
    </summary>
    
    
      <category term="Distributed" scheme="http://yoursite.com/categories/Distributed/"/>
    
    
  </entry>
  
</feed>
