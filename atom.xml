<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>lihuimintu</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-03-07T04:05:57.845Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>图</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ZooKeeper 监控指标</title>
    <link href="http://yoursite.com/2020/03/07/Zookeeper-Monitor/"/>
    <id>http://yoursite.com/2020/03/07/Zookeeper-Monitor/</id>
    <published>2020-03-06T16:00:00.000Z</published>
    <updated>2020-03-07T04:05:57.845Z</updated>
    
    <content type="html"><![CDATA[<hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>ZooKeeper 是一个开放源码的分布式应用程序协调服务，很多分布式应用程序可以基于它实现同步服务，如 HDFS、HBase、Kafka</p><h4 id="状态"><a href="#状态" class="headerlink" title="状态"></a>状态</h4><p>ZooKeeper 提供了四字命令(The Four Letter Words)，用来获取 ZooKeeper 服务的当前状态及相关信息</p><p><strong>有哪些命令可以使用？</strong></p><table><thead><tr><th>四字命令</th><th>功能描述</th></tr></thead><tbody><tr><td>conf</td><td>打印配置</td></tr><tr><td>cons</td><td>列出所有连接到这台服务器的客户端全部连接/会话详细信息。包括”接受/发送”的包数量、会话id、操作延迟、最后的操作执行等等信息</td></tr><tr><td>crst</td><td>重置所有连接的连接和会话统计信息</td></tr><tr><td>dump</td><td>列出那些比较重要的会话和临时节点。这个命令只能在leader节点上有用</td></tr><tr><td>envi</td><td>打印出服务环境的详细信息</td></tr><tr><td>reqs</td><td>列出未经处理的请求</td></tr><tr><td>ruok</td><td>即”Are you ok”，测试服务是否处于正确状态。如果确实如此，那么服务返回”imok”，否则不做任何相应</td></tr><tr><td>stat</td><td>输出关于性能和连接的客户端的列表</td></tr><tr><td>srst</td><td>重置服务器的统计</td></tr><tr><td>srvr</td><td>列出连接服务器的详细信息</td></tr><tr><td>wchs</td><td>列出服务器watch的详细信息</td></tr><tr><td>wchc</td><td>通过session列出服务器watch的详细信息，它的输出是一个与watch相关的会话的列表</td></tr><tr><td>wchp</td><td>通过路径列出服务器watch的详细信息。它输出一个与session相关的路径</td></tr><tr><td>mntr</td><td>输出可用于检测集群健康状态的变量列表</td></tr></tbody></table><p>通过下列命令来获取这些监控信息 <code>echo commands  |  nc ip port</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">$ echo mntr | nc 192.168.1.229 2181</span><br><span class="line">zk_version      3.4.5-cdh6.3.1--1, built on 09&#x2F;26&#x2F;2019 09:28 GMT    # 版本</span><br><span class="line">zk_avg_latency  0                                                   # 平均延时</span><br><span class="line">zk_max_latency  51                                                  # 最大延时</span><br><span class="line">zk_min_latency  0                                                   # 最小延时</span><br><span class="line">zk_packets_received     825166                                      # 收包数</span><br><span class="line">zk_packets_sent 844514                                              # 发包数</span><br><span class="line">zk_num_alive_connections        10                                  # 连接数</span><br><span class="line">zk_outstanding_requests 0                                           # 堆积请求数</span><br><span class="line">zk_server_state leader                                              # 状态</span><br><span class="line">zk_znode_count  399                                                 # znode 数量</span><br><span class="line">zk_watch_count  74                                                  # watch 数量</span><br><span class="line">zk_ephemerals_count     9                                           # 临时节点(znode)</span><br><span class="line">zk_approximate_data_size        28043                               # 数据大小</span><br><span class="line">zk_open_file_descriptor_count   59                                  # 打开的文件描述符数量</span><br><span class="line">zk_max_file_descriptor_count    32768                               # 最大文件描述符数量</span><br><span class="line">zk_fsync_threshold_exceed_count 0</span><br><span class="line">zk_followers    2                                                   # follower 数量，leader角色才会有这个输出</span><br><span class="line">zk_synced_followers     2                                           # 同步的 follower 数量</span><br><span class="line">zk_pending_syncs        0                                           # 准备同步数，leader角色才会有这个输出</span><br><span class="line">zk_last_proposal_size   32                                          # 最近一次 Proposal 消息大小</span><br><span class="line">zk_max_proposal_size    1337                                        # 最大 Proposal 消息大小</span><br><span class="line">zk_min_proposal_size    32                                          # 最小 Proposal 消息大小</span><br></pre></td></tr></table></figure><p>还有很多信息可以获取，更多可以自行搜索。</p><h4 id="监控"><a href="#监控" class="headerlink" title="监控"></a>监控</h4><p>监控平台的话目前业界已经比较成熟的阿里开源 ZooKeeper 监控框架 <a href="https://github.com/alibaba/taokeeper" target="_blank" rel="noopener">TaoKeeper</a></p><p>TaoKeeper 同作原理通过SSH连接到ZooKeeper部署的机器上，再在上面执行 ZooKeeper 的四字命令来得到统计信息，再分析保存到 MySQL 数据库中。</p><p>当然也可以使用四字命令+nc写脚本去做告警也行，针对以下几个维度去</p><p>open_connections  单机打开连接连接数，建议超过平常的两倍就报警，这样就能知道比平常连接数激增</p><p>latency  响应一个客户端请求的时间，建议这个时间大于10个TickTime就报警</p><blockquote><p>tickTime 这个时间是作为 Zookeeper 服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个 tickTime 时间就会发送一个心跳。默认是 2000 毫秒</p></blockquote><p>outstanding_requests   排队请求的数量，当ZooKeeper超过了它的处理能力时，这个值会增大，建议设置报警阀值为10</p><blockquote><p>zookeeper globalOutstandingLimit 配置指定了等待处理的最大请求数量的限制</p></blockquote><p>open_file_descriptor_count   打开文件数量，当这个值大于允许值”zk_max_file_descriptor_count” 85% 时报警</p><p>Java Heap Size  JVM 堆栈内存使用情况大于分配的最大内存 80% 时报警</p><hr><p>参考链接</p><ul><li><a href="https://www.cnblogs.com/xinfang520/p/10721770.html" target="_blank" rel="noopener">Hadoop记录- zookeeper 监控指标</a></li><li><a href="https://www.cnblogs.com/allenhaozi/p/11416817.html" target="_blank" rel="noopener">zookeeper globalOutstandingLimit</a></li><li><a href="https://www.cnblogs.com/linuxbug/p/4840506.html" target="_blank" rel="noopener">zookeeper监控告警</a></li><li><a href="https://blog.csdn.net/hackerwin7/article/details/43985049" target="_blank" rel="noopener">Zookeeper 监控原型开发</a></li><li><a href="https://www.jianshu.com/p/4f11d7bfc9ce" target="_blank" rel="noopener">ZooKeeper监控工具(六)</a></li><li><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-zookeeper/index.html" target="_blank" rel="noopener">分布式服务框架 Zookeeper — 管理分布式环境中的数据</a></li></ul>]]></content>
    
    <summary type="html">
    
      ZooKeeper 监控指标
    
    </summary>
    
    
      <category term="Zookeeper" scheme="http://yoursite.com/categories/Zookeeper/"/>
    
    
  </entry>
  
  <entry>
    <title>HBase 读取性能优化</title>
    <link href="http://yoursite.com/2020/03/05/HBase-read-optimization/"/>
    <id>http://yoursite.com/2020/03/05/HBase-read-optimization/</id>
    <published>2020-03-04T16:00:00.000Z</published>
    <updated>2020-03-05T09:59:32.972Z</updated>
    
    <content type="html"><![CDATA[<p>HBase 读取性能优化</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>本文主要借阅于《HBase原理与实践》这本书，可以自行查阅 13.4 和 13.5 章</p><p>读请求延迟较大通常存在三种场景，分别为：</p><ol><li><p>仅有某业务延迟较大，集群其他业务都正常</p></li><li><p>整个集群所有业务都反映延迟较大</p></li><li><p>某个业务起来之后集群其他部分业务延迟较大</p></li></ol><p>这三种场景是表象，通常某业务反应延迟异常，首先需要明确具体是哪种场景，然后针对性解决问题。</p><p>主要分为四个方面: 客户端优化、服务器端优化、列族设计优化以及 HDFS 相关优化</p><h4 id="客户端优化"><a href="#客户端优化" class="headerlink" title="客户端优化"></a>客户端优化</h4><p>客户端作为业务读写的入口，姿势使用不正确通常会导致本业务读延迟较高实际上存在一些使用姿势的推荐用法</p><h5 id="scan-缓存是否设置合理？"><a href="#scan-缓存是否设置合理？" class="headerlink" title="scan 缓存是否设置合理？"></a>scan 缓存是否设置合理？</h5><p>优化原理: HBase业务通常一次 scan 就会返回大量数据，因此客户端发起一次 scan 请求，实际并不会一次就将所有数据加载到本地，而是分成多次 RPC 请求进行加载，这样设计一方面因为大量数据请求可能会导致网络带宽严重消耗进而影响其他业务，另一方面因为数据量太大可能导致本地客户端发生OOM。在这样的设计体系下，用户会首先加载一部分数据到本地，然后遍历处理，再加载下一部分数据到本地处理，如此往复，直至所有数据都加载完成。数据加载到本地就存放在scan缓存中，默认为100条数据。</p><p>通常情况下，默认的scan缓存设置是可以正常工作的。但是对于一些大scan（一次scan可能需要查询几万甚至几十万行数据），每次请求100条数据意味着一次scan需要几百甚至几千次RPC请求，这种交互的代价无疑是很大的。因此可以考虑将scan缓存设置增大，比如设为500或者1000条可能更加合适。《HBase原理与实践》作者之前做过一次试验，在一次scan 10w+条数据量的条件下，将scan缓存从100增加到1000条，可以有效降低scan请求的总体延迟，延迟降低了25%左右。</p><p>优化建议: 大scan场景下将scan缓存从100增大到500或者1000，用以减少RPC次数。</p><h5 id="get-是否使用批量请求？"><a href="#get-是否使用批量请求？" class="headerlink" title="get 是否使用批量请求？"></a>get 是否使用批量请求？</h5><p>优化原理: HBase分别提供了单条get以及批量get的API接口，使用批量get接口可以减少客户端到RegionServer之间的RPC连接数，提高读取吞吐量。另外需要注意的是，批量get请求要么成功返回所有请求数据，要么抛出异常。</p><p>优化建议: 使用批量get进行读取请求。需要注意的是，对读取延迟非常敏感的业务，批量请求时每次批量数不能太大，最好进行测试。</p><h5 id="请求是否可以显式指定列簇或者列？"><a href="#请求是否可以显式指定列簇或者列？" class="headerlink" title="请求是否可以显式指定列簇或者列？"></a>请求是否可以显式指定列簇或者列？</h5><p>优化原理: HBase是典型的列簇数据库，意味着同一列簇的数据存储在一起，不同列簇的数据分开存储在不同的目录下。一个表有多个列簇，如果只是根据rowkey而不指定列簇进行检索，不同列簇的数据需要独立进行检索，性能必然会比指定列簇的查询差很多，很多情况下甚至会有2～3倍的性能损失。</p><p>优化建议：尽量指定列簇或者列进行精确查找。</p><h5 id="离线批量读取请求是否设置禁止缓存？"><a href="#离线批量读取请求是否设置禁止缓存？" class="headerlink" title="离线批量读取请求是否设置禁止缓存？"></a>离线批量读取请求是否设置禁止缓存？</h5><p>优化原理: 通常在离线批量读取数据时会进行一次性全表扫描，一方面数据量很大，另一方面请求只会执行一次。这种场景下如果使用scan默认设置，就会将数据从HDFS加载出来放到缓存。可想而知，大量数据进入缓存必将其他实时业务热点数据挤出，其他业务不得不从HDFS加载，进而造成明显的读延迟毛刺。</p><p>优化建议: 离线批量读取请求设置禁用缓存，scan.setCacheBlocks (false)。</p><h4 id="服务端优化"><a href="#服务端优化" class="headerlink" title="服务端优化"></a>服务端优化</h4><p>一般服务端端问题一旦导致业务读请求延迟较大的话，通常是集群级别的，即整个集群的业务都会反映读延迟较大。</p><h5 id="读请求是否均衡？"><a href="#读请求是否均衡？" class="headerlink" title="读请求是否均衡？"></a>读请求是否均衡？</h5><p>优化原理: 假如业务所有读请求都落在集群某一台RegionServer上的某几个Region上，很显然，这一方面不能发挥整个集群的并发处理能力，另一方面势必造成此台 RegionServer 资源严重消耗（比如IO耗尽、handler耗尽等），导致落在该台 RegionServer 上的其他业务受到波及。也就是说读请求不均衡不仅会造成本身业务性能很差，还会严重影响其他业务。</p><p>观察确认: 观察所有RegionServer的读请求QPS曲线，确认是否存在读请求不均衡现象。</p><p>优化建议: Rowkey必须进行散列化处理（比如MD5散列），同时建表必须进行预分区处理。</p><h5 id="BlockCache设置是否合理？"><a href="#BlockCache设置是否合理？" class="headerlink" title="BlockCache设置是否合理？"></a>BlockCache设置是否合理？</h5><p>优化原理: BlockCache作为读缓存，对于读性能至关重要。默认情况下BlockCache和MemStore的配置相对比较均衡（各占40%），可以根据集群业务进行修正，比如读多写少业务可以将BlockCache占比调大。另一方面，BlockCache的策略选择也很重要，不同策略对读性能来说影响并不是很大，但是对GC的影响却相当显著，尤其在BucketCache的offheap模式下GC表现非常优秀。</p><p>观察确认: 观察所有 RegionServer 的缓存未命中率、配置文件相关配置项以及GC日志，确认 BlockCache 是否可以优化。</p><p>优化建议: 如果JVM内存配置量小于20G，BlockCache策略选择LRUBlockCache；否则选择BucketCache策略的 offheap 模式。</p><h5 id="HFile文件是否太多？"><a href="#HFile文件是否太多？" class="headerlink" title="HFile文件是否太多？"></a>HFile文件是否太多？</h5><p>优化原理: HBase在读取数据时通常先到MemStore和BlockCache中检索（读取最近写入数据和热点数据），如果查找不到则到文件中检索。HBase的类LSM树结构导致每个store包含多个HFile文件，文件越多，检索所需的IO次数越多，读取延迟也就越高。文件数量通常取决于Compaction的执行策略，一般和两个配置参数有关：hbase.hstore. compactionThreshold和hbase.hstore.compaction.max.size，前者表示一个store中的文件数超过阈值就应该进行合并，后者表示参与合并的文件大小最大是多少，超过此大小的文件不能参与合并。这两个参数需要谨慎设置，如果前者设置太大，后者设置太小，就会导致Compaction合并文件的实际效果不明显，很多文件得不到合并，进而导致HFile文件数变多。</p><p>观察确认: 观察RegionServer级别以及Region级别的HFile数，确认HFile文件是否过多。</p><p>优化建议: hbase.hstore.compactionThreshold设置不能太大，默认为3个。</p><h5 id="Compaction是否消耗系统资源过多？"><a href="#Compaction是否消耗系统资源过多？" class="headerlink" title="Compaction是否消耗系统资源过多？"></a>Compaction是否消耗系统资源过多？</h5><p>优化原理: Compaction是将小文件合并为大文件，提高后续业务随机读性能，但是也会带来IO放大以及带宽消耗问题（数据远程读取以及三副本写入都会消耗系统带宽）。正常配置情况下，Minor Compaction并不会带来很大的系统资源消耗，除非因为配置不合理导致MinorCompaction太过频繁，或者Region设置太大发生Major Compaction。</p><p>观察确认: 观察系统IO资源以及带宽资源使用情况，再观察Compaction队列长度，确认是否由于Compaction导致系统资源消耗过多。</p><p>优化建议: 对于大Region读延迟敏感的业务（100G以上）通常不建议开启自动MajorCompaction，手动低峰期触发。小Region或者延迟不敏感的业务可以开启MajorCompaction，但建议限制流量。</p><h4 id="列簇设计优化"><a href="#列簇设计优化" class="headerlink" title="列簇设计优化"></a>列簇设计优化</h4><h5 id="布隆过滤器是否设置？"><a href="#布隆过滤器是否设置？" class="headerlink" title="布隆过滤器是否设置？"></a>布隆过滤器是否设置？</h5><p>优化原理: 布隆过滤器主要用来过滤不存在待检索rowkey的HFile文件，避免无用的IO操作。</p><p>布隆过滤器取值有两个——row以及rowcol，需要根据业务来确定具体使用哪种。如果业务中大多数随机查询仅仅使用row作为查询条件，布隆过滤器一定要设置为row；如果大多数随机查询使用row+column作为查询条件，布隆过滤器需要设置为rowcol。如果不确定业务查询类型，则设置为row。</p><p>优化建议: 任何业务都应该设置布隆过滤器，通常设置为row，除非确认业务随机查询类型为row+column，则设置为rowcol。<strong>默认为 row</strong></p><h5 id="TTL-是否设置合理？"><a href="#TTL-是否设置合理？" class="headerlink" title="TTL 是否设置合理？"></a>TTL 是否设置合理？</h5><p>优化原理: TTL(Time to Live) 用于限定数据的超时时间，HBase cell 超过时间后会被自动删除，对某些数据不是永久保存，并大量写入的场景下非常适用，减少数据规模</p><p>优化建议: CF 默认的 TTL 值是 FOREVER，也就是永不过期，可以根据具体的业务场景设置超时时间</p><h4 id="HDFS-相关优化"><a href="#HDFS-相关优化" class="headerlink" title="HDFS 相关优化"></a>HDFS 相关优化</h4><h5 id="数据本地率是不是很低？"><a href="#数据本地率是不是很低？" class="headerlink" title="数据本地率是不是很低？"></a>数据本地率是不是很低？</h5><p>优化原理: 如果数据本地率很低，数据读取时会产生大量网络IO请求，导致读延迟较高。</p><p>观察确认: 观察所有RegionServer的数据本地率（见jmx中指标PercentFileLocal，在TableWeb UI可以看到各个Region的Locality）。</p><p>优化建议: 尽量避免Region无故迁移。对于本地率较低的节点，可以在业务低峰期执行major_compact。</p><blockquote><p>执行major_compact提升数据本地率的理论依据是，major_compact本质上是将Region中的所有文件读取出来然后写到一个大文件，写大文件必然会在本地DataNode生成一个副本，这样Region的数据本地率就会提升到100%。</p></blockquote><h5 id="Short-Circuit-Local-Read功能是否开启？"><a href="#Short-Circuit-Local-Read功能是否开启？" class="headerlink" title="Short-Circuit Local Read功能是否开启？"></a>Short-Circuit Local Read功能是否开启？</h5><p>优化原理: 当前HDFS读取数据都需要经过 DataNode，客户端会向DataNode发送读取数据的请求，DataNode接受到请求之后从硬盘中将文件读出来，再通过TCP发送给客户端。Short Circuit策略允许客户端绕过DataNode直接读取本地数据。</p><p>优化建议: 开启Short Circuit Local Read 功能，需要在<code>hbase-site.xml</code>或者<code>hdfs-site.xml</code>配置文件中增加如下配置项</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.client.read.shortcircuit&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.domain.socket.path&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;&#x2F;var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;dn_socket&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.client.read.shortcircuit.buffer.size&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;131072&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure><blockquote><p>需要注意的是，dfs.client.read.shortcircuit.buffer.size参数默认是1M，对于HBase系统来说有可能会造成OOM，详见HBASE-8143 HBase on Hadoop 2 with local short circuit reads(ssr) causes OOM</p></blockquote><h5 id="Hedged-Read功能是否开启？"><a href="#Hedged-Read功能是否开启？" class="headerlink" title="Hedged Read功能是否开启？"></a>Hedged Read功能是否开启？</h5><p>优化原理: HBase数据在HDFS中默认存储三个副本，通常情况下HBase会根据一定算法优先选择一个DataNode进行数据读取。然而在某些情况下，有可能因为磁盘问题或者网络问题等引起读取超时，根据Hedged Read策略，如果在指定时间内读取请求没有返回，HDFS客户端将会向第二个副本发送第二次数据请求，并且谁先返回就使用谁，之后返回的将会被丢弃。</p><p>优化建议: 开启Hedged Read功能，需要在<code>hbase-site.xml</code>配置文件中增加如下配置项</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.client.hedged.read.threadpool.size&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;20&lt;&#x2F;value&gt;&lt;!-- 20 threads --&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.client.hedged.read.threshold.millis&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;10&lt;&#x2F;value&gt;&lt;!-- 10 milliseconds --&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure><blockquote><p>参数dfs.client.hedged.read.threadpool.size表示用于hedged read的线程池线程数量，默认为0，表示关闭hedged read功能；参数dfs.client.hedged.read.threshold.millis表示HDFS数据读取超时时间，超过这个阈值，HDFS客户端将会再发起一次读取请求。</p></blockquote><h4 id="读性能优化归纳"><a href="#读性能优化归纳" class="headerlink" title="读性能优化归纳"></a>读性能优化归纳</h4><p>提到读延迟较大无非三种常见的表象，单个业务慢、集群随机读慢以及某个业务随机读之后其他业务受到影响导致随机读延迟很大。</p><p>了解完常见的可能导致读延迟较大的一些问题之后，我们将这些问题进行如下归类，读者可以在看到现象之后在对应的问题列表中进行具体定位</p><p><img src="/images/blog/2020-03-05-3.png" alt></p><hr><p>参考链接</p><ul><li><a href="https://www.cnblogs.com/yingjie2222/p/6084255.html" target="_blank" rel="noopener">HBase读延迟的12种优化套路</a></li><li><a href="https://www.cnblogs.com/yanzibuaa/p/7568528.html" target="_blank" rel="noopener">HBase的TTL介绍</a></li><li><a href="https://blog.csdn.net/jewes/article/details/40189263" target="_blank" rel="noopener">详解HDFS Short Circuit Local Reads</a></li></ul>]]></content>
    
    <summary type="html">
    
      HBase 读优化套路
    
    </summary>
    
    
      <category term="HBase" scheme="http://yoursite.com/categories/HBase/"/>
    
    
  </entry>
  
  <entry>
    <title>ZooKeeper 客户端连接数</title>
    <link href="http://yoursite.com/2020/03/05/Zookeeper-maxClientCnxns/"/>
    <id>http://yoursite.com/2020/03/05/Zookeeper-maxClientCnxns/</id>
    <published>2020-03-04T16:00:00.000Z</published>
    <updated>2020-03-07T02:33:12.519Z</updated>
    
    <content type="html"><![CDATA[<p>配置参数将限制连接到 ZooKeeper 的客户端的数量，限制并发连接的数量</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>Zookeeper 容易出现连接数暴增事件，因此可以通过相应的配置来限制某 IP 的连接数</p><h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><p><strong>maxClientCnxns</strong></p><p>Limits the number of concurrent connections (at the socket level) that a single client, identified by IP address, may make to a single member of the ZooKeeper ensemble. This is used to prevent certain classes of DoS attacks, including file descriptor exhaustion. The default is 60. Setting this to 0 entirely removes the limit on concurrent connections.</p><blockquote><p>翻译过来就是将单个客户端（通过IP地址标识）可以与ZooKeeper集成中的单个成员建立的并发连接数（在套接字级别）受到限制。 这用于防止某些类的DoS攻击，包括文件描述符耗尽。 默认值为60。将其设置为0将完全消除并发连接的限制。</p></blockquote><p>需要明确地是这个限制的使用范围，仅仅是单台客户端机器与单台ZK服务器之间的连接数限制，不是针对指定客户端ID，也不是ZK集群的连接数限制，也不是单台ZK对所有客户端的连接数限制。</p><p>如果某 IP 连接数过多会报如下错误</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2011-10-28 09:39:44,856 – WARN  [NIOServerCxn.Factory:0.0.0.0&#x2F;0.0.0.0:5858:NIOServerCnxn$Factory@253] – Too many connections from &#x2F;172.*.*.* – max is 60</span><br></pre></td></tr></table></figure><hr><p>参考链接</p><ul><li><a href="https://blog.51cto.com/zlfwmm/1712403" target="_blank" rel="noopener">ZooKeeper客户端连接数过多</a></li><li><a href="https://blog.csdn.net/weixin_34238633/article/details/92595022" target="_blank" rel="noopener">记一次zookeeper连接数暴增事件</a></li><li><a href="https://www.jianshu.com/p/63f147de795b" target="_blank" rel="noopener">集群 zk 连接数过多问题排查</a></li></ul>]]></content>
    
    <summary type="html">
    
      ZooKeeper 限制客户端连接数
    
    </summary>
    
    
      <category term="Zookeeper" scheme="http://yoursite.com/categories/Zookeeper/"/>
    
    
  </entry>
  
  <entry>
    <title>NameNode 在高可用中防止脑裂</title>
    <link href="http://yoursite.com/2020/03/05/NameNode-HA-fencing/"/>
    <id>http://yoursite.com/2020/03/05/NameNode-HA-fencing/</id>
    <published>2020-03-04T16:00:00.000Z</published>
    <updated>2020-03-05T04:16:43.378Z</updated>
    
    <content type="html"><![CDATA[<p>CDH 从图表生成器添加图表</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>在分布式系统中脑裂又称为双主现象，由于网络问题可能会导致出现两个 NameNode 同时为 Active 状态，此时两个 NameNode 都可以对外提供服务，无法保证数据一致性。ActiveStandbyElector 通过 Fencing 机制防止脑裂现象。</p><h4 id="机制"><a href="#机制" class="headerlink" title="机制"></a>机制</h4><p>当某个 NameNode 竞选成功，成功创建 ActiveStandbyElectorLock 临时节点后会创建另一个名为 ActiveBreadCrumb 的持久节点，该节点保存了 NameNode 的地址信息，正常情况下删除 ActiveStandbyElectorLock 节点时会主动删除 ActiveBreadCrumb，但如果由于异常情况导致 Zookeeper Session关闭，此时临时节点 ActiveStandbyElectorLock 会被删除，但持久节点 ActiveBreadCrumb 并不会删除，当有新的 NameNode 竞选成功后它会发现已经存在一个旧的 NameNode 遗留下来的 ActiveBreadCrumb 节点，此时会通知 ZKFC 对旧的 ANN 进行 fencing，</p><p>在进行 fencing 的时候，会执行以下的操作:</p><ul><li>首先尝试调用这个旧 Active NameNode 的 HAServiceProtocol RPC 接口的 transitionToStandby 方法，看能不能把它转换为 Standby 状态。</li><li>如果 transitionToStandby 方法调用失败，那么就执行 Hadoop 配置文件之中预定义的隔离措施，Hadoop 目前主要提供两种隔离措施，通常会选择 sshfence：<ul><li>sshfence: 通过 SSH 登录到目标机器上，执行命令 fuser 将对应的进程杀死；</li><li>shellfence: 执行一个用户自定义的 shell 脚本来将对应的进程隔离；</li></ul></li></ul><p>具体如何使用可以参阅<a href="https://blog.csdn.net/tom_fans/article/details/89677681" target="_blank" rel="noopener">Hadoop NameNode HA fencing</a></p><p>只有在成功地执行完成 fencing 之后，选主成功的 ActiveStandbyElector 才会回调 ZKFailoverController 的 becomeActive 方法将对应的 NameNode 转换为 Active 状态，开始对外提供服务</p><p><img src="/images/blog/2020-03-05-1.png" alt></p><p><img src="/images/blog/2020-03-05-2.png" alt></p><h4 id="JournalNode-方面"><a href="#JournalNode-方面" class="headerlink" title="JournalNode 方面"></a>JournalNode 方面</h4><p>同一时刻只有一个 NameNode 可以写入 QJM, 这样就不担心脑裂问题。</p><p>Epoch 是一个单调递增的整数，用来标识每一次 Active NameNode 的生命周期，每发生一次 NameNode 的主备切换，Epoch 就会加 1。这实际上是一种 fencing 机制。</p><p>Epoch 的过程完全类似:</p><ol><li>Active NameNode 首先向 JournalNode 集群发送 getJournalState RPC 请求，每个 JournalNode 会返回自己保存的最近的那个 Epoch(代码中叫 lastPromisedEpoch)。</li><li>NameNode 收到大多数的 JournalNode 返回的 Epoch 之后，在其中选择最大的一个加 1 作为当前的新 Epoch，然后向各个 JournalNode 发送 newEpoch RPC 请求，把这个新的 Epoch 发给各个 JournalNode。</li><li>每一个 JournalNode 在收到新的 Epoch 之后，首先检查这个新的 Epoch 是否比它本地保存的 lastPromisedEpoch 大，如果大的话就把 lastPromisedEpoch 更新为这个新的 Epoch，并且向 NameNode 返回它自己的本地磁盘上最新的一个 EditLogSegment 的起始事务 id，为后面的数据恢复过程做好准备。如果小于或等于的话就向 NameNode 返回错误。</li><li>NameNode 收到大多数 JournalNode 对 newEpoch 的成功响应之后，就会认为生成新的 Epoch 成功</li></ol><p>在生成新的 Epoch 之后，每次 NameNode 在向 JournalNode 集群提交 EditLog 的时候，都会把这个 Epoch 作为参数传递过去。每个 JournalNode 会比较传过来的 Epoch 和它自己保存的 lastPromisedEpoch 的大小，如果传过来的 epoch 的值比它自己保存的 lastPromisedEpoch 小的话，那么这次写相关操作会被拒绝。一旦大多数 JournalNode 都拒绝了这次写操作，那么这次写操作就失败了。如果原来的 Active NameNode 恢复正常之后再向 JournalNode 写 EditLog，那么因为它的 Epoch 肯定比新生成的 Epoch 小，并且大多数的 JournalNode 都接受了这个新生成的 Epoch，所以拒绝写入的 JournalNode 数目至少是大多数，这样原来的 Active NameNode 写 EditLog 就肯定会失败，失败之后这个 NameNode 进程会直接退出，这样就实现了对原来的 Active NameNode 的隔离了。</p><hr><p>参考链接</p><ul><li><a href="https://www.jianshu.com/p/139fb06bea48" target="_blank" rel="noopener">NameNode在高可用中的防止脑裂</a></li><li><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-name-node/index.html" target="_blank" rel="noopener">Hadoop NameNode 高可用 (High Availability) 实现解析</a></li><li><a href="https://blog.csdn.net/tom_fans/article/details/89677681" target="_blank" rel="noopener">Hadoop NameNode HA fencing</a></li></ul>]]></content>
    
    <summary type="html">
    
      NameNode 在高可用中防止脑裂
    
    </summary>
    
    
      <category term="HDFS" scheme="http://yoursite.com/categories/HDFS/"/>
    
    
  </entry>
  
  <entry>
    <title>CDH 从图表生成器添加图表</title>
    <link href="http://yoursite.com/2020/03/03/CDH-chart/"/>
    <id>http://yoursite.com/2020/03/03/CDH-chart/</id>
    <published>2020-03-02T16:00:00.000Z</published>
    <updated>2020-03-05T11:32:50.037Z</updated>
    
    <content type="html"><![CDATA[<p>CDH 从图表生成器添加图表</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>CDH 是一款开源的集部署、监控、操作等于一体的 Hadoop 生态组件管理工具，也提供收费版（比免费版多提供数据备份恢复、故障定位等特性）。</p><p>CDH 提供的监控界面在体验上是非常优秀的，但是有时候有些监控项需要点到具体的组件实例了查看，挺麻烦的，因此我想直接加在组件面板里</p><h4 id="图表"><a href="#图表" class="headerlink" title="图表"></a>图表</h4><h5 id="HBase-各个RS读写请求"><a href="#HBase-各个RS读写请求" class="headerlink" title="HBase 各个RS读写请求"></a>HBase 各个RS读写请求</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">select read_requests_rate, write_requests_rate</span><br><span class="line">&#96;&#96;&#96; </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">![](&#x2F;images&#x2F;blog&#x2F;2020-03-03-1.png)</span><br><span class="line"></span><br><span class="line">##### Zookeeper 打开的连接数</span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96; </span><br><span class="line">SELECT open_connections</span><br></pre></td></tr></table></figure><p><img src="/images/blog/2020-03-03-4.png" alt></p>]]></content>
    
    <summary type="html">
    
      CDH 从图表生成器添加图表
    
    </summary>
    
    
      <category term="CDH" scheme="http://yoursite.com/categories/CDH/"/>
    
    
  </entry>
  
  <entry>
    <title>Unix 中的 I/O 模型</title>
    <link href="http://yoursite.com/2020/02/29/Unix-IO/"/>
    <id>http://yoursite.com/2020/02/29/Unix-IO/</id>
    <published>2020-02-28T16:00:00.000Z</published>
    <updated>2020-02-29T12:24:45.303Z</updated>
    
    <content type="html"><![CDATA[<p>叙述一下 Unix 下为解决不同 I/O 问题所设计的 I/O 模型</p><hr><h4 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h4><p>Unix 下存在五种 I/O 模型:      </p><ul><li>阻塞 I/O</li><li>非阻塞 I/O</li><li>I/O 复用（select和poll）</li><li>信号驱动 I/O（SIGIO）</li><li>异步 I/O</li></ul><p>水平有限，可以阅读下方链接即可</p><ul><li><a href="https://zhuanlan.zhihu.com/p/45745980" target="_blank" rel="noopener">UNIX 中的 I/O 模型</a></li><li><a href="https://blog.csdn.net/lmy86263/article/details/55681371" target="_blank" rel="noopener">Unix中的IO模型：帮你弄清阻塞VS非阻塞、同步VS异步</a></li><li><a href="https://www.cnblogs.com/51try-again/p/11078674.html" target="_blank" rel="noopener">Linux IO模式及 select、poll、epoll详解</a></li><li><a href="https://www.ibm.com/developerworks/cn/linux/l-async/index.html" target="_blank" rel="noopener">使用异步 I/O 大大提高应用程序的性能</a></li></ul>]]></content>
    
    <summary type="html">
    
      叙述一下 Unix 下为解决不同 I/O 问题所设计的 I/O 模型
    
    </summary>
    
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
  </entry>
  
  <entry>
    <title>NameNode RPC 延迟</title>
    <link href="http://yoursite.com/2020/02/27/NameNode-RPC-Latency/"/>
    <id>http://yoursite.com/2020/02/27/NameNode-RPC-Latency/</id>
    <published>2020-02-26T16:00:00.000Z</published>
    <updated>2020-02-27T08:22:44.996Z</updated>
    
    <content type="html"><![CDATA[<p>NameNode RPC 延迟分析</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>CDH 有个 NameNode RPC 延迟测试，用于检查NameNode响应请求所花费的平均时间的移动平均值不超过某个值。 </p><p>此运行状况测试失败，可能表明 NameNode 配置错误，NameNode 写入其数据目录之一时遇到问题，或者可能表明容量规划问题。</p><h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><p>检查 NameNode <code>RpcQueueTime_avg_time</code> 是否异常，如果耗费时间较长，这表明大部分RPC延迟都花在了请求排队上，请尝试增加 NameNode NameNode Handler Count，即<code>dfs.namenode.handler.count</code></p><p>如果 NameNode <code>RpcProcessingTime_avg_time</code> 指示大部分 RPC 延迟是由于请求处理引起的，请检查以确保存储 HDFS 元数据的每个目录运行正常，比如权限异常、磁盘问题</p><p>在 CDH NameNode 图表面板上可以看到这两个监控项曲线</p><p><img src="/images/blog/2020-02-27-1.png" alt></p><p>当然更多时候是人员不规范使用 HDFS 造成的，这种更多需要借助 hdfs-audit 分析 rpc 画像，找到幕后真凶，请参考之前写的<a href="https://lihuimintu.github.io/2019/12/25/elk-hdfs/" target="_blank" rel="noopener">采集 hdfs-audit 分析 rpc 画像</a></p><h4 id="监控设置"><a href="#监控设置" class="headerlink" title="监控设置"></a>监控设置</h4><p>可以使用 NameNode RPC 延迟阈值 和 NameNode RPC 延迟监视窗口 来配置此监控</p><p><img src="/images/blog/2020-02-27-2.png" alt></p><hr><p>参考链接</p><ul><li><a href="https://docs.cloudera.com/documentation/enterprise/latest/topics/cm_ht_namenode.html#concept_kx8_nzn_yk" target="_blank" rel="noopener">NameNode RPC Latency</a></li></ul>]]></content>
    
    <summary type="html">
    
      NameNode RPC 延迟分析
    
    </summary>
    
    
      <category term="HDFS" scheme="http://yoursite.com/categories/HDFS/"/>
    
    
  </entry>
  
  <entry>
    <title>IDEA 远程调试</title>
    <link href="http://yoursite.com/2020/02/25/IDEA-Remote/"/>
    <id>http://yoursite.com/2020/02/25/IDEA-Remote/</id>
    <published>2020-02-24T16:00:00.000Z</published>
    <updated>2020-03-07T02:28:34.008Z</updated>
    
    <content type="html"><![CDATA[<p>使用IDEA进行远程调试</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>想远程调试下测试环境的代码，因此记录下使用过程</p><h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><p><strong>Edit Configurations</strong></p><p><img src="/images/blog/2020-02-25-1.png" alt></p><p><strong>Remote</strong></p><p><img src="/images/blog/2020-02-25-2.png" alt></p><p><strong>Config</strong></p><p><img src="/images/blog/2020-02-25-3.png" alt></p><h4 id="启动远程项目"><a href="#启动远程项目" class="headerlink" title="启动远程项目"></a>启动远程项目</h4><p>正常启动命令如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -jar ***.jar</span><br></pre></td></tr></table></figure><p>开启远程调试</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -jar -agentlib:jdwp&#x3D;transport&#x3D;dt_socket,server&#x3D;y,suspend&#x3D;n,address&#x3D;5005 **.jar</span><br></pre></td></tr></table></figure><blockquote><p>其中 <code>-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005</code> 是从上一步方框里拷贝出来的</p></blockquote><h4 id="设置断点，开始调试"><a href="#设置断点，开始调试" class="headerlink" title="设置断点，开始调试"></a>设置断点，开始调试</h4><p>远程 debug 模式已经开启，现在可以在需要调试的代码中打断点了</p><p><img src="/images/blog/2020-02-25-4.png" alt></p><p>如图中所示，如果断点内有√，则表示选取的断点正确</p><h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><p>要保证远程调试监听的端口没有被占用，其次防火墙对端口放行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netstat -anlp | grep 5005</span><br></pre></td></tr></table></figure><hr><p>参考链接</p><ul><li><a href="https://www.cnblogs.com/victorbu/p/10904732.html" target="_blank" rel="noopener">IDEA 远程调试</a></li><li><a href="https://www.jianshu.com/p/302dc10217c0" target="_blank" rel="noopener">使用IDEA进行远程调试</a></li></ul>]]></content>
    
    <summary type="html">
    
      使用IDEA进行远程调试
    
    </summary>
    
    
      <category term="Tools" scheme="http://yoursite.com/categories/Tools/"/>
    
    
  </entry>
  
  <entry>
    <title>TCP 可信传输协议</title>
    <link href="http://yoursite.com/2020/02/16/TCP/"/>
    <id>http://yoursite.com/2020/02/16/TCP/</id>
    <published>2020-02-15T16:00:00.000Z</published>
    <updated>2020-03-02T06:13:11.314Z</updated>
    
    <content type="html"><![CDATA[<p>TCP 是基于全双工的可信传输协议</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><h4 id="TCP-状态"><a href="#TCP-状态" class="headerlink" title="TCP 状态"></a>TCP 状态</h4><p>了解 TCP 各个状态，对理解知识和定位问题有很大帮助，具体参看<a href="https://mp.weixin.qq.com/s/5XSM-yLwLc-HyoDdCCNyCA" target="_blank" rel="noopener">TCP连接的状态详解以及故障排查（上）</a>，下方列出会用到的状态</p><p>LISTENING: 表示侦听来自远方的TCP端口的连接请求</p><p>SYN-SENT: 客户端 SYN_SENT 状态</p><p>SYN-RECEIVED: 服务器端状态 SYN_RCVD</p><p>ESTABLISHED: 代表一个打开的连接</p><p>FIN-WAIT-1: 等待远程TCP连接中断请求，或先前的连接中断请求的确认</p><p>FIN-WAIT-2: 从远程TCP等待连接中断请求</p><p>CLOSE-WAIT: 等待从本地用户发来的连接中断请求</p><p>LAST-ACK: 等待原来的发向远程TCP的连接中断请求的确认</p><p>TIME-WAIT: 等待足够的时间以确保远程TCP接收到连接中断请求的确认</p><p>CLOSED: 没有任何连接状态</p><h4 id="三次握手、四次挥手"><a href="#三次握手、四次挥手" class="headerlink" title="三次握手、四次挥手"></a>三次握手、四次挥手</h4><p>具体可以参看<a href="https://mp.weixin.qq.com/s/NL7Jzh0lYoA395yzaGxBHw" target="_blank" rel="noopener">跟着动画学习TCP三次握手和四次挥手</a>，动画做的挺好的，也讲的挺通俗易懂的。</p><p>可以使用<code>tcpdump</code>命令来抓包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ tcpdump -iany -nn tcp port 80</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</span><br><span class="line">listening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes</span><br><span class="line">19:18:41.179701 IP 127.0.0.1.42524 &gt; 127.0.0.1.80: Flags [S], seq 712584015, win 43690, options [mss 65495,sackOK,TS val 4236991780 ecr 0,nop,wscale 7], length 0</span><br><span class="line">14:59:13.050531 IP 127.0.0.1.80 &gt; 127.0.0.1.42524: Flags [S.], seq 2917476214, ack 712584016, win 43690, options [mss 65495,sackOK,TS val 4236991780 ecr 4236991780,nop,wscale 7], length 0</span><br><span class="line">19:18:41.179727 IP 127.0.0.1.42524 &gt; 127.0.0.1.80: Flags [.], ack 1, win 342, options [nop,nop,TS val 4236991780 ecr 4236991780], length 0</span><br></pre></td></tr></table></figure><blockquote><p>-nn: 表示不解析域名，直接显示IP，在netstat命令中，也有这个选项<br>-i: 指定监听的网卡，如果为 -iany 则表示监听所有的网卡</p></blockquote><h4 id="TCP-特点"><a href="#TCP-特点" class="headerlink" title="TCP 特点"></a>TCP 特点</h4><p>首先，TCP 是<strong>基于连接的</strong>，也就是在进行数据传输之前，客户端与服务端(或者说是通信的双方)需要先建立一个可信的连接。在数据传输结束后，再通过一种协定的方式断开连接，由通信的双方释放资源。这里涉及到的，就是常说的”三次握手”、”四次挥手”</p><p>其次，TCP 是可靠的，它定义了一种数据包的”超时重传机制”，简单说，就是每一个数据包在发送出去后的都会等待一个响应。如果指定时间内没有收到响应，由发送方进行一定次数的重传来保证数据的可靠传输。</p><p>最后，TCP 是基于流的，这是指在传输数据时应用层不需要关注数据包的边界，TCP在数据传输时会自动根据网络环境将数据进行<font color="#FF0000">缓冲、分组、合并</font>。这点跟基于报文的协议(UDP)是截然不同的。当然，基于流的传输也保证了数据收发的有序性，因此每个数据包都附带上一个属于当前连接的序列号。</p><h4 id="TCP-与-UDP-区别"><a href="#TCP-与-UDP-区别" class="headerlink" title="TCP 与 UDP 区别"></a>TCP 与 UDP 区别</h4><table><thead><tr><th>TCP</th><th>UDP</th></tr></thead><tbody><tr><td>面向连接</td><td>无连接</td></tr><tr><td>提供可靠服务</td><td>不保证可靠</td></tr><tr><td>点到点</td><td>一对一，一对多，多对一和多对多</td></tr><tr><td>对系统资源要求较多</td><td>对系统资源要求较少</td></tr><tr><td>实时性低</td><td>实时性高</td></tr><tr><td>流模式</td><td>数据报模式，基于报文的</td></tr></tbody></table><hr><p>参考链接</p><ul><li><a href="https://mp.weixin.qq.com/s/NL7Jzh0lYoA395yzaGxBHw" target="_blank" rel="noopener">跟着动画学习TCP三次握手和四次挥手</a></li><li><a href="https://mp.weixin.qq.com/s/5XSM-yLwLc-HyoDdCCNyCA" target="_blank" rel="noopener">TCP连接的状态详解以及故障排查（上）</a></li><li><a href="https://mp.weixin.qq.com/s/Y4EELMzinGunW7MEAPjPgA" target="_blank" rel="noopener">TCP连接的状态详解以及故障排查（下）</a></li><li><a href="https://mp.weixin.qq.com/s/_UmaGl8yYLtajSgqMPjNYw" target="_blank" rel="noopener">校招面试必考的TCP</a></li><li><a href="https://www.jellythink.com/archives/478" target="_blank" rel="noopener">Linux tcpdump命令详解</a></li><li><a href="https://www.cnblogs.com/jason2013/articles/4346639.html" target="_blank" rel="noopener">TCP流模式和UDP数据报模式的区别</a></li><li><a href="https://mp.weixin.qq.com/s/Je5ZHyGPQe5p0ANV5B-gOQ" target="_blank" rel="noopener">TCP三次握手和四次挥手详解</a></li><li><a href="https://mp.weixin.qq.com/s/1eEBzkGp6WI5bp6697X3Gw" target="_blank" rel="noopener">搞了两周Socket通信，终于弄明白了！</a></li></ul>]]></content>
    
    <summary type="html">
    
      TCP 是基于全双工的可信传输协议
    
    </summary>
    
    
      <category term="Network" scheme="http://yoursite.com/categories/Network/"/>
    
    
  </entry>
  
  <entry>
    <title>Hive 查询表行数</title>
    <link href="http://yoursite.com/2020/02/13/Hive-RowNums/"/>
    <id>http://yoursite.com/2020/02/13/Hive-RowNums/</id>
    <published>2020-02-12T16:00:00.000Z</published>
    <updated>2020-02-16T08:51:11.513Z</updated>
    
    <content type="html"><![CDATA[<p>在 Hive 中比较快速地查到表的行数信息</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>有时在大数据处理时需要查询 Hive 一些数据表的行数，使用关系型数据库久了会使用<code>select count(*) from &lt;table&gt;;</code>方式进行查询，但是这种方式执行时间比较长，Hive 本身就有记录数据行数，所以可以直接拿到数据行数</p><h4 id="方式"><a href="#方式" class="headerlink" title="方式"></a>方式</h4><h5 id="元数据表"><a href="#元数据表" class="headerlink" title="元数据表"></a>元数据表</h5><p>从元数据表中查询元数据</p><p>查询表的总条数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">select a.TBL_ID, a.TBL_NAME, b.PARAM_KEY, b.PARAM_VALUE </span><br><span class="line">  from TBLS as a join TABLE_PARAMS as b </span><br><span class="line">  where a.TBL_ID &#x3D; b.TBL_ID and TBL_NAME&#x3D;&quot;call_center&quot; and PARAM_KEY&#x3D;&quot;numRows&quot;;</span><br><span class="line">+--------+-------------+-----------+-------------+</span><br><span class="line">| TBL_ID | TBL_NAME    | PARAM_KEY | PARAM_VALUE |</span><br><span class="line">+--------+-------------+-----------+-------------+</span><br><span class="line">|    134 | call_center | numRows   | 60          |</span><br><span class="line">+--------+-------------+-----------+-------------+</span><br></pre></td></tr></table></figure><h5 id="explain"><a href="#explain" class="headerlink" title="explain"></a>explain</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">explain select * from xxx limit 2;</span><br></pre></td></tr></table></figure><h5 id="HUE"><a href="#HUE" class="headerlink" title="HUE"></a>HUE</h5><p>从HUE上直接查看</p><p><img src="/images/blog/2020-02-13-1.png" alt></p><hr><p>参考链接</p><ul><li><a href="https://www.jianshu.com/p/b46c8d1f13f1" target="_blank" rel="noopener">Hive查询表行数</a></li><li><a href="https://blog.csdn.net/name_szd/article/details/80618515" target="_blank" rel="noopener">hive查询数据库总条数</a></li><li><a href="https://www.cnblogs.com/qingyunzong/p/8710356.html#_label2_0" target="_blank" rel="noopener">Hive学习之路 （三）Hive元数据信息对应MySQL数据库表</a></li></ul>]]></content>
    
    <summary type="html">
    
      Hive 查询表行数
    
    </summary>
    
    
      <category term="Hive" scheme="http://yoursite.com/categories/Hive/"/>
    
    
  </entry>
  
  <entry>
    <title>Linux CPU、Memory、IO、Network 分析</title>
    <link href="http://yoursite.com/2020/02/07/CPU-Memory-IO-Network/"/>
    <id>http://yoursite.com/2020/02/07/CPU-Memory-IO-Network/</id>
    <published>2020-02-06T16:00:00.000Z</published>
    <updated>2020-02-07T13:52:51.547Z</updated>
    
    <content type="html"><![CDATA[<p>Linux性能监控 - CPU、Memory、IO、Network</p><hr><h4 id="CPU"><a href="#CPU" class="headerlink" title="CPU"></a>CPU</h4><p>CPU 查看这些重要参数: 中断、上下文切换、可运行队列、CPU 利用率来监测性能。</p><p>每个参数的健康区间:</p><ul><li>CPU利用率: User Time &lt;= 70%，System Time &lt;= 35%，User Time + System Time &lt;= 70%。</li><li>上下文切换: 与CPU利用率相关联，如果CPU利用率状态良好，大量的上下文切换也是可以接受的。</li><li>可运行队列: 每个处理器的可运行队列 &lt;=3 个线程。如双处理器系统的可运行队列里不应该超过6个线程。</li></ul><p>运用到的工具有<a href="https://www.jellythink.com/archives/419" target="_blank" rel="noopener">Linux vmstat命令详解</a>、<a href="https://www.jellythink.com/archives/421" target="_blank" rel="noopener">Linux top命令详解</a>、<a href="https://www.cnblogs.com/mayou18/p/9546431.html" target="_blank" rel="noopener">Linux mpstat-显示各个可用CPU的状态</a></p><p>top 查看 CPU 利用率，如果 User Time 比较高，使用 <a href="https://github.com/oldratlee/useful-scripts/blob/dev-2.x/docs/java.md#-show-busy-java-threads" target="_blank" rel="noopener">show-busy-java-threads</a> 工具排查，其会从所有运行的Java进程中找出最消耗CPU的线程（缺省5个），打印出其线程栈。<a href="https://lihuimintu.github.io/2019/06/03/Occupying-CPU-high/" target="_blank" rel="noopener">Linux 系统 CPU 过高异常排查</a></p><p>如果 System Time 比 User Time 高，以及高频度的上下文切换（cs），说明应用程序进行了大量的系统调用。</p><p>vmstat 输出例子</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----</span><br><span class="line"> r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st</span><br><span class="line"> 3  0 361396 196772  55820 359372    0    0    13    21    1    1  2  0 98  0  0</span><br><span class="line"> 1  0 361392 196524  55820 359616    8    0   236     0  411  527  1  0 90  9  0</span><br><span class="line"> 2  1 361392 196524  55828 359608    0    0     0    48  370  503  1  1 98  0  0</span><br><span class="line"> 4  0 361392 196524  55828 359616    0    0     0     0  442  559  1  0 99  0  0</span><br></pre></td></tr></table></figure><p>需要关注的参数</p><ul><li>r: 当前运行队列中线程的数目，代表线程处于可运行状态，但CPU还未能执行</li><li>b: 等待IO的进程数量；<strong>如果该值一直都很大，说明IO比较繁忙，处理较慢</strong></li><li>in: 每秒中断数</li><li>cs: 每秒上下文切换数</li><li>us: 用户占用CPU的百分比</li><li>sys: 内核占用CPU的百分比</li><li>id: CPU 完全空闲的百分比</li></ul><h4 id="Memory"><a href="#Memory" class="headerlink" title="Memory"></a>Memory</h4><p>分析 vmstat 输出</p><ul><li>si: 每秒从交换区写到内存的大小</li><li>so: 每秒写入交换区的内存大小</li></ul><p>内存够用的时候，这2个值都是0，如果这2个值长期大于0时，系统性能会受到影响，磁盘IO和CPU资源都会被消耗。有时我们看到空闲内存（free）很少的或接近于0时，就认为内存不够用了，不能光看这一点，还要结合si和so，如果free很少，但是si和so也很少（大多时候是0），那么不用担心，系统性能这时不会受到影响的</p><p><a href="https://mp.weixin.qq.com/s/XAhASXPkIELTvwwY8QPXng" target="_blank" rel="noopener">查询进程占用内存情况方法</a></p><h4 id="IO"><a href="#IO" class="headerlink" title="IO"></a>IO</h4><p>通过 <code>vmstat</code> 的输出，重点关注<code>b</code>、<code>bi</code>、<code>bo</code>和<code>wa</code>字段。这几个值变大，都意味着IO的消耗增加。</p><p>对于读请求大的服务器，一般<code>b</code>、<code>bi</code>、<code>wa</code>都会比较大，而对于写入量大的服务器，一般<code>b</code>、<code>bo</code>、<code>wa</code>都会比较大。</p><p>借助<a href="https://www.jellythink.com/archives/438" target="_blank" rel="noopener">Linux iostat命令详解</a>可以查看相关参数</p><ul><li>%iowait: 如果该值较高，表示磁盘存在I/O瓶颈</li><li>await: 平均每次设备I/O操作的等待时间 (毫秒)，一般地，系统I/O响应时间应该低于5ms，如果大于 10ms就比较大了</li><li>%util：一秒中有百分之多少的时间用于I/O操作，即被IO消耗的CPU百分比，一般地，如果该参数是100%表示设备已经接近满负荷运行了</li></ul><p>借助<code>pidstat -d 1</code>定位出导致瓶颈的进程，参阅<a href="https://www.jellythink.com/archives/444" target="_blank" rel="noopener">Linux pidstat命令详解</a></p><p>现在定位到进程级别了，可能需要知道这个进程到底打开了哪些文件，借助<code>lsof -p 20711</code>命令列出指定20711进程打开的文件列表，参阅<a href="https://www.jellythink.com/archives/449" target="_blank" rel="noopener">Linux lsof命令详解</a></p><h4 id="Network"><a href="#Network" class="headerlink" title="Network"></a>Network</h4><p>网络的监测是所有 Linux 子系统里面最复杂的，这里贴了解到的运维命令</p><p>iftop 命令查看端到端流量</p><p>NetHogs实时监控进程的网络带宽占用情况，参阅<a href="https://lihuimintu.github.io/2019/12/06/Linux-nethogs/" target="_blank" rel="noopener">Linux NetHogs 监控工具</a></p><p>还有些<code>ip</code>，<code>netstat</code>，<code>tcpdump</code>，<code>sar</code> 来分析网络性能问题，参阅<a href="https://www.jellythink.com/archives/486" target="_blank" rel="noopener">Linux性能监测：网络篇</a></p><p><strong>UDP监控</strong></p><p>对于UDP服务，查看所有监听的UDP端口的网络情况</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$  watch netstat -unlp</span><br><span class="line">Every 2.0s: netstat -unlp                                                                                                                      Fri Feb  7 21:36:55 2020</span><br><span class="line"></span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         StatePID&#x2F;Program name</span><br><span class="line">udp        0  0 0.0.0.0:627             0.0.0.0:*                           6813&#x2F;rpcbind</span><br><span class="line">udp        0  0 0.0.0.0:7191            0.0.0.0:*                           8137&#x2F;python2</span><br><span class="line">udp        0  0 0.0.0.0:68              0.0.0.0:*                           780&#x2F;dhclient</span><br><span class="line">udp        0  0 0.0.0.0:111             0.0.0.0:*                           1&#x2F;systemd</span><br><span class="line">udp        0  0 10.0.0.122:123          0.0.0.0:*                           3924&#x2F;ntpd</span><br><span class="line">udp        0  0 127.0.0.1:123           0.0.0.0:*                           3924&#x2F;ntpd</span><br><span class="line">udp        0  0 0.0.0.0:123             0.0.0.0:*                           3924&#x2F;ntpd</span><br><span class="line">udp6   0  0 :::627                  :::*                                6813&#x2F;rpcbind</span><br><span class="line">udp6   0  0 :::7191                 :::*                                8137&#x2F;python2</span><br><span class="line">udp6   0  0 :::111                  :::*                                6813&#x2F;rpcbind</span><br><span class="line">udp6   0  0 :::123                  :::*                                3924&#x2F;ntpd</span><br></pre></td></tr></table></figure><p>对于<code>Recv-Q</code>和<code>Send-Q</code>两个指标值为0，或者没有长时间大于0的数值是比较正常的。</p><p>对于UDP服务，查看丢包情况（网卡收到了，但是应用层没有处理过来造成的丢包）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ netstat -su</span><br><span class="line">...</span><br><span class="line">Udp:</span><br><span class="line">    14706185 packets received</span><br><span class="line">    272545 packets to unknown port received.</span><br><span class="line">    11026 packet receive errors</span><br><span class="line">    46474392 packets sent</span><br><span class="line">    10964 receive buffer errors</span><br><span class="line">    0 send buffer errors</span><br><span class="line">    InCsumErrors: 62</span><br><span class="line">....</span><br></pre></td></tr></table></figure><p><code>packet receive errors</code> 这一项数值增长了，则表明在丢包。</p><p><strong>TCP监控</strong></p><p>对于TCP服务而言，这个就比较复杂；因为TCP涉及到重传，所以我们就需要重点关注这个重传率。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ netstat -st | grep segments</span><br><span class="line">    689039681 segments received</span><br><span class="line">    864949096 segments send out</span><br><span class="line">    2233493 segments retransmited</span><br><span class="line">    5186 bad segments received.</span><br></pre></td></tr></table></figure><p>查看<code>segments send out</code>和<code>segments retransmited</code>指标，对比一段时间内，这两个指标的增长率就是对应的重传率(<a href="https://www.jellythink.com/archives/486" target="_blank" rel="noopener">Linux性能监测：网络篇</a>文末尾有计算重传率的Shell脚本)，发生重传说明网络传输有丢包，基本上从3个点去定位：客户端网络情况、服务端网络情况、中间链路网络情况。</p><p><strong>网卡吞吐率</strong></p><p>可以通过<code>sar -n DEV 2 3</code>命令来查看</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ sar -n DEV 2 3</span><br><span class="line">Linux 3.10.0-862.14.4.el7.x86_64 (region-0-0-122) 2020年02月07日 _x86_64_(4 CPU)</span><br><span class="line"></span><br><span class="line">21时43分32秒     IFACE   rxpck&#x2F;s   txpck&#x2F;s    rxkB&#x2F;s    txkB&#x2F;s   rxcmp&#x2F;s   txcmp&#x2F;s  rxmcst&#x2F;s</span><br><span class="line">21时43分34秒      eth0    116.50    119.50     11.77     19.72      0.00      0.00      0.00</span><br><span class="line">21时43分34秒        lo     66.00     66.00    144.26    144.26      0.00      0.00      0.00</span><br><span class="line"></span><br><span class="line">21时43分34秒     IFACE   rxpck&#x2F;s   txpck&#x2F;s    rxkB&#x2F;s    txkB&#x2F;s   rxcmp&#x2F;s   txcmp&#x2F;s  rxmcst&#x2F;s</span><br><span class="line">21时43分36秒      eth0    155.00    152.00     19.60     44.66      0.00      0.00      0.00</span><br><span class="line">21时43分36秒        lo     29.00     29.00     94.53     94.53      0.00      0.00      0.00</span><br><span class="line"></span><br><span class="line">21时43分36秒     IFACE   rxpck&#x2F;s   txpck&#x2F;s    rxkB&#x2F;s    txkB&#x2F;s   rxcmp&#x2F;s   txcmp&#x2F;s  rxmcst&#x2F;s</span><br><span class="line">21时43分38秒      eth0    116.00    118.00     11.86     19.80      0.00      0.00      0.00</span><br><span class="line">21时43分38秒        lo     33.00     33.00     68.67     68.67      0.00      0.00      0.00</span><br><span class="line"></span><br><span class="line">平均时间:     IFACE   rxpck&#x2F;s   txpck&#x2F;s    rxkB&#x2F;s    txkB&#x2F;s   rxcmp&#x2F;s   txcmp&#x2F;s  rxmcst&#x2F;s</span><br><span class="line">平均时间:      eth0    129.17    129.83     14.41     28.06      0.00      0.00      0.00</span><br><span class="line">平均时间:        lo     42.67     42.67    102.49    102.49      0.00      0.00      0.00</span><br></pre></td></tr></table></figure><p>将<code>rxkB/s</code>和<code>txkB/s</code>进行相加，得到网卡设备的实际吞吐率，然后再和网卡的硬件指标进行比对即可。</p><p>比如一个网卡的<code>rxkB/s</code>指标为21999.10，<code>txkB/s</code>指标为482.56，那这个网卡的吞吐率大概在<code>22Mbytes/s</code>，即<code>176 Mbits/sec</code>，没有达到<code>1Gbit/sec</code>的硬件上限。</p><hr><p>参考链接</p><ul><li><a href="https://www.cnblogs.com/linuxbug/p/4909980.html" target="_blank" rel="noopener">Linux性能监控 - CPU、Memory、IO、Network</a></li></ul>]]></content>
    
    <summary type="html">
    
      Linux 性能监控
    
    </summary>
    
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
  </entry>
  
  <entry>
    <title>缺页中断算法 - LRU</title>
    <link href="http://yoursite.com/2020/02/05/LRU/"/>
    <id>http://yoursite.com/2020/02/05/LRU/</id>
    <published>2020-02-04T16:00:00.000Z</published>
    <updated>2020-02-05T08:09:48.486Z</updated>
    
    <content type="html"><![CDATA[<p>最近最久未使用置换算法</p><hr><h4 id="缺页中断"><a href="#缺页中断" class="headerlink" title="缺页中断"></a>缺页中断</h4><p>缺页中断就是 CPU 要访问的页不在主存，需要操作系统将其调入主存后再进行访问</p><p>访问的页面不在内存时，会产生一次缺页中断，缺页中断是由于所要访问的页面不存在主内存时触发，属于由硬件所产生的一种特殊的中断，也称之为硬中断。</p><p>缺页本身是一种中断，与软中断一样，需要经过4个处理步骤</p><ol><li>保护CPU现场 </li><li>分析中断原因 </li><li>转入缺页中断处理程序进行处理 </li><li>恢复CPU现场，继续执行</li></ol><p>缺页中断更多可以阅读<a href="https://liam.page/2017/09/01/page-fault/" target="_blank" rel="noopener">程序员的自我修养（七）：内存缺页错误</a></p><h4 id="页面置换算法"><a href="#页面置换算法" class="headerlink" title="页面置换算法"></a>页面置换算法</h4><p>进程运行过程中，如果发生缺页中断，而此时内存中有没有空闲的物理块时，<br>为了能够把所缺的页面装入内存，系统必须从内存中选择一页调出到磁盘的对换区。<br>但此时应该把那个页面换出，则需要根据一定的页面置换算法（Page Replacement Algorithm)来确定。</p><p>页面置换算法有 OPT、FIFO、LRU 三种算法。OPT、FIFO 大家自行阅读<a href="https://blog.csdn.net/Youth_Mr6/article/details/82767332" target="_blank" rel="noopener">缺页中断算法(FIFO,LRU)</a></p><h4 id="LRU"><a href="#LRU" class="headerlink" title="LRU"></a>LRU</h4><p>最近最久未使用置换算法（Least Recently Used）</p><p>置换最近一段时间以来最长时间未访问过的页面。根据程序局部性原理，刚被访问的页面，可能马上又要被访问；而较长时间内没有被访问的页面，可能最近不会被访问。 </p><p>采用固定分配局部置换的策略，假定系统为某进程在内存中分配了3个物理页，页面访问顺序为2、3、2、1、5、2、4、5、3、2、5、2。假定系统未采用预调页策略，即未事先调入任何页面。</p><p><img src="/images/blog/2020-02-05-1.png" alt></p><p>中断次数为7，缺页中断率为7/12*100% = 58.3%</p><h4 id="数据结构实现实现-LRU"><a href="#数据结构实现实现-LRU" class="headerlink" title="数据结构实现实现 LRU"></a>数据结构实现实现 LRU</h4><p>哈希表 + 双向链表</p><p>用哈希表，辅以双向链表记录键值对的信息。所以可以在 <code>O(1)</code> 时间内完成 put 和 get 操作，同时也支持 <code>O(1)</code> 删除第一个添加的节点。</p><p><img src="/images/blog/2020-02-05-2.png" alt></p><p>使用双向链表的一个好处是不需要额外信息删除一个节点，同时可以在常数时间内从头部或尾部插入删除节点。</p><p>一个需要注意的是，在双向链表实现中，这里使用一个伪头部和伪尾部标记界限，这样在更新的时候就不需要检查是否是 null 节点。</p><p><img src="/images/blog/2020-02-05-3.png" alt></p><p>感兴趣可以到 LeetCode 做这道题——<a href="https://leetcode-cn.com/problems/lru-cache/solution/lru-huan-cun-ji-zhi-by-leetcode/" target="_blank" rel="noopener">146. LRU缓存机制</a></p><hr><p>参考链接</p><ul><li><a href="https://blog.csdn.net/Youth_Mr6/article/details/82767332" target="_blank" rel="noopener">缺页中断算法(FIFO,LRU)</a></li><li><a href="https://leetcode-cn.com/problems/lru-cache/solution/lru-huan-cun-ji-zhi-by-leetcode/" target="_blank" rel="noopener">146. LRU缓存机制</a></li></ul>]]></content>
    
    <summary type="html">
    
      最近最久未使用置换算法
    
    </summary>
    
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
  </entry>
  
  <entry>
    <title>Zookeeper 解决脑裂原理</title>
    <link href="http://yoursite.com/2020/02/04/ZK-Split-Brain/"/>
    <id>http://yoursite.com/2020/02/04/ZK-Split-Brain/</id>
    <published>2020-02-03T16:00:00.000Z</published>
    <updated>2020-02-17T05:30:16.496Z</updated>
    
    <content type="html"><![CDATA[<p>这是分布式系统中一个很实际的问题</p><hr><h4 id="转载来源"><a href="#转载来源" class="headerlink" title="转载来源"></a>转载来源</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">作者：1点25</span><br><span class="line">链接：https:&#x2F;&#x2F;juejin.im&#x2F;post&#x2F;5d36c2f25188257f6a209d37</span><br><span class="line">来源: 掘金</span><br></pre></td></tr></table></figure><h4 id="脑裂"><a href="#脑裂" class="headerlink" title="脑裂"></a>脑裂</h4><p>脑裂(split-brain)就是“大脑分裂”，也就是本来一个“大脑”被拆分了两个或多个“大脑”，我们都知道，如果一个人有多个大脑，并且相互独立的话，那么会导致人体“手舞足蹈”，“不听使唤”。<br>脑裂通常会出现在集群环境中，比如ElasticSearch、Zookeeper集群，而这些集群环境有一个统一的特点，就是它们有一个大脑，比如ElasticSearch集群中有Master节点，Zookeeper集群中有Leader节点。</p><h4 id="Zookeeper-集群中的脑裂场景"><a href="#Zookeeper-集群中的脑裂场景" class="headerlink" title="Zookeeper 集群中的脑裂场景"></a>Zookeeper 集群中的脑裂场景</h4><p>对于一个集群，想要提高这个集群的可用性，通常会采用多机房部署，比如现在有一个由6台zkServer所组成的一个集群，部署在了两个机房</p><p><img src="/images/blog/2020-02-04-1.png" alt></p><p>正常情况下，此集群只会有一个Leader，那么如果机房之间的网络断了之后，两个机房内的zkServer还是可以相互通信的，如果<strong>不考虑过半机制</strong>，那么就会出现每个机房内部都将选出一个Leader。</p><p><img src="/images/blog/2020-02-04-2.png" alt></p><p>这就相当于原本一个集群，被分成了两个集群，出现了两个“大脑”，这就是脑裂。<br>对于这种情况，我们也可以看出来，原本应该是统一的一个集群对外提供服务的，现在变成了两个集群同时对外提供服务，如果过了一会，断了的网络突然联通了，那么此时就会出现问题了，两个集群刚刚都对外提供服务了，数据该怎么合并，数据冲突怎么解决等等问题。<br>刚刚在说明脑裂场景时，有一个前提条件就是没有考虑过半机制，所以实际上Zookeeper集群中是不会出现脑裂问题的，而不会出现的原因就跟过半机制有关。</p><h4 id="过半机制"><a href="#过半机制" class="headerlink" title="过半机制"></a>过半机制</h4><p>在领导者选举的过程中，如果某台zkServer获得了超过半数的选票，则此zkServer就可以成为Leader了。</p><p>过半机制的源码实现其实非常简单：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">public class QuorumMaj implements QuorumVerifier &#123;</span><br><span class="line">    private static final Logger LOG &#x3D; LoggerFactory.getLogger(QuorumMaj.class);</span><br><span class="line">    </span><br><span class="line">    int half;</span><br><span class="line">    </span><br><span class="line">    &#x2F;&#x2F; n表示集群中zkServer的个数（准确的说是参与者的个数，参与者不包括观察者节点）</span><br><span class="line">    public QuorumMaj(int n)&#123;</span><br><span class="line">        this.half &#x3D; n&#x2F;2;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; 验证是否符合过半机制</span><br><span class="line">    public boolean containsQuorum(Set&lt;Long&gt; set)&#123;</span><br><span class="line">        &#x2F;&#x2F; half是在构造方法里赋值的</span><br><span class="line">        &#x2F;&#x2F; set.size()表示某台zkServer获得的票数</span><br><span class="line">        return (set.size() &gt; half);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>大家仔细看一下上面方法中的注释，核心代码就是下面两行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">this.half &#x3D; n&#x2F;2;</span><br><span class="line">return (set.size() &gt; half);</span><br></pre></td></tr></table></figure><p>举个简单的例子： 如果现在集群中有5台zkServer，那么half=5/2=2，那么也就是说，领导者选举的过程中至少要有三台zkServer投了同一个zkServer，才会符合过半机制，才能选出来一个Leader。</p><p>那么有一个问题我们想一下，<strong>选举的过程中为什么一定要有一个过半机制验证？</strong>因为这样不需要等待所有zkServer都投了同一个zkServer就可以选举出来一个Leader了，这样比较快，所以叫快速领导者选举算法呗。</p><p>那么再来想一个问题，<strong>过半机制中为什么是大于，而不是大于等于呢？</strong></p><p>这就是跟脑裂问题有关系了，比如回到上文出现脑裂问题的场景：</p><p><img src="/images/blog/2020-02-04-3.png" alt></p><p>当机房中间的网络断掉之后，机房1内的三台服务器会进行领导者选举，但是此时过半机制的条件是set.size() &gt; 3，也就是说至少要4台zkServer才能选出来一个Leader，所以对于机房1来说它不能选出一个Leader，同样机房2也不能选出一个Leader，这种情况下整个集群当机房间的网络断掉后，整个集群将没有Leader。</p><p>而如果过半机制的条件是set.size() &gt;= 3，那么机房1和机房2都会选出一个Leader，这样就出现了脑裂。所以我们就知道了，为什么过半机制中是大于，而不是大于等于。就是为了防止脑裂。</p><p>如果假设我们现在只有5台机器，也部署在两个机房</p><p><img src="/images/blog/2020-02-04-4.png" alt></p><p>此时过半机制的条件是set.size() &gt; 2，也就是至少要3台服务器才能选出一个Leader，此时机房件的网络断开了，对于机房1来说是没有影响的，Leader依然还是Leader，对于机房2来说是选不出来Leader的，此时整个集群中只有一个Leader。</p><p>所以，我们可以总结得出，有了过半机制，对于一个Zookeeper集群，要么没有Leader，要没只有1个Leader，这样就避免了脑裂问题。</p><blockquote><p>Q: 在阅读时我就想如果机房1有1个leader和1个follower,机房2有3个follower, 这种情况呢？<br>A: 每当新leader产生时，会生成一个epoch，这个epoch是递增的，followers如果确认了新的leader存在，知道其epoch，就会拒绝epoch小于现任leader epoch的所有请求。所以旧leader无法被follower认可。</p><p>假设某个leader假死，其余的followers选举出了一个新的leader。这时，旧的leader复活并且仍然认为自己是leader，这个时候它向其他followers发出写请求也是会被拒绝的。因为每当新leader产生时，会生成一个epoch，这个epoch是递增的，followers如果确认了新的leader存在，知道其epoch，就会拒绝epoch小于现任leader epoch的所有请求。那有没有follower不知道新的leader存在呢，有可能，但肯定不是大多数，否则新leader无法产生。Zookeeper的写也遵循quorum机制，因此，得不到大多数支持的写是无效的，旧leader即使各种认为自己是leader，依然没有什么作用。</p></blockquote><h4 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h4><p>解决 Split-Brain 的问题，一般有3种方式</p><ul><li>Quorums（法定人数）: 比如3个节点的集群，Quorums = 2, 也就是说集群可以容忍1个节点失效，这时候还能选举出1个lead，集群还可用。比如4个节点的集群，它的Quorums = 3，Quorums要超过3，相当于集群的容忍度还是1，如果2个节点失效，那么整个集群还是无效的</li><li>Redundant communications: 冗余通信的方式，集群中采用多种通信方式，防止一种通信方式失效导致集群中的节点无法通信。</li><li>Fencing: 共享资源的方式，比如能看到共享资源就表示在集群中，能够获得共享资源的锁的就是Leader，看不到共享资源的，就不在集群中。HDFS NameNode 使用的就是这种方式</li></ul><p>ZooKeeper默认采用了Quorums这种方式，即只有集群中超过半数节点投票才能选举出Leader。这样的方式可以确保leader的唯一性,要么选出唯一的一个leader，要么选举失败。</p><p>在ZooKeeper中Quorums有2个作用:</p><ul><li>集群中最少的节点数用来选举Leader保证集群可用</li><li>通知客户端数据已经安全保存，前集群中最少数量的节点数已经保存了该数据。一旦这些节点保存了该数据，客户端将被通知已经安全保存了，可以继续其他任务。而集群中剩余的节点将会最终也保存了该数据</li></ul><hr><p>参考链接</p><ul><li><a href="https://blog.csdn.net/u013374645/article/details/93140148" target="_blank" rel="noopener">面试题-Zookeeper是如何解决脑裂问题</a></li><li><a href="https://blog.csdn.net/bigtree_3721/article/details/78389492" target="_blank" rel="noopener">ZooKeeper的Quorums机制</a></li></ul>]]></content>
    
    <summary type="html">
    
      脑裂是什么？Zookeeper是如何解决的？
    
    </summary>
    
    
      <category term="Zookeeper" scheme="http://yoursite.com/categories/Zookeeper/"/>
    
    
  </entry>
  
  <entry>
    <title>CDH 集群中 DN 热换盘处理</title>
    <link href="http://yoursite.com/2020/02/03/DN-Fault-Raid/"/>
    <id>http://yoursite.com/2020/02/03/DN-Fault-Raid/</id>
    <published>2020-02-02T16:00:00.000Z</published>
    <updated>2020-02-06T03:30:13.320Z</updated>
    
    <content type="html"><![CDATA[<p>在集群使用的过程中会遇到数据节点的磁盘故障，在不停数据节点的情况下，如何为数据节点进行热插拔换盘操作</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>HDFS 随着使用时间会出现数据借点磁盘故障现象，网上查阅到的大部分都是停止数据节点，换盘后重启。我司处理的方法是热插拔处理。</p><h4 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h4><p>具体参阅 Fayson 的<a href="https://cloud.tencent.com/developer/article/1158329" target="_blank" rel="noopener">如何在CDH集群中为数据节点热插拔硬盘</a></p><p>Fayson 自己也说操作步骤更类似于加盘操作，磁盘坏掉如果磁盘的盘符未变更则只需要将磁盘格式化挂载在原来的目录下。</p><p>变通方式是先将其取消坏盘挂载，更换新盘后格式化再挂载回来，DN 执行刷新数据目录操作即可。</p><p>这样会出现节点内磁盘不均衡的现象，需要借助节点内平衡 DiskBalancer，更多参考 <a href="https://issues.apache.org/jira/browse/HDFS-1312" target="_blank" rel="noopener">HDFS-1312</a></p><hr><p>参考链接</p><ul><li><a href="https://cloud.tencent.com/developer/article/1158329" target="_blank" rel="noopener">如何在CDH集群中为数据节点热插拔硬盘</a></li><li><a href="https://ilinuxkernel.com/?p=958" target="_blank" rel="noopener">Linux硬盘盘符分配</a></li></ul>]]></content>
    
    <summary type="html">
    
      如何在CDH集群中为数据节点热插拔硬盘
    
    </summary>
    
    
      <category term="CDH" scheme="http://yoursite.com/categories/CDH/"/>
    
    
  </entry>
  
  <entry>
    <title>内存缺页错误</title>
    <link href="http://yoursite.com/2020/02/01/Page-Fault/"/>
    <id>http://yoursite.com/2020/02/01/Page-Fault/</id>
    <published>2020-01-31T16:00:00.000Z</published>
    <updated>2020-02-24T03:05:19.008Z</updated>
    
    <content type="html"><![CDATA[<p>缺页中断就是要访问的页不在主存，需要操作系统将其调入主存后再进行访问。</p><hr><h4 id="转载来源"><a href="#转载来源" class="headerlink" title="转载来源"></a>转载来源</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">作者：Liam Huang</span><br><span class="line">链接：https:&#x2F;&#x2F;liam.page&#x2F;2017&#x2F;09&#x2F;01&#x2F;page-fault&#x2F;</span><br></pre></td></tr></table></figure><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>众所周知，CPU 不能直接和硬盘进行交互。CPU 所作的一切运算，都是通过 CPU 缓存间接与内存进行操作的。若是 CPU 请求的内存数据在物理内存中不存在，那么 CPU 就会报告「缺页错误（Page Fault）」，提示内核产生中断，将所缺的页面装入内存。</p><p>在内核处理缺页错误时，就有可能进行磁盘的读写操作。这样的操作，相对 CPU 的处理是非常缓慢的。因此，发生大量的缺页错误，势必会对程序的性能造成很大影响。因此，在对性能要求很高的环境下，应当尽可能避免这种情况。</p><p>此篇介绍缺页错误本身，并结合一个实际示例作出一些实践分析。这里主要在 Linux 的场景下做讨论；其他现代操作系统，基本也是类似的。</p><h4 id="内存页和缺页错误"><a href="#内存页和缺页错误" class="headerlink" title="内存页和缺页错误"></a>内存页和缺页错误</h4><p>现代 CPU 都支持分段和分页的内存寻址模式。在 Linux 当中，实际起作用的只有分页模式。</p><p>具体来说，分页模式在逻辑上将虚拟内存和物理内存同时等分成固定大小的块。这些块在虚拟内存上称之为「页」，而在物理内存上称之为「页帧」，并交由 CPU 中的 MMU 模块(内存管理单元)来负责页帧和页之间的映射管理。</p><p>引入分页模式的好处，可以大致概括为两个方面: </p><ul><li>允许虚存空间远大于实际物理内存大小的情况。这是因为，分页之后，操作系统读入磁盘的文件时，无需以文件为单位全部读入，而可以以内存页为单位，分片读入。同时，考虑到 CPU 不可能一次性需要使用整个内存中的数据，因此可以交由特定的算法，进行内存调度: 将长时间不用的页帧内的数据暂存到磁盘上。</li><li>减少了内存碎片的产生。这是因为，引入分页之后，内存的分配管理都是以页大小（通常是 4KiB，扩展分页模式下是 4MiB）为单位的；虚拟内存中的页总是对应物理内存中实际的页帧。这样一来，在虚拟内存空间中，页内连续的内存在物理内存上也一定是连续的，不会产生碎片。</li></ul><h4 id="缺页错误"><a href="#缺页错误" class="headerlink" title="缺页错误"></a>缺页错误</h4><p>当进程在进行一些计算时，CPU 会请求内存中存储的数据。在这个请求过程中，CPU 发出的地址是逻辑地址（虚拟地址），然后交由 CPU 当中的 MMU 单元进行内存寻址，找到实际物理内存上的内容。若是目标虚存空间中的内存页（因为某种原因），在物理内存中没有对应的页帧，那么 CPU 就无法获取数据。这种情况下，CPU 是无法进行计算的，于是它就会报告一个缺页错误（Page Fault）。</p><p>因为 CPU 无法继续进行进程请求的计算，并报告了缺页错误，用户进程必然就中断了。这样的中断称之为缺页中断，因为由 CPU 以外的硬件产生的中断，也称之为硬中断。在报告 Page Fault 之后，进程会从用户态切换到系统态，交由操作系统内核的 Page Fault Handler 处理缺页错误。</p><h4 id="分类和处理"><a href="#分类和处理" class="headerlink" title="分类和处理"></a>分类和处理</h4><p>基本来说，缺页错误可以分为两类: 硬缺页错误（Hard Page Fault）和软缺页错误（Soft Page Fault）。这里，前者又称为主要缺页错误（Major Page Fault）；后者又称为次要缺页错误（Minor Page Fault）。当缺页中断发生后，Page Fault Handler 会判断缺页的类型，进而处理缺页错误，最终将控制权交给用户态代码。</p><p>若是此时物理内存里，已经有一个页帧正是此时 CPU 请求的内存页，那么这是一个软缺页错误；于是，Page Fault Hander 会指示 MMU 建立相应的页帧到页的映射关系。这一操作的实质是进程间共享内存——比如动态库（共享对象），比如 mmap 的文件。</p><p>若是此时物理内存中，没有相应的页帧，那么这就是一个硬缺页错误；于是 Page Fault Hander 会指示 CPU，从已经打开的磁盘文件中读取相应的内容到物理内存，而后交由 MMU 建立这份页帧到页的映射关系。</p><p>不难发现，软缺页错误只是在内核态里轻轻地走了一遭，而硬缺页错误则涉及到磁盘 I/O。因此，处理起来，硬缺页错误要比软缺页错误耗时长得多。这就是为什么我们要求高性能程序必须在对外提供服务时，尽可能少地发生硬缺页错误。</p><blockquote><p>除了硬缺页错误和软缺页错误之外，还有一类缺页错误是因为访问非法内存引起的。前两类缺页错误中，进程尝试访问的虚存地址尚为合法有效的地址，只是对应的物理内存页帧没有在物理内存当中。后者则不然，进程尝试访问的虚存地址是非法无效的地址。比如尝试对 nullptr 解引用，就会访问地址为 0x0 的虚存地址，这是非法地址。此时 CPU 报出无效缺页错误（Invalid Page Fault）。操作系统对无效缺页错误的处理各不相同：Windows 会使用异常机制向进程报告；*nix 则会通过向进程发送 SIGSEGV 信号（11），引发<a href="https://liam.page/2017/05/27/tutorial-to-GDB-taking-ncurses-as-an-example/" target="_blank" rel="noopener">内存转储</a>。</p></blockquote><h4 id="缺页错误的原因"><a href="#缺页错误的原因" class="headerlink" title="缺页错误的原因"></a>缺页错误的原因</h4><p>之前提到，物理内存中没有 CPU 所需的页帧，就会引发缺页错误。这一现象背后的原因可能有很多。</p><p>例如说，进程通过 mmap 系统调用，直接建立了磁盘文件和虚拟内存的映射关系。然而，在 mmap 调用之后，并不会立即从磁盘上读取这一文件。而是在实际需要文件内容时，通过 CPU 触发缺页错误，要求 Page Fault Handler 去将文件内容读入内存。</p><p>又例如说，一个进程启动了很久，但是长时间没有活动。若是计算机处在很高的内存压力下，则操作系统会将这一进程长期未使用的页帧内容，从物理内存转储到磁盘上。这个过程称为换出（swap out）。在 *nix 系统下，用于转储这部分内存内容的磁盘空间，称为交换空间；在 Windows 上，这部分磁盘空间，则被称为虚拟内存，对应磁盘上的文件则称为页面文件。在这个过程中，进程在内存中保存的任意内容，都可能被换出到交换空间：可以是数据内容，也可以是进程的代码段内容。</p><p>Windows 用户看到这里，应该能明白这部分空间为什么叫做「虚拟内存」——因为它于真实的内存条相对，是在硬盘上虚拟出来的一份内存。通过这样的方式，「好像」将内存的容量扩大了。同样，为什么叫「页面文件」也一目了然。因为事实上，文件内保存的就是一个个内存页帧。在 Windows 上经常能观察到「假死」的现象，就和缺页错误有关。这种现象，实际就是长期不运行某个程序，导致程序对应的内存被换出到磁盘；在需要响应时，由于需要从磁盘上读取大量内容，导致响应很慢，产生假死现象。这种现象发生时，若是监控系统硬错误数量，就会发现在短时间内，目标进程产生了大量的硬错误。</p><p>在 Windows XP 流行的年代，有很多来路不明的「系统优化建议」。其中一条就是「扩大页面文件的大小，有助于加快系统速度」。事实上，这种方式只能加大内存「看起来」的容量，却给内存整体（将物理内存和磁盘页面文件看做一个整体）的响应速度带来了巨大的负面影响。因为，尽管容量增大了，但是访问这部分增大的容量时，进程实际上需要先陷入内核态，从磁盘上读取内容做好映射，再继续执行。更有甚者，这些建议会要求「将页面文件分散在多个不同磁盘分区」，并美其名曰「分散压力」。事实上，从页面文件中读取内存页帧本就已经很慢；若是还要求磁盘不断在不同分区上寻址，那就更慢了。可见谣言害死人。</p><h4 id="观察缺页错误"><a href="#观察缺页错误" class="headerlink" title="观察缺页错误"></a>观察缺页错误</h4><p>通过<code>top</code> 命令查看观察软中断和硬中断的 CPU 使用率，hi(hard interrupt) 表示硬中断、si(soft interrupt) 表示软中断</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%Cpu(s): 53.3 us,  1.2 sy,  0.0 ni, 44.6 id,  0.9 wa,  0.0 hi,  0.1 si,  0.0 st</span><br></pre></td></tr></table></figure><p>通过<code>mpstat -P ALL 2</code> 每隔两秒查看下所有核状态信息，其中<code>%irq</code>为硬中断，<code>%soft</code>为软中断，如果硬中断比较高可以看看是不是大量读取磁盘引起的(<code>iostat -d 1</code>每隔1s 查看磁盘读写速度)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">$ mpstat -P ALL 2</span><br><span class="line">Linux 3.10.0-862.14.4.el7.x86_64 (region-0-0-122) 2020年02月07日 _x86_64_(4 CPU)</span><br><span class="line"></span><br><span class="line">16时23分31秒  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle</span><br><span class="line">16时23分33秒  all   52.64    0.00    0.88    0.38    0.00    0.00    0.00    0.00    0.00   46.10</span><br><span class="line">16时23分33秒    0  100.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00</span><br><span class="line">16时23分33秒    1    4.06    0.00    1.52    1.52    0.00    0.00    0.00    0.00    0.00   92.89</span><br><span class="line">16时23分33秒    2   99.50    0.00    0.50    0.00    0.00    0.00    0.00    0.00    0.00    0.00</span><br><span class="line">16时23分33秒    3    5.08    0.00    1.52    0.00    0.00    0.00    0.00    0.00    0.00   93.40</span><br><span class="line"></span><br><span class="line">16时23分33秒  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle</span><br><span class="line">16时23分35秒  all   52.41    0.00    1.01    0.13    0.00    0.13    0.00    0.00    0.00   46.33</span><br><span class="line">16时23分35秒    0  100.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00</span><br><span class="line">16时23分35秒    1    4.57    0.00    2.54    0.51    0.00    0.00    0.00    0.00    0.00   92.39</span><br><span class="line">16时23分35秒    2  100.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00</span><br><span class="line">16时23分35秒    3    3.55    0.00    2.03    0.51    0.00    0.51    0.00    0.00    0.00   93.40</span><br><span class="line"></span><br><span class="line">16时23分35秒  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle</span><br><span class="line">16时23分37秒  all   54.74    0.00    1.52    0.13    0.00    0.25    0.00    0.00    0.00   43.36</span><br><span class="line">16时23分37秒    0   99.50    0.00    0.50    0.00    0.00    0.00    0.00    0.00    0.00    0.00</span><br><span class="line">16时23分37秒    1    9.28    0.00    2.06    0.00    0.00    0.52    0.00    0.00    0.00   88.14</span><br><span class="line">16时23分37秒    2  100.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00</span><br><span class="line">16时23分37秒    3    8.16    0.00    3.57    0.00    0.00    0.51    0.00    0.00    0.00   87.76</span><br></pre></td></tr></table></figure><p><code>dstat</code>可以查看 CPU 软硬中断次数，hiq、siq分别为硬中断和软中断次数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ dstat</span><br><span class="line">You did not select any stats, using -cdngy by default.</span><br><span class="line">----total-cpu-usage---- -dsk&#x2F;total- -net&#x2F;total- ---paging-- ---system--</span><br><span class="line">usr sys idl wai hiq siq| read  writ| recv  send|  in   out | int   csw</span><br><span class="line"> 62   1  36   0   0   0|  84k  355k|   0     0 |   0     0 |5766  4315</span><br><span class="line"> 52   1  47   0   0   0|   0   232k|  21k   54k|   0     0 |5279  4004</span><br><span class="line"> 52   1  47   0   0   0|   0    56k|  15k   22k|   0     0 |5327  4045</span><br></pre></td></tr></table></figure><p>ps 是一个强大的命令，我们可以用 -o 选项指定希望关注的项目。比如</p><ul><li>min_flt: 进程启动至今软缺页中断数量</li><li>maj_flt: 进程启动至今硬缺页中断数量</li><li>cmd: 执行的命令</li><li>args: 执行的命令的参数（从 $0$ 开始）</li><li>uid: 执行命令的用户的 ID</li><li>gid: 执行命令的用户所在组的 ID</li></ul><p>因此，我们可以用 <code>ps -o min_flt,maj_flt,cmd,args,uid,gid 14434</code> 来观察进程号为 14434 的进程的缺页错误</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ps -o min_flt,maj_flt,cmd,args,uid,gid 14434</span><br><span class="line">   MINFL  MAJFL CMD                         COMMAND                       UID   GID</span><br><span class="line">  2413550   355 &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_181-clou &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_181-clou   981   978</span><br></pre></td></tr></table></figure><p>结合 <code>watch</code> 命令，则可关注进程当前出发缺页中断的状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">watch -n 1 --difference &quot;ps -o min_flt,maj_flt,cmd,args,uid,gid 14434&quot;</span><br></pre></td></tr></table></figure><p>你还可以结合 <code>sort</code> 命令，动态观察产生缺页错误最多的几个进程。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ watch -n 1 &quot;ps -eo min_flt,maj_flt,cmd,args,uid,gid | sort -nrk1 | head -n 8&quot;</span><br><span class="line">Every 1.0s: ps -eo min_flt,maj_flt,cmd,args,uid,gid | sort -nrk1 | head -n 8                                                                   Fri Feb  7 17:18:43 2020</span><br><span class="line"></span><br><span class="line">437395316 262 &#x2F;usr&#x2F;bin&#x2F;python2 &#x2F;opt&#x2F;cloud &#x2F;usr&#x2F;bin&#x2F;python2 &#x2F;opt&#x2F;cloud     0     0</span><br><span class="line">69510242   75 &#x2F;bin&#x2F;bash &#x2F;opt&#x2F;cloudera&#x2F;cm- &#x2F;bin&#x2F;bash &#x2F;opt&#x2F;cloudera&#x2F;cm-   981   978</span><br><span class="line">23151349 1164 &#x2F;usr&#x2F;local&#x2F;aegis&#x2F;aegis_clie &#x2F;usr&#x2F;local&#x2F;aegis&#x2F;aegis_clie     0     0</span><br><span class="line">5772594  1197 &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;systemd-jo &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;systemd-jo     0     0</span><br><span class="line">2902146    46 &#x2F;usr&#x2F;sbin&#x2F;irqbalance --fore &#x2F;usr&#x2F;sbin&#x2F;irqbalance --fore     0     0</span><br><span class="line">2413598   355 &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_181-clou &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_181-clou   981   978</span><br><span class="line">1591327   619 &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_181-clou &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_181-clou   997   995</span><br><span class="line">1588418   197 &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;systemd -- &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;systemd --     0     0</span><br></pre></td></tr></table></figure><h4 id="一个硬缺页错误导致的问题"><a href="#一个硬缺页错误导致的问题" class="headerlink" title="一个硬缺页错误导致的问题"></a>一个硬缺页错误导致的问题</h4><p>我司的某一高性能服务采取了 mmap 的方式，从磁盘加载大量数据。由于调研测试需要，多名组内成员共享一台调研机器。现在的问题是，当共享的人数较多时，新启动的服务进程会在启动时耗费大量时间——以几十分钟计。那么，这是为什么呢？</p><blockquote><p>因为涉及到公司机密，这里不方便给截图。留待以后，做模拟实验后给出。</p></blockquote><p>以 top 命令观察，机器卡顿时，CPU 负载并不高：32 核只有 1.3 左右的 1min 平均负载。但是，iostat 观察到，磁盘正在以 10MiB/s 级别的速度，不断进行读取。由此判断，这种情况下，目标进程一定有大量的 Page Fault 产生。使用上述 <code>watch -n 1 --difference &quot;ps -o min_flt,maj_flt,cmd,args,uid,gid &lt;pid&gt;&quot;</code> 观察，发现目标进程确实有大量硬缺页错误产生，肯定了这一推断。</p><p>然而，诚然进程需要载入大量数据，但是以 mmap 的方式映射，为何会已有大量同类服务存在的情况下，大量读取硬盘呢？这就需要更加深入的分析了。</p><p>事实上，这里隐含了一个非常细小的矛盾。一方面，该服务需要从磁盘加载大量数据；另一方面，该服务对性能要求非常高。我们知道，mmap 只是对文件做了映射，不会在调用 mmap 时立即将文件内容加载进内存。这就导致了一个问题：当服务启动对外提供服务时，可能还有数据未能加载进内存；而这种加载是非常慢的，严重影响服务性能。因此，可以推断，为了解决这个问题，程序必然在 mmap 之后，尝试将所有数据加载进物理内存。</p><p>这样一来，先前遇到的现象就很容易解释了。</p><ul><li>一方面，因为公用机器的人很多，必然造成内存压力大，从而存在大量换出的内存；</li><li>另一方面，新启动的进程，会逐帧地扫描文件；</li><li>这样一来，新启动的进程，就必须在极大的内存压力下，不断逼迫系统将其它进程的内存换出，而后换入自己需要的内存，不断进行磁盘 I/O；</li><li>故此，新启动的进程会耗费大量时间进行不必要的磁盘 I/O。</li></ul>]]></content>
    
    <summary type="html">
    
      缺页错误（Page Fault）
    
    </summary>
    
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
  </entry>
  
  <entry>
    <title>HDFS 处理小文件方法</title>
    <link href="http://yoursite.com/2020/01/28/HDFS-small-file/"/>
    <id>http://yoursite.com/2020/01/28/HDFS-small-file/</id>
    <published>2020-01-27T16:00:00.000Z</published>
    <updated>2020-02-26T14:46:23.924Z</updated>
    
    <content type="html"><![CDATA[<p>Hadoop 中处理小文件</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>HDFS 随着使用时间会存在小文件现象，这里将网上调研的处理方法收集一下</p><p>解决此问题的方法主要为两个方面</p><ol><li><p>从源头解决小文件问题，此方法需要业务方配合解决</p></li><li><p>合并平台上已有的小文件。因此更多工作是围绕第二种个方法来做的，有多种方法，比如HAR 归档啊，利用Hive针对parquet 文件格式合并、Ozone 小文件利器。</p></li></ol><p>HAR 把历史数据进行归档，读取它还多了一层索引查找，比较鸡肋，不怎么好用。Ozone 目前技术较新，对Hadoop 有版本要求，需要Hadoop 3.x 版本。因此平常更多是利用 Hive 来针对 parquet 文件格式合并</p><h4 id="处理方法"><a href="#处理方法" class="headerlink" title="处理方法"></a>处理方法</h4><p>什么是小文件？小文件是怎么来的？可以参阅<a href="https://mp.weixin.qq.com/s/QdaNFfq37bBlAQtNDGLmVw" target="_blank" rel="noopener">如何在Hadoop中处理小文件</a></p><p>利用 HAR 方法处理小文件可以参阅<a href="https://mp.weixin.qq.com/s/rlM3ZPmWz-ZT72w59lvrRw" target="_blank" rel="noopener">0508-如何使用Hadoop的Archive处理小文件</a></p><p>利用 Hive 方法处理小文件可以参阅<a href="https://mp.weixin.qq.com/s/xw6MjNvpI97m0aygyW1HaA" target="_blank" rel="noopener">如何在Hadoop中处理小文件-续</a>、<a href="https://mp.weixin.qq.com/s/ki2k9fBPco6PwMZEzff60A" target="_blank" rel="noopener">0704-5.16.2-如何使用Hive合并小文件</a></p><p>利用 Impala 方法处理小文件可以参阅<a href="https://mp.weixin.qq.com/s/BTBqUqfbczq08LKRCuC_Ig" target="_blank" rel="noopener">如何使用Impala合并小文件</a></p><p>利用 Ozone 来解决<a href="https://mp.weixin.qq.com/s/x_xCjSs5Aed8z6moxp_uYQ" target="_blank" rel="noopener">Hadoop小文件利器Ozone</a></p><p>在有赞使用 SparkSQL 中提到存在小文件现象，采用社区 <a href="https://issues.apache.org/jira/browse/SPARK-24940" target="_blank" rel="noopener">SPARK-24940</a>方式处理，借助 SQL hint 的方式合并小文件。参阅<a href="https://www.iteblog.com/archives/2501.html" target="_blank" rel="noopener">Spark SQL 查询中 Coalesce 和 Repartition 暗示（Hint）</a></p><p>利用 Spark 处理小文件跟 Spark SQL 也是差不多的，通过 Spark 的 coalesce() 方法和 repartition() 方法，设置并行个数可以通过输入文件的总大小和期望输出文件的大小进行预计算而得。</p><h4 id="小文件寻找"><a href="#小文件寻找" class="headerlink" title="小文件寻找"></a>小文件寻找</h4><p>知道处理方法后，问题就是如何寻找小文件，最先之前是利用<code>haddop fs -count &lt;path&gt;</code>人工肉眼寻找</p><p>高级操作还是得借用 HDFS 的元数据 FsImage 来分析，感兴趣可以自行参阅下方博客</p><ul><li><a href="https://mp.weixin.qq.com/s/TWGROH7TRgXRwl3TBlGLhw" target="_blank" rel="noopener">0464-如何离线分析HDFS的FsImage查找集群小文件</a></li><li><a href="https://mp.weixin.qq.com/s/RT8tCF_ZEvMYewjGgABSYQ" target="_blank" rel="noopener">hdfs如何通过解析fsimage来监控目录</a></li></ul><hr><p>参考链接</p><ul><li><a href="http://bigdatadecode.club/HDFS-little-file-action.html" target="_blank" rel="noopener">HDFS小文件合并实战</a></li><li><a href="https://juejin.im/post/5c3f2713f265da61285a5b75#heading-8" target="_blank" rel="noopener">SparkSQL 在有赞的实践</a></li><li><a href="https://blog.csdn.net/weixin_37944880/article/details/86694829" target="_blank" rel="noopener">Spark小文件合并</a></li></ul>]]></content>
    
    <summary type="html">
    
      Hadoop 中处理小文件
    
    </summary>
    
    
      <category term="HDFS" scheme="http://yoursite.com/categories/HDFS/"/>
    
    
  </entry>
  
  <entry>
    <title>Hive set 命令使用</title>
    <link href="http://yoursite.com/2020/01/27/Hive-set/"/>
    <id>http://yoursite.com/2020/01/27/Hive-set/</id>
    <published>2020-01-26T16:00:00.000Z</published>
    <updated>2020-02-03T10:02:20.441Z</updated>
    
    <content type="html"><![CDATA[<p>Hive set 命令使用</p><hr><h4 id="转载来源"><a href="#转载来源" class="headerlink" title="转载来源"></a>转载来源</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">作者：lwf006164</span><br><span class="line">链接：https:&#x2F;&#x2F;blog.csdn.net&#x2F;lwf006164&#x2F;article&#x2F;details&#x2F;96754526</span><br><span class="line">来源：Hive篇.set命令使用</span><br></pre></td></tr></table></figure><h4 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h4><p>Hive 命令行下执行 set 命令，仅当前会话有效</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 设置显示当前使用的数据库</span><br><span class="line">set hive.cli.print.current.db&#x3D;true;</span><br><span class="line"></span><br><span class="line">#  设置显示表名</span><br><span class="line">set hive.cli.print.header&#x3D;true;</span><br></pre></td></tr></table></figure><p>Hive 脚本中配置 set 命令，当前机器用户有效   </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 进入编辑模式，在执行 hive 命令进入 shell 命令行的时候默认会加载这个脚本</span><br><span class="line">vi ~&#x2F;.hiverc</span><br><span class="line"></span><br><span class="line"># 将下面的两行命令添加到该文件中</span><br><span class="line"># 设置显示当前使用的数据库</span><br><span class="line">set hive.cli.print.current.db&#x3D;true;</span><br><span class="line"></span><br><span class="line"># 设置显示表名</span><br><span class="line">set hive.cli.print.header&#x3D;true;</span><br></pre></td></tr></table></figure><p>查看 Hive 历史操作命令集可以浏览 <code>~/.hivehistory</code></p>]]></content>
    
    <summary type="html">
    
      Hive set 命令使用
    
    </summary>
    
    
      <category term="Hive" scheme="http://yoursite.com/categories/Hive/"/>
    
    
  </entry>
  
  <entry>
    <title>HDFS 中 atime 与 mtime 解析</title>
    <link href="http://yoursite.com/2020/01/27/HDFS-atime-ctime/"/>
    <id>http://yoursite.com/2020/01/27/HDFS-atime-ctime/</id>
    <published>2020-01-26T16:00:00.000Z</published>
    <updated>2020-02-06T07:55:22.814Z</updated>
    
    <content type="html"><![CDATA[<p>HDFS 中 atime 和 mtime</p><hr><h4 id="转载来源"><a href="#转载来源" class="headerlink" title="转载来源"></a>转载来源</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">作者：混绅士</span><br><span class="line">链接：http:&#x2F;&#x2F;bigdatadecode.club&#x2F;HDFS%E4%B8%ADatime%E5%92%8Cmtime.html</span><br><span class="line">来源：HDFS中atime与mtime解析</span><br></pre></td></tr></table></figure><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>如果不知道 atime、mtime 的建议先了解下 Linux 中 atime 和 mtime，这样有助于学习 HDFS 中 atime 与 mtime</p><h4 id="查看"><a href="#查看" class="headerlink" title="查看"></a>查看</h4><p>转载文章中未介绍如何查看 HDFS atime、mtime，因此说明下。</p><p>跟 Linux 一样，也是使用 stat 命令来查看</p><p>用法: <code>hadoop fs -stat [format] &lt;path&gt; ...</code></p><blockquote><p>以指定的格式打印有关 <path></path> 处的文件/目录的统计信息。格式接受八进制（％a）和符号（％A），字节（％b）文件大小，类型（％F），所有者组名称（％g），名称（％n），块大小（％o），复制（％r），<br>所有者用户名（％u），访问日期（％x，％X）和修改日期（％y，％Y）。％x和％y将UTC日期显示为“yyyy-MM-dd HH:mm:ss”，并且％X和％Y显示自1970年1月1日UTC以来的毫秒数。如果未指定格式，则默认使用％y。 —— <a href="https://www.jianshu.com/p/49d89c197310" target="_blank" rel="noopener">https://www.jianshu.com/p/49d89c197310</a></p></blockquote><p>如查看 HBase 根目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -stat &quot;type:%F perm:%a %u:%g size:%b mtime:%y atime:%x name:%n&quot; &#x2F;hbase</span><br><span class="line">type:directory perm:755 hbase:hbase size:0 mtime:2020-01-14 12:54:42 atime:1970-01-01 00:00:00 name:hbase</span><br></pre></td></tr></table></figure><h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><p>先来了解下 HDFS 中的这 atime 与 mtime 的变化规则</p><p>在看代码之前，先想下 atime 和 mtime 有可能在哪些地方会修改</p><p>hdfs底层api对文件都有哪些操作？</p><p>无非就是读写两种操作，读肯定修改的是atime，写修改的是mtime，是否修改atime还得确认。<br>这里我们还漏掉一种操作，那就是mv。</p><h5 id="atime"><a href="#atime" class="headerlink" title="atime"></a>atime</h5><p>去年写过一篇文章 <a href="http://bigdatadecode.club/HDFS%20read%E8%A7%A3%E6%9E%90.html" target="_blank" rel="noopener">HDFS read解析(一)之Open文件流</a>介绍HDFS读操作流程，这里就不再累赘了，直接贴出关键代码。(有兴趣的同学可以自行查看)</p><blockquote><p>这里还是简单说下读的流程：客户端向NN发起一个读请求，NN将相关的block信息返回给客户端，客户端再与对应的DN建立连接读取信息。<br>  在这个过程中，先与NN交互然后再与DN交互，那么每个文件的atime相关元数据信息都存在NN中，那么atime相关的修改也肯定发生在与NN交互的这个过程中。</p></blockquote><p>从之前的文章中可知入口函数是 <code>FSNamesystem.getBlockLocations</code>，关键代码如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">LocatedBlocks getBlockLocations(String clientMachine, String srcArg,</span><br><span class="line">    long offset, long length) throws IOException &#123;</span><br><span class="line">...</span><br><span class="line">  if (res.updateAccessTime()) &#123;</span><br><span class="line">    String src &#x3D; srcArg;</span><br><span class="line">    writeLock();</span><br><span class="line">    final long now &#x3D; now();</span><br><span class="line">    try &#123;</span><br><span class="line">      final INodesInPath iip &#x3D; dir.resolvePath(pc, src);</span><br><span class="line">      src &#x3D; iip.getPath();</span><br><span class="line">      INode inode &#x3D; iip.getLastINode();</span><br><span class="line">      &#x2F;&#x2F; 再次判断是否可以更新atime</span><br><span class="line">      boolean updateAccessTime &#x3D; inode !&#x3D; null &amp;&amp;</span><br><span class="line">          now &gt; inode.getAccessTime() + getAccessTimePrecision();</span><br><span class="line">      if (!isInSafeMode() &amp;&amp; updateAccessTime) &#123;</span><br><span class="line">        &#x2F;&#x2F; 设置atime</span><br><span class="line">        boolean changed &#x3D; FSDirAttrOp.setTimes(dir,</span><br><span class="line">            inode, -1, now, false, iip.getLatestSnapshotId());</span><br><span class="line">        if (changed) &#123;</span><br><span class="line">          getEditLog().logTimes(src, -1, now);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; catch (Throwable e) &#123;</span><br><span class="line">      LOG.warn(&quot;Failed to update the access time of &quot; + src, e);</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">      writeUnlock(operationName);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>res.updateAccessTime()</code> 决定了是否更新atime，其值是在getBlockLocationsInt中赋值的，代码如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">boolean updateAccessTime &#x3D; isAccessTimeSupported() &amp;&amp; !isInSafeMode()</span><br><span class="line">        &amp;&amp; !iip.isSnapshot()</span><br><span class="line">        &amp;&amp; now &gt; inode.getAccessTime() + getAccessTimePrecision();</span><br></pre></td></tr></table></figure><p>其中关键的因素是<code>isAccessTimeSupported()</code>和<code>getAccessTimePrecision()</code>，这个两个方法都与<code>accessTimePrecision</code>有关，<br>此值是由<code>dfs.namenode.accesstime.precision</code>设置的，默认是3600000。</p><p>当此值大于0，<code>isAccessTimeSupported()</code>返回true，<code>getAccessTimePrecision()</code>得到的值是<code>dfs.namenode.accesstime.precision</code>的值。</p><p>从上述代码中可以看出更新atime的一个条件是<strong>两次读取间隔相隔<code>dfs.namenode.accesstime.precision</code>秒，默认是1小时。</strong></p><p>这里遗留两个问题:</p><ol><li>新建文件时atime如何赋值</li><li>修改文件内容时atime如何赋值</li></ol><p>关于这个两个问题我在下一节在写流程中解答。请继续向下看</p><h5 id="mtime"><a href="#mtime" class="headerlink" title="mtime"></a>mtime</h5><p>同样去年也写过一篇关于写的文章<a href="http://bigdatadecode.club/HDFS%20write%E8%A7%A3%E6%9E%90.html" target="_blank" rel="noopener">HDFS write解析</a>介绍HDFS读操作流程，这里就不再累赘了，直接贴出关键代码。(有兴趣的同学可以自行查看)</p><p>写相关的操作包括create、close和append<br>写文件有两种方式，一种是调用<code>create(Path)</code>方法，另一种是调用<code>append(Path)</code>方法</p><p><strong>create</strong></p><p>通过调用<code>create(Path)</code>，最终会调用<code>FSDirectory.addFile</code>方法，<br>在此方法中会new一个<code>INodeFile</code>，此时会设置 mtime 和 atime 为同一个值，代码如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">INodesInPath addFile(INodesInPath existing, String localName, PermissionStatus</span><br><span class="line">    permissions, short replication, long preferredBlockSize,</span><br><span class="line">    String clientName, String clientMachine)</span><br><span class="line">  throws FileAlreadyExistsException, QuotaExceededException,</span><br><span class="line">    UnresolvedLinkException, SnapshotAccessControlException, AclException &#123;</span><br><span class="line">  long modTime &#x3D; now();</span><br><span class="line">  INodeFile newNode &#x3D; newINodeFile(allocateNewInodeId(), permissions, modTime,</span><br><span class="line">      modTime, replication, preferredBlockSize);</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>有打开一个文件就有关闭一个文件，接下来看下关闭文件时 atime 和 mtime 会有什么变化。</p><p><strong>close</strong></p><p><code>closeFile()</code>在<code>finalizeINodeFileUnderConstruction</code>中调用，在此方法中会设置mtime，看下代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">private void finalizeINodeFileUnderConstruction(String src,</span><br><span class="line">    INodeFile pendingFile, int latestSnapshot) throws IOException &#123;</span><br><span class="line">...</span><br><span class="line">  pendingFile.toCompleteFile(now());</span><br><span class="line">...</span><br><span class="line">  closeFile(src, pendingFile);</span><br><span class="line">...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; INodeFile.java</span><br><span class="line">public INodeFile toCompleteFile(long mtime) &#123;</span><br><span class="line">  Preconditions.checkState(isUnderConstruction(),</span><br><span class="line">      &quot;file is no longer under construction&quot;);</span><br><span class="line">  FileUnderConstructionFeature uc &#x3D; getFileUnderConstructionFeature();</span><br><span class="line">  if (uc !&#x3D; null) &#123;</span><br><span class="line">    assertAllBlocksComplete();</span><br><span class="line">    removeFeature(uc);</span><br><span class="line">    this.setModificationTime(mtime);</span><br><span class="line">  &#125;</span><br><span class="line">  return this;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从代码中可以看出，close文件时只对mtime进行了修改。</p><p><strong>append</strong></p><p>append只是打开一个文件流，并不会修改mtime或者atime，只是在close的时候修改mtime</p><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p><strong>atime</strong></p><ol><li>两次读间隔大于默认的1小时时，更新atime。默认间隔通过<code>dfs.namenode.accesstime.precision</code>控制。</li><li>新建一个文件时atime赋值为当前时间(注意，当关闭一个文件时atime不会修改)</li></ol><p><strong>mtime</strong></p><ol><li>新建一个文件时mtime赋值为当前时间(同时会修改atime)</li><li>关闭一个文件时mtime赋值为当前时间(此时并不会修改atime)</li></ol>]]></content>
    
    <summary type="html">
    
      HDFS 中 atime 与 mtime 解析
    
    </summary>
    
    
      <category term="HDFS" scheme="http://yoursite.com/categories/HDFS/"/>
    
    
  </entry>
  
  <entry>
    <title>Kafka 零拷贝原理</title>
    <link href="http://yoursite.com/2020/01/24/Kafka-Zero-Copy/"/>
    <id>http://yoursite.com/2020/01/24/Kafka-Zero-Copy/</id>
    <published>2020-01-23T16:00:00.000Z</published>
    <updated>2020-02-05T13:37:24.348Z</updated>
    
    <content type="html"><![CDATA[<p>从字面意思理解就是数据不需要来回的拷贝，大大提升了系统的性能</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>“零拷贝”是指计算机操作的过程中，CPU 不需要为数据在内存之间的拷贝消耗资源。而它通常是指计算机在网络上发送文件时，不需要将文件内容拷贝到用户空间（User Space）而直接在内核空间（Kernel Space）中传输到网络的方式。</p><p>零拷贝并不是不需要拷贝，而是减少不必要的拷贝次数。通常是说在 IO 读写过程中</p><h4 id="缓冲区"><a href="#缓冲区" class="headerlink" title="缓冲区"></a>缓冲区</h4><p>缓冲区是所有 I/O 的基础，I/O 讲的无非就是把数据移进或移出缓冲区</p><p>进程执行I/O操作，就是向操作系统发出请求，让它要么把缓冲区的数据排干(写)，要么填充缓冲区(读)</p><p>下面看一个 Java 进程发起读请求加载数据大致的流程图</p><p><img src="/images/blog/2020-01-24-1.png" alt></p><ol><li><p>进程发起读请求之后，内核接收到读请求之后，会先检查内核空间中是否已经存在进程所需要的数据，<br>如果已经存在，则直接把数据拷贝给进程的缓冲区</p></li><li><p>如果内核缓冲区没有命中随即向磁盘控制器发出命令，要求从磁盘读取数据，磁盘控制器把数据直接写入内核read缓冲区，这一步通过DMA完成</p></li><li><p>接下来内核将数据拷贝到进程的缓冲区</p></li></ol><p>DMA 是一种硬件和软件之间的数据传输的技术，且 DMA 进行数据传输的过程中几乎不需要 CPU 参与</p><h4 id="虚拟内存"><a href="#虚拟内存" class="headerlink" title="虚拟内存"></a>虚拟内存</h4><p>所有现代操作系统都使用虚拟内存，使用虚拟的地址取代物理地址，这样有两个好处:</p><ol><li>一个以上的虚拟地址可以指向同一个物理内存地址</li><li>虚拟内存空间可大于实际可用的物理地址</li></ol><p>利用第一条特性可以把内核空间地址和用户空间的虚拟地址映射到同一个物理地址，<br>这样DMA就可以填充对内核和用户空间进程同时可见的缓冲区了，大致如下图所示:</p><p><img src="/images/blog/2020-01-24-3.png" alt></p><h4 id="零拷贝技术"><a href="#零拷贝技术" class="headerlink" title="零拷贝技术"></a>零拷贝技术</h4><p>本人水平有限，零拷贝技术可以阅读下方链接</p><ul><li><a href="https://www.jianshu.com/p/497e7640b57c" target="_blank" rel="noopener">零拷贝的原理及Java实现</a></li><li><a href="https://blog.csdn.net/lzb348110175/article/details/100853071" target="_blank" rel="noopener">Kafka 的零拷贝技术</a></li><li><a href="https://mp.weixin.qq.com/s/otuUvACiVDafGgDl6xNd2A" target="_blank" rel="noopener">面试被问到“零拷贝”！你真的理解吗？</a></li><li><a href="https://mp.weixin.qq.com/s/vaKRVvfUnFjhHzfkwZDbKQ" target="_blank" rel="noopener">图解Kafka的零拷贝技术到底有多牛？</a></li><li><a href="https://blog.csdn.net/ljheee/article/details/99652448" target="_blank" rel="noopener">Kafka零拷贝</a></li></ul><p>Kafka 通过 sendfile 实现的零拷贝 I/O，理想状态下的零拷贝I/O需要接触 DMA 完成。</p>]]></content>
    
    <summary type="html">
    
      Kafka 零拷贝原理
    
    </summary>
    
    
      <category term="Kafka" scheme="http://yoursite.com/categories/Kafka/"/>
    
    
  </entry>
  
  <entry>
    <title>HDFS Federation 发展历程</title>
    <link href="http://yoursite.com/2020/01/22/HDFS-Federation/"/>
    <id>http://yoursite.com/2020/01/22/HDFS-Federation/</id>
    <published>2020-01-21T16:00:00.000Z</published>
    <updated>2020-02-17T05:21:56.316Z</updated>
    
    <content type="html"><![CDATA[<p>HDFS Federation（联邦）相关知识，如 HDFS Router-based Federation</p><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>在阅读一些 Hadoop3 新特性的文章中，我看到有提”多 NameNode”，然后简单某度搜索到的链接都是提 HDFS Federation（联邦）</p><p><img src="/images/blog/2020-01-22-2.png" alt></p><p>下意识以为”多 NameNode”指的是 Federation 正式在 Hadoop3 上实现可用，在一次面试过程中跟面试官扯 Hadoop3 Federation 新特性，人家反问一句这不是 Hadoop2 就有了吗。。。我瞬间懵了。</p><p>事后回顾到才知道 HDFS RBF 才是 Hadoop3 Router-based Federation 新特性。”多 NameNode” 是指 HDFS HA 支持多 Standby 节点机制 <a href="https://issues.apache.org/jira/browse/HDFS-6440" target="_blank" rel="noopener">HDFS-6440</a></p><p>在配置平衡策略时看到有个”BlockPool”，想到这是跟 Federation 有关，因此想整理一下对这方面的知识程度</p><p><img src="/images/blog/2020-01-22-1.png" alt></p><h4 id="HDFS-Federation"><a href="#HDFS-Federation" class="headerlink" title="HDFS Federation"></a>HDFS Federation</h4><p>对 Federation 学习个人大部分是通过以下链接学习了解，大家可以按链接顺序自行学习。</p><ul><li><a href="https://mp.weixin.qq.com/s/8VrQFIuQUVEa20JfG2EZkQ" target="_blank" rel="noopener">HDFS Federation（联邦）简介</a></li><li><a href="https://mp.weixin.qq.com/s/YCzhYCv-fcbOHfM9wvMchg" target="_blank" rel="noopener">如何通过CM为HDFS启用Federation</a></li><li><a href="https://mp.weixin.qq.com/s/X-8DmvgAsLsFEZADYGklBA" target="_blank" rel="noopener">如何通过CM禁用Federation</a></li><li><a href="https://mp.weixin.qq.com/s/2QDnm54_ObpXMyD952GovQ" target="_blank" rel="noopener">HDFS Router-based Federation</a></li><li><a href="https://mp.weixin.qq.com/s/BPfhDxiSaUkoOSyrMuAHIw" target="_blank" rel="noopener">Router-Based HDFS Federation 在滴滴大数据的应用</a></li><li><a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs-rbf/HDFSRouterFederation.html" target="_blank" rel="noopener">HDFS Router-based Federation 官方文档</a></li><li><a href="https://mp.weixin.qq.com/s/0qY-LMHQT-yGJPh-ybbZUg" target="_blank" rel="noopener">Apache Hadoop 的 HDFS federation 前世今生</a></li></ul><p>由于 Router-based HDFS federation 还算比较新的特性，所以社区分了几个阶段修复或添加了一些新的功能，<br>社区在 2.9 和 3.0 版本中发布 HDFS RBF 方案解决统一命名空间问题，参见 <a href="https://issues.apache.org/jira/browse/HDFS-10467" target="_blank" rel="noopener">HDFS-10467</a>，<br>在 Apache Hadoop 3.2.0 版本修复或添加了一些功能，参见 <a href="https://issues.apache.org/jira/browse/HDFS-12615" target="_blank" rel="noopener">HDFS-12615</a>，<br>以及 Router-based HDFS federation 稳定性相关的 ISSUE <a href="https://issues.apache.org/jira/browse/HDFS-13891" target="_blank" rel="noopener">HDFS-13891</a>，<br>这个 ISSUE 可能会在 Apache Hadoop 3.3.0 版本发布(过往记忆大数据的<a href="https://mp.weixin.qq.com/s/0qY-LMHQT-yGJPh-ybbZUg" target="_blank" rel="noopener">Apache Hadoop 的 HDFS federation 前世今生</a>文末提的。。。不知道会不会发布)。</p><p>小米在<a href="https://mp.weixin.qq.com/s/2QDnm54_ObpXMyD952GovQ" target="_blank" rel="noopener">HDFS Router-based Federation</a>文末提了 Rebalance 社区没有实现跨子集群的迁移。</p><p>翻看官方文档的确有提不支持跨子集群的迁移:</p><blockquote><p>Some operations are not available in Router-based federation. The Router throws exceptions for those. Examples users may encounter include the following.</p><ul><li>Rename file/folder in two different nameservices.</li><li>Copy file/folder in two different nameservices.</li><li>Write into a file/folder being rebalanced.</li></ul></blockquote><p>但是跨集群的数据平衡已经在做了，可以参见 <a href="https://issues.apache.org/jira/browse/HDFS-13123" target="_blank" rel="noopener">HDFS-13123</a>。<br>小米自己也说内部实现了跨子集群的 Federation Rename，可以在未来将这一特性合并到社区的 Rebalance 方案中，期待ing。</p><hr><p>参考链接</p><ul><li><a href="https://mp.weixin.qq.com/s/CEIKrDZrR6EZIKJ9x2Se0Q" target="_blank" rel="noopener">HDFS3.2升级在滴滴的实践</a></li><li><a href="http://www.voidcn.com/article/p-umrtxeag-pk.html" target="_blank" rel="noopener">HDFS HA支持多Standby节点机制</a></li></ul>]]></content>
    
    <summary type="html">
    
      HDFS Federation 发展历程
    
    </summary>
    
    
      <category term="HDFS" scheme="http://yoursite.com/categories/HDFS/"/>
    
    
  </entry>
  
</feed>
